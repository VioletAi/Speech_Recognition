{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b791c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_dataloader\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a749c177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d',\n",
       " 'dcl',\n",
       " 'l',\n",
       " 'ae',\n",
       " 'aa',\n",
       " 'p',\n",
       " 'n',\n",
       " 'w',\n",
       " 'pcl',\n",
       " 'sh',\n",
       " 'sil',\n",
       " 'uh',\n",
       " 'b',\n",
       " 'ah',\n",
       " 'ix',\n",
       " 'ih',\n",
       " 'iy',\n",
       " 'tcl',\n",
       " 'ax-h',\n",
       " 'ey',\n",
       " 'q',\n",
       " 's',\n",
       " 'ao',\n",
       " 'eh',\n",
       " 'el',\n",
       " 'en',\n",
       " 'kcl',\n",
       " 'er',\n",
       " 'oy',\n",
       " 'k',\n",
       " 'th',\n",
       " 'm',\n",
       " 'jh',\n",
       " 'nx',\n",
       " 'pau',\n",
       " 'h#',\n",
       " 'hh',\n",
       " 'dh',\n",
       " 'ux',\n",
       " 'r',\n",
       " 't',\n",
       " 'g',\n",
       " 'v',\n",
       " 'gcl',\n",
       " 'z',\n",
       " 'y',\n",
       " 'aw',\n",
       " 'eng',\n",
       " 'epi',\n",
       " 'zh',\n",
       " 'ax',\n",
       " 'ay',\n",
       " 'dx',\n",
       " 'em',\n",
       " 'hv',\n",
       " 'bcl',\n",
       " 'ng',\n",
       " 'ow',\n",
       " 'ch',\n",
       " 'f',\n",
       " 'uw',\n",
       " 'axr']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file line by line\n",
    "with open(\"phone_map\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Extracting phone numbers\n",
    "phones = [line.split(':')[0].strip() for line in lines]\n",
    "\n",
    "# Getting unique phone numbers\n",
    "unique_phones_set = set(phones)\n",
    "unique_phones_list = list(unique_phones_set)\n",
    "\n",
    "unique_phones_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b883dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_phones_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389a8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_phones_list=['_']+unique_phones_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12027a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab_61.txt\", 'w') as output_file:\n",
    "        for phone in unique_phones_list:\n",
    "            output_file.write(phone + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65348bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find the unique phones in train.json, and then\n",
    "# create a file named vocab.txt, each line in this \n",
    "# file is a unique phone, in total there should be \n",
    "# 40 lines\n",
    "\n",
    "vocab = {}\n",
    "phonemes = []\n",
    "with open(\"vocab_61.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab[text.strip()] = id\n",
    "        phonemes.append(text)\n",
    "phonemes = phonemes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641fef24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_': 0,\n",
       " 'd': 1,\n",
       " 'dcl': 2,\n",
       " 'l': 3,\n",
       " 'ae': 4,\n",
       " 'aa': 5,\n",
       " 'p': 6,\n",
       " 'n': 7,\n",
       " 'w': 8,\n",
       " 'pcl': 9,\n",
       " 'sh': 10,\n",
       " 'sil': 11,\n",
       " 'uh': 12,\n",
       " 'b': 13,\n",
       " 'ah': 14,\n",
       " 'ix': 15,\n",
       " 'ih': 16,\n",
       " 'iy': 17,\n",
       " 'tcl': 18,\n",
       " 'ax-h': 19,\n",
       " 'ey': 20,\n",
       " 'q': 21,\n",
       " 's': 22,\n",
       " 'ao': 23,\n",
       " 'eh': 24,\n",
       " 'el': 25,\n",
       " 'en': 26,\n",
       " 'kcl': 27,\n",
       " 'er': 28,\n",
       " 'oy': 29,\n",
       " 'k': 30,\n",
       " 'th': 31,\n",
       " 'm': 32,\n",
       " 'jh': 33,\n",
       " 'nx': 34,\n",
       " 'pau': 35,\n",
       " 'h#': 36,\n",
       " 'hh': 37,\n",
       " 'dh': 38,\n",
       " 'ux': 39,\n",
       " 'r': 40,\n",
       " 't': 41,\n",
       " 'g': 42,\n",
       " 'v': 43,\n",
       " 'gcl': 44,\n",
       " 'z': 45,\n",
       " 'y': 46,\n",
       " 'aw': 47,\n",
       " 'eng': 48,\n",
       " 'epi': 49,\n",
       " 'zh': 50,\n",
       " 'ax': 51,\n",
       " 'ay': 52,\n",
       " 'dx': 53,\n",
       " 'em': 54,\n",
       " 'hv': 55,\n",
       " 'bcl': 56,\n",
       " 'ng': 57,\n",
       " 'ow': 58,\n",
       " 'ch': 59,\n",
       " 'f': 60,\n",
       " 'uw': 61,\n",
       " 'axr': 62}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a83486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_39 = {}\n",
    "with open(\"vocab_39.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab_39[text.strip()] = id\n",
    "        #phonemes.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da2829e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently using cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"currently using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"currently using cpu only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e976cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c46efcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dropout tuning For 2 Layer LSTM, with Dropout between layer\n",
      "Currently using dropout rate of 0.5\n",
      "Total number of model parameters is 568127\n",
      "\n",
      "{'_': 0, 'd': 1, 'dcl': 2, 'l': 3, 'ae': 4, 'aa': 5, 'p': 6, 'n': 7, 'w': 8, 'pcl': 9, 'sh': 10, 'sil': 11, 'uh': 12, 'b': 13, 'ah': 14, 'ix': 15, 'ih': 16, 'iy': 17, 'tcl': 18, 'ax-h': 19, 'ey': 20, 'q': 21, 's': 22, 'ao': 23, 'eh': 24, 'el': 25, 'en': 26, 'kcl': 27, 'er': 28, 'oy': 29, 'k': 30, 'th': 31, 'm': 32, 'jh': 33, 'nx': 34, 'pau': 35, 'h#': 36, 'hh': 37, 'dh': 38, 'ux': 39, 'r': 40, 't': 41, 'g': 42, 'v': 43, 'gcl': 44, 'z': 45, 'y': 46, 'aw': 47, 'eng': 48, 'epi': 49, 'zh': 50, 'ax': 51, 'ay': 52, 'dx': 53, 'em': 54, 'hv': 55, 'bcl': 56, 'ng': 57, 'ow': 58, 'ch': 59, 'f': 60, 'uw': 61, 'axr': 62}\n",
      "\n",
      "{'_': 0, 'uh': 1, 'n': 2, 's': 3, 'th': 4, 'y': 5, 'ih': 6, 'ch': 7, 'aa': 8, 'l': 9, 'uw': 10, 'ah': 11, 'b': 12, 'ow': 13, 'dh': 14, 'd': 15, 'aw': 16, 't': 17, 'r': 18, 'w': 19, 'm': 20, 'v': 21, 'ay': 22, 'f': 23, 'p': 24, 'sh': 25, 'eh': 26, 'oy': 27, 'sil': 28, 'hh': 29, 'dx': 30, 'jh': 31, 'er': 32, 'iy': 33, 'g': 34, 'ae': 35, 'ey': 36, 'z': 37, 'k': 38, 'ng': 39}\n",
      "\n",
      "{11: 28, 5: 8, 4: 35, 14: 11, 23: 8, 47: 16, 51: 11, 19: 11, 62: 32, 52: 22, 13: 12, 56: 28, 59: 7, 1: 15, 2: 28, 38: 14, 53: 30, 24: 26, 25: 9, 54: 20, 26: 2, 48: 39, 49: 28, 28: 32, 20: 36, 60: 23, 42: 34, 44: 28, 36: 28, 37: 29, 55: 29, 16: 6, 15: 6, 17: 33, 33: 31, 30: 38, 27: 28, 3: 9, 32: 20, 57: 39, 7: 2, 34: 2, 58: 13, 29: 27, 6: 24, 35: 28, 9: 28, 40: 18, 22: 3, 10: 25, 41: 17, 18: 28, 31: 4, 12: 1, 61: 10, 39: 10, 43: 21, 8: 19, 46: 5, 45: 37, 50: 25, 0: 0}\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.7127338218688966\n",
      "  batch 100 loss: 4.046548843383789\n",
      "  batch 150 loss: 3.8809189796447754\n",
      "  batch 200 loss: 3.776601858139038\n",
      "  batch 250 loss: 3.685465531349182\n",
      "  batch 300 loss: 3.523019323348999\n",
      "  batch 350 loss: 3.342341966629028\n",
      "  batch 400 loss: 3.224484014511108\n",
      "  batch 450 loss: 3.054233293533325\n",
      "  batch 500 loss: 2.8914224815368654\n",
      "  batch 550 loss: 2.767561855316162\n",
      "  batch 600 loss: 2.6155224227905274\n",
      "  batch 650 loss: 2.4782934379577637\n",
      "  batch 700 loss: 2.4187934017181396\n",
      "  batch 750 loss: 2.3231186056137085\n",
      "  batch 800 loss: 2.2484674882888793\n",
      "  batch 850 loss: 2.1590465545654296\n",
      "  batch 900 loss: 2.1395219659805296\n",
      "LOSS train 2.13952 valid 7.45642, valid PER 63.59%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 2.031524295806885\n",
      "  batch 100 loss: 2.008863868713379\n",
      "  batch 150 loss: 1.900759162902832\n",
      "  batch 200 loss: 1.8948761677742005\n",
      "  batch 250 loss: 1.8484989190101624\n",
      "  batch 300 loss: 1.8212290620803833\n",
      "  batch 350 loss: 1.7982022643089295\n",
      "  batch 400 loss: 1.7492196464538574\n",
      "  batch 450 loss: 1.721820604801178\n",
      "  batch 500 loss: 1.6795043277740478\n",
      "  batch 550 loss: 1.681934642791748\n",
      "  batch 600 loss: 1.631407790184021\n",
      "  batch 650 loss: 1.5669626593589783\n",
      "  batch 700 loss: 1.5734083890914916\n",
      "  batch 750 loss: 1.5613019585609436\n",
      "  batch 800 loss: 1.5203819727897645\n",
      "  batch 850 loss: 1.5329148149490357\n",
      "  batch 900 loss: 1.502426245212555\n",
      "LOSS train 1.50243 valid 8.87400, valid PER 37.78%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4208507013320923\n",
      "  batch 100 loss: 1.4904036927223205\n",
      "  batch 150 loss: 1.4981988525390626\n",
      "  batch 200 loss: 1.4336654710769654\n",
      "  batch 250 loss: 1.4152528619766236\n",
      "  batch 300 loss: 1.428551743030548\n",
      "  batch 350 loss: 1.4290201044082642\n",
      "  batch 400 loss: 1.4182133460044861\n",
      "  batch 450 loss: 1.3910506868362427\n",
      "  batch 500 loss: 1.3637998509407043\n",
      "  batch 550 loss: 1.3834795427322388\n",
      "  batch 600 loss: 1.3327042984962463\n",
      "  batch 650 loss: 1.346205439567566\n",
      "  batch 700 loss: 1.3498477816581727\n",
      "  batch 750 loss: 1.3394263792037964\n",
      "  batch 800 loss: 1.3876406359672546\n",
      "  batch 850 loss: 1.314320013523102\n",
      "  batch 900 loss: 1.3536664080619811\n",
      "LOSS train 1.35367 valid 10.53038, valid PER 32.56%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.3232082974910737\n",
      "  batch 100 loss: 1.2689874053001404\n",
      "  batch 150 loss: 1.2871069025993347\n",
      "  batch 200 loss: 1.2731406116485595\n",
      "  batch 250 loss: 1.2749579977989196\n",
      "  batch 300 loss: 1.2750091028213502\n",
      "  batch 350 loss: 1.2497627675533294\n",
      "  batch 400 loss: 1.230275422334671\n",
      "  batch 450 loss: 1.2324862945079804\n",
      "  batch 500 loss: 1.3295508658885955\n",
      "  batch 550 loss: 1.2417934763431548\n",
      "  batch 600 loss: 1.2389451313018798\n",
      "  batch 650 loss: 1.2614957761764527\n",
      "  batch 700 loss: 1.2822237503528595\n",
      "  batch 750 loss: 1.269256989955902\n",
      "  batch 800 loss: 1.2242705142498016\n",
      "  batch 850 loss: 1.2401188445091247\n",
      "  batch 900 loss: 1.228197407722473\n",
      "LOSS train 1.22820 valid 7.91310, valid PER 30.77%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.183654589653015\n",
      "  batch 100 loss: 1.1764692270755768\n",
      "  batch 150 loss: 1.1914522171020507\n",
      "  batch 200 loss: 1.2161383974552153\n",
      "  batch 250 loss: 1.15049267411232\n",
      "  batch 300 loss: 1.224024587869644\n",
      "  batch 350 loss: 1.1893051409721374\n",
      "  batch 400 loss: 1.1832207930088043\n",
      "  batch 450 loss: 1.1782315158843994\n",
      "  batch 500 loss: 1.1521783185005188\n",
      "  batch 550 loss: 1.1870315849781037\n",
      "  batch 600 loss: 1.1927097642421722\n",
      "  batch 650 loss: 1.179173423051834\n",
      "  batch 700 loss: 1.1600409126281739\n",
      "  batch 750 loss: 1.1550591862201691\n",
      "  batch 800 loss: 1.2031694543361664\n",
      "  batch 850 loss: 1.207008534669876\n",
      "  batch 900 loss: 1.170934430360794\n",
      "LOSS train 1.17093 valid 8.19138, valid PER 30.38%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.1142580604553223\n",
      "  batch 100 loss: 1.134141048192978\n",
      "  batch 150 loss: 1.1226751482486725\n",
      "  batch 200 loss: 1.0824161350727082\n",
      "  batch 250 loss: 1.1074337768554687\n",
      "  batch 300 loss: 1.126119714975357\n",
      "  batch 350 loss: 1.1291327226161956\n",
      "  batch 400 loss: 1.0968580663204193\n",
      "  batch 450 loss: 1.135231932401657\n",
      "  batch 500 loss: 1.0727435517311097\n",
      "  batch 550 loss: 1.0757683181762696\n",
      "  batch 600 loss: 1.1065571367740632\n",
      "  batch 650 loss: 1.0836355841159822\n",
      "  batch 700 loss: 1.0797932052612305\n",
      "  batch 750 loss: 1.1367853677272797\n",
      "  batch 800 loss: 1.104386032819748\n",
      "  batch 850 loss: 1.1030631399154662\n",
      "  batch 900 loss: 1.1247619593143463\n",
      "LOSS train 1.12476 valid 8.08978, valid PER 28.84%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.039256546497345\n",
      "  batch 100 loss: 1.0960163867473602\n",
      "  batch 150 loss: 1.0554362225532532\n",
      "  batch 200 loss: 1.0478963553905487\n",
      "  batch 250 loss: 1.109973850250244\n",
      "  batch 300 loss: 1.071951960325241\n",
      "  batch 350 loss: 1.092892359495163\n",
      "  batch 400 loss: 1.0426957988739014\n",
      "  batch 450 loss: 1.0373858416080475\n",
      "  batch 500 loss: 1.0435391581058502\n",
      "  batch 550 loss: 1.0311280608177185\n",
      "  batch 600 loss: 1.0537768340110778\n",
      "  batch 650 loss: 1.0384761440753936\n",
      "  batch 700 loss: 1.0748674392700195\n",
      "  batch 750 loss: 1.0448183119297028\n",
      "  batch 800 loss: 1.0242707669734954\n",
      "  batch 850 loss: 1.0423240220546723\n",
      "  batch 900 loss: 1.0230839383602142\n",
      "LOSS train 1.02308 valid 7.79208, valid PER 28.10%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.0169366478919983\n",
      "  batch 100 loss: 0.9892160153388977\n",
      "  batch 150 loss: 1.0116298985481262\n",
      "  batch 200 loss: 1.0279420828819275\n",
      "  batch 250 loss: 1.0044354844093322\n",
      "  batch 300 loss: 0.9708976948261261\n",
      "  batch 350 loss: 1.0252652764320374\n",
      "  batch 400 loss: 1.014217553138733\n",
      "  batch 450 loss: 1.053039356470108\n",
      "  batch 500 loss: 1.0101540756225587\n",
      "  batch 550 loss: 1.0119120657444\n",
      "  batch 600 loss: 0.9724074625968933\n",
      "  batch 650 loss: 1.0029759979248047\n",
      "  batch 700 loss: 1.033340939283371\n",
      "  batch 750 loss: 1.0036540639400482\n",
      "  batch 800 loss: 1.0169296312332152\n",
      "  batch 850 loss: 1.0089152145385742\n",
      "  batch 900 loss: 1.000953048467636\n",
      "LOSS train 1.00095 valid 8.17592, valid PER 27.32%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.9406536495685578\n",
      "  batch 100 loss: 0.9422307658195496\n",
      "  batch 150 loss: 0.9545614445209503\n",
      "  batch 200 loss: 0.9668191647529603\n",
      "  batch 250 loss: 0.940552841424942\n",
      "  batch 300 loss: 0.9504677188396454\n",
      "  batch 350 loss: 0.9606835699081421\n",
      "  batch 400 loss: 0.9705081343650818\n",
      "  batch 450 loss: 0.9883949732780457\n",
      "  batch 500 loss: 0.9345441746711731\n",
      "  batch 550 loss: 0.9762807631492615\n",
      "  batch 600 loss: 1.0010916924476623\n",
      "  batch 650 loss: 0.9973351812362671\n",
      "  batch 700 loss: 0.9626042294502258\n",
      "  batch 750 loss: 0.9745428001880646\n",
      "  batch 800 loss: 1.0169001138210296\n",
      "  batch 850 loss: 0.9751249778270722\n",
      "  batch 900 loss: 0.9352785575389863\n",
      "LOSS train 0.93528 valid 9.24448, valid PER 26.59%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.9352896797657013\n",
      "  batch 100 loss: 0.9296794021129609\n",
      "  batch 150 loss: 0.9381176280975342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.9256658458709717\n",
      "  batch 250 loss: 0.9220187711715698\n",
      "  batch 300 loss: 0.933674408197403\n",
      "  batch 350 loss: 0.9115529894828797\n",
      "  batch 400 loss: 0.9036956608295441\n",
      "  batch 450 loss: 0.9199896728992463\n",
      "  batch 500 loss: 0.9267417562007904\n",
      "  batch 550 loss: 0.9420150065422058\n",
      "  batch 600 loss: 0.9381523501873016\n",
      "  batch 650 loss: 0.9565025448799134\n",
      "  batch 700 loss: 0.9723470556735992\n",
      "  batch 750 loss: 0.967600382566452\n",
      "  batch 800 loss: 0.9598461091518402\n",
      "  batch 850 loss: 0.9359593617916108\n",
      "  batch 900 loss: 0.9270258128643036\n",
      "LOSS train 0.92703 valid 8.10100, valid PER 25.52%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.9140496742725372\n",
      "  batch 100 loss: 0.8988940513134003\n",
      "  batch 150 loss: 0.8823165214061737\n",
      "  batch 200 loss: 0.867505464553833\n",
      "  batch 250 loss: 0.8885717809200286\n",
      "  batch 300 loss: 0.8762462043762207\n",
      "  batch 350 loss: 0.9288914179801941\n",
      "  batch 400 loss: 0.8883568215370178\n",
      "  batch 450 loss: 0.8917292201519013\n",
      "  batch 500 loss: 0.893832323551178\n",
      "  batch 550 loss: 0.9281637966632843\n",
      "  batch 600 loss: 0.9019370448589324\n",
      "  batch 650 loss: 0.90286860704422\n",
      "  batch 700 loss: 0.9738214159011841\n",
      "  batch 750 loss: 0.8908457362651825\n",
      "  batch 800 loss: 0.9023269450664521\n",
      "  batch 850 loss: 0.9202986204624176\n",
      "  batch 900 loss: 0.9362438428401947\n",
      "Epoch 00011: reducing learning rate of group 0 to 3.5000e-01.\n",
      "LOSS train 0.93624 valid 7.80019, valid PER 25.81%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.8294428384304047\n",
      "  batch 100 loss: 0.7955527329444885\n",
      "  batch 150 loss: 0.826560492515564\n",
      "  batch 200 loss: 0.8329715979099274\n",
      "  batch 250 loss: 0.8145304703712464\n",
      "  batch 300 loss: 0.8312780284881591\n",
      "  batch 350 loss: 0.7898967730998993\n",
      "  batch 400 loss: 0.8231899738311768\n",
      "  batch 450 loss: 0.7966358411312103\n",
      "  batch 500 loss: 0.8126211577653885\n",
      "  batch 550 loss: 0.8133137917518616\n",
      "  batch 600 loss: 0.8117157971858978\n",
      "  batch 650 loss: 0.784386590719223\n",
      "  batch 700 loss: 0.7954100906848908\n",
      "  batch 750 loss: 0.8359850633144379\n",
      "  batch 800 loss: 0.7710872745513916\n",
      "  batch 850 loss: 0.8004891741275787\n",
      "  batch 900 loss: 0.8470781695842743\n",
      "LOSS train 0.84708 valid 8.02231, valid PER 24.22%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7464598298072815\n",
      "  batch 100 loss: 0.7683358418941498\n",
      "  batch 150 loss: 0.7816807878017425\n",
      "  batch 200 loss: 0.7390980124473572\n",
      "  batch 250 loss: 0.7631669485569\n",
      "  batch 300 loss: 0.8023992061614991\n",
      "  batch 350 loss: 0.7469221329689026\n",
      "  batch 400 loss: 0.7597194427251815\n",
      "  batch 450 loss: 0.782301185131073\n",
      "  batch 500 loss: 0.7617997360229493\n",
      "  batch 550 loss: 0.8206272947788239\n",
      "  batch 600 loss: 0.7756453633308411\n",
      "  batch 650 loss: 0.7797054052352905\n",
      "  batch 700 loss: 0.777808780670166\n",
      "  batch 750 loss: 0.7508720409870148\n",
      "  batch 800 loss: 0.7885166108608246\n",
      "  batch 850 loss: 0.7674127835035324\n",
      "  batch 900 loss: 0.800517873764038\n",
      "LOSS train 0.80052 valid 7.86539, valid PER 23.33%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.7401131319999695\n",
      "  batch 100 loss: 0.7101046818494797\n",
      "  batch 150 loss: 0.7571490609645843\n",
      "  batch 200 loss: 0.7430842781066894\n",
      "  batch 250 loss: 0.7366911923885345\n",
      "  batch 300 loss: 0.7406120717525482\n",
      "  batch 350 loss: 0.743714781999588\n",
      "  batch 400 loss: 0.765101068019867\n",
      "  batch 450 loss: 0.7352888947725296\n",
      "  batch 500 loss: 0.7452648115158081\n",
      "  batch 550 loss: 0.7511745083332062\n",
      "  batch 600 loss: 0.7564392340183258\n",
      "  batch 650 loss: 0.7689459121227264\n",
      "  batch 700 loss: 0.7548160398006439\n",
      "  batch 750 loss: 0.754376118183136\n",
      "  batch 800 loss: 0.7536221671104432\n",
      "  batch 850 loss: 0.7988668251037597\n",
      "  batch 900 loss: 0.7837511259317398\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.7500e-01.\n",
      "LOSS train 0.78375 valid 7.24529, valid PER 23.62%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6807804870605468\n",
      "  batch 100 loss: 0.6899118137359619\n",
      "  batch 150 loss: 0.7040841162204743\n",
      "  batch 200 loss: 0.7217361265420914\n",
      "  batch 250 loss: 0.7245520234107972\n",
      "  batch 300 loss: 0.7185644543170929\n",
      "  batch 350 loss: 0.6936136543750763\n",
      "  batch 400 loss: 0.7074761652946472\n",
      "  batch 450 loss: 0.6870927679538726\n",
      "  batch 500 loss: 0.6745935207605362\n",
      "  batch 550 loss: 0.7502154678106308\n",
      "  batch 600 loss: 0.7206722855567932\n",
      "  batch 650 loss: 0.7014418190717697\n",
      "  batch 700 loss: 0.6837559199333191\n",
      "  batch 750 loss: 0.7153599107265473\n",
      "  batch 800 loss: 0.7011631721258164\n",
      "  batch 850 loss: 0.6909257543087005\n",
      "  batch 900 loss: 0.6618053144216538\n",
      "LOSS train 0.66181 valid 7.38464, valid PER 23.10%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.6943185406923295\n",
      "  batch 100 loss: 0.6689208650588989\n",
      "  batch 150 loss: 0.6942738747596741\n",
      "  batch 200 loss: 0.7031411105394363\n",
      "  batch 250 loss: 0.6787740910053253\n",
      "  batch 300 loss: 0.6899457103013993\n",
      "  batch 350 loss: 0.6849247217178345\n",
      "  batch 400 loss: 0.6836340874433517\n",
      "  batch 450 loss: 0.6967034727334976\n",
      "  batch 500 loss: 0.6795766150951386\n",
      "  batch 550 loss: 0.6610416114330292\n",
      "  batch 600 loss: 0.7279119139909744\n",
      "  batch 650 loss: 0.6894417321681976\n",
      "  batch 700 loss: 0.6596921616792679\n",
      "  batch 750 loss: 0.6870517265796662\n",
      "  batch 800 loss: 0.6563732165098191\n",
      "  batch 850 loss: 0.6752480345964432\n",
      "  batch 900 loss: 0.6886143881082535\n",
      "LOSS train 0.68861 valid 7.50058, valid PER 23.00%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.7034877550601959\n",
      "  batch 100 loss: 0.6314425432682037\n",
      "  batch 150 loss: 0.6814765989780426\n",
      "  batch 200 loss: 0.6349913412332535\n",
      "  batch 250 loss: 0.6831410670280457\n",
      "  batch 300 loss: 0.6489278155565262\n",
      "  batch 350 loss: 0.6789904147386551\n",
      "  batch 400 loss: 0.6717376101016999\n",
      "  batch 450 loss: 0.660617516040802\n",
      "  batch 500 loss: 0.6703162461519241\n",
      "  batch 550 loss: 0.6735382670164108\n",
      "  batch 600 loss: 0.720940210223198\n",
      "  batch 650 loss: 0.6673753595352173\n",
      "  batch 700 loss: 0.6739353227615357\n",
      "  batch 750 loss: 0.6309822380542756\n",
      "  batch 800 loss: 0.6955317920446396\n",
      "  batch 850 loss: 0.6718750232458115\n",
      "  batch 900 loss: 0.6728313648700714\n",
      "LOSS train 0.67283 valid 7.24141, valid PER 22.58%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.6312210118770599\n",
      "  batch 100 loss: 0.6552127778530121\n",
      "  batch 150 loss: 0.6989195287227631\n",
      "  batch 200 loss: 0.6680579787492752\n",
      "  batch 250 loss: 0.6293550205230712\n",
      "  batch 300 loss: 0.6560295969247818\n",
      "  batch 350 loss: 0.6348947548866272\n",
      "  batch 400 loss: 0.63683653652668\n",
      "  batch 450 loss: 0.675760322213173\n",
      "  batch 500 loss: 0.6635937494039535\n",
      "  batch 550 loss: 0.6785118782520294\n",
      "  batch 600 loss: 0.6645758843421936\n",
      "  batch 650 loss: 0.6684731715917587\n",
      "  batch 700 loss: 0.6702795803546906\n",
      "  batch 750 loss: 0.6772064834833145\n",
      "  batch 800 loss: 0.6777731961011887\n",
      "  batch 850 loss: 0.669931058883667\n",
      "  batch 900 loss: 0.6714367055892945\n",
      "LOSS train 0.67144 valid 7.39517, valid PER 22.45%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.6500495535135269\n",
      "  batch 100 loss: 0.6571761894226075\n",
      "  batch 150 loss: 0.6336332088708878\n",
      "  batch 200 loss: 0.625132127404213\n",
      "  batch 250 loss: 0.6619403421878814\n",
      "  batch 300 loss: 0.6807610535621643\n",
      "  batch 350 loss: 0.6608762061595916\n",
      "  batch 400 loss: 0.6452319598197938\n",
      "  batch 450 loss: 0.6210075050592423\n",
      "  batch 500 loss: 0.633331760764122\n",
      "  batch 550 loss: 0.673870460987091\n",
      "  batch 600 loss: 0.6666935819387436\n",
      "  batch 650 loss: 0.6662268453836441\n",
      "  batch 700 loss: 0.6664176404476165\n",
      "  batch 750 loss: 0.6373683059215546\n",
      "  batch 800 loss: 0.6508872807025909\n",
      "  batch 850 loss: 0.674522465467453\n",
      "  batch 900 loss: 0.6542757868766784\n",
      "Epoch 00019: reducing learning rate of group 0 to 8.7500e-02.\n",
      "LOSS train 0.65428 valid 7.68835, valid PER 22.62%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.6178551638126373\n",
      "  batch 100 loss: 0.6113486385345459\n",
      "  batch 150 loss: 0.6252816194295883\n",
      "  batch 200 loss: 0.6278883588314056\n",
      "  batch 250 loss: 0.6470467519760131\n",
      "  batch 300 loss: 0.6402287244796753\n",
      "  batch 350 loss: 0.5986198997497558\n",
      "  batch 400 loss: 0.6168769109249115\n",
      "  batch 450 loss: 0.6082984095811844\n",
      "  batch 500 loss: 0.6339954203367233\n",
      "  batch 550 loss: 0.6315268987417221\n",
      "  batch 600 loss: 0.6322900074720382\n",
      "  batch 650 loss: 0.621735582947731\n",
      "  batch 700 loss: 0.6249138206243515\n",
      "  batch 750 loss: 0.6088589054346084\n",
      "  batch 800 loss: 0.6270631402730942\n",
      "  batch 850 loss: 0.6400865191221237\n",
      "  batch 900 loss: 0.6186095380783081\n",
      "LOSS train 0.61861 valid 7.63062, valid PER 22.20%\n",
      "Training finished in 5.0 minutes.\n",
      "Model saved to checkpoints/20231212_095116/model_20\n",
      "End tuning For 2 Layer LSTM\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout_between_layer\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler_61_phone import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from phone_61_decoder import decode\n",
    "\n",
    "print(\"Start dropout tuning For 2 Layer LSTM, with Dropout between layer\")\n",
    "\n",
    "\n",
    "\n",
    "args = {'seed': 123,\n",
    "    'train_json': 'train_fbank_p61.json',\n",
    "    'val_json': 'dev_fbank.json',\n",
    "    'test_json': 'test_fbank.json',\n",
    "    'batch_size': 4,\n",
    "    'num_layers': 2,\n",
    "    'fbank_dims': 23,\n",
    "    'model_dims': 128,\n",
    "    'concat': 1,\n",
    "    'lr': 0.7,\n",
    "    'vocab': vocab,\n",
    "    'vocab_39': vocab_39,\n",
    "    'report_interval': 50,\n",
    "    'num_epochs': 20,\n",
    "    'device': device,\n",
    "   }\n",
    "\n",
    "args = namedtuple('x', args)(**args)\n",
    "\n",
    "\n",
    "print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "model_with_dropout = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "print('Total number of model parameters is {}'.format(num_params))\n",
    "start = datetime.now()\n",
    "model_with_dropout.to(args.device)\n",
    "model_path = sgd_trainer(model_with_dropout, args)\n",
    "end = datetime.now()\n",
    "duration = (end - start).total_seconds()\n",
    "print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "print('Model saved to {}'.format(model_path))\n",
    "\n",
    "print(\"End tuning For 2 Layer LSTM\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add33319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3048a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_regularisation_dropout_between_layer\n",
    "args = {'seed': 123,\n",
    "    'train_json': 'train_fbank_p61.json',\n",
    "    'val_json': 'dev_fbank.json',\n",
    "    'test_json': 'test_fbank.json',\n",
    "    'batch_size': 4,\n",
    "    'num_layers': 2,\n",
    "    'fbank_dims': 23,\n",
    "    'model_dims': 128,\n",
    "    'concat': 1,\n",
    "    'lr': 0.5,\n",
    "    'vocab': vocab,\n",
    "    'vocab_39': vocab_39,\n",
    "    'report_interval': 50,\n",
    "    'num_epochs': 20,\n",
    "    'device': \"cpu\",\n",
    "   }\n",
    "\n",
    "args = namedtuple('x', args)(**args)\n",
    "model = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "628f0c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints/20231212_095116/model_20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (lstm): LSTM(23, 128, num_layers=2, bidirectional=True)\n",
       "  (proj): Linear(in_features=256, out_features=63, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print('Loading model from {}'.format(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a376b944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB: 14.79%, DEL: 5.96%, INS: 3.02%, COR: 79.24%, PER: 23.77%\n"
     ]
    }
   ],
   "source": [
    "from phone_61_decoder import decode\n",
    "results = decode(model, args, args.test_json)\n",
    "print(\"SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%\".format(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad93fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84664f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a456fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
