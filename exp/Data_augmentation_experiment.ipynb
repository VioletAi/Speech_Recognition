{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adff7fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_dataloader\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25165d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find the unique phones in train.json, and then\n",
    "# create a file named vocab.txt, each line in this \n",
    "# file is a unique phone, in total there should be \n",
    "# 40 lines\n",
    "\n",
    "vocab = {}\n",
    "phonemes = []\n",
    "with open(\"vocab_39.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab[text.strip()] = id\n",
    "        phonemes.append(text)\n",
    "phonemes = phonemes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5918c426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently using cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"currently using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"currently using cpu only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e27ccc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0, 0.1, 0.3, 0.5]\n",
    "Optimiser = [\"SGD_Scheduler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db4a1060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dropout tuning For 2 Layer LSTM, with Dropout between layer\n",
      "Currently using SGD_Scheduler optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.953719806671143\n",
      "  batch 100 loss: 3.3993154096603395\n",
      "  batch 150 loss: 3.300382390022278\n",
      "  batch 200 loss: 3.1780371713638305\n",
      "  batch 250 loss: 3.064969687461853\n",
      "  batch 300 loss: 2.8370530080795286\n",
      "  batch 350 loss: 2.6840357875823972\n",
      "  batch 400 loss: 2.575184178352356\n",
      "  batch 450 loss: 2.462020764350891\n",
      "  batch 500 loss: 2.3452905082702635\n",
      "  batch 550 loss: 2.2567735862731935\n",
      "  batch 600 loss: 2.200996849536896\n",
      "  batch 650 loss: 2.107234065532684\n",
      "  batch 700 loss: 2.0393861436843874\n",
      "  batch 750 loss: 1.9655284953117371\n",
      "  batch 800 loss: 1.911541314125061\n",
      "  batch 850 loss: 1.9214751029014587\n",
      "  batch 900 loss: 1.8436394572257995\n",
      "  batch 950 loss: 1.800590009689331\n",
      "  batch 1000 loss: 1.7497910404205321\n",
      "  batch 1050 loss: 1.6814700603485107\n",
      "  batch 1100 loss: 1.6279713416099548\n",
      "  batch 1150 loss: 1.6326284623146057\n",
      "  batch 1200 loss: 1.600464382171631\n",
      "  batch 1250 loss: 1.6186462092399596\n",
      "  batch 1300 loss: 1.514974775314331\n",
      "  batch 1350 loss: 1.5106215572357178\n",
      "  batch 1400 loss: 1.4968688416481017\n",
      "  batch 1450 loss: 1.3915811157226563\n",
      "  batch 1500 loss: 1.3978799414634704\n",
      "  batch 1550 loss: 1.3566129660606385\n",
      "  batch 1600 loss: 1.3741099691390992\n",
      "  batch 1650 loss: 1.2955077576637268\n",
      "  batch 1700 loss: 1.3167269206047059\n",
      "  batch 1750 loss: 1.268619211912155\n",
      "  batch 1800 loss: 1.2795173597335816\n",
      "  batch 1850 loss: 1.2620125544071197\n",
      "  batch 1900 loss: 1.2305581760406494\n",
      "  batch 1950 loss: 1.2477925026416778\n",
      "  batch 2000 loss: 1.253996958732605\n",
      "  batch 2050 loss: 1.1776480293273925\n",
      "  batch 2100 loss: 1.2105989754199982\n",
      "  batch 2150 loss: 1.1650351202487945\n",
      "  batch 2200 loss: 1.2110681414604187\n",
      "  batch 2250 loss: 1.191192750930786\n",
      "  batch 2300 loss: 1.1764232587814332\n",
      "  batch 2350 loss: 1.1649400186538696\n",
      "  batch 2400 loss: 1.1307914233207703\n",
      "  batch 2450 loss: 1.1361888253688812\n",
      "  batch 2500 loss: 1.1011138844490052\n",
      "  batch 2550 loss: 1.1082768869400024\n",
      "  batch 2600 loss: 1.0742875063419342\n",
      "  batch 2650 loss: 1.1234747207164764\n",
      "  batch 2700 loss: 1.1182584428787232\n",
      "  batch 2750 loss: 1.1093690848350526\n",
      "LOSS train 1.10937 valid 1.11071, valid PER 34.05%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.0735940253734588\n",
      "  batch 100 loss: 1.056636562347412\n",
      "  batch 150 loss: 1.0595078074932098\n",
      "  batch 200 loss: 1.037081060409546\n",
      "  batch 250 loss: 1.0587892091274262\n",
      "  batch 300 loss: 1.035017580986023\n",
      "  batch 350 loss: 1.001768374443054\n",
      "  batch 400 loss: 1.007700296640396\n",
      "  batch 450 loss: 1.0472142553329469\n",
      "  batch 500 loss: 1.0553822278976441\n",
      "  batch 550 loss: 1.0280195820331572\n",
      "  batch 600 loss: 1.0411839926242827\n",
      "  batch 650 loss: 0.9838652014732361\n",
      "  batch 700 loss: 0.9808330428600311\n",
      "  batch 750 loss: 0.9682562696933746\n",
      "  batch 800 loss: 0.9843046474456787\n",
      "  batch 850 loss: 0.9930899047851562\n",
      "  batch 900 loss: 1.010415186882019\n",
      "  batch 950 loss: 0.9924934756755829\n",
      "  batch 1000 loss: 0.9987963175773621\n",
      "  batch 1050 loss: 0.9997894465923309\n",
      "  batch 1100 loss: 0.9331219363212585\n",
      "  batch 1150 loss: 0.9570833623409272\n",
      "  batch 1200 loss: 0.9741566586494446\n",
      "  batch 1250 loss: 0.9995481872558594\n",
      "  batch 1300 loss: 0.9665496838092804\n",
      "  batch 1350 loss: 0.9551468634605408\n",
      "  batch 1400 loss: 0.948340641260147\n",
      "  batch 1450 loss: 0.9529044306278229\n",
      "  batch 1500 loss: 0.9459471940994263\n",
      "  batch 1550 loss: 0.9359663426876068\n",
      "  batch 1600 loss: 0.9844194781780243\n",
      "  batch 1650 loss: 0.9100600504875183\n",
      "  batch 1700 loss: 0.9301564574241639\n",
      "  batch 1750 loss: 0.9216768872737885\n",
      "  batch 1800 loss: 0.8781951999664307\n",
      "  batch 1850 loss: 0.9277823770046234\n",
      "  batch 1900 loss: 0.9404625737667084\n",
      "  batch 1950 loss: 0.8944469726085663\n",
      "  batch 2000 loss: 0.8914561760425568\n",
      "  batch 2050 loss: 0.8963236033916473\n",
      "  batch 2100 loss: 0.8606720042228698\n",
      "  batch 2150 loss: 0.8832761514186859\n",
      "  batch 2200 loss: 0.9112849843502044\n",
      "  batch 2250 loss: 0.866248015165329\n",
      "  batch 2300 loss: 0.89488649725914\n",
      "  batch 2350 loss: 0.8952204740047455\n",
      "  batch 2400 loss: 0.8904138588905335\n",
      "  batch 2450 loss: 0.8370527148246765\n",
      "  batch 2500 loss: 0.8718337738513946\n",
      "  batch 2550 loss: 0.8785219347476959\n",
      "  batch 2600 loss: 0.8596694302558899\n",
      "  batch 2650 loss: 0.9011728250980378\n",
      "  batch 2700 loss: 0.8239072406291962\n",
      "  batch 2750 loss: 0.8730448722839356\n",
      "LOSS train 0.87304 valid 0.93184, valid PER 29.66%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 0.8361380481719971\n",
      "  batch 100 loss: 0.8641145598888397\n",
      "  batch 150 loss: 0.8574317026138306\n",
      "  batch 200 loss: 0.8207035553455353\n",
      "  batch 250 loss: 0.8012987339496612\n",
      "  batch 300 loss: 0.8066903221607208\n",
      "  batch 350 loss: 0.8129715156555176\n",
      "  batch 400 loss: 0.8101631259918213\n",
      "  batch 450 loss: 0.8046724832057953\n",
      "  batch 500 loss: 0.805607088804245\n",
      "  batch 550 loss: 0.8591667783260345\n",
      "  batch 600 loss: 0.7962080907821655\n",
      "  batch 650 loss: 0.7900412392616272\n",
      "  batch 700 loss: 0.7928042042255402\n",
      "  batch 750 loss: 0.8222563219070435\n",
      "  batch 800 loss: 0.7900632333755493\n",
      "  batch 850 loss: 0.8133156418800354\n",
      "  batch 900 loss: 0.7968025374412536\n",
      "  batch 950 loss: 0.8250773704051971\n",
      "  batch 1000 loss: 0.8227135944366455\n",
      "  batch 1050 loss: 0.8406893849372864\n",
      "  batch 1100 loss: 0.7978189712762833\n",
      "  batch 1150 loss: 0.7891172194480895\n",
      "  batch 1200 loss: 0.7818507790565491\n",
      "  batch 1250 loss: 0.7809949934482574\n",
      "  batch 1300 loss: 0.7882111793756486\n",
      "  batch 1350 loss: 0.797071019411087\n",
      "  batch 1400 loss: 0.792514374256134\n",
      "  batch 1450 loss: 0.7869057905673981\n",
      "  batch 1500 loss: 0.7881359374523162\n",
      "  batch 1550 loss: 0.7867164754867554\n",
      "  batch 1600 loss: 0.781719172000885\n",
      "  batch 1650 loss: 0.8078709483146668\n",
      "  batch 1700 loss: 0.7956330823898315\n",
      "  batch 1750 loss: 0.7655401104688644\n",
      "  batch 1800 loss: 0.7773001110553741\n",
      "  batch 1850 loss: 0.7516560781002045\n",
      "  batch 1900 loss: 0.7520559561252594\n",
      "  batch 1950 loss: 0.7556512069702148\n",
      "  batch 2000 loss: 0.7819255095720291\n",
      "  batch 2050 loss: 0.7665477174520493\n",
      "  batch 2100 loss: 0.7659707534313202\n",
      "  batch 2150 loss: 0.7801506346464158\n",
      "  batch 2200 loss: 0.7846727418899536\n",
      "  batch 2250 loss: 0.7550998246669769\n",
      "  batch 2300 loss: 0.7841246557235718\n",
      "  batch 2350 loss: 0.7756801629066468\n",
      "  batch 2400 loss: 0.7493716764450074\n",
      "  batch 2450 loss: 0.7184943449497223\n",
      "  batch 2500 loss: 0.7418382430076599\n",
      "  batch 2550 loss: 0.6962744396924972\n",
      "  batch 2600 loss: 0.7453274673223496\n",
      "  batch 2650 loss: 0.746290300488472\n",
      "  batch 2700 loss: 0.7375486779212952\n",
      "  batch 2750 loss: 0.7471743953227997\n",
      "LOSS train 0.74717 valid 0.84091, valid PER 26.55%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.7566227936744689\n",
      "  batch 100 loss: 0.683283970952034\n",
      "  batch 150 loss: 0.7137220954895019\n",
      "  batch 200 loss: 0.6924392712116242\n",
      "  batch 250 loss: 0.713928393125534\n",
      "  batch 300 loss: 0.7193051308393479\n",
      "  batch 350 loss: 0.722063182592392\n",
      "  batch 400 loss: 0.7078528273105621\n",
      "  batch 450 loss: 0.7126262652873993\n",
      "  batch 500 loss: 0.7258376145362854\n",
      "  batch 550 loss: 0.6966599082946777\n",
      "  batch 600 loss: 0.6956020081043244\n",
      "  batch 650 loss: 0.6874836653470993\n",
      "  batch 700 loss: 0.7122212421894073\n",
      "  batch 750 loss: 0.6936503630876542\n",
      "  batch 800 loss: 0.7018575870990753\n",
      "  batch 850 loss: 0.6894213551282883\n",
      "  batch 900 loss: 0.6999055755138397\n",
      "  batch 950 loss: 0.7432081127166748\n",
      "  batch 1000 loss: 0.6962644785642624\n",
      "  batch 1050 loss: 0.6874954718351364\n",
      "  batch 1100 loss: 0.6556194376945496\n",
      "  batch 1150 loss: 0.6943847012519836\n",
      "  batch 1200 loss: 0.7048828619718551\n",
      "  batch 1250 loss: 0.669788944721222\n",
      "  batch 1300 loss: 0.7160003811120987\n",
      "  batch 1350 loss: 0.7250341111421585\n",
      "  batch 1400 loss: 0.6695272296667099\n",
      "  batch 1450 loss: 0.7007060700654983\n",
      "  batch 1500 loss: 0.6824588757753373\n",
      "  batch 1550 loss: 0.6739413058757782\n",
      "  batch 1600 loss: 0.6359480410814286\n",
      "  batch 1650 loss: 0.6693297600746155\n",
      "  batch 1700 loss: 0.6687054193019867\n",
      "  batch 1750 loss: 0.6764307594299317\n",
      "  batch 1800 loss: 0.6578171247243881\n",
      "  batch 1850 loss: 0.6731379324197769\n",
      "  batch 1900 loss: 0.6612040895223618\n",
      "  batch 1950 loss: 0.6700560420751571\n",
      "  batch 2000 loss: 0.6862103754281997\n",
      "  batch 2050 loss: 0.6647235018014908\n",
      "  batch 2100 loss: 0.6855974471569062\n",
      "  batch 2150 loss: 0.6777552604675293\n",
      "  batch 2200 loss: 0.7095809429883957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 2250 loss: 0.6603223425149918\n",
      "  batch 2300 loss: 0.6594460576772689\n",
      "  batch 2350 loss: 0.6609499651193619\n",
      "  batch 2400 loss: 0.6409342026710511\n",
      "  batch 2450 loss: 0.6322945392131806\n",
      "  batch 2500 loss: 0.686556544303894\n",
      "  batch 2550 loss: 0.6657145524024963\n",
      "  batch 2600 loss: 0.6866445565223693\n",
      "  batch 2650 loss: 0.6462947624921799\n",
      "  batch 2700 loss: 0.6766492688655853\n",
      "  batch 2750 loss: 0.6341919189691544\n",
      "LOSS train 0.63419 valid 0.83288, valid PER 25.81%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.6331386977434158\n",
      "  batch 100 loss: 0.6089944469928742\n",
      "  batch 150 loss: 0.6204289311170578\n",
      "  batch 200 loss: 0.6151063221693039\n",
      "  batch 250 loss: 0.579046545624733\n",
      "  batch 300 loss: 0.6445787811279297\n",
      "  batch 350 loss: 0.6220481461286544\n",
      "  batch 400 loss: 0.5797956824302674\n",
      "  batch 450 loss: 0.6365955978631973\n",
      "  batch 500 loss: 0.6200397503376007\n",
      "  batch 550 loss: 0.6106108516454697\n",
      "  batch 600 loss: 0.5838797509670257\n",
      "  batch 650 loss: 0.6128009176254272\n",
      "  batch 700 loss: 0.6067919433116913\n",
      "  batch 750 loss: 0.5975795793533325\n",
      "  batch 800 loss: 0.6253193193674087\n",
      "  batch 850 loss: 0.6276639878749848\n",
      "  batch 900 loss: 0.6056935733556748\n",
      "  batch 950 loss: 0.6245164012908936\n",
      "  batch 1000 loss: 0.6339029270410538\n",
      "  batch 1050 loss: 0.6034399080276489\n",
      "  batch 1100 loss: 0.62728569149971\n",
      "  batch 1150 loss: 0.5853156685829163\n",
      "  batch 1200 loss: 0.6046004384756088\n",
      "  batch 1250 loss: 0.6206363481283188\n",
      "  batch 1300 loss: 0.6294826811552048\n",
      "  batch 1350 loss: 0.6052882248163223\n",
      "  batch 1400 loss: 0.6120620846748352\n",
      "  batch 1450 loss: 0.642283040881157\n",
      "  batch 1500 loss: 0.6132546293735505\n",
      "  batch 1550 loss: 0.6039214742183685\n",
      "  batch 1600 loss: 0.6214508122205734\n",
      "  batch 1650 loss: 0.5930055928230286\n",
      "  batch 1700 loss: 0.6024544012546539\n",
      "  batch 1750 loss: 0.5958067160844803\n",
      "  batch 1800 loss: 0.5958728885650635\n",
      "  batch 1850 loss: 0.615622135400772\n",
      "  batch 1900 loss: 0.6191712468862534\n",
      "  batch 1950 loss: 0.6027223300933838\n",
      "  batch 2000 loss: 0.5736045181751251\n",
      "  batch 2050 loss: 0.586766402721405\n",
      "  batch 2100 loss: 0.614361680150032\n",
      "  batch 2150 loss: 0.6202370816469193\n",
      "  batch 2200 loss: 0.5943909102678299\n",
      "  batch 2250 loss: 0.5701259499788285\n",
      "  batch 2300 loss: 0.6191302704811096\n",
      "  batch 2350 loss: 0.6408807873725891\n",
      "  batch 2400 loss: 0.5952574080228805\n",
      "  batch 2450 loss: 0.6221605896949768\n",
      "  batch 2500 loss: 0.5966881769895553\n",
      "  batch 2550 loss: 0.5953228348493576\n",
      "  batch 2600 loss: 0.6017127829790115\n",
      "  batch 2650 loss: 0.5695416563749314\n",
      "  batch 2700 loss: 0.5638218927383423\n",
      "  batch 2750 loss: 0.5999871504306793\n",
      "LOSS train 0.59999 valid 0.79452, valid PER 24.44%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.5487315624952316\n",
      "  batch 100 loss: 0.5333709985017776\n",
      "  batch 150 loss: 0.547932203412056\n",
      "  batch 200 loss: 0.5750016433000564\n",
      "  batch 250 loss: 0.5142772567272186\n",
      "  batch 300 loss: 0.5466502118110657\n",
      "  batch 350 loss: 0.5656981313228607\n",
      "  batch 400 loss: 0.5861866790056228\n",
      "  batch 450 loss: 0.5704397821426391\n",
      "  batch 500 loss: 0.5430601662397385\n",
      "  batch 550 loss: 0.5562238985300064\n",
      "  batch 600 loss: 0.5229019558429718\n",
      "  batch 650 loss: 0.5706759423017502\n",
      "  batch 700 loss: 0.5483507633209228\n",
      "  batch 750 loss: 0.548558429479599\n",
      "  batch 800 loss: 0.5529124838113785\n",
      "  batch 850 loss: 0.5050410115718842\n",
      "  batch 900 loss: 0.5406787002086639\n",
      "  batch 950 loss: 0.545902459025383\n",
      "  batch 1000 loss: 0.5461882746219635\n",
      "  batch 1050 loss: 0.5390165799856186\n",
      "  batch 1100 loss: 0.5459447485208512\n",
      "  batch 1150 loss: 0.5648762273788452\n",
      "  batch 1200 loss: 0.5346873933076859\n",
      "  batch 1250 loss: 0.5529287350177765\n",
      "  batch 1300 loss: 0.5391875410079956\n",
      "  batch 1350 loss: 0.5416547346115113\n",
      "  batch 1400 loss: 0.5745528107881546\n",
      "  batch 1450 loss: 0.557036184668541\n",
      "  batch 1500 loss: 0.5479319787025452\n",
      "  batch 1550 loss: 0.5307093322277069\n",
      "  batch 1600 loss: 0.5664029449224472\n",
      "  batch 1650 loss: 0.557745281457901\n",
      "  batch 1700 loss: 0.5696755516529083\n",
      "  batch 1750 loss: 0.5613427031040191\n",
      "  batch 1800 loss: 0.5339065754413604\n",
      "  batch 1850 loss: 0.5412934958934784\n",
      "  batch 1900 loss: 0.5630770242214203\n",
      "  batch 1950 loss: 0.5425320249795914\n",
      "  batch 2000 loss: 0.5359797507524491\n",
      "  batch 2050 loss: 0.5275699299573898\n",
      "  batch 2100 loss: 0.5385691386461258\n",
      "  batch 2150 loss: 0.5326292532682418\n",
      "  batch 2200 loss: 0.5514844697713852\n",
      "  batch 2250 loss: 0.5366388809680939\n",
      "  batch 2300 loss: 0.5290901112556458\n",
      "  batch 2350 loss: 0.537550340294838\n",
      "  batch 2400 loss: 0.5114215511083603\n",
      "  batch 2450 loss: 0.5575453478097916\n",
      "  batch 2500 loss: 0.5329536408185959\n",
      "  batch 2550 loss: 0.550400567650795\n",
      "  batch 2600 loss: 0.520373752117157\n",
      "  batch 2650 loss: 0.528166646361351\n",
      "  batch 2700 loss: 0.5346591776609421\n",
      "  batch 2750 loss: 0.5422975182533264\n",
      "LOSS train 0.54230 valid 0.79313, valid PER 24.36%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.4853441959619522\n",
      "  batch 100 loss: 0.5089635914564132\n",
      "  batch 150 loss: 0.521401344537735\n",
      "  batch 200 loss: 0.47141260087490083\n",
      "  batch 250 loss: 0.5098902893066406\n",
      "  batch 300 loss: 0.46874790251255033\n",
      "  batch 350 loss: 0.4772089147567749\n",
      "  batch 400 loss: 0.4943197411298752\n",
      "  batch 450 loss: 0.4659723007678986\n",
      "  batch 500 loss: 0.47889574885368347\n",
      "  batch 550 loss: 0.4783075654506683\n",
      "  batch 600 loss: 0.49658446192741396\n",
      "  batch 650 loss: 0.48642903566360474\n",
      "  batch 700 loss: 0.48595251858234406\n",
      "  batch 750 loss: 0.49461105823516843\n",
      "  batch 800 loss: 0.4932073795795441\n",
      "  batch 850 loss: 0.5100084120035171\n",
      "  batch 900 loss: 0.5073008644580841\n",
      "  batch 950 loss: 0.5069549417495728\n",
      "  batch 1000 loss: 0.4635547453165054\n",
      "  batch 1050 loss: 0.5029655128717423\n",
      "  batch 1100 loss: 0.506125939488411\n",
      "  batch 1150 loss: 0.4855739903450012\n",
      "  batch 1200 loss: 0.47626211822032927\n",
      "  batch 1250 loss: 0.4511631101369858\n",
      "  batch 1300 loss: 0.46922948002815246\n",
      "  batch 1350 loss: 0.5019682449102402\n",
      "  batch 1400 loss: 0.4918469315767288\n",
      "  batch 1450 loss: 0.4742990258336067\n",
      "  batch 1500 loss: 0.5012585282325744\n",
      "  batch 1550 loss: 0.4777141129970551\n",
      "  batch 1600 loss: 0.4825780326128006\n",
      "  batch 1650 loss: 0.49143983483314513\n",
      "  batch 1700 loss: 0.49131280958652496\n",
      "  batch 1750 loss: 0.48965048134326933\n",
      "  batch 1800 loss: 0.48942486584186556\n",
      "  batch 1850 loss: 0.4777380722761154\n",
      "  batch 1900 loss: 0.4832593709230423\n",
      "  batch 1950 loss: 0.4840766489505768\n",
      "  batch 2000 loss: 0.46923047602176665\n",
      "  batch 2050 loss: 0.4875830096006393\n",
      "  batch 2100 loss: 0.4901506844162941\n",
      "  batch 2150 loss: 0.5040311390161514\n",
      "  batch 2200 loss: 0.4881868588924408\n",
      "  batch 2250 loss: 0.47065616726875303\n",
      "  batch 2300 loss: 0.4918229931592941\n",
      "  batch 2350 loss: 0.46871536850929263\n",
      "  batch 2400 loss: 0.4758726599812508\n",
      "  batch 2450 loss: 0.4920339080691338\n",
      "  batch 2500 loss: 0.49422443330287935\n",
      "  batch 2550 loss: 0.47513764768838884\n",
      "  batch 2600 loss: 0.4400218930840492\n",
      "  batch 2650 loss: 0.46485424280166626\n",
      "  batch 2700 loss: 0.49067408323287964\n",
      "  batch 2750 loss: 0.46890905559062956\n",
      "Epoch 00007: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.46891 valid 0.79938, valid PER 23.55%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.39572382360696795\n",
      "  batch 100 loss: 0.3723585611581802\n",
      "  batch 150 loss: 0.3927775853872299\n",
      "  batch 200 loss: 0.3742271587252617\n",
      "  batch 250 loss: 0.3624039590358734\n",
      "  batch 300 loss: 0.38239111602306364\n",
      "  batch 350 loss: 0.377235147356987\n",
      "  batch 400 loss: 0.38006845325231553\n",
      "  batch 450 loss: 0.3694994357228279\n",
      "  batch 500 loss: 0.3771443784236908\n",
      "  batch 550 loss: 0.3557948160171509\n",
      "  batch 600 loss: 0.35256792306900026\n",
      "  batch 650 loss: 0.3828764662146568\n",
      "  batch 700 loss: 0.37469856441020966\n",
      "  batch 750 loss: 0.3714794501662254\n",
      "  batch 800 loss: 0.39221775203943254\n",
      "  batch 850 loss: 0.36379300117492674\n",
      "  batch 900 loss: 0.3716262689232826\n",
      "  batch 950 loss: 0.3766215318441391\n",
      "  batch 1000 loss: 0.3761435204744339\n",
      "  batch 1050 loss: 0.35280231058597566\n",
      "  batch 1100 loss: 0.36842176198959353\n",
      "  batch 1150 loss: 0.3895983785390854\n",
      "  batch 1200 loss: 0.3478300181031227\n",
      "  batch 1250 loss: 0.37117437571287154\n",
      "  batch 1300 loss: 0.3671118932962418\n",
      "  batch 1350 loss: 0.3366044703125954\n",
      "  batch 1400 loss: 0.38114445775747297\n",
      "  batch 1450 loss: 0.3654925188422203\n",
      "  batch 1500 loss: 0.3557592836022377\n",
      "  batch 1550 loss: 0.3296461313962936\n",
      "  batch 1600 loss: 0.37858051151037214\n",
      "  batch 1650 loss: 0.3885683462023735\n",
      "  batch 1700 loss: 0.35141980051994326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1750 loss: 0.3581231477856636\n",
      "  batch 1800 loss: 0.3444445192813873\n",
      "  batch 1850 loss: 0.3627458393573761\n",
      "  batch 1900 loss: 0.35728987425565717\n",
      "  batch 1950 loss: 0.3488490375876427\n",
      "  batch 2000 loss: 0.33862262666225434\n",
      "  batch 2050 loss: 0.367991014122963\n",
      "  batch 2100 loss: 0.3611968460679054\n",
      "  batch 2150 loss: 0.3819046404957771\n",
      "  batch 2200 loss: 0.3519177129864693\n",
      "  batch 2250 loss: 0.3503365993499756\n",
      "  batch 2300 loss: 0.35073968917131426\n",
      "  batch 2350 loss: 0.36448561400175095\n",
      "  batch 2400 loss: 0.3584690853953362\n",
      "  batch 2450 loss: 0.32283335238695143\n",
      "  batch 2500 loss: 0.3499716010689735\n",
      "  batch 2550 loss: 0.36583689600229263\n",
      "  batch 2600 loss: 0.34491838842630385\n",
      "  batch 2650 loss: 0.35270298421382906\n",
      "  batch 2700 loss: 0.3336675837635994\n",
      "  batch 2750 loss: 0.363292818069458\n",
      "LOSS train 0.36329 valid 0.77704, valid PER 22.59%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.3169567310810089\n",
      "  batch 100 loss: 0.30438849300146104\n",
      "  batch 150 loss: 0.30472632229328156\n",
      "  batch 200 loss: 0.3110214206576347\n",
      "  batch 250 loss: 0.3277635782957077\n",
      "  batch 300 loss: 0.3111167544126511\n",
      "  batch 350 loss: 0.31117518395185473\n",
      "  batch 400 loss: 0.31866980731487277\n",
      "  batch 450 loss: 0.3120387160778046\n",
      "  batch 500 loss: 0.3025549334287643\n",
      "  batch 550 loss: 0.3243237373232841\n",
      "  batch 600 loss: 0.32129092782735824\n",
      "  batch 650 loss: 0.3363706856966019\n",
      "  batch 700 loss: 0.3228953728079796\n",
      "  batch 750 loss: 0.3247325836122036\n",
      "  batch 800 loss: 0.304847249686718\n",
      "  batch 850 loss: 0.31678318589925764\n",
      "  batch 900 loss: 0.3226615834236145\n",
      "  batch 950 loss: 0.34169649362564086\n",
      "  batch 1000 loss: 0.31806664139032365\n",
      "  batch 1050 loss: 0.32506414979696274\n",
      "  batch 1100 loss: 0.30939469367265704\n",
      "  batch 1150 loss: 0.3066409969329834\n",
      "  batch 1200 loss: 0.3080586636066437\n",
      "  batch 1250 loss: 0.31099888175725937\n",
      "  batch 1300 loss: 0.3445057538151741\n",
      "  batch 1350 loss: 0.3028797671198845\n",
      "  batch 1400 loss: 0.2892319732904434\n",
      "  batch 1450 loss: 0.31146217495203016\n",
      "  batch 1500 loss: 0.31797836512327193\n",
      "  batch 1550 loss: 0.303047656416893\n",
      "  batch 1600 loss: 0.29992659389972687\n",
      "  batch 1650 loss: 0.31130442082881926\n",
      "  batch 1700 loss: 0.3239132758975029\n",
      "  batch 1750 loss: 0.325028675198555\n",
      "  batch 1800 loss: 0.3081406745314598\n",
      "  batch 1850 loss: 0.33234004616737367\n",
      "  batch 1900 loss: 0.33026167839765547\n",
      "  batch 1950 loss: 0.29821664065122605\n",
      "  batch 2000 loss: 0.3222100806236267\n",
      "  batch 2050 loss: 0.3026474602520466\n",
      "  batch 2100 loss: 0.31247793227434156\n",
      "  batch 2150 loss: 0.33049200057983397\n",
      "  batch 2200 loss: 0.31179784059524535\n",
      "  batch 2250 loss: 0.2975488129258156\n",
      "  batch 2300 loss: 0.3013569450378418\n",
      "  batch 2350 loss: 0.30271649271249773\n",
      "  batch 2400 loss: 0.3488972526788712\n",
      "  batch 2450 loss: 0.30137956887483597\n",
      "  batch 2500 loss: 0.2993029925227165\n",
      "  batch 2550 loss: 0.3206727260351181\n",
      "  batch 2600 loss: 0.3120621493458748\n",
      "  batch 2650 loss: 0.2974278788268566\n",
      "  batch 2700 loss: 0.28551336348056794\n",
      "  batch 2750 loss: 0.3017120108008385\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.30171 valid 0.80764, valid PER 22.45%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.2805278581380844\n",
      "  batch 100 loss: 0.27512248665094374\n",
      "  batch 150 loss: 0.2485818776488304\n",
      "  batch 200 loss: 0.24218881607055665\n",
      "  batch 250 loss: 0.2637279969453812\n",
      "  batch 300 loss: 0.25193405896425247\n",
      "  batch 350 loss: 0.26287864774465564\n",
      "  batch 400 loss: 0.23524576053023338\n",
      "  batch 450 loss: 0.2602534893155098\n",
      "  batch 500 loss: 0.2787193778157234\n",
      "  batch 550 loss: 0.22714639008045195\n",
      "  batch 600 loss: 0.25190326914191247\n",
      "  batch 650 loss: 0.26083960235118864\n",
      "  batch 700 loss: 0.24599758982658387\n",
      "  batch 750 loss: 0.24381447732448577\n",
      "  batch 800 loss: 0.23558711141347885\n",
      "  batch 850 loss: 0.24188816249370576\n",
      "  batch 900 loss: 0.23455827340483665\n",
      "  batch 950 loss: 0.24931739240884782\n",
      "  batch 1000 loss: 0.26382056310772894\n",
      "  batch 1050 loss: 0.25165616393089296\n",
      "  batch 1100 loss: 0.252028568983078\n",
      "  batch 1150 loss: 0.2540597268939018\n",
      "  batch 1200 loss: 0.25060057029128074\n",
      "  batch 1250 loss: 0.2484237790107727\n",
      "  batch 1300 loss: 0.24926836863160134\n",
      "  batch 1350 loss: 0.2454253289103508\n",
      "  batch 1400 loss: 0.24422364681959152\n",
      "  batch 1450 loss: 0.25748755484819413\n",
      "  batch 1500 loss: 0.24588236570358277\n",
      "  batch 1550 loss: 0.2394559408724308\n",
      "  batch 1600 loss: 0.2438835009932518\n",
      "  batch 1650 loss: 0.24111205458641052\n",
      "  batch 1700 loss: 0.2264811162650585\n",
      "  batch 1750 loss: 0.24449053063988685\n",
      "  batch 1800 loss: 0.2484683108329773\n",
      "  batch 1850 loss: 0.25308642610907556\n",
      "  batch 1900 loss: 0.2515838874876499\n",
      "  batch 1950 loss: 0.2410448008775711\n",
      "  batch 2000 loss: 0.24900266095995904\n",
      "  batch 2050 loss: 0.24219828188419343\n",
      "  batch 2100 loss: 0.24047863259911537\n",
      "  batch 2150 loss: 0.2599579498171806\n",
      "  batch 2200 loss: 0.24360226452350617\n",
      "  batch 2250 loss: 0.23682192027568816\n",
      "  batch 2300 loss: 0.23546631753444672\n",
      "  batch 2350 loss: 0.24054789930582046\n",
      "  batch 2400 loss: 0.24523878887295722\n",
      "  batch 2450 loss: 0.25277351260185243\n",
      "  batch 2500 loss: 0.2371288213133812\n",
      "  batch 2550 loss: 0.24225425690412522\n",
      "  batch 2600 loss: 0.24211633771657945\n",
      "  batch 2650 loss: 0.25862608462572095\n",
      "  batch 2700 loss: 0.25186711639165876\n",
      "  batch 2750 loss: 0.23689907759428025\n",
      "Epoch 00010: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.23690 valid 0.81987, valid PER 21.95%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.20633479773998262\n",
      "  batch 100 loss: 0.21730397447943686\n",
      "  batch 150 loss: 0.23124995827674866\n",
      "  batch 200 loss: 0.21677738696336746\n",
      "  batch 250 loss: 0.21767389968037607\n",
      "  batch 300 loss: 0.1935266475379467\n",
      "  batch 350 loss: 0.21093197003006936\n",
      "  batch 400 loss: 0.2061265690624714\n",
      "  batch 450 loss: 0.1932155403494835\n",
      "  batch 500 loss: 0.1936310788989067\n",
      "  batch 550 loss: 0.22478107154369353\n",
      "  batch 600 loss: 0.2296217016875744\n",
      "  batch 650 loss: 0.19419832438230514\n",
      "  batch 700 loss: 0.19373470962047576\n",
      "  batch 750 loss: 0.2152336597442627\n",
      "  batch 800 loss: 0.2107715755701065\n",
      "  batch 850 loss: 0.21564196556806564\n",
      "  batch 900 loss: 0.22915177747607232\n",
      "  batch 950 loss: 0.19553581342101098\n",
      "  batch 1000 loss: 0.19899871736764907\n",
      "  batch 1050 loss: 0.2133799271285534\n",
      "  batch 1100 loss: 0.20887480393052102\n",
      "  batch 1150 loss: 0.20608412459492684\n",
      "  batch 1200 loss: 0.18710325345396994\n",
      "  batch 1250 loss: 0.20647905498743058\n",
      "  batch 1300 loss: 0.2115144895017147\n",
      "  batch 1350 loss: 0.20301948353648186\n",
      "  batch 1400 loss: 0.18845754936337472\n",
      "  batch 1450 loss: 0.208394156396389\n",
      "  batch 1500 loss: 0.2153603230416775\n",
      "  batch 1550 loss: 0.2068246480822563\n",
      "  batch 1600 loss: 0.2130152216553688\n",
      "  batch 1650 loss: 0.20890962690114975\n",
      "  batch 1700 loss: 0.20611741468310357\n",
      "  batch 1750 loss: 0.2080981746315956\n",
      "  batch 1800 loss: 0.20077634006738662\n",
      "  batch 1850 loss: 0.22132897675037383\n",
      "  batch 1900 loss: 0.20848515272140503\n",
      "  batch 1950 loss: 0.21137568086385727\n",
      "  batch 2000 loss: 0.2216333144903183\n",
      "  batch 2050 loss: 0.20230214416980744\n",
      "  batch 2100 loss: 0.22146907284855843\n",
      "  batch 2150 loss: 0.20052989244461059\n",
      "  batch 2200 loss: 0.20135101929306984\n",
      "  batch 2250 loss: 0.2155284257233143\n",
      "  batch 2300 loss: 0.22767128735780717\n",
      "  batch 2350 loss: 0.20635121390223504\n",
      "  batch 2400 loss: 0.1929955340921879\n",
      "  batch 2450 loss: 0.2073195394873619\n",
      "  batch 2500 loss: 0.1904962906241417\n",
      "  batch 2550 loss: 0.1945571121573448\n",
      "  batch 2600 loss: 0.20620022490620613\n",
      "  batch 2650 loss: 0.19963334530591964\n",
      "  batch 2700 loss: 0.2028592260181904\n",
      "  batch 2750 loss: 0.20387336030602454\n",
      "Epoch 00011: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.20387 valid 0.83591, valid PER 22.12%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.19215897098183632\n",
      "  batch 100 loss: 0.18665058121085168\n",
      "  batch 150 loss: 0.18265775807201862\n",
      "  batch 200 loss: 0.18028705045580865\n",
      "  batch 250 loss: 0.19665177762508393\n",
      "  batch 300 loss: 0.1916587781906128\n",
      "  batch 350 loss: 0.17278195038437844\n",
      "  batch 400 loss: 0.19749882072210312\n",
      "  batch 450 loss: 0.1824803978204727\n",
      "  batch 500 loss: 0.17769283890724183\n",
      "  batch 550 loss: 0.19716224133968352\n",
      "  batch 600 loss: 0.18544502317905426\n",
      "  batch 650 loss: 0.18747681483626366\n",
      "  batch 700 loss: 0.18428777962923049\n",
      "  batch 750 loss: 0.17847225993871688\n",
      "  batch 800 loss: 0.17393697455525398\n",
      "  batch 850 loss: 0.19038082480430604\n",
      "  batch 900 loss: 0.19251209884881973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 950 loss: 0.18678005650639534\n",
      "  batch 1000 loss: 0.18697609066963194\n",
      "  batch 1050 loss: 0.18464520432054995\n",
      "  batch 1100 loss: 0.17821577459573745\n",
      "  batch 1150 loss: 0.18456487104296684\n",
      "  batch 1200 loss: 0.18207444801926612\n",
      "  batch 1250 loss: 0.18611753433942796\n",
      "  batch 1300 loss: 0.17698888897895812\n",
      "  batch 1350 loss: 0.1901748187839985\n",
      "  batch 1400 loss: 0.18531769126653672\n",
      "  batch 1450 loss: 0.18777516454458237\n",
      "  batch 1500 loss: 0.19808893144130707\n",
      "  batch 1550 loss: 0.1907767315208912\n",
      "  batch 1600 loss: 0.18621637269854546\n",
      "  batch 1650 loss: 0.20473864793777466\n",
      "  batch 1700 loss: 0.19696079179644585\n",
      "  batch 1750 loss: 0.17612159952521325\n",
      "  batch 1800 loss: 0.17171434685587883\n",
      "  batch 1850 loss: 0.18141204185783863\n",
      "  batch 1900 loss: 0.19442686542868615\n",
      "  batch 1950 loss: 0.1837778042256832\n",
      "  batch 2000 loss: 0.18425996944308282\n",
      "  batch 2050 loss: 0.18283584341406822\n",
      "  batch 2100 loss: 0.18681820705533028\n",
      "  batch 2150 loss: 0.17854492127895355\n",
      "  batch 2200 loss: 0.17579241439700127\n",
      "  batch 2250 loss: 0.1840982346236706\n",
      "  batch 2300 loss: 0.17941391780972482\n",
      "  batch 2350 loss: 0.18400463044643403\n",
      "  batch 2400 loss: 0.1895555093884468\n",
      "  batch 2450 loss: 0.20072278887033462\n",
      "  batch 2500 loss: 0.18227265536785126\n",
      "  batch 2550 loss: 0.19140613496303557\n",
      "  batch 2600 loss: 0.1939099498093128\n",
      "  batch 2650 loss: 0.19207316368818284\n",
      "  batch 2700 loss: 0.18887024670839309\n",
      "  batch 2750 loss: 0.17069534212350845\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.17070 valid 0.84774, valid PER 22.09%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.1760811112821102\n",
      "  batch 100 loss: 0.17078408047556878\n",
      "  batch 150 loss: 0.17735146127641202\n",
      "  batch 200 loss: 0.17370058462023735\n",
      "  batch 250 loss: 0.18349081858992577\n",
      "  batch 300 loss: 0.18661437198519706\n",
      "  batch 350 loss: 0.16497929990291596\n",
      "  batch 400 loss: 0.17478436037898062\n",
      "  batch 450 loss: 0.18288024216890336\n",
      "  batch 500 loss: 0.18140493899583818\n",
      "  batch 550 loss: 0.17067285783588887\n",
      "  batch 600 loss: 0.1939602790772915\n",
      "  batch 650 loss: 0.16754175782203673\n",
      "  batch 700 loss: 0.18761152505874634\n",
      "  batch 750 loss: 0.17054492592811585\n",
      "  batch 800 loss: 0.18559005975723267\n",
      "  batch 850 loss: 0.1772021447122097\n",
      "  batch 900 loss: 0.16410453915596007\n",
      "  batch 950 loss: 0.1708747760951519\n",
      "  batch 1000 loss: 0.1857479403913021\n",
      "  batch 1050 loss: 0.1898539823293686\n",
      "  batch 1100 loss: 0.17429515913128854\n",
      "  batch 1150 loss: 0.17039905562996865\n",
      "  batch 1200 loss: 0.1720832645893097\n",
      "  batch 1250 loss: 0.1807411590963602\n",
      "  batch 1300 loss: 0.18477378994226457\n",
      "  batch 1350 loss: 0.17583028778433799\n",
      "  batch 1400 loss: 0.18816109940409662\n",
      "  batch 1450 loss: 0.1765730708837509\n",
      "  batch 1500 loss: 0.1822350563108921\n",
      "  batch 1550 loss: 0.16462440073490142\n",
      "  batch 1600 loss: 0.17715631052851677\n",
      "  batch 1650 loss: 0.17256350859999656\n",
      "  batch 1700 loss: 0.16648793920874597\n",
      "  batch 1750 loss: 0.17394165217876434\n",
      "  batch 1800 loss: 0.16242492944002151\n",
      "  batch 1850 loss: 0.17566620439291\n",
      "  batch 1900 loss: 0.16696031883358955\n",
      "  batch 1950 loss: 0.16690106093883514\n",
      "  batch 2000 loss: 0.17882786378264426\n",
      "  batch 2050 loss: 0.1709672823548317\n",
      "  batch 2100 loss: 0.16593629047274588\n",
      "  batch 2150 loss: 0.17726353958249091\n",
      "  batch 2200 loss: 0.16948707789182663\n",
      "  batch 2250 loss: 0.17808375224471093\n",
      "  batch 2300 loss: 0.16358682468533517\n",
      "  batch 2350 loss: 0.16859180599451065\n",
      "  batch 2400 loss: 0.1752721218764782\n",
      "  batch 2450 loss: 0.18194569647312164\n",
      "  batch 2500 loss: 0.16218303069472312\n",
      "  batch 2550 loss: 0.17575269401073457\n",
      "  batch 2600 loss: 0.16745117858052253\n",
      "  batch 2650 loss: 0.1733303599059582\n",
      "  batch 2700 loss: 0.18472313672304153\n",
      "  batch 2750 loss: 0.17527931094169616\n",
      "Epoch 00013: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.17528 valid 0.85481, valid PER 22.05%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.17696475237607956\n",
      "  batch 100 loss: 0.16123958691954612\n",
      "  batch 150 loss: 0.1707221306860447\n",
      "  batch 200 loss: 0.17055777579545975\n",
      "  batch 250 loss: 0.16880730338394642\n",
      "  batch 300 loss: 0.16289238914847373\n",
      "  batch 350 loss: 0.166455407589674\n",
      "  batch 400 loss: 0.17873874187469482\n",
      "  batch 450 loss: 0.1820583926141262\n",
      "  batch 500 loss: 0.1781476405262947\n",
      "  batch 550 loss: 0.1576040341705084\n",
      "  batch 600 loss: 0.1647024305164814\n",
      "  batch 650 loss: 0.16615447878837586\n",
      "  batch 700 loss: 0.17146160677075387\n",
      "  batch 750 loss: 0.1616509471088648\n",
      "  batch 800 loss: 0.17465195193886757\n",
      "  batch 850 loss: 0.15477010861039162\n",
      "  batch 900 loss: 0.1826828411221504\n",
      "  batch 950 loss: 0.16740084320306778\n",
      "  batch 1000 loss: 0.1764777173101902\n",
      "  batch 1050 loss: 0.1643503026664257\n",
      "  batch 1100 loss: 0.15820351779460906\n",
      "  batch 1150 loss: 0.16150001853704452\n",
      "  batch 1200 loss: 0.17354277282953262\n",
      "  batch 1250 loss: 0.17005971416831017\n",
      "  batch 1300 loss: 0.17519800812005998\n",
      "  batch 1350 loss: 0.18668120875954627\n",
      "  batch 1400 loss: 0.16073869913816452\n",
      "  batch 1450 loss: 0.1701427961885929\n",
      "  batch 1500 loss: 0.16573623850941657\n",
      "  batch 1550 loss: 0.16817101642489432\n",
      "  batch 1600 loss: 0.1676666371524334\n",
      "  batch 1650 loss: 0.17241153687238694\n",
      "  batch 1700 loss: 0.16407345116138458\n",
      "  batch 1750 loss: 0.16795385383069517\n",
      "  batch 1800 loss: 0.15739931255578996\n",
      "  batch 1850 loss: 0.16403332903981208\n",
      "  batch 1900 loss: 0.1697110991179943\n",
      "  batch 1950 loss: 0.17128219678997994\n",
      "  batch 2000 loss: 0.18559652343392372\n",
      "  batch 2050 loss: 0.16936716198921203\n",
      "  batch 2100 loss: 0.1701517218351364\n",
      "  batch 2150 loss: 0.172934839874506\n",
      "  batch 2200 loss: 0.16498162880539893\n",
      "  batch 2250 loss: 0.1768886613845825\n",
      "  batch 2300 loss: 0.17274806290864944\n",
      "  batch 2350 loss: 0.15678644448518753\n",
      "  batch 2400 loss: 0.1715734262764454\n",
      "  batch 2450 loss: 0.16779269687831402\n",
      "  batch 2500 loss: 0.17140974313020707\n",
      "  batch 2550 loss: 0.16964000433683396\n",
      "  batch 2600 loss: 0.17273615956306457\n",
      "  batch 2650 loss: 0.15967040598392487\n",
      "  batch 2700 loss: 0.1679001532495022\n",
      "  batch 2750 loss: 0.16834791615605355\n",
      "Epoch 00014: reducing learning rate of group 0 to 3.9062e-03.\n",
      "LOSS train 0.16835 valid 0.85745, valid PER 22.05%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.16647404581308364\n",
      "  batch 100 loss: 0.15919179171323777\n",
      "  batch 150 loss: 0.16849833458662034\n",
      "  batch 200 loss: 0.1506073722243309\n",
      "  batch 250 loss: 0.1707659973204136\n",
      "  batch 300 loss: 0.17036126032471657\n",
      "  batch 350 loss: 0.16903420507907868\n",
      "  batch 400 loss: 0.18365135237574579\n",
      "  batch 450 loss: 0.15811440765857696\n",
      "  batch 500 loss: 0.16206756755709648\n",
      "  batch 550 loss: 0.1802033093571663\n",
      "  batch 600 loss: 0.17560742661356926\n",
      "  batch 650 loss: 0.16685627207159995\n",
      "  batch 700 loss: 0.18200886741280556\n",
      "  batch 750 loss: 0.16217349737882614\n",
      "  batch 800 loss: 0.15269776090979575\n",
      "  batch 850 loss: 0.169420545399189\n",
      "  batch 900 loss: 0.17440215200185777\n",
      "  batch 950 loss: 0.17053562887012957\n",
      "  batch 1000 loss: 0.1613165009021759\n",
      "  batch 1050 loss: 0.1616584151983261\n",
      "  batch 1100 loss: 0.15991035163402556\n",
      "  batch 1150 loss: 0.1768345382809639\n",
      "  batch 1200 loss: 0.165144651979208\n",
      "  batch 1250 loss: 0.17117534548044205\n",
      "  batch 1300 loss: 0.1642969709634781\n",
      "  batch 1350 loss: 0.14534003511071206\n",
      "  batch 1400 loss: 0.1606636978685856\n",
      "  batch 1450 loss: 0.1618879720568657\n",
      "  batch 1500 loss: 0.1588982817530632\n",
      "  batch 1550 loss: 0.16804907530546187\n",
      "  batch 1600 loss: 0.16510928906500338\n",
      "  batch 1650 loss: 0.16443247452378273\n",
      "  batch 1700 loss: 0.1715244935452938\n",
      "  batch 1750 loss: 0.15596759513020517\n",
      "  batch 1800 loss: 0.16679181210696697\n",
      "  batch 1850 loss: 0.1771680473536253\n",
      "  batch 1900 loss: 0.1639097136259079\n",
      "  batch 1950 loss: 0.16066811203956605\n",
      "  batch 2000 loss: 0.17641767844557762\n",
      "  batch 2050 loss: 0.17611684188246726\n",
      "  batch 2100 loss: 0.16754757806658746\n",
      "  batch 2150 loss: 0.16466498345136643\n",
      "  batch 2200 loss: 0.1684330578148365\n",
      "  batch 2250 loss: 0.1676112301647663\n",
      "  batch 2300 loss: 0.15567451894283293\n",
      "  batch 2350 loss: 0.1786729146540165\n",
      "  batch 2400 loss: 0.1633160765469074\n",
      "  batch 2450 loss: 0.16242161430418492\n",
      "  batch 2500 loss: 0.17278997480869293\n",
      "  batch 2550 loss: 0.15732932716608047\n",
      "  batch 2600 loss: 0.15763459712266922\n",
      "  batch 2650 loss: 0.16783461287617685\n",
      "  batch 2700 loss: 0.16520729407668114\n",
      "  batch 2750 loss: 0.15729998037219048\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.9531e-03.\n",
      "LOSS train 0.15730 valid 0.85896, valid PER 22.02%\n",
      "EPOCH 16:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 50 loss: 0.1754765547811985\n",
      "  batch 100 loss: 0.1617314687371254\n",
      "  batch 150 loss: 0.17069957703351973\n",
      "  batch 200 loss: 0.1602345472574234\n",
      "  batch 250 loss: 0.16497291818261148\n",
      "  batch 300 loss: 0.165514215528965\n",
      "  batch 350 loss: 0.16981702148914338\n",
      "  batch 400 loss: 0.1668749901652336\n",
      "  batch 450 loss: 0.15690707445144653\n",
      "  batch 500 loss: 0.17490482419729234\n",
      "  batch 550 loss: 0.15982845827937125\n",
      "  batch 600 loss: 0.161205098554492\n",
      "  batch 650 loss: 0.17022019132971763\n",
      "  batch 700 loss: 0.16750350549817086\n",
      "  batch 750 loss: 0.15492497146129608\n",
      "  batch 800 loss: 0.16567156627774238\n",
      "  batch 850 loss: 0.15849783539772033\n",
      "  batch 900 loss: 0.1612556102871895\n",
      "  batch 950 loss: 0.1466603533178568\n",
      "  batch 1000 loss: 0.17151930853724479\n",
      "  batch 1050 loss: 0.17835172563791274\n",
      "  batch 1100 loss: 0.16304952129721642\n",
      "  batch 1150 loss: 0.16401251137256623\n",
      "  batch 1200 loss: 0.15943723246455194\n",
      "  batch 1250 loss: 0.1645649553835392\n",
      "  batch 1300 loss: 0.17839066058397293\n",
      "  batch 1350 loss: 0.16299106523394585\n",
      "  batch 1400 loss: 0.1692052401602268\n",
      "  batch 1450 loss: 0.16969440251588822\n",
      "  batch 1500 loss: 0.15843263044953346\n",
      "  batch 1550 loss: 0.1552181239426136\n",
      "  batch 1600 loss: 0.16650125339627267\n",
      "  batch 1650 loss: 0.16717839896678924\n",
      "  batch 1700 loss: 0.15794208131730555\n",
      "  batch 1750 loss: 0.16343286782503127\n",
      "  batch 1800 loss: 0.16834654971957208\n",
      "  batch 1850 loss: 0.1578563205897808\n",
      "  batch 1900 loss: 0.17598748594522476\n",
      "  batch 1950 loss: 0.15122410386800766\n",
      "  batch 2000 loss: 0.1556165884435177\n",
      "  batch 2050 loss: 0.15732650518417357\n",
      "  batch 2100 loss: 0.18667602956295012\n",
      "  batch 2150 loss: 0.15652302414178848\n",
      "  batch 2200 loss: 0.16735530629754067\n",
      "  batch 2250 loss: 0.1617315310239792\n",
      "  batch 2300 loss: 0.1555623595416546\n",
      "  batch 2350 loss: 0.17083668768405913\n",
      "  batch 2400 loss: 0.17958846405148507\n",
      "  batch 2450 loss: 0.16873545102775098\n",
      "  batch 2500 loss: 0.15649379298090935\n",
      "  batch 2550 loss: 0.16143381878733634\n",
      "  batch 2600 loss: 0.15311698839068413\n",
      "  batch 2650 loss: 0.1696843184530735\n",
      "  batch 2700 loss: 0.15954042062163354\n",
      "  batch 2750 loss: 0.16593562364578246\n",
      "Epoch 00016: reducing learning rate of group 0 to 9.7656e-04.\n",
      "LOSS train 0.16594 valid 0.85983, valid PER 22.02%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.1629522657394409\n",
      "  batch 100 loss: 0.1647608198225498\n",
      "  batch 150 loss: 0.16079382747411727\n",
      "  batch 200 loss: 0.16833848565816878\n",
      "  batch 250 loss: 0.15308338336646557\n",
      "  batch 300 loss: 0.1528875069320202\n",
      "  batch 350 loss: 0.16567190036177634\n",
      "  batch 400 loss: 0.17859035179018976\n",
      "  batch 450 loss: 0.179025749117136\n",
      "  batch 500 loss: 0.15797774374485016\n",
      "  batch 550 loss: 0.1575942827761173\n",
      "  batch 600 loss: 0.16530304901301862\n",
      "  batch 650 loss: 0.16231995493173598\n",
      "  batch 700 loss: 0.17063844859600066\n",
      "  batch 750 loss: 0.15056009121239186\n",
      "  batch 800 loss: 0.15042088255286218\n",
      "  batch 850 loss: 0.16070313528180122\n",
      "  batch 900 loss: 0.1608043298870325\n",
      "  batch 950 loss: 0.16302939057350158\n",
      "  batch 1000 loss: 0.1499899473786354\n",
      "  batch 1050 loss: 0.17424072086811065\n",
      "  batch 1100 loss: 0.14711635887622834\n",
      "  batch 1150 loss: 0.1779918900132179\n",
      "  batch 1200 loss: 0.17017193660140037\n",
      "  batch 1250 loss: 0.15230077870190142\n",
      "  batch 1300 loss: 0.15576573342084885\n",
      "  batch 1350 loss: 0.1635751622915268\n",
      "  batch 1400 loss: 0.1425949165970087\n",
      "  batch 1450 loss: 0.17564119458198546\n",
      "  batch 1500 loss: 0.1498203107714653\n",
      "  batch 1550 loss: 0.14761727660894394\n",
      "  batch 1600 loss: 0.16758636333048343\n",
      "  batch 1650 loss: 0.16073448240756988\n",
      "  batch 1700 loss: 0.16573075383901595\n",
      "  batch 1750 loss: 0.16008191168308258\n",
      "  batch 1800 loss: 0.1672791302204132\n",
      "  batch 1850 loss: 0.17176130935549735\n",
      "  batch 1900 loss: 0.1669233998656273\n",
      "  batch 1950 loss: 0.1754038229584694\n",
      "  batch 2000 loss: 0.16722483336925506\n",
      "  batch 2050 loss: 0.16301321655511855\n",
      "  batch 2100 loss: 0.15122320041060447\n",
      "  batch 2150 loss: 0.17498601973056793\n",
      "  batch 2200 loss: 0.1505601742863655\n",
      "  batch 2250 loss: 0.1699083635210991\n",
      "  batch 2300 loss: 0.15989495545625687\n",
      "  batch 2350 loss: 0.16715980663895608\n",
      "  batch 2400 loss: 0.1680883400142193\n",
      "  batch 2450 loss: 0.17650852672755718\n",
      "  batch 2500 loss: 0.16318359360098839\n",
      "  batch 2550 loss: 0.1562064516544342\n",
      "  batch 2600 loss: 0.1664152316749096\n",
      "  batch 2650 loss: 0.18068299561738968\n",
      "  batch 2700 loss: 0.1716974540054798\n",
      "  batch 2750 loss: 0.17190722584724427\n",
      "Epoch 00017: reducing learning rate of group 0 to 4.8828e-04.\n",
      "LOSS train 0.17191 valid 0.86048, valid PER 22.04%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.1495884682238102\n",
      "  batch 100 loss: 0.15672811597585679\n",
      "  batch 150 loss: 0.16404063612222672\n",
      "  batch 200 loss: 0.14528740480542182\n",
      "  batch 250 loss: 0.15749366447329521\n",
      "  batch 300 loss: 0.15975972533226013\n",
      "  batch 350 loss: 0.16210912227630614\n",
      "  batch 400 loss: 0.15126550942659378\n",
      "  batch 450 loss: 0.1748231156170368\n",
      "  batch 500 loss: 0.16380761727690696\n",
      "  batch 550 loss: 0.1745526349544525\n",
      "  batch 600 loss: 0.16161863051354886\n",
      "  batch 650 loss: 0.14682335197925567\n",
      "  batch 700 loss: 0.1698458634316921\n",
      "  batch 750 loss: 0.1619436752796173\n",
      "  batch 800 loss: 0.15953647166490556\n",
      "  batch 850 loss: 0.16873810648918153\n",
      "  batch 900 loss: 0.17001129791140557\n",
      "  batch 950 loss: 0.15292552851140498\n",
      "  batch 1000 loss: 0.1667054495215416\n",
      "  batch 1050 loss: 0.15846914619207383\n",
      "  batch 1100 loss: 0.16244067788124084\n",
      "  batch 1150 loss: 0.16302008524537087\n",
      "  batch 1200 loss: 0.15598962612450123\n",
      "  batch 1250 loss: 0.1515347246825695\n",
      "  batch 1300 loss: 0.16325793609023095\n",
      "  batch 1350 loss: 0.16434780076146127\n",
      "  batch 1400 loss: 0.17037795424461366\n",
      "  batch 1450 loss: 0.17557657301425933\n",
      "  batch 1500 loss: 0.15460997924208641\n",
      "  batch 1550 loss: 0.17188398763537407\n",
      "  batch 1600 loss: 0.15680828794836998\n",
      "  batch 1650 loss: 0.1754276178777218\n",
      "  batch 1700 loss: 0.17261668428778648\n",
      "  batch 1750 loss: 0.16118056014180182\n",
      "  batch 1800 loss: 0.1674865511059761\n",
      "  batch 1850 loss: 0.1542762327194214\n",
      "  batch 1900 loss: 0.16310931548476218\n",
      "  batch 1950 loss: 0.17515190854668616\n",
      "  batch 2000 loss: 0.1639855482429266\n",
      "  batch 2050 loss: 0.1629468397796154\n",
      "  batch 2100 loss: 0.15033984124660493\n",
      "  batch 2150 loss: 0.1620684866607189\n",
      "  batch 2200 loss: 0.17823800176382065\n",
      "  batch 2250 loss: 0.1647384163737297\n",
      "  batch 2300 loss: 0.17367742121219634\n",
      "  batch 2350 loss: 0.16376493722200394\n",
      "  batch 2400 loss: 0.16773926466703415\n",
      "  batch 2450 loss: 0.16208056688308717\n",
      "  batch 2500 loss: 0.16715980038046838\n",
      "  batch 2550 loss: 0.16401257880032064\n",
      "  batch 2600 loss: 0.16426693454384803\n",
      "  batch 2650 loss: 0.162707060277462\n",
      "  batch 2700 loss: 0.16083817183971405\n",
      "  batch 2750 loss: 0.16707253262400626\n",
      "Epoch 00018: reducing learning rate of group 0 to 2.4414e-04.\n",
      "LOSS train 0.16707 valid 0.86069, valid PER 22.06%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.1640956288576126\n",
      "  batch 100 loss: 0.17177353754639627\n",
      "  batch 150 loss: 0.16679072007536888\n",
      "  batch 200 loss: 0.15728823363780975\n",
      "  batch 250 loss: 0.15959976635873319\n",
      "  batch 300 loss: 0.16047698721289635\n",
      "  batch 350 loss: 0.15567313000559807\n",
      "  batch 400 loss: 0.1527158072590828\n",
      "  batch 450 loss: 0.169859127253294\n",
      "  batch 500 loss: 0.16653022184967994\n",
      "  batch 550 loss: 0.16010848239064215\n",
      "  batch 600 loss: 0.1650303180515766\n",
      "  batch 650 loss: 0.17013956382870674\n",
      "  batch 700 loss: 0.16766058430075645\n",
      "  batch 750 loss: 0.16568354189395904\n",
      "  batch 800 loss: 0.16761329896748067\n",
      "  batch 850 loss: 0.15858792200684546\n",
      "  batch 900 loss: 0.16535810649394989\n",
      "  batch 950 loss: 0.14758988380432128\n",
      "  batch 1000 loss: 0.1641010797023773\n",
      "  batch 1050 loss: 0.15933751747012137\n",
      "  batch 1100 loss: 0.16839128807187081\n",
      "  batch 1150 loss: 0.16380310252308847\n",
      "  batch 1200 loss: 0.16319838896393776\n",
      "  batch 1250 loss: 0.15062589883804323\n",
      "  batch 1300 loss: 0.1578187683224678\n",
      "  batch 1350 loss: 0.16178630277514458\n",
      "  batch 1400 loss: 0.16292497888207436\n",
      "  batch 1450 loss: 0.1718195478618145\n",
      "  batch 1500 loss: 0.1556217660009861\n",
      "  batch 1550 loss: 0.17425185680389405\n",
      "  batch 1600 loss: 0.16286582604050637\n",
      "  batch 1650 loss: 0.16275407508015632\n",
      "  batch 1700 loss: 0.15450552150607108\n",
      "  batch 1750 loss: 0.16173298209905623\n",
      "  batch 1800 loss: 0.15338071018457414\n",
      "  batch 1850 loss: 0.1652233688533306\n",
      "  batch 1900 loss: 0.1632629670202732\n",
      "  batch 1950 loss: 0.15256422959268093\n",
      "  batch 2000 loss: 0.17221977114677428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 2050 loss: 0.16119898468255997\n",
      "  batch 2100 loss: 0.16538225322961808\n",
      "  batch 2150 loss: 0.17166501730680467\n",
      "  batch 2200 loss: 0.16480281800031663\n",
      "  batch 2250 loss: 0.15695223361253738\n",
      "  batch 2300 loss: 0.16064973950386047\n",
      "  batch 2350 loss: 0.16216407105326652\n",
      "  batch 2400 loss: 0.1605139757692814\n",
      "  batch 2450 loss: 0.16934005685150624\n",
      "  batch 2500 loss: 0.15787366658449173\n",
      "  batch 2550 loss: 0.16344405695796013\n",
      "  batch 2600 loss: 0.16994589753448963\n",
      "  batch 2650 loss: 0.1608096908032894\n",
      "  batch 2700 loss: 0.16727915972471238\n",
      "  batch 2750 loss: 0.16089907497167588\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.2207e-04.\n",
      "LOSS train 0.16090 valid 0.86079, valid PER 22.08%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.1637587985396385\n",
      "  batch 100 loss: 0.17124918222427368\n",
      "  batch 150 loss: 0.17031085327267648\n",
      "  batch 200 loss: 0.14507515400648116\n",
      "  batch 250 loss: 0.15264624878764152\n",
      "  batch 300 loss: 0.1512027721107006\n",
      "  batch 350 loss: 0.15805489614605903\n",
      "  batch 400 loss: 0.15187787875533104\n",
      "  batch 450 loss: 0.16845012664794923\n",
      "  batch 500 loss: 0.16454240381717683\n",
      "  batch 550 loss: 0.155244460105896\n",
      "  batch 600 loss: 0.150245451182127\n",
      "  batch 650 loss: 0.17319818511605262\n",
      "  batch 700 loss: 0.17837341040372848\n",
      "  batch 750 loss: 0.1532253149151802\n",
      "  batch 800 loss: 0.17081103309988976\n",
      "  batch 850 loss: 0.17002774856984615\n",
      "  batch 900 loss: 0.15671802058815956\n",
      "  batch 950 loss: 0.16816358745098114\n",
      "  batch 1000 loss: 0.16773645728826522\n",
      "  batch 1050 loss: 0.16801049396395684\n",
      "  batch 1100 loss: 0.15437070816755294\n",
      "  batch 1150 loss: 0.16581477388739585\n",
      "  batch 1200 loss: 0.15415432497859002\n",
      "  batch 1250 loss: 0.15255060732364656\n",
      "  batch 1300 loss: 0.1573325827717781\n",
      "  batch 1350 loss: 0.180618307441473\n",
      "  batch 1400 loss: 0.16564021661877631\n",
      "  batch 1450 loss: 0.1597014045715332\n",
      "  batch 1500 loss: 0.17100140795111657\n",
      "  batch 1550 loss: 0.16297089979052543\n",
      "  batch 1600 loss: 0.16488245010375976\n",
      "  batch 1650 loss: 0.1646387392282486\n",
      "  batch 1700 loss: 0.161413269713521\n",
      "  batch 1750 loss: 0.1604990068078041\n",
      "  batch 1800 loss: 0.159014805406332\n",
      "  batch 1850 loss: 0.1694578404724598\n",
      "  batch 1900 loss: 0.1766088119149208\n",
      "  batch 1950 loss: 0.1622218082100153\n",
      "  batch 2000 loss: 0.15788657061755657\n",
      "  batch 2050 loss: 0.1711932922899723\n",
      "  batch 2100 loss: 0.17281611233949662\n",
      "  batch 2150 loss: 0.17229521930217742\n",
      "  batch 2200 loss: 0.16386903539299966\n",
      "  batch 2250 loss: 0.14697734862565995\n",
      "  batch 2300 loss: 0.15869097039103508\n",
      "  batch 2350 loss: 0.16670793682336807\n",
      "  batch 2400 loss: 0.17052875459194183\n",
      "  batch 2450 loss: 0.1652469515800476\n",
      "  batch 2500 loss: 0.1801398278772831\n",
      "  batch 2550 loss: 0.16745301119983197\n",
      "  batch 2600 loss: 0.1531238143146038\n",
      "  batch 2650 loss: 0.149157839640975\n",
      "  batch 2700 loss: 0.15660605601966382\n",
      "  batch 2750 loss: 0.15458571255207063\n",
      "Epoch 00020: reducing learning rate of group 0 to 6.1035e-05.\n",
      "LOSS train 0.15459 valid 0.86084, valid PER 22.07%\n",
      "Training finished in 17.0 minutes.\n",
      "Model saved to checkpoints/20231211_140855/model_8\n",
      "Currently using dropout rate of 0.1\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.003544487953186\n",
      "  batch 100 loss: 3.396976284980774\n",
      "  batch 150 loss: 3.2943828296661377\n",
      "  batch 200 loss: 3.1734512853622436\n",
      "  batch 250 loss: 3.0687922620773316\n",
      "  batch 300 loss: 2.86239737033844\n",
      "  batch 350 loss: 2.6498245668411253\n",
      "  batch 400 loss: 2.5387462759017945\n",
      "  batch 450 loss: 2.4047731876373293\n",
      "  batch 500 loss: 2.3163883686065674\n",
      "  batch 550 loss: 2.2211385440826414\n",
      "  batch 600 loss: 2.1812213110923766\n",
      "  batch 650 loss: 2.087606987953186\n",
      "  batch 700 loss: 2.023928716182709\n",
      "  batch 750 loss: 1.9500569581985474\n",
      "  batch 800 loss: 1.8984940600395204\n",
      "  batch 850 loss: 1.899877679347992\n",
      "  batch 900 loss: 1.8316830945014955\n",
      "  batch 950 loss: 1.7813343715667724\n",
      "  batch 1000 loss: 1.754707360267639\n",
      "  batch 1050 loss: 1.6830584263801576\n",
      "  batch 1100 loss: 1.6158606362342836\n",
      "  batch 1150 loss: 1.639640700817108\n",
      "  batch 1200 loss: 1.586159954071045\n",
      "  batch 1250 loss: 1.6136074471473694\n",
      "  batch 1300 loss: 1.4858591651916504\n",
      "  batch 1350 loss: 1.4841595005989074\n",
      "  batch 1400 loss: 1.4778440117835998\n",
      "  batch 1450 loss: 1.3745224952697754\n",
      "  batch 1500 loss: 1.3707156801223754\n",
      "  batch 1550 loss: 1.345408754348755\n",
      "  batch 1600 loss: 1.3656538367271422\n",
      "  batch 1650 loss: 1.2972974896430969\n",
      "  batch 1700 loss: 1.3036767256259918\n",
      "  batch 1750 loss: 1.2614137136936188\n",
      "  batch 1800 loss: 1.2927535700798034\n",
      "  batch 1850 loss: 1.2690367472171784\n",
      "  batch 1900 loss: 1.2360194516181946\n",
      "  batch 1950 loss: 1.2482609021663666\n",
      "  batch 2000 loss: 1.234414438009262\n",
      "  batch 2050 loss: 1.1736239683628082\n",
      "  batch 2100 loss: 1.202823657989502\n",
      "  batch 2150 loss: 1.1783705580234527\n",
      "  batch 2200 loss: 1.2019618833065033\n",
      "  batch 2250 loss: 1.18902615070343\n",
      "  batch 2300 loss: 1.1741776585578918\n",
      "  batch 2350 loss: 1.177535195350647\n",
      "  batch 2400 loss: 1.1522742629051208\n",
      "  batch 2450 loss: 1.1337296915054322\n",
      "  batch 2500 loss: 1.1023341071605683\n",
      "  batch 2550 loss: 1.102572523355484\n",
      "  batch 2600 loss: 1.09701185464859\n",
      "  batch 2650 loss: 1.1359985721111299\n",
      "  batch 2700 loss: 1.1262713479995727\n",
      "  batch 2750 loss: 1.1120801544189454\n",
      "LOSS train 1.11208 valid 1.14786, valid PER 34.84%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.0289777958393096\n",
      "  batch 100 loss: 1.0305482482910155\n",
      "  batch 150 loss: 1.0811976933479308\n",
      "  batch 200 loss: 1.0640680551528932\n",
      "  batch 250 loss: 1.0372640156745911\n",
      "  batch 300 loss: 1.0767758774757386\n",
      "  batch 350 loss: 1.0132601928710938\n",
      "  batch 400 loss: 1.03361745595932\n",
      "  batch 450 loss: 1.026640853881836\n",
      "  batch 500 loss: 1.0039461851119995\n",
      "  batch 550 loss: 1.0214423871040343\n",
      "  batch 600 loss: 1.034743182659149\n",
      "  batch 650 loss: 1.04886403799057\n",
      "  batch 700 loss: 1.0165729558467864\n",
      "  batch 750 loss: 1.0001902520656585\n",
      "  batch 800 loss: 1.0116573679447174\n",
      "  batch 850 loss: 0.9820974743366242\n",
      "  batch 900 loss: 1.021229065656662\n",
      "  batch 950 loss: 1.0225624763965606\n",
      "  batch 1000 loss: 1.0095151746273041\n",
      "  batch 1050 loss: 1.0043437159061432\n",
      "  batch 1100 loss: 0.9673531913757324\n",
      "  batch 1150 loss: 0.974851895570755\n",
      "  batch 1200 loss: 1.012005478143692\n",
      "  batch 1250 loss: 0.981127815246582\n",
      "  batch 1300 loss: 0.9840526902675628\n",
      "  batch 1350 loss: 0.9869962406158447\n",
      "  batch 1400 loss: 0.9695998930931091\n",
      "  batch 1450 loss: 0.9744833266735077\n",
      "  batch 1500 loss: 0.9607650136947632\n",
      "  batch 1550 loss: 0.9619400548934937\n",
      "  batch 1600 loss: 0.9547707986831665\n",
      "  batch 1650 loss: 0.9598314762115479\n",
      "  batch 1700 loss: 0.9202843475341796\n",
      "  batch 1750 loss: 0.9774415624141694\n",
      "  batch 1800 loss: 0.9731319200992584\n",
      "  batch 1850 loss: 0.9913153195381165\n",
      "  batch 1900 loss: 0.9655439627170562\n",
      "  batch 1950 loss: 0.9365767085552216\n",
      "  batch 2000 loss: 0.9057664883136749\n",
      "  batch 2050 loss: 0.918806893825531\n",
      "  batch 2100 loss: 0.9330853021144867\n",
      "  batch 2150 loss: 0.924970862865448\n",
      "  batch 2200 loss: 0.9251427018642425\n",
      "  batch 2250 loss: 0.9000705087184906\n",
      "  batch 2300 loss: 0.9178064525127411\n",
      "  batch 2350 loss: 0.9103130555152893\n",
      "  batch 2400 loss: 0.8953627169132232\n",
      "  batch 2450 loss: 0.9029574716091155\n",
      "  batch 2500 loss: 0.9293838810920715\n",
      "  batch 2550 loss: 0.9259918701648712\n",
      "  batch 2600 loss: 0.8697549569606781\n",
      "  batch 2650 loss: 0.8964784359931945\n",
      "  batch 2700 loss: 0.9082570087909698\n",
      "  batch 2750 loss: 0.8771650791168213\n",
      "LOSS train 0.87717 valid 0.92298, valid PER 28.81%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 0.8576232290267944\n",
      "  batch 100 loss: 0.8540140926837921\n",
      "  batch 150 loss: 0.8423572075366974\n",
      "  batch 200 loss: 0.8528668940067291\n",
      "  batch 250 loss: 0.8351512312889099\n",
      "  batch 300 loss: 0.8298351097106934\n",
      "  batch 350 loss: 0.857296644449234\n",
      "  batch 400 loss: 0.8352886140346527\n",
      "  batch 450 loss: 0.8353014767169953\n",
      "  batch 500 loss: 0.8501611590385437\n",
      "  batch 550 loss: 0.863223375082016\n",
      "  batch 600 loss: 0.8352343463897705\n",
      "  batch 650 loss: 0.8284291541576385\n",
      "  batch 700 loss: 0.8313389432430267\n",
      "  batch 750 loss: 0.8281834876537323\n",
      "  batch 800 loss: 0.8538864493370056\n",
      "  batch 850 loss: 0.8345646226406097\n",
      "  batch 900 loss: 0.8332429367303849\n",
      "  batch 950 loss: 0.7858531188964843\n",
      "  batch 1000 loss: 0.8424918663501739\n",
      "  batch 1050 loss: 0.8229647767543793\n",
      "  batch 1100 loss: 0.799924840927124\n",
      "  batch 1150 loss: 0.8123194670677185\n",
      "  batch 1200 loss: 0.8129715627431869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1250 loss: 0.8149964022636413\n",
      "  batch 1300 loss: 0.8067138302326202\n",
      "  batch 1350 loss: 0.8225009000301361\n",
      "  batch 1400 loss: 0.8537977349758148\n",
      "  batch 1450 loss: 0.803494348526001\n",
      "  batch 1500 loss: 0.8439032423496247\n",
      "  batch 1550 loss: 0.850817676782608\n",
      "  batch 1600 loss: 0.8070595419406891\n",
      "  batch 1650 loss: 0.8100879681110382\n",
      "  batch 1700 loss: 0.7865436780452728\n",
      "  batch 1750 loss: 0.8304861295223236\n",
      "  batch 1800 loss: 0.8107601594924927\n",
      "  batch 1850 loss: 0.824907636642456\n",
      "  batch 1900 loss: 0.777960877418518\n",
      "  batch 1950 loss: 0.8088335263729095\n",
      "  batch 2000 loss: 0.7943071436882019\n",
      "  batch 2050 loss: 0.7896440136432648\n",
      "  batch 2100 loss: 0.7871558880805969\n",
      "  batch 2150 loss: 0.791410346031189\n",
      "  batch 2200 loss: 0.7777875596284867\n",
      "  batch 2250 loss: 0.7846190774440766\n",
      "  batch 2300 loss: 0.797517192363739\n",
      "  batch 2350 loss: 0.7929212760925293\n",
      "  batch 2400 loss: 0.7543191576004028\n",
      "  batch 2450 loss: 0.7949833059310913\n",
      "  batch 2500 loss: 0.7770532608032227\n",
      "  batch 2550 loss: 0.7730299437046051\n",
      "  batch 2600 loss: 0.7799861907958985\n",
      "  batch 2650 loss: 0.7854568684101104\n",
      "  batch 2700 loss: 0.7873339474201202\n",
      "  batch 2750 loss: 0.7453441941738128\n",
      "LOSS train 0.74534 valid 0.85992, valid PER 26.95%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.7503157365322113\n",
      "  batch 100 loss: 0.7962751722335816\n",
      "  batch 150 loss: 0.7513212847709656\n",
      "  batch 200 loss: 0.7160842561721802\n",
      "  batch 250 loss: 0.7367874312400818\n",
      "  batch 300 loss: 0.7290513694286347\n",
      "  batch 350 loss: 0.736638867855072\n",
      "  batch 400 loss: 0.729034789800644\n",
      "  batch 450 loss: 0.7532480216026306\n",
      "  batch 500 loss: 0.7445346957445145\n",
      "  batch 550 loss: 0.7446418583393097\n",
      "  batch 600 loss: 0.7429550468921662\n",
      "  batch 650 loss: 0.743631734251976\n",
      "  batch 700 loss: 0.7340059632062912\n",
      "  batch 750 loss: 0.7280151927471161\n",
      "  batch 800 loss: 0.7457820904254914\n",
      "  batch 850 loss: 0.731935099363327\n",
      "  batch 900 loss: 0.7361128127574921\n",
      "  batch 950 loss: 0.7486040669679642\n",
      "  batch 1000 loss: 0.7331576240062714\n",
      "  batch 1050 loss: 0.7297658354043961\n",
      "  batch 1100 loss: 0.7149215590953827\n",
      "  batch 1150 loss: 0.6887965679168702\n",
      "  batch 1200 loss: 0.7248770242929459\n",
      "  batch 1250 loss: 0.7904807907342911\n",
      "  batch 1300 loss: 0.7036370003223419\n",
      "  batch 1350 loss: 0.6823466700315476\n",
      "  batch 1400 loss: 0.7034469360113144\n",
      "  batch 1450 loss: 0.6759869331121444\n",
      "  batch 1500 loss: 0.7146009188890458\n",
      "  batch 1550 loss: 0.7739503407478332\n",
      "  batch 1600 loss: 0.7101278913021087\n",
      "  batch 1650 loss: 0.7029472714662552\n",
      "  batch 1700 loss: 0.6866032069921494\n",
      "  batch 1750 loss: 0.7125827038288116\n",
      "  batch 1800 loss: 0.7276526367664338\n",
      "  batch 1850 loss: 0.7104396706819535\n",
      "  batch 1900 loss: 0.6794670766592026\n",
      "  batch 1950 loss: 0.7167540299892425\n",
      "  batch 2000 loss: 0.7344388622045517\n",
      "  batch 2050 loss: 0.7170749890804291\n",
      "  batch 2100 loss: 0.7418622839450836\n",
      "  batch 2150 loss: 0.68399906873703\n",
      "  batch 2200 loss: 0.6547116637229919\n",
      "  batch 2250 loss: 0.6693797618150711\n",
      "  batch 2300 loss: 0.7051299750804901\n",
      "  batch 2350 loss: 0.6903503966331482\n",
      "  batch 2400 loss: 0.68800950050354\n",
      "  batch 2450 loss: 0.7121496558189392\n",
      "  batch 2500 loss: 0.7235815232992172\n",
      "  batch 2550 loss: 0.7145694720745087\n",
      "  batch 2600 loss: 0.7271072071790695\n",
      "  batch 2650 loss: 0.7226182019710541\n",
      "  batch 2700 loss: 0.6653238654136657\n",
      "  batch 2750 loss: 0.662470953464508\n",
      "LOSS train 0.66247 valid 0.83287, valid PER 25.18%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.686777435541153\n",
      "  batch 100 loss: 0.6590584468841553\n",
      "  batch 150 loss: 0.7087979108095169\n",
      "  batch 200 loss: 0.6628447419404984\n",
      "  batch 250 loss: 0.6477887803316116\n",
      "  batch 300 loss: 0.6660377299785614\n",
      "  batch 350 loss: 0.6950903975963593\n",
      "  batch 400 loss: 0.6703314703702926\n",
      "  batch 450 loss: 0.660353045463562\n",
      "  batch 500 loss: 0.6833649575710297\n",
      "  batch 550 loss: 0.6283716571331024\n",
      "  batch 600 loss: 0.6477214390039444\n",
      "  batch 650 loss: 0.6545325940847397\n",
      "  batch 700 loss: 0.6642043972015381\n",
      "  batch 750 loss: 0.6807945692539215\n",
      "  batch 800 loss: 0.6691610085964202\n",
      "  batch 850 loss: 0.6560766524076462\n",
      "  batch 900 loss: 0.6604725366830826\n",
      "  batch 950 loss: 0.6659239596128463\n",
      "  batch 1000 loss: 0.6265831619501114\n",
      "  batch 1050 loss: 0.6676296675205231\n",
      "  batch 1100 loss: 0.6208886337280274\n",
      "  batch 1150 loss: 0.6311927783489227\n",
      "  batch 1200 loss: 0.6341734939813614\n",
      "  batch 1250 loss: 0.6239079517126084\n",
      "  batch 1300 loss: 0.6281451570987702\n",
      "  batch 1350 loss: 0.662466396689415\n",
      "  batch 1400 loss: 0.637002226114273\n",
      "  batch 1450 loss: 0.6396952003240586\n",
      "  batch 1500 loss: 0.6613671457767487\n",
      "  batch 1550 loss: 0.6627155029773713\n",
      "  batch 1600 loss: 0.6324913841485977\n",
      "  batch 1650 loss: 0.6217957180738449\n",
      "  batch 1700 loss: 0.6557549017667771\n",
      "  batch 1750 loss: 0.6406574267148971\n",
      "  batch 1800 loss: 0.625332943201065\n",
      "  batch 1850 loss: 0.6307558119297028\n",
      "  batch 1900 loss: 0.6780383622646332\n",
      "  batch 1950 loss: 0.6472930341959\n",
      "  batch 2000 loss: 0.644769378900528\n",
      "  batch 2050 loss: 0.6743705010414124\n",
      "  batch 2100 loss: 0.65508838057518\n",
      "  batch 2150 loss: 0.6545935368537903\n",
      "  batch 2200 loss: 0.640656064748764\n",
      "  batch 2250 loss: 0.6457817304134369\n",
      "  batch 2300 loss: 0.6083102232217789\n",
      "  batch 2350 loss: 0.629012199640274\n",
      "  batch 2400 loss: 0.6289401698112488\n",
      "  batch 2450 loss: 0.6119735276699066\n",
      "  batch 2500 loss: 0.613947925567627\n",
      "  batch 2550 loss: 0.6392543703317642\n",
      "  batch 2600 loss: 0.6503298312425614\n",
      "  batch 2650 loss: 0.62027299284935\n",
      "  batch 2700 loss: 0.6168084877729416\n",
      "  batch 2750 loss: 0.5999102455377578\n",
      "LOSS train 0.59991 valid 0.80644, valid PER 24.58%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.5885999846458435\n",
      "  batch 100 loss: 0.5968551415205002\n",
      "  batch 150 loss: 0.5874964410066604\n",
      "  batch 200 loss: 0.5784460437297821\n",
      "  batch 250 loss: 0.5979919284582138\n",
      "  batch 300 loss: 0.5847176367044449\n",
      "  batch 350 loss: 0.5873326796293259\n",
      "  batch 400 loss: 0.6018955236673356\n",
      "  batch 450 loss: 0.603828701376915\n",
      "  batch 500 loss: 0.6128268885612488\n",
      "  batch 550 loss: 0.5793561893701553\n",
      "  batch 600 loss: 0.5873803025484086\n",
      "  batch 650 loss: 0.622906180024147\n",
      "  batch 700 loss: 0.6072893041372299\n",
      "  batch 750 loss: 0.5936636972427368\n",
      "  batch 800 loss: 0.58652492582798\n",
      "  batch 850 loss: 0.5808948546648025\n",
      "  batch 900 loss: 0.5939451539516449\n",
      "  batch 950 loss: 0.5897299265861511\n",
      "  batch 1000 loss: 0.5937800014019012\n",
      "  batch 1050 loss: 0.5883265411853791\n",
      "  batch 1100 loss: 0.6006654471158981\n",
      "  batch 1150 loss: 0.5966390854120255\n",
      "  batch 1200 loss: 0.6075511771440506\n",
      "  batch 1250 loss: 0.5906298583745957\n",
      "  batch 1300 loss: 0.5563132172822952\n",
      "  batch 1350 loss: 0.6265966379642487\n",
      "  batch 1400 loss: 0.5896666538715363\n",
      "  batch 1450 loss: 0.5685324960947037\n",
      "  batch 1500 loss: 0.5826887935400009\n",
      "  batch 1550 loss: 0.5847246503829956\n",
      "  batch 1600 loss: 0.5969963872432709\n",
      "  batch 1650 loss: 0.5844348037242889\n",
      "  batch 1700 loss: 0.5658296030759812\n",
      "  batch 1750 loss: 0.5913017135858536\n",
      "  batch 1800 loss: 0.6080599850416184\n",
      "  batch 1850 loss: 0.5536269915103912\n",
      "  batch 1900 loss: 0.5753499764204025\n",
      "  batch 1950 loss: 0.5662364047765732\n",
      "  batch 2000 loss: 0.5764779150485992\n",
      "  batch 2050 loss: 0.5414487874507904\n",
      "  batch 2100 loss: 0.5829480004310608\n",
      "  batch 2150 loss: 0.5711824440956116\n",
      "  batch 2200 loss: 0.5828473740816116\n",
      "  batch 2250 loss: 0.5667013263702393\n",
      "  batch 2300 loss: 0.5688258945941925\n",
      "  batch 2350 loss: 0.6017578876018524\n",
      "  batch 2400 loss: 0.5667187649011612\n",
      "  batch 2450 loss: 0.5367366975545883\n",
      "  batch 2500 loss: 0.5461617743968964\n",
      "  batch 2550 loss: 0.5843140071630478\n",
      "  batch 2600 loss: 0.5935833704471588\n",
      "  batch 2650 loss: 0.598672434091568\n",
      "  batch 2700 loss: 0.5805467158555985\n",
      "  batch 2750 loss: 0.5484997516870499\n",
      "LOSS train 0.54850 valid 0.79965, valid PER 24.40%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.5317646872997284\n",
      "  batch 100 loss: 0.534889760017395\n",
      "  batch 150 loss: 0.5658622241020203\n",
      "  batch 200 loss: 0.5522538828849792\n",
      "  batch 250 loss: 0.5190955525636674\n",
      "  batch 300 loss: 0.5192805832624435\n",
      "  batch 350 loss: 0.5542634320259094\n",
      "  batch 400 loss: 0.5332328647375106\n",
      "  batch 450 loss: 0.5308561086654663\n",
      "  batch 500 loss: 0.5530217921733857\n",
      "  batch 550 loss: 0.5371926265954972\n",
      "  batch 600 loss: 0.5180280685424805\n",
      "  batch 650 loss: 0.5440992999076844\n",
      "  batch 700 loss: 0.5412730520963669\n",
      "  batch 750 loss: 0.5275981545448303\n",
      "  batch 800 loss: 0.561502650976181\n",
      "  batch 850 loss: 0.5380064398050308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 900 loss: 0.5330305236577988\n",
      "  batch 950 loss: 0.5303064233064652\n",
      "  batch 1000 loss: 0.5467915225028992\n",
      "  batch 1050 loss: 0.5234055888652801\n",
      "  batch 1100 loss: 0.5265372186899185\n",
      "  batch 1150 loss: 0.5394180148839951\n",
      "  batch 1200 loss: 0.5490663731098175\n",
      "  batch 1250 loss: 0.5588401943445206\n",
      "  batch 1300 loss: 0.5709744572639466\n",
      "  batch 1350 loss: 0.4965359193086624\n",
      "  batch 1400 loss: 0.5183021306991578\n",
      "  batch 1450 loss: 0.5279060959815979\n",
      "  batch 1500 loss: 0.5013817924261094\n",
      "  batch 1550 loss: 0.5444025576114655\n",
      "  batch 1600 loss: 0.5368912118673325\n",
      "  batch 1650 loss: 0.5152462863922119\n",
      "  batch 1700 loss: 0.5503312271833419\n",
      "  batch 1750 loss: 0.4896565079689026\n",
      "  batch 1800 loss: 0.5111396026611328\n",
      "  batch 1850 loss: 0.5419363248348236\n",
      "  batch 1900 loss: 0.5064775037765503\n",
      "  batch 1950 loss: 0.5310086888074875\n",
      "  batch 2000 loss: 0.5015270274877548\n",
      "  batch 2050 loss: 0.5441862630844116\n",
      "  batch 2100 loss: 0.534551482796669\n",
      "  batch 2150 loss: 0.5290987724065781\n",
      "  batch 2200 loss: 0.5203839588165283\n",
      "  batch 2250 loss: 0.5348705220222473\n",
      "  batch 2300 loss: 0.5333390873670578\n",
      "  batch 2350 loss: 0.5344910001754761\n",
      "  batch 2400 loss: 0.5241539323329926\n",
      "  batch 2450 loss: 0.5240966778993607\n",
      "  batch 2500 loss: 0.530108500123024\n",
      "  batch 2550 loss: 0.5290962648391724\n",
      "  batch 2600 loss: 0.531413791179657\n",
      "  batch 2650 loss: 0.49798556089401247\n",
      "  batch 2700 loss: 0.5182314831018447\n",
      "  batch 2750 loss: 0.5273596113920211\n",
      "LOSS train 0.52736 valid 0.79745, valid PER 23.67%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.48883641958236695\n",
      "  batch 100 loss: 0.4884052985906601\n",
      "  batch 150 loss: 0.4825205498933792\n",
      "  batch 200 loss: 0.47154259532690046\n",
      "  batch 250 loss: 0.4835624420642853\n",
      "  batch 300 loss: 0.47666975677013396\n",
      "  batch 350 loss: 0.5295787793397904\n",
      "  batch 400 loss: 0.5167944538593292\n",
      "  batch 450 loss: 0.5075507879257202\n",
      "  batch 500 loss: 0.468003171980381\n",
      "  batch 550 loss: 0.4932473301887512\n",
      "  batch 600 loss: 0.47251412987709046\n",
      "  batch 650 loss: 0.46728830695152285\n",
      "  batch 700 loss: 0.48393276661634443\n",
      "  batch 750 loss: 0.4905804520845413\n",
      "  batch 800 loss: 0.46693403720855714\n",
      "  batch 850 loss: 0.4865146666765213\n",
      "  batch 900 loss: 0.47642349421978\n",
      "  batch 950 loss: 0.5234431064128876\n",
      "  batch 1000 loss: 0.49628304958343505\n",
      "  batch 1050 loss: 0.4839802557229996\n",
      "  batch 1100 loss: 0.4668288779258728\n",
      "  batch 1150 loss: 0.49758250296115875\n",
      "  batch 1200 loss: 0.49163139164447783\n",
      "  batch 1250 loss: 0.48271402388811113\n",
      "  batch 1300 loss: 0.5027817958593368\n",
      "  batch 1350 loss: 0.4820759445428848\n",
      "  batch 1400 loss: 0.48442575871944427\n",
      "  batch 1450 loss: 0.475280784368515\n",
      "  batch 1500 loss: 0.4995486348867416\n",
      "  batch 1550 loss: 0.5230817919969559\n",
      "  batch 1600 loss: 0.5121177536249161\n",
      "  batch 1650 loss: 0.47172459721565246\n",
      "  batch 1700 loss: 0.4833082318305969\n",
      "  batch 1750 loss: 0.4955373650789261\n",
      "  batch 1800 loss: 0.4914428955316544\n",
      "  batch 1850 loss: 0.4615092611312866\n",
      "  batch 1900 loss: 0.49788594722747803\n",
      "  batch 1950 loss: 0.47912432670593263\n",
      "  batch 2000 loss: 0.4887390613555908\n",
      "  batch 2050 loss: 0.47405404210090635\n",
      "  batch 2100 loss: 0.48132047057151794\n",
      "  batch 2150 loss: 0.4978075051307678\n",
      "  batch 2200 loss: 0.4691050219535828\n",
      "  batch 2250 loss: 0.49707298576831815\n",
      "  batch 2300 loss: 0.4669489634037018\n",
      "  batch 2350 loss: 0.4761086803674698\n",
      "  batch 2400 loss: 0.48877870440483095\n",
      "  batch 2450 loss: 0.46550062745809556\n",
      "  batch 2500 loss: 0.48000063717365266\n",
      "  batch 2550 loss: 0.47056387722492216\n",
      "  batch 2600 loss: 0.4834193766117096\n",
      "  batch 2650 loss: 0.4699745148420334\n",
      "  batch 2700 loss: 0.5019812917709351\n",
      "  batch 2750 loss: 0.4884151017665863\n",
      "LOSS train 0.48842 valid 0.79398, valid PER 23.06%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.43155313074588775\n",
      "  batch 100 loss: 0.46625795364379885\n",
      "  batch 150 loss: 0.43920651108026504\n",
      "  batch 200 loss: 0.45281650841236115\n",
      "  batch 250 loss: 0.4431608605384827\n",
      "  batch 300 loss: 0.4357533371448517\n",
      "  batch 350 loss: 0.46060751616954804\n",
      "  batch 400 loss: 0.4591404157876968\n",
      "  batch 450 loss: 0.4235962927341461\n",
      "  batch 500 loss: 0.45305310308933255\n",
      "  batch 550 loss: 0.47369386374950406\n",
      "  batch 600 loss: 0.4561159747838974\n",
      "  batch 650 loss: 0.4555291557312012\n",
      "  batch 700 loss: 0.4451135835051537\n",
      "  batch 750 loss: 0.4626189851760864\n",
      "  batch 800 loss: 0.4452412161231041\n",
      "  batch 850 loss: 0.48361622512340546\n",
      "  batch 900 loss: 0.4994395625591278\n",
      "  batch 950 loss: 0.46053397059440615\n",
      "  batch 1000 loss: 0.4721789294481277\n",
      "  batch 1050 loss: 0.4687925070524216\n",
      "  batch 1100 loss: 0.46003018915653227\n",
      "  batch 1150 loss: 0.4459285604953766\n",
      "  batch 1200 loss: 0.43396032571792603\n",
      "  batch 1250 loss: 0.4580251336097717\n",
      "  batch 1300 loss: 0.4705846518278122\n",
      "  batch 1350 loss: 0.438159658908844\n",
      "  batch 1400 loss: 0.47349567770957945\n",
      "  batch 1450 loss: 0.4616731822490692\n",
      "  batch 1500 loss: 0.43132884323596954\n",
      "  batch 1550 loss: 0.4387389302253723\n",
      "  batch 1600 loss: 0.41820054888725283\n",
      "  batch 1650 loss: 0.4522748029232025\n",
      "  batch 1700 loss: 0.44094827890396115\n",
      "  batch 1750 loss: 0.44828618824481964\n",
      "  batch 1800 loss: 0.4013767635822296\n",
      "  batch 1850 loss: 0.47624529153108597\n",
      "  batch 1900 loss: 0.45844094812870023\n",
      "  batch 1950 loss: 0.4635774791240692\n",
      "  batch 2000 loss: 0.4375394576787949\n",
      "  batch 2050 loss: 0.45107567965984346\n",
      "  batch 2100 loss: 0.4618686902523041\n",
      "  batch 2150 loss: 0.4711344033479691\n",
      "  batch 2200 loss: 0.4523427492380142\n",
      "  batch 2250 loss: 0.44628023624420166\n",
      "  batch 2300 loss: 0.42916589438915254\n",
      "  batch 2350 loss: 0.45046956062316895\n",
      "  batch 2400 loss: 0.4756921672821045\n",
      "  batch 2450 loss: 0.45300217747688293\n",
      "  batch 2500 loss: 0.4480520009994507\n",
      "  batch 2550 loss: 0.45349457919597624\n",
      "  batch 2600 loss: 0.42018133133649826\n",
      "  batch 2650 loss: 0.4586846911907196\n",
      "  batch 2700 loss: 0.41647963047027586\n",
      "  batch 2750 loss: 0.4538562273979187\n",
      "Epoch 00009: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.45386 valid 0.82881, valid PER 23.39%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.3886534509062767\n",
      "  batch 100 loss: 0.3789934867620468\n",
      "  batch 150 loss: 0.3740174862742424\n",
      "  batch 200 loss: 0.3468798103928566\n",
      "  batch 250 loss: 0.342185195684433\n",
      "  batch 300 loss: 0.3879220324754715\n",
      "  batch 350 loss: 0.3576861900091171\n",
      "  batch 400 loss: 0.3694001072645187\n",
      "  batch 450 loss: 0.34944200396537783\n",
      "  batch 500 loss: 0.3338740926980972\n",
      "  batch 550 loss: 0.3374745300412178\n",
      "  batch 600 loss: 0.3412936666607857\n",
      "  batch 650 loss: 0.34883571922779083\n",
      "  batch 700 loss: 0.37118603885173795\n",
      "  batch 750 loss: 0.3284122425317764\n",
      "  batch 800 loss: 0.33837144762277604\n",
      "  batch 850 loss: 0.34157157242298125\n",
      "  batch 900 loss: 0.34438935458660125\n",
      "  batch 950 loss: 0.3561783567070961\n",
      "  batch 1000 loss: 0.3393503299355507\n",
      "  batch 1050 loss: 0.33275351077318194\n",
      "  batch 1100 loss: 0.31859118402004244\n",
      "  batch 1150 loss: 0.3311803057789803\n",
      "  batch 1200 loss: 0.3460348090529442\n",
      "  batch 1250 loss: 0.3222889637947082\n",
      "  batch 1300 loss: 0.34026686668395995\n",
      "  batch 1350 loss: 0.3597508719563484\n",
      "  batch 1400 loss: 0.3317922079563141\n",
      "  batch 1450 loss: 0.3102074673771858\n",
      "  batch 1500 loss: 0.3441119846701622\n",
      "  batch 1550 loss: 0.34191154420375824\n",
      "  batch 1600 loss: 0.3410118407011032\n",
      "  batch 1650 loss: 0.32353465735912323\n",
      "  batch 1700 loss: 0.3549272954463959\n",
      "  batch 1750 loss: 0.3406947726011276\n",
      "  batch 1800 loss: 0.32817029148340227\n",
      "  batch 1850 loss: 0.34468750178813934\n",
      "  batch 1900 loss: 0.32242919921875\n",
      "  batch 1950 loss: 0.2994926717877388\n",
      "  batch 2000 loss: 0.32706635564565656\n",
      "  batch 2050 loss: 0.34247878432273865\n",
      "  batch 2100 loss: 0.32643874883651736\n",
      "  batch 2150 loss: 0.30317548006772993\n",
      "  batch 2200 loss: 0.33499340087175367\n",
      "  batch 2250 loss: 0.3263443386554718\n",
      "  batch 2300 loss: 0.3291034905612469\n",
      "  batch 2350 loss: 0.3392400598526001\n",
      "  batch 2400 loss: 0.32981319457292557\n",
      "  batch 2450 loss: 0.31413945466279986\n",
      "  batch 2500 loss: 0.3121970424056053\n",
      "  batch 2550 loss: 0.3319266924262047\n",
      "  batch 2600 loss: 0.3259773141145706\n",
      "  batch 2650 loss: 0.31357665807008744\n",
      "  batch 2700 loss: 0.3276412072777748\n",
      "  batch 2750 loss: 0.3338755691051483\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.33388 valid 0.80395, valid PER 22.06%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.2755657339096069\n",
      "  batch 100 loss: 0.29936056822538376\n",
      "  batch 150 loss: 0.26479948937892916\n",
      "  batch 200 loss: 0.27229732275009155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 250 loss: 0.27728149145841596\n",
      "  batch 300 loss: 0.2895902115106583\n",
      "  batch 350 loss: 0.2856204405426979\n",
      "  batch 400 loss: 0.28121084690093995\n",
      "  batch 450 loss: 0.27859010845422744\n",
      "  batch 500 loss: 0.2859617409110069\n",
      "  batch 550 loss: 0.28804107785224914\n",
      "  batch 600 loss: 0.28548723191022873\n",
      "  batch 650 loss: 0.2630582246184349\n",
      "  batch 700 loss: 0.26530607134103773\n",
      "  batch 750 loss: 0.27682259410619736\n",
      "  batch 800 loss: 0.277895981669426\n",
      "  batch 850 loss: 0.2859682211279869\n",
      "  batch 900 loss: 0.2621074056625366\n",
      "  batch 950 loss: 0.27790908694267275\n",
      "  batch 1000 loss: 0.2765858566761017\n",
      "  batch 1050 loss: 0.24775049477815628\n",
      "  batch 1100 loss: 0.26967536717653273\n",
      "  batch 1150 loss: 0.26703612983226777\n",
      "  batch 1200 loss: 0.27961874663829805\n",
      "  batch 1250 loss: 0.283763110935688\n",
      "  batch 1300 loss: 0.2694987803697586\n",
      "  batch 1350 loss: 0.2710930714011192\n",
      "  batch 1400 loss: 0.25163019984960555\n",
      "  batch 1450 loss: 0.27653350442647934\n",
      "  batch 1500 loss: 0.2542334453761578\n",
      "  batch 1550 loss: 0.28054853796958923\n",
      "  batch 1600 loss: 0.26595029890537264\n",
      "  batch 1650 loss: 0.2790479692816734\n",
      "  batch 1700 loss: 0.2626271218061447\n",
      "  batch 1750 loss: 0.27136299550533294\n",
      "  batch 1800 loss: 0.27553610265254974\n",
      "  batch 1850 loss: 0.2691468641161919\n",
      "  batch 1900 loss: 0.27220809951424596\n",
      "  batch 1950 loss: 0.2828543746471405\n",
      "  batch 2000 loss: 0.2606804093718529\n",
      "  batch 2050 loss: 0.257970897257328\n",
      "  batch 2100 loss: 0.28765216678380967\n",
      "  batch 2150 loss: 0.2752019248902798\n",
      "  batch 2200 loss: 0.26527593344449996\n",
      "  batch 2250 loss: 0.2606205010414124\n",
      "  batch 2300 loss: 0.27209680795669555\n",
      "  batch 2350 loss: 0.2491878193616867\n",
      "  batch 2400 loss: 0.2626603808999062\n",
      "  batch 2450 loss: 0.2612224002182484\n",
      "  batch 2500 loss: 0.27289597153663636\n",
      "  batch 2550 loss: 0.2657217299938202\n",
      "  batch 2600 loss: 0.29259723275899885\n",
      "  batch 2650 loss: 0.29297405540943144\n",
      "  batch 2700 loss: 0.2819824489951134\n",
      "  batch 2750 loss: 0.26271111369132993\n",
      "Epoch 00011: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.26271 valid 0.80998, valid PER 22.20%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.2382812087237835\n",
      "  batch 100 loss: 0.2380738651752472\n",
      "  batch 150 loss: 0.24422741383314134\n",
      "  batch 200 loss: 0.2576557847857475\n",
      "  batch 250 loss: 0.24977578997612\n",
      "  batch 300 loss: 0.23748720645904542\n",
      "  batch 350 loss: 0.25933820724487305\n",
      "  batch 400 loss: 0.22897900342941285\n",
      "  batch 450 loss: 0.24742108166217805\n",
      "  batch 500 loss: 0.2514181238412857\n",
      "  batch 550 loss: 0.234647499024868\n",
      "  batch 600 loss: 0.23626903355121612\n",
      "  batch 650 loss: 0.24554193511605263\n",
      "  batch 700 loss: 0.23820437848567963\n",
      "  batch 750 loss: 0.22099805727601052\n",
      "  batch 800 loss: 0.24640048980712892\n",
      "  batch 850 loss: 0.23318031311035156\n",
      "  batch 900 loss: 0.23866567969322205\n",
      "  batch 950 loss: 0.22987229615449906\n",
      "  batch 1000 loss: 0.24110422119498254\n",
      "  batch 1050 loss: 0.23725362300872802\n",
      "  batch 1100 loss: 0.22764634534716607\n",
      "  batch 1150 loss: 0.24239062681794166\n",
      "  batch 1200 loss: 0.24705520436167716\n",
      "  batch 1250 loss: 0.23555614069104194\n",
      "  batch 1300 loss: 0.24983529776334762\n",
      "  batch 1350 loss: 0.24952130854129792\n",
      "  batch 1400 loss: 0.24808891177177428\n",
      "  batch 1450 loss: 0.25506894767284394\n",
      "  batch 1500 loss: 0.24931235060095788\n",
      "  batch 1550 loss: 0.229477399289608\n",
      "  batch 1600 loss: 0.2359270039200783\n",
      "  batch 1650 loss: 0.23066225200891494\n",
      "  batch 1700 loss: 0.22171947598457337\n",
      "  batch 1750 loss: 0.24280448347330094\n",
      "  batch 1800 loss: 0.25316634148359296\n",
      "  batch 1850 loss: 0.2301846319437027\n",
      "  batch 1900 loss: 0.2258941811323166\n",
      "  batch 1950 loss: 0.24580381602048873\n",
      "  batch 2000 loss: 0.24734968066215515\n",
      "  batch 2050 loss: 0.23009765967726709\n",
      "  batch 2100 loss: 0.23955790646374225\n",
      "  batch 2150 loss: 0.2293406119942665\n",
      "  batch 2200 loss: 0.24816054999828338\n",
      "  batch 2250 loss: 0.25014266654849054\n",
      "  batch 2300 loss: 0.2436094956099987\n",
      "  batch 2350 loss: 0.23908066108822823\n",
      "  batch 2400 loss: 0.21993398442864417\n",
      "  batch 2450 loss: 0.2379387103021145\n",
      "  batch 2500 loss: 0.216871857047081\n",
      "  batch 2550 loss: 0.26437399357557295\n",
      "  batch 2600 loss: 0.23335896998643876\n",
      "  batch 2650 loss: 0.23813106447458268\n",
      "  batch 2700 loss: 0.22913289099931716\n",
      "  batch 2750 loss: 0.24674080282449723\n",
      "Epoch 00012: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.24674 valid 0.82803, valid PER 22.16%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.22113588377833365\n",
      "  batch 100 loss: 0.22220724254846572\n",
      "  batch 150 loss: 0.23972485318779946\n",
      "  batch 200 loss: 0.2474594995379448\n",
      "  batch 250 loss: 0.21018889158964157\n",
      "  batch 300 loss: 0.21779451191425322\n",
      "  batch 350 loss: 0.22108765304088593\n",
      "  batch 400 loss: 0.21670333102345465\n",
      "  batch 450 loss: 0.21999012917280197\n",
      "  batch 500 loss: 0.22315627381205558\n",
      "  batch 550 loss: 0.21738172754645346\n",
      "  batch 600 loss: 0.22115529775619508\n",
      "  batch 650 loss: 0.23290687024593354\n",
      "  batch 700 loss: 0.21583179220557214\n",
      "  batch 750 loss: 0.23993698745965958\n",
      "  batch 800 loss: 0.22019312232732774\n",
      "  batch 850 loss: 0.21061672016978264\n",
      "  batch 900 loss: 0.24813141375780107\n",
      "  batch 950 loss: 0.2108326970040798\n",
      "  batch 1000 loss: 0.21335100173950194\n",
      "  batch 1050 loss: 0.22900728657841682\n",
      "  batch 1100 loss: 0.2320967461168766\n",
      "  batch 1150 loss: 0.22634724095463754\n",
      "  batch 1200 loss: 0.2154480139911175\n",
      "  batch 1250 loss: 0.21674488380551338\n",
      "  batch 1300 loss: 0.2173460952937603\n",
      "  batch 1350 loss: 0.2214143207669258\n",
      "  batch 1400 loss: 0.2246347510814667\n",
      "  batch 1450 loss: 0.22581958621740342\n",
      "  batch 1500 loss: 0.22670558631420135\n",
      "  batch 1550 loss: 0.22433478385210037\n",
      "  batch 1600 loss: 0.22107695877552033\n",
      "  batch 1650 loss: 0.21360061690211296\n",
      "  batch 1700 loss: 0.22434720486402512\n",
      "  batch 1750 loss: 0.21460674256086348\n",
      "  batch 1800 loss: 0.22179607450962066\n",
      "  batch 1850 loss: 0.22569944202899933\n",
      "  batch 1900 loss: 0.23597089886665346\n",
      "  batch 1950 loss: 0.2018178640305996\n",
      "  batch 2000 loss: 0.23142315447330475\n",
      "  batch 2050 loss: 0.1989172665774822\n",
      "  batch 2100 loss: 0.23092925652861596\n",
      "  batch 2150 loss: 0.22002382427453995\n",
      "  batch 2200 loss: 0.21728262886404992\n",
      "  batch 2250 loss: 0.21516364857554435\n",
      "  batch 2300 loss: 0.22662121444940567\n",
      "  batch 2350 loss: 0.21171652764081955\n",
      "  batch 2400 loss: 0.2152251859009266\n",
      "  batch 2450 loss: 0.2156132446229458\n",
      "  batch 2500 loss: 0.21471585646271707\n",
      "  batch 2550 loss: 0.20677624091506006\n",
      "  batch 2600 loss: 0.21558535426855088\n",
      "  batch 2650 loss: 0.2170604233443737\n",
      "  batch 2700 loss: 0.20901117995381355\n",
      "  batch 2750 loss: 0.2268511463701725\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.22685 valid 0.83164, valid PER 22.06%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.2081312096118927\n",
      "  batch 100 loss: 0.21158361122012137\n",
      "  batch 150 loss: 0.21352466106414794\n",
      "  batch 200 loss: 0.2044799430668354\n",
      "  batch 250 loss: 0.20494318157434463\n",
      "  batch 300 loss: 0.2261766880750656\n",
      "  batch 350 loss: 0.2218553301692009\n",
      "  batch 400 loss: 0.21411697536706925\n",
      "  batch 450 loss: 0.23260356336832047\n",
      "  batch 500 loss: 0.21105551838874817\n",
      "  batch 550 loss: 0.19818204388022423\n",
      "  batch 600 loss: 0.23106965243816377\n",
      "  batch 650 loss: 0.20997573003172876\n",
      "  batch 700 loss: 0.209427160769701\n",
      "  batch 750 loss: 0.21653308108448982\n",
      "  batch 800 loss: 0.20459640353918077\n",
      "  batch 850 loss: 0.21566825360059738\n",
      "  batch 900 loss: 0.21101518705487252\n",
      "  batch 950 loss: 0.23274385288357735\n",
      "  batch 1000 loss: 0.22936397403478623\n",
      "  batch 1050 loss: 0.21389035195112227\n",
      "  batch 1100 loss: 0.20862967863678933\n",
      "  batch 1150 loss: 0.19978639394044875\n",
      "  batch 1200 loss: 0.19182700172066688\n",
      "  batch 1250 loss: 0.22183270364999771\n",
      "  batch 1300 loss: 0.22877789065241813\n",
      "  batch 1350 loss: 0.20114029675722123\n",
      "  batch 1400 loss: 0.20871371209621428\n",
      "  batch 1450 loss: 0.21358830660581588\n",
      "  batch 1500 loss: 0.19355272233486176\n",
      "  batch 1550 loss: 0.20854865700006486\n",
      "  batch 1600 loss: 0.21807701230049134\n",
      "  batch 1650 loss: 0.21201155439019204\n",
      "  batch 1700 loss: 0.23028874546289443\n",
      "  batch 1750 loss: 0.21288340389728547\n",
      "  batch 1800 loss: 0.20381877332925796\n",
      "  batch 1850 loss: 0.22601348504424096\n",
      "  batch 1900 loss: 0.20594889536499977\n",
      "  batch 1950 loss: 0.22150640189647675\n",
      "  batch 2000 loss: 0.19918230608105658\n",
      "  batch 2050 loss: 0.20350924402475357\n",
      "  batch 2100 loss: 0.20180818796157837\n",
      "  batch 2150 loss: 0.21506512820720672\n",
      "  batch 2200 loss: 0.20738914459943772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 2250 loss: 0.20834460407495498\n",
      "  batch 2300 loss: 0.20901915460824966\n",
      "  batch 2350 loss: 0.20926114723086356\n",
      "  batch 2400 loss: 0.224500602632761\n",
      "  batch 2450 loss: 0.20586979925632476\n",
      "  batch 2500 loss: 0.2143600106239319\n",
      "  batch 2550 loss: 0.22060076966881753\n",
      "  batch 2600 loss: 0.20150316804647445\n",
      "  batch 2650 loss: 0.20638425692915915\n",
      "  batch 2700 loss: 0.19934888258576394\n",
      "  batch 2750 loss: 0.22576652273535727\n",
      "Epoch 00014: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.22577 valid 0.83286, valid PER 22.10%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.20025224804878236\n",
      "  batch 100 loss: 0.20949839562177658\n",
      "  batch 150 loss: 0.2195408633351326\n",
      "  batch 200 loss: 0.20798778608441354\n",
      "  batch 250 loss: 0.19043301522731781\n",
      "  batch 300 loss: 0.21326522141695023\n",
      "  batch 350 loss: 0.20282391235232353\n",
      "  batch 400 loss: 0.2022451750934124\n",
      "  batch 450 loss: 0.2127455660700798\n",
      "  batch 500 loss: 0.21037504807114601\n",
      "  batch 550 loss: 0.20124362409114838\n",
      "  batch 600 loss: 0.21857730016112328\n",
      "  batch 650 loss: 0.21139029458165168\n",
      "  batch 700 loss: 0.21267214313149452\n",
      "  batch 750 loss: 0.19576792627573014\n",
      "  batch 800 loss: 0.20979709029197693\n",
      "  batch 850 loss: 0.20548163935542108\n",
      "  batch 900 loss: 0.21077482521533966\n",
      "  batch 950 loss: 0.1924974128603935\n",
      "  batch 1000 loss: 0.2081321431696415\n",
      "  batch 1050 loss: 0.21221321940422058\n",
      "  batch 1100 loss: 0.18907118141651152\n",
      "  batch 1150 loss: 0.19915837228298186\n",
      "  batch 1200 loss: 0.20494253873825075\n",
      "  batch 1250 loss: 0.20645719543099403\n",
      "  batch 1300 loss: 0.1939450140297413\n",
      "  batch 1350 loss: 0.22530644357204438\n",
      "  batch 1400 loss: 0.20931659117341042\n",
      "  batch 1450 loss: 0.20698380947113038\n",
      "  batch 1500 loss: 0.21465580224990843\n",
      "  batch 1550 loss: 0.21983203202486037\n",
      "  batch 1600 loss: 0.22114761263132096\n",
      "  batch 1650 loss: 0.20948870360851288\n",
      "  batch 1700 loss: 0.22246383249759674\n",
      "  batch 1750 loss: 0.2143010391294956\n",
      "  batch 1800 loss: 0.20239030689001083\n",
      "  batch 1850 loss: 0.21711252003908157\n",
      "  batch 1900 loss: 0.1997944623231888\n",
      "  batch 1950 loss: 0.2079745339602232\n",
      "  batch 2000 loss: 0.19801247701048852\n",
      "  batch 2050 loss: 0.209943707883358\n",
      "  batch 2100 loss: 0.2043313865363598\n",
      "  batch 2150 loss: 0.21548814713954925\n",
      "  batch 2200 loss: 0.21492041394114494\n",
      "  batch 2250 loss: 0.21187622770667075\n",
      "  batch 2300 loss: 0.19229982033371926\n",
      "  batch 2350 loss: 0.2085773341357708\n",
      "  batch 2400 loss: 0.2045247757434845\n",
      "  batch 2450 loss: 0.21686860382556916\n",
      "  batch 2500 loss: 0.19754210755228996\n",
      "  batch 2550 loss: 0.20154744982719422\n",
      "  batch 2600 loss: 0.20847936138510703\n",
      "  batch 2650 loss: 0.19656758531928062\n",
      "  batch 2700 loss: 0.21441057831048965\n",
      "  batch 2750 loss: 0.21218875527381897\n",
      "Epoch 00015: reducing learning rate of group 0 to 3.9062e-03.\n",
      "LOSS train 0.21219 valid 0.83615, valid PER 22.20%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.20573108062148093\n",
      "  batch 100 loss: 0.2104301017522812\n",
      "  batch 150 loss: 0.20597161546349527\n",
      "  batch 200 loss: 0.19753389149904252\n",
      "  batch 250 loss: 0.2018101568520069\n",
      "  batch 300 loss: 0.1964358912408352\n",
      "  batch 350 loss: 0.19896969184279442\n",
      "  batch 400 loss: 0.21089776679873468\n",
      "  batch 450 loss: 0.20624919787049292\n",
      "  batch 500 loss: 0.20510532289743424\n",
      "  batch 550 loss: 0.21286477789282798\n",
      "  batch 600 loss: 0.19622548043727875\n",
      "  batch 650 loss: 0.21683104783296586\n",
      "  batch 700 loss: 0.21337469458580016\n",
      "  batch 750 loss: 0.2132108937203884\n",
      "  batch 800 loss: 0.19210536450147628\n",
      "  batch 850 loss: 0.2075599180161953\n",
      "  batch 900 loss: 0.19440404042601586\n",
      "  batch 950 loss: 0.21128762483596802\n",
      "  batch 1000 loss: 0.2219432082772255\n",
      "  batch 1050 loss: 0.2103446628153324\n",
      "  batch 1100 loss: 0.19179477497935296\n",
      "  batch 1150 loss: 0.19984790414571763\n",
      "  batch 1200 loss: 0.21491701588034628\n",
      "  batch 1250 loss: 0.18311161652207375\n",
      "  batch 1300 loss: 0.21867278337478638\n",
      "  batch 1350 loss: 0.2088383887708187\n",
      "  batch 1400 loss: 0.19145848616957664\n",
      "  batch 1450 loss: 0.2105936561524868\n",
      "  batch 1500 loss: 0.2182907411456108\n",
      "  batch 1550 loss: 0.20567578956484794\n",
      "  batch 1600 loss: 0.20411126360297202\n",
      "  batch 1650 loss: 0.20713837385177614\n",
      "  batch 1700 loss: 0.18072108134627343\n",
      "  batch 1750 loss: 0.20674193531274795\n",
      "  batch 1800 loss: 0.19991422802209854\n",
      "  batch 1850 loss: 0.1988651543855667\n",
      "  batch 1900 loss: 0.2031192909181118\n",
      "  batch 1950 loss: 0.2003209564834833\n",
      "  batch 2000 loss: 0.20854659929871558\n",
      "  batch 2050 loss: 0.1974297833442688\n",
      "  batch 2100 loss: 0.20102692082524298\n",
      "  batch 2150 loss: 0.21704356864094734\n",
      "  batch 2200 loss: 0.2153271631896496\n",
      "  batch 2250 loss: 0.20058510169386864\n",
      "  batch 2300 loss: 0.21886345475912095\n",
      "  batch 2350 loss: 0.20061831369996072\n",
      "  batch 2400 loss: 0.22100681602954864\n",
      "  batch 2450 loss: 0.205908921957016\n",
      "  batch 2500 loss: 0.1970430800318718\n",
      "  batch 2550 loss: 0.21251432508230209\n",
      "  batch 2600 loss: 0.20826059713959694\n",
      "  batch 2650 loss: 0.2058471792936325\n",
      "  batch 2700 loss: 0.20779020637273787\n",
      "  batch 2750 loss: 0.19908743873238563\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.9531e-03.\n",
      "LOSS train 0.19909 valid 0.83748, valid PER 22.11%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.20280834525823593\n",
      "  batch 100 loss: 0.20197213277220727\n",
      "  batch 150 loss: 0.22218462347984314\n",
      "  batch 200 loss: 0.21004058808088302\n",
      "  batch 250 loss: 0.19398933053016662\n",
      "  batch 300 loss: 0.20385699227452278\n",
      "  batch 350 loss: 0.2153116823732853\n",
      "  batch 400 loss: 0.18456474781036378\n",
      "  batch 450 loss: 0.1997811932861805\n",
      "  batch 500 loss: 0.2033475974202156\n",
      "  batch 550 loss: 0.2065114386379719\n",
      "  batch 600 loss: 0.19267016023397446\n",
      "  batch 650 loss: 0.19816335722804068\n",
      "  batch 700 loss: 0.19629460632801055\n",
      "  batch 750 loss: 0.20114737272262573\n",
      "  batch 800 loss: 0.22130666583776473\n",
      "  batch 850 loss: 0.1928820213675499\n",
      "  batch 900 loss: 0.19345566585659982\n",
      "  batch 950 loss: 0.21524812355637551\n",
      "  batch 1000 loss: 0.20438918873667716\n",
      "  batch 1050 loss: 0.2130433887243271\n",
      "  batch 1100 loss: 0.2040150211751461\n",
      "  batch 1150 loss: 0.1993286807835102\n",
      "  batch 1200 loss: 0.21160361930727958\n",
      "  batch 1250 loss: 0.2095096991956234\n",
      "  batch 1300 loss: 0.2042682583630085\n",
      "  batch 1350 loss: 0.2165910029411316\n",
      "  batch 1400 loss: 0.2101338216662407\n",
      "  batch 1450 loss: 0.2178012391924858\n",
      "  batch 1500 loss: 0.19816215053200723\n",
      "  batch 1550 loss: 0.20193500831723213\n",
      "  batch 1600 loss: 0.2135329020023346\n",
      "  batch 1650 loss: 0.20656455785036087\n",
      "  batch 1700 loss: 0.20516571179032325\n",
      "  batch 1750 loss: 0.1997956131398678\n",
      "  batch 1800 loss: 0.19523863524198531\n",
      "  batch 1850 loss: 0.20327981904149056\n",
      "  batch 1900 loss: 0.18888615921139718\n",
      "  batch 1950 loss: 0.19617222756147384\n",
      "  batch 2000 loss: 0.2056063511967659\n",
      "  batch 2050 loss: 0.20073344185948372\n",
      "  batch 2100 loss: 0.19121233478188515\n",
      "  batch 2150 loss: 0.19652798473834993\n",
      "  batch 2200 loss: 0.1929452010989189\n",
      "  batch 2250 loss: 0.21222517400979996\n",
      "  batch 2300 loss: 0.22186073422431946\n",
      "  batch 2350 loss: 0.203429863601923\n",
      "  batch 2400 loss: 0.20260907962918281\n",
      "  batch 2450 loss: 0.19453150346875192\n",
      "  batch 2500 loss: 0.22157860904932022\n",
      "  batch 2550 loss: 0.20442285627126694\n",
      "  batch 2600 loss: 0.209851391017437\n",
      "  batch 2650 loss: 0.22043229013681412\n",
      "  batch 2700 loss: 0.2046444058418274\n",
      "  batch 2750 loss: 0.19546914771199225\n",
      "Epoch 00017: reducing learning rate of group 0 to 9.7656e-04.\n",
      "LOSS train 0.19547 valid 0.83902, valid PER 22.06%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.20171404778957366\n",
      "  batch 100 loss: 0.19487395748496056\n",
      "  batch 150 loss: 0.19932596728205682\n",
      "  batch 200 loss: 0.18807996451854705\n",
      "  batch 250 loss: 0.213762549161911\n",
      "  batch 300 loss: 0.18832609713077544\n",
      "  batch 350 loss: 0.19871509104967117\n",
      "  batch 400 loss: 0.22067443162202835\n",
      "  batch 450 loss: 0.2092484875023365\n",
      "  batch 500 loss: 0.22269210278987883\n",
      "  batch 550 loss: 0.21041465803980827\n",
      "  batch 600 loss: 0.19310796439647673\n",
      "  batch 650 loss: 0.21140264347195625\n",
      "  batch 700 loss: 0.1945646507292986\n",
      "  batch 750 loss: 0.2244733452796936\n",
      "  batch 800 loss: 0.20503443628549575\n",
      "  batch 850 loss: 0.22122009381651878\n",
      "  batch 900 loss: 0.19523009672760963\n",
      "  batch 950 loss: 0.2085866704583168\n",
      "  batch 1000 loss: 0.19627330362796783\n",
      "  batch 1050 loss: 0.21768604665994645\n",
      "  batch 1100 loss: 0.204378582239151\n",
      "  batch 1150 loss: 0.1938232320547104\n",
      "  batch 1200 loss: 0.20676563397049905\n",
      "  batch 1250 loss: 0.21374554187059402\n",
      "  batch 1300 loss: 0.2135222390294075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1350 loss: 0.1993356542289257\n",
      "  batch 1400 loss: 0.20398598209023475\n",
      "  batch 1450 loss: 0.1956988087296486\n",
      "  batch 1500 loss: 0.19626488134264947\n",
      "  batch 1550 loss: 0.1974722048640251\n",
      "  batch 1600 loss: 0.20159361451864244\n",
      "  batch 1650 loss: 0.1965295284986496\n",
      "  batch 1700 loss: 0.20830517157912254\n",
      "  batch 1750 loss: 0.19299522206187247\n",
      "  batch 1800 loss: 0.19528258442878724\n",
      "  batch 1850 loss: 0.20994090750813485\n",
      "  batch 1900 loss: 0.20032187655568123\n",
      "  batch 1950 loss: 0.20334867745637894\n",
      "  batch 2000 loss: 0.22872849866747857\n",
      "  batch 2050 loss: 0.19549178957939148\n",
      "  batch 2100 loss: 0.21772972300648688\n",
      "  batch 2150 loss: 0.20159623995423318\n",
      "  batch 2200 loss: 0.1999000833928585\n",
      "  batch 2250 loss: 0.2070457910001278\n",
      "  batch 2300 loss: 0.19337164163589476\n",
      "  batch 2350 loss: 0.20964196249842643\n",
      "  batch 2400 loss: 0.21048405185341834\n",
      "  batch 2450 loss: 0.2001415039598942\n",
      "  batch 2500 loss: 0.18920308127999305\n",
      "  batch 2550 loss: 0.20626488849520683\n",
      "  batch 2600 loss: 0.1908569486439228\n",
      "  batch 2650 loss: 0.2151473231613636\n",
      "  batch 2700 loss: 0.19518760904669763\n",
      "  batch 2750 loss: 0.20754970937967301\n",
      "Epoch 00018: reducing learning rate of group 0 to 4.8828e-04.\n",
      "LOSS train 0.20755 valid 0.83921, valid PER 22.05%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.1979489493370056\n",
      "  batch 100 loss: 0.2094828486442566\n",
      "  batch 150 loss: 0.20508474230766296\n",
      "  batch 200 loss: 0.2001821829378605\n",
      "  batch 250 loss: 0.19737631633877753\n",
      "  batch 300 loss: 0.20417662605643272\n",
      "  batch 350 loss: 0.1856420098245144\n",
      "  batch 400 loss: 0.19716329962015153\n",
      "  batch 450 loss: 0.20987174570560455\n",
      "  batch 500 loss: 0.21101441726088524\n",
      "  batch 550 loss: 0.21863550052046776\n",
      "  batch 600 loss: 0.21106314077973365\n",
      "  batch 650 loss: 0.20483788579702378\n",
      "  batch 700 loss: 0.20397626727819443\n",
      "  batch 750 loss: 0.20797693073749543\n",
      "  batch 800 loss: 0.19954197198152543\n",
      "  batch 850 loss: 0.20322811603546143\n",
      "  batch 900 loss: 0.21504830524325372\n",
      "  batch 950 loss: 0.1949378338456154\n",
      "  batch 1000 loss: 0.20703834936022758\n",
      "  batch 1050 loss: 0.2201526527106762\n",
      "  batch 1100 loss: 0.19980240479111672\n",
      "  batch 1150 loss: 0.20936973318457602\n",
      "  batch 1200 loss: 0.2010951928794384\n",
      "  batch 1250 loss: 0.21136506840586664\n",
      "  batch 1300 loss: 0.19559070810675622\n",
      "  batch 1350 loss: 0.21000032290816306\n",
      "  batch 1400 loss: 0.19737498223781585\n",
      "  batch 1450 loss: 0.1999354800581932\n",
      "  batch 1500 loss: 0.21026378884911537\n",
      "  batch 1550 loss: 0.20301549538969993\n",
      "  batch 1600 loss: 0.19982172966003417\n",
      "  batch 1650 loss: 0.2094838809967041\n",
      "  batch 1700 loss: 0.18598446041345595\n",
      "  batch 1750 loss: 0.19495715081691742\n",
      "  batch 1800 loss: 0.2150283382833004\n",
      "  batch 1850 loss: 0.20394085213541985\n",
      "  batch 1900 loss: 0.20667533010244368\n",
      "  batch 1950 loss: 0.21088765263557435\n",
      "  batch 2000 loss: 0.21244671434164047\n",
      "  batch 2050 loss: 0.20164459437131882\n",
      "  batch 2100 loss: 0.2033120174705982\n",
      "  batch 2150 loss: 0.1860387447476387\n",
      "  batch 2200 loss: 0.20563331380486488\n",
      "  batch 2250 loss: 0.21090513661503793\n",
      "  batch 2300 loss: 0.19253681972622871\n",
      "  batch 2350 loss: 0.19102221339941025\n",
      "  batch 2400 loss: 0.204121226221323\n",
      "  batch 2450 loss: 0.19414515197277069\n",
      "  batch 2500 loss: 0.2022520562261343\n",
      "  batch 2550 loss: 0.1979010097682476\n",
      "  batch 2600 loss: 0.21409692347049714\n",
      "  batch 2650 loss: 0.2030637151002884\n",
      "  batch 2700 loss: 0.19231820866465568\n",
      "  batch 2750 loss: 0.2084977561235428\n",
      "Epoch 00019: reducing learning rate of group 0 to 2.4414e-04.\n",
      "LOSS train 0.20850 valid 0.83946, valid PER 22.08%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.19184104926884174\n",
      "  batch 100 loss: 0.21132090091705322\n",
      "  batch 150 loss: 0.18918278247117995\n",
      "  batch 200 loss: 0.2156658200919628\n",
      "  batch 250 loss: 0.21557406693696976\n",
      "  batch 300 loss: 0.19931580543518065\n",
      "  batch 350 loss: 0.21420389488339425\n",
      "  batch 400 loss: 0.1937298433482647\n",
      "  batch 450 loss: 0.1958115927875042\n",
      "  batch 500 loss: 0.19620204344391823\n",
      "  batch 550 loss: 0.1906006269156933\n",
      "  batch 600 loss: 0.21815004721283912\n",
      "  batch 650 loss: 0.19591952115297318\n",
      "  batch 700 loss: 0.1971711552143097\n",
      "  batch 750 loss: 0.19501977011561394\n",
      "  batch 800 loss: 0.20286799266934394\n",
      "  batch 850 loss: 0.19386698335409164\n",
      "  batch 900 loss: 0.2068653590977192\n",
      "  batch 950 loss: 0.20312265932559967\n",
      "  batch 1000 loss: 0.20395705550909043\n",
      "  batch 1050 loss: 0.2031368401646614\n",
      "  batch 1100 loss: 0.19478687390685082\n",
      "  batch 1150 loss: 0.20396041065454484\n",
      "  batch 1200 loss: 0.1956192933768034\n",
      "  batch 1250 loss: 0.21047427654266357\n",
      "  batch 1300 loss: 0.1960979448258877\n",
      "  batch 1350 loss: 0.19986006274819373\n",
      "  batch 1400 loss: 0.20055022045969964\n",
      "  batch 1450 loss: 0.2123247829079628\n",
      "  batch 1500 loss: 0.20428166970610617\n",
      "  batch 1550 loss: 0.21726641148328782\n",
      "  batch 1600 loss: 0.20906542301177977\n",
      "  batch 1650 loss: 0.203758926987648\n",
      "  batch 1700 loss: 0.20352520287036896\n",
      "  batch 1750 loss: 0.2084876848757267\n",
      "  batch 1800 loss: 0.202619798630476\n",
      "  batch 1850 loss: 0.2092687602341175\n",
      "  batch 1900 loss: 0.19460127800703048\n",
      "  batch 1950 loss: 0.20540326580405235\n",
      "  batch 2000 loss: 0.20780536368489266\n",
      "  batch 2050 loss: 0.1975412490963936\n",
      "  batch 2100 loss: 0.1910674314200878\n",
      "  batch 2150 loss: 0.19030606433749198\n",
      "  batch 2200 loss: 0.20572071760892868\n",
      "  batch 2250 loss: 0.2203908781707287\n",
      "  batch 2300 loss: 0.22118833228945733\n",
      "  batch 2350 loss: 0.21150273486971854\n",
      "  batch 2400 loss: 0.21111588269472123\n",
      "  batch 2450 loss: 0.19449711292982103\n",
      "  batch 2500 loss: 0.2004423688352108\n",
      "  batch 2550 loss: 0.21457981273531915\n",
      "  batch 2600 loss: 0.2192743580043316\n",
      "  batch 2650 loss: 0.18884985715150834\n",
      "  batch 2700 loss: 0.19144604086875916\n",
      "  batch 2750 loss: 0.19745611503720284\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.2207e-04.\n",
      "LOSS train 0.19746 valid 0.83956, valid PER 22.07%\n",
      "Training finished in 17.0 minutes.\n",
      "Model saved to checkpoints/20231211_142644/model_8\n",
      "Currently using dropout rate of 0.3\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.948751101493835\n",
      "  batch 100 loss: 3.3835076093673706\n",
      "  batch 150 loss: 3.291892800331116\n",
      "  batch 200 loss: 3.171583342552185\n",
      "  batch 250 loss: 3.069647555351257\n",
      "  batch 300 loss: 2.8560848951339723\n",
      "  batch 350 loss: 2.6975346422195434\n",
      "  batch 400 loss: 2.592257146835327\n",
      "  batch 450 loss: 2.4838414144515992\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m adam_trainer(model_with_dropout, args)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[43msgd_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_with_dropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     62\u001b[0m duration \u001b[38;5;241m=\u001b[39m (end \u001b[38;5;241m-\u001b[39m start)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "File \u001b[0;32m/rds/user/wa285/hpc-work/MLMI2/exp/trainer_SGD_Scheduler.py:67\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     66\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 67\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     70\u001b[0m running_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n",
      "File \u001b[0;32m/rds/user/wa285/hpc-work/MLMI2/exp/trainer_SGD_Scheduler.py:29\u001b[0m, in \u001b[0;36mtrain.<locals>.train_one_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     26\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     27\u001b[0m last_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     30\u001b[0m     inputs, in_lens, trans, _ \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     31\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/rds/user/wa285/hpc-work/MLMI2/exp/dataloader.py:22\u001b[0m, in \u001b[0;36mTDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_idx[index]]\n\u001b[1;32m     21\u001b[0m data_path \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfbank\u001b[39m\u001b[38;5;124m\"\u001b[39m] \n\u001b[0;32m---> 22\u001b[0m fbank \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m fbank_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(fbank, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m fbank_std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstd(fbank, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout_between_layer\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start dropout tuning For 2 Layer LSTM, with Dropout between layer\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank_augmentation.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank_augmentation.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For 2 Layer LSTM\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f9e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0.5]\n",
    "Optimiser = [\"SGD_Scheduler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f854071b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dropout tuning For 2 Layer LSTM, with Dropout between layer\n",
      "Currently using SGD_Scheduler optimiser\n",
      "Currently using dropout rate of 0.5\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.963497977256775\n",
      "  batch 100 loss: 3.384019055366516\n",
      "  batch 150 loss: 3.299621024131775\n",
      "  batch 200 loss: 3.1785534524917605\n",
      "  batch 250 loss: 3.0771847105026247\n",
      "  batch 300 loss: 2.8530867671966553\n",
      "  batch 350 loss: 2.684001531600952\n",
      "  batch 400 loss: 2.5836263513565063\n",
      "  batch 450 loss: 2.481904878616333\n",
      "  batch 500 loss: 2.361795382499695\n",
      "  batch 550 loss: 2.2778086948394773\n",
      "  batch 600 loss: 2.2288330817222595\n",
      "  batch 650 loss: 2.1413475918769835\n",
      "  batch 700 loss: 2.075978944301605\n",
      "  batch 750 loss: 1.999926404953003\n",
      "  batch 800 loss: 1.9528973960876466\n",
      "  batch 850 loss: 1.9586704325675965\n",
      "  batch 900 loss: 1.8926297879219056\n",
      "  batch 950 loss: 1.8473450422286988\n",
      "  batch 1000 loss: 1.8088407802581787\n",
      "  batch 1050 loss: 1.7400964522361755\n",
      "  batch 1100 loss: 1.6704342699050903\n",
      "  batch 1150 loss: 1.6831332302093507\n",
      "  batch 1200 loss: 1.638839428424835\n",
      "  batch 1250 loss: 1.6595231866836548\n",
      "  batch 1300 loss: 1.5559598302841187\n",
      "  batch 1350 loss: 1.5563930201530456\n",
      "  batch 1400 loss: 1.5277193975448609\n",
      "  batch 1450 loss: 1.4428192853927613\n",
      "  batch 1500 loss: 1.4325593757629393\n",
      "  batch 1550 loss: 1.4040909147262572\n",
      "  batch 1600 loss: 1.4220273900032043\n",
      "  batch 1650 loss: 1.3507649564743043\n",
      "  batch 1700 loss: 1.3699221396446228\n",
      "  batch 1750 loss: 1.3204268538951873\n",
      "  batch 1800 loss: 1.3409141969680787\n",
      "  batch 1850 loss: 1.3457596015930176\n",
      "  batch 1900 loss: 1.3050496697425842\n",
      "  batch 1950 loss: 1.2986390066146851\n",
      "  batch 2000 loss: 1.3045846366882323\n",
      "  batch 2050 loss: 1.238085904121399\n",
      "  batch 2100 loss: 1.2646103358268739\n",
      "  batch 2150 loss: 1.230875301361084\n",
      "  batch 2200 loss: 1.2633454990386963\n",
      "  batch 2250 loss: 1.2561982703208923\n",
      "  batch 2300 loss: 1.246846741437912\n",
      "  batch 2350 loss: 1.254093050956726\n",
      "  batch 2400 loss: 1.2092169535160064\n",
      "  batch 2450 loss: 1.1994570994377136\n",
      "  batch 2500 loss: 1.1807297086715698\n",
      "  batch 2550 loss: 1.1695493757724762\n",
      "  batch 2600 loss: 1.1656073248386383\n",
      "  batch 2650 loss: 1.1849680042266846\n",
      "  batch 2700 loss: 1.1829509544372558\n",
      "  batch 2750 loss: 1.174580991268158\n",
      "LOSS train 1.17458 valid 1.15213, valid PER 34.61%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.094808622598648\n",
      "  batch 100 loss: 1.1280529284477234\n",
      "  batch 150 loss: 1.1562093460559846\n",
      "  batch 200 loss: 1.1269506859779357\n",
      "  batch 250 loss: 1.0885364997386933\n",
      "  batch 300 loss: 1.13985182762146\n",
      "  batch 350 loss: 1.0912773323059082\n",
      "  batch 400 loss: 1.0920098519325256\n",
      "  batch 450 loss: 1.090972341299057\n",
      "  batch 500 loss: 1.0660774028301239\n",
      "  batch 550 loss: 1.0804617965221406\n",
      "  batch 600 loss: 1.1054370272159577\n",
      "  batch 650 loss: 1.1195976269245147\n",
      "  batch 700 loss: 1.0663511264324188\n",
      "  batch 750 loss: 1.0783031010627746\n",
      "  batch 800 loss: 1.088994722366333\n",
      "  batch 850 loss: 1.0526053738594054\n",
      "  batch 900 loss: 1.0619907689094543\n",
      "  batch 950 loss: 1.1041803777217865\n",
      "  batch 1000 loss: 1.05898854970932\n",
      "  batch 1050 loss: 1.0505408799648286\n",
      "  batch 1100 loss: 1.0246643400192261\n",
      "  batch 1150 loss: 1.0403914070129394\n",
      "  batch 1200 loss: 1.0644308638572693\n",
      "  batch 1250 loss: 1.0382075750827788\n",
      "  batch 1300 loss: 1.055146279335022\n",
      "  batch 1350 loss: 1.0402774596214295\n",
      "  batch 1400 loss: 1.031554890871048\n",
      "  batch 1450 loss: 1.0388423228263854\n",
      "  batch 1500 loss: 1.0299171006679535\n",
      "  batch 1550 loss: 1.0196057295799255\n",
      "  batch 1600 loss: 1.0331035232543946\n",
      "  batch 1650 loss: 1.0176455903053283\n",
      "  batch 1700 loss: 0.9643076050281525\n",
      "  batch 1750 loss: 1.0581742107868195\n",
      "  batch 1800 loss: 1.0336756360530854\n",
      "  batch 1850 loss: 1.0247078514099122\n",
      "  batch 1900 loss: 1.014196628332138\n",
      "  batch 1950 loss: 0.9956937849521637\n",
      "  batch 2000 loss: 0.9624391424655915\n",
      "  batch 2050 loss: 0.9967397141456604\n",
      "  batch 2100 loss: 0.9711955261230468\n",
      "  batch 2150 loss: 0.9965463840961456\n",
      "  batch 2200 loss: 0.9890009236335754\n",
      "  batch 2250 loss: 0.941452910900116\n",
      "  batch 2300 loss: 0.999510474205017\n",
      "  batch 2350 loss: 0.9730709362030029\n",
      "  batch 2400 loss: 0.9644773614406585\n",
      "  batch 2450 loss: 0.98942826628685\n",
      "  batch 2500 loss: 0.980236999988556\n",
      "  batch 2550 loss: 0.9606182324886322\n",
      "  batch 2600 loss: 0.9482672917842865\n",
      "  batch 2650 loss: 0.9552304458618164\n",
      "  batch 2700 loss: 0.9716976416110993\n",
      "  batch 2750 loss: 0.9331965112686157\n",
      "LOSS train 0.93320 valid 0.94741, valid PER 29.30%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 0.9426352012157441\n",
      "  batch 100 loss: 0.9385829257965088\n",
      "  batch 150 loss: 0.9105721187591552\n",
      "  batch 200 loss: 0.8987472558021545\n",
      "  batch 250 loss: 0.9213944172859192\n",
      "  batch 300 loss: 0.8907402312755585\n",
      "  batch 350 loss: 0.9249021255970001\n",
      "  batch 400 loss: 0.9036457216739655\n",
      "  batch 450 loss: 0.9049673068523407\n",
      "  batch 500 loss: 0.9181365835666656\n",
      "  batch 550 loss: 0.9424913215637207\n",
      "  batch 600 loss: 0.8969348764419556\n",
      "  batch 650 loss: 0.9180167412757874\n",
      "  batch 700 loss: 0.9085886979103088\n",
      "  batch 750 loss: 0.914201090335846\n",
      "  batch 800 loss: 0.9099226093292236\n",
      "  batch 850 loss: 0.9071385526657104\n",
      "  batch 900 loss: 0.9017843246459961\n",
      "  batch 950 loss: 0.8859528887271881\n",
      "  batch 1000 loss: 0.9005524146556855\n",
      "  batch 1050 loss: 0.8682407283782959\n",
      "  batch 1100 loss: 0.871380717754364\n",
      "  batch 1150 loss: 0.8970287895202637\n",
      "  batch 1200 loss: 0.8760517793893814\n",
      "  batch 1250 loss: 0.8743430650234223\n",
      "  batch 1300 loss: 0.8937696659564972\n",
      "  batch 1350 loss: 0.8670676469802856\n",
      "  batch 1400 loss: 0.9222327101230622\n",
      "  batch 1450 loss: 0.8623658692836762\n",
      "  batch 1500 loss: 0.9125773799419403\n",
      "  batch 1550 loss: 0.908046635389328\n",
      "  batch 1600 loss: 0.8600480592250824\n",
      "  batch 1650 loss: 0.8866431045532227\n",
      "  batch 1700 loss: 0.8474604201316833\n",
      "  batch 1750 loss: 0.9073723900318146\n",
      "  batch 1800 loss: 0.8806412875652313\n",
      "  batch 1850 loss: 0.8902082479000092\n",
      "  batch 1900 loss: 0.857849451303482\n",
      "  batch 1950 loss: 0.8469825077056885\n",
      "  batch 2000 loss: 0.8499987065792084\n",
      "  batch 2050 loss: 0.8733049774169922\n",
      "  batch 2100 loss: 0.8650495529174804\n",
      "  batch 2150 loss: 0.8471004211902619\n",
      "  batch 2200 loss: 0.8645296311378479\n",
      "  batch 2250 loss: 0.8725192201137543\n",
      "  batch 2300 loss: 0.8603877711296082\n",
      "  batch 2350 loss: 0.8439002710580826\n",
      "  batch 2400 loss: 0.8022817528247833\n",
      "  batch 2450 loss: 0.8653051519393921\n",
      "  batch 2500 loss: 0.8732383680343628\n",
      "  batch 2550 loss: 0.8523267543315888\n",
      "  batch 2600 loss: 0.8655629420280456\n",
      "  batch 2650 loss: 0.8549093341827393\n",
      "  batch 2700 loss: 0.8657872700691223\n",
      "  batch 2750 loss: 0.8202360594272613\n",
      "LOSS train 0.82024 valid 0.83299, valid PER 26.30%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.817918393611908\n",
      "  batch 100 loss: 0.8746893560886383\n",
      "  batch 150 loss: 0.8158984220027924\n",
      "  batch 200 loss: 0.7893944025039673\n",
      "  batch 250 loss: 0.7998540210723877\n",
      "  batch 300 loss: 0.78950319647789\n",
      "  batch 350 loss: 0.8036700296401977\n",
      "  batch 400 loss: 0.7884818494319916\n",
      "  batch 450 loss: 0.8297278892993927\n",
      "  batch 500 loss: 0.8240490239858628\n",
      "  batch 550 loss: 0.7855085134506226\n",
      "  batch 600 loss: 0.8231944441795349\n",
      "  batch 650 loss: 0.795232241153717\n",
      "  batch 700 loss: 0.799182904958725\n",
      "  batch 750 loss: 0.7978899765014649\n",
      "  batch 800 loss: 0.8170513474941253\n",
      "  batch 850 loss: 0.8240711188316345\n",
      "  batch 900 loss: 0.8193205440044403\n",
      "  batch 950 loss: 0.8277808046340942\n",
      "  batch 1000 loss: 0.7929441475868225\n",
      "  batch 1050 loss: 0.8081903314590454\n",
      "  batch 1100 loss: 0.7874804621934891\n",
      "  batch 1150 loss: 0.7723556900024414\n",
      "  batch 1200 loss: 0.8051711165904999\n",
      "  batch 1250 loss: 0.8651829338073731\n",
      "  batch 1300 loss: 0.7764326989650726\n",
      "  batch 1350 loss: 0.739035114645958\n",
      "  batch 1400 loss: 0.7693171423673629\n",
      "  batch 1450 loss: 0.7639323472976685\n",
      "  batch 1500 loss: 0.8016644823551178\n",
      "  batch 1550 loss: 0.8612806427478791\n",
      "  batch 1600 loss: 0.7913344061374664\n",
      "  batch 1650 loss: 0.7880934184789657\n",
      "  batch 1700 loss: 0.756933661699295\n",
      "  batch 1750 loss: 0.7967891418933868\n",
      "  batch 1800 loss: 0.7779080498218537\n",
      "  batch 1850 loss: 0.774625506401062\n",
      "  batch 1900 loss: 0.7473547607660294\n",
      "  batch 1950 loss: 0.8020953154563903\n",
      "  batch 2000 loss: 0.8065821552276611\n",
      "  batch 2050 loss: 0.8054714727401734\n",
      "  batch 2100 loss: 0.8080041587352753\n",
      "  batch 2150 loss: 0.7741261667013168\n",
      "  batch 2200 loss: 0.7378623855113983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 2250 loss: 0.7382354551553726\n",
      "  batch 2300 loss: 0.7816479676961898\n",
      "  batch 2350 loss: 0.7619472283124924\n",
      "  batch 2400 loss: 0.7660547268390655\n",
      "  batch 2450 loss: 0.7867212772369385\n",
      "  batch 2500 loss: 0.791603547334671\n",
      "  batch 2550 loss: 0.7764487832784652\n",
      "  batch 2600 loss: 0.8015234822034836\n",
      "  batch 2650 loss: 0.8040702295303345\n",
      "  batch 2700 loss: 0.7320561671257019\n",
      "  batch 2750 loss: 0.7660239523649216\n",
      "LOSS train 0.76602 valid 0.82979, valid PER 25.86%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.7538781034946441\n",
      "  batch 100 loss: 0.7248894530534744\n",
      "  batch 150 loss: 0.76694344997406\n",
      "  batch 200 loss: 0.7351259028911591\n",
      "  batch 250 loss: 0.738396846652031\n",
      "  batch 300 loss: 0.7325045424699783\n",
      "  batch 350 loss: 0.7741005194187164\n",
      "  batch 400 loss: 0.7429319965839386\n",
      "  batch 450 loss: 0.7318109476566315\n",
      "  batch 500 loss: 0.7649670577049256\n",
      "  batch 550 loss: 0.714766366481781\n",
      "  batch 600 loss: 0.7244398188591004\n",
      "  batch 650 loss: 0.7246204060316086\n",
      "  batch 700 loss: 0.7715640312433243\n",
      "  batch 750 loss: 0.7551899319887161\n",
      "  batch 800 loss: 0.7441845107078552\n",
      "  batch 850 loss: 0.7092767655849457\n",
      "  batch 900 loss: 0.7533529454469681\n",
      "  batch 950 loss: 0.7445663595199585\n",
      "  batch 1000 loss: 0.7209235250949859\n",
      "  batch 1050 loss: 0.7392033421993256\n",
      "  batch 1100 loss: 0.7148778080940247\n",
      "  batch 1150 loss: 0.7288430321216584\n",
      "  batch 1200 loss: 0.7417351722717285\n",
      "  batch 1250 loss: 0.7091560906171799\n",
      "  batch 1300 loss: 0.6917506766319275\n",
      "  batch 1350 loss: 0.7529450738430024\n",
      "  batch 1400 loss: 0.7207637995481491\n",
      "  batch 1450 loss: 0.7228596228361129\n",
      "  batch 1500 loss: 0.743083074092865\n",
      "  batch 1550 loss: 0.7551283419132233\n",
      "  batch 1600 loss: 0.7211567807197571\n",
      "  batch 1650 loss: 0.7019098490476608\n",
      "  batch 1700 loss: 0.7446833336353302\n",
      "  batch 1750 loss: 0.7265367543697357\n",
      "  batch 1800 loss: 0.7168458139896393\n",
      "  batch 1850 loss: 0.7150218307971954\n",
      "  batch 1900 loss: 0.7561031472682953\n",
      "  batch 1950 loss: 0.7131610232591629\n",
      "  batch 2000 loss: 0.7263331627845764\n",
      "  batch 2050 loss: 0.7381866478919983\n",
      "  batch 2100 loss: 0.7322230923175812\n",
      "  batch 2150 loss: 0.7319263279438019\n",
      "  batch 2200 loss: 0.7339746820926666\n",
      "  batch 2250 loss: 0.7502863097190857\n",
      "  batch 2300 loss: 0.7094557547569275\n",
      "  batch 2350 loss: 0.7352323573827744\n",
      "  batch 2400 loss: 0.7349123352766037\n",
      "  batch 2450 loss: 0.6938662147521972\n",
      "  batch 2500 loss: 0.7017154836654663\n",
      "  batch 2550 loss: 0.714334317445755\n",
      "  batch 2600 loss: 0.7329358184337615\n",
      "  batch 2650 loss: 0.7016738975048065\n",
      "  batch 2700 loss: 0.693287062048912\n",
      "  batch 2750 loss: 0.6935561263561248\n",
      "LOSS train 0.69356 valid 0.79053, valid PER 24.19%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.6993534249067307\n",
      "  batch 100 loss: 0.6941328954696655\n",
      "  batch 150 loss: 0.6820578998327256\n",
      "  batch 200 loss: 0.672557669878006\n",
      "  batch 250 loss: 0.6960232973098754\n",
      "  batch 300 loss: 0.6895694130659104\n",
      "  batch 350 loss: 0.7261822235584259\n",
      "  batch 400 loss: 0.7011873811483383\n",
      "  batch 450 loss: 0.696912556886673\n",
      "  batch 500 loss: 0.6945439034700394\n",
      "  batch 550 loss: 0.6492241275310516\n",
      "  batch 600 loss: 0.6833321762084961\n",
      "  batch 650 loss: 0.7199699121713639\n",
      "  batch 700 loss: 0.693975904583931\n",
      "  batch 750 loss: 0.6679394483566284\n",
      "  batch 800 loss: 0.6935536020994186\n",
      "  batch 850 loss: 0.6701453316211701\n",
      "  batch 900 loss: 0.7003296661376953\n",
      "  batch 950 loss: 0.6848366743326187\n",
      "  batch 1000 loss: 0.6910854637622833\n",
      "  batch 1050 loss: 0.6786465674638749\n",
      "  batch 1100 loss: 0.701738646030426\n",
      "  batch 1150 loss: 0.6829811096191406\n",
      "  batch 1200 loss: 0.6918888747692108\n",
      "  batch 1250 loss: 0.689070103764534\n",
      "  batch 1300 loss: 0.661272047162056\n",
      "  batch 1350 loss: 0.7297853642702102\n",
      "  batch 1400 loss: 0.6776358103752136\n",
      "  batch 1450 loss: 0.6915372455120087\n",
      "  batch 1500 loss: 0.6913707208633423\n",
      "  batch 1550 loss: 0.6908845555782318\n",
      "  batch 1600 loss: 0.7124267649650574\n",
      "  batch 1650 loss: 0.6824856388568878\n",
      "  batch 1700 loss: 0.6840330201387406\n",
      "  batch 1750 loss: 0.6859319621324539\n",
      "  batch 1800 loss: 0.6841193854808807\n",
      "  batch 1850 loss: 0.6541881167888641\n",
      "  batch 1900 loss: 0.6527541661262513\n",
      "  batch 1950 loss: 0.6707417446374894\n",
      "  batch 2000 loss: 0.684301283955574\n",
      "  batch 2050 loss: 0.6674670857191086\n",
      "  batch 2100 loss: 0.6974577105045319\n",
      "  batch 2150 loss: 0.6551711529493331\n",
      "  batch 2200 loss: 0.68095434486866\n",
      "  batch 2250 loss: 0.6485354191064835\n",
      "  batch 2300 loss: 0.666159160733223\n",
      "  batch 2350 loss: 0.6952422964572906\n",
      "  batch 2400 loss: 0.6627279329299927\n",
      "  batch 2450 loss: 0.6658765262365341\n",
      "  batch 2500 loss: 0.6471363699436188\n",
      "  batch 2550 loss: 0.668703265786171\n",
      "  batch 2600 loss: 0.6722147393226624\n",
      "  batch 2650 loss: 0.6706250709295273\n",
      "  batch 2700 loss: 0.661410573720932\n",
      "  batch 2750 loss: 0.6426031267642975\n",
      "LOSS train 0.64260 valid 0.77191, valid PER 23.74%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.6171879470348358\n",
      "  batch 100 loss: 0.6492082190513611\n",
      "  batch 150 loss: 0.6385751938819886\n",
      "  batch 200 loss: 0.6236921817064285\n",
      "  batch 250 loss: 0.6250739550590515\n",
      "  batch 300 loss: 0.6294620633125305\n",
      "  batch 350 loss: 0.6446060675382614\n",
      "  batch 400 loss: 0.6356308704614639\n",
      "  batch 450 loss: 0.6246843594312668\n",
      "  batch 500 loss: 0.6406188154220581\n",
      "  batch 550 loss: 0.633821502327919\n",
      "  batch 600 loss: 0.6168511998653412\n",
      "  batch 650 loss: 0.6401235175132751\n",
      "  batch 700 loss: 0.6320258796215057\n",
      "  batch 750 loss: 0.626740283370018\n",
      "  batch 800 loss: 0.6526002371311188\n",
      "  batch 850 loss: 0.6617724233865738\n",
      "  batch 900 loss: 0.6597076112031937\n",
      "  batch 950 loss: 0.6267227482795715\n",
      "  batch 1000 loss: 0.666616445183754\n",
      "  batch 1050 loss: 0.6276935905218124\n",
      "  batch 1100 loss: 0.6230242854356766\n",
      "  batch 1150 loss: 0.6189539754390716\n",
      "  batch 1200 loss: 0.6491799712181091\n",
      "  batch 1250 loss: 0.6580060559511185\n",
      "  batch 1300 loss: 0.6758639413118362\n",
      "  batch 1350 loss: 0.6108172130584717\n",
      "  batch 1400 loss: 0.6380634319782257\n",
      "  batch 1450 loss: 0.6645438867807388\n",
      "  batch 1500 loss: 0.6286979240179061\n",
      "  batch 1550 loss: 0.6525330179929734\n",
      "  batch 1600 loss: 0.6443160200119018\n",
      "  batch 1650 loss: 0.6325626581907272\n",
      "  batch 1700 loss: 0.6601770466566086\n",
      "  batch 1750 loss: 0.605164121389389\n",
      "  batch 1800 loss: 0.6397458946704865\n",
      "  batch 1850 loss: 0.6656828951835633\n",
      "  batch 1900 loss: 0.6391811800003052\n",
      "  batch 1950 loss: 0.6605048948526382\n",
      "  batch 2000 loss: 0.6239049816131592\n",
      "  batch 2050 loss: 0.6539999198913574\n",
      "  batch 2100 loss: 0.6465944039821625\n",
      "  batch 2150 loss: 0.6384869420528412\n",
      "  batch 2200 loss: 0.6326662611961364\n",
      "  batch 2250 loss: 0.6513403350114823\n",
      "  batch 2300 loss: 0.6280830931663514\n",
      "  batch 2350 loss: 0.6544164264202118\n",
      "  batch 2400 loss: 0.6205146539211274\n",
      "  batch 2450 loss: 0.6465570276975632\n",
      "  batch 2500 loss: 0.6389793109893799\n",
      "  batch 2550 loss: 0.6192184615135193\n",
      "  batch 2600 loss: 0.6393517649173737\n",
      "  batch 2650 loss: 0.5962833052873612\n",
      "  batch 2700 loss: 0.6382413339614869\n",
      "  batch 2750 loss: 0.6477026361227035\n",
      "LOSS train 0.64770 valid 0.74890, valid PER 23.19%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.5980870634317398\n",
      "  batch 100 loss: 0.598872013092041\n",
      "  batch 150 loss: 0.5967202931642532\n",
      "  batch 200 loss: 0.5745439857244492\n",
      "  batch 250 loss: 0.5940942758321762\n",
      "  batch 300 loss: 0.6068363296985626\n",
      "  batch 350 loss: 0.6233750933408737\n",
      "  batch 400 loss: 0.6219162726402283\n",
      "  batch 450 loss: 0.6027965807914734\n",
      "  batch 500 loss: 0.5772090858221054\n",
      "  batch 550 loss: 0.6399712443351746\n",
      "  batch 600 loss: 0.5959159904718399\n",
      "  batch 650 loss: 0.6057678633928298\n",
      "  batch 700 loss: 0.6065218275785447\n",
      "  batch 750 loss: 0.6051469123363495\n",
      "  batch 800 loss: 0.5785868704319\n",
      "  batch 850 loss: 0.5939733892679214\n",
      "  batch 900 loss: 0.5890133684873581\n",
      "  batch 950 loss: 0.6384497076272965\n",
      "  batch 1000 loss: 0.6076286065578461\n",
      "  batch 1050 loss: 0.5896414923667908\n",
      "  batch 1100 loss: 0.5819362926483155\n",
      "  batch 1150 loss: 0.6300145554542541\n",
      "  batch 1200 loss: 0.60165676176548\n",
      "  batch 1250 loss: 0.611881451010704\n",
      "  batch 1300 loss: 0.6161392450332641\n",
      "  batch 1350 loss: 0.6357550150156022\n",
      "  batch 1400 loss: 0.6329469710588456\n",
      "  batch 1450 loss: 0.6276390379667283\n",
      "  batch 1500 loss: 0.5833697098493577\n",
      "  batch 1550 loss: 0.6365344017744065\n",
      "  batch 1600 loss: 0.6255913209915162\n",
      "  batch 1650 loss: 0.5996359729766846\n",
      "  batch 1700 loss: 0.578370651602745\n",
      "  batch 1750 loss: 0.5937367588281631\n",
      "  batch 1800 loss: 0.6246250927448272\n",
      "  batch 1850 loss: 0.5811186772584915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1900 loss: 0.6263420355319976\n",
      "  batch 1950 loss: 0.5834772741794586\n",
      "  batch 2000 loss: 0.6091418641805649\n",
      "  batch 2050 loss: 0.579633520245552\n",
      "  batch 2100 loss: 0.5923056781291962\n",
      "  batch 2150 loss: 0.6093237096071243\n",
      "  batch 2200 loss: 0.5883041900396347\n",
      "  batch 2250 loss: 0.6148463207483291\n",
      "  batch 2300 loss: 0.5728337800502777\n",
      "  batch 2350 loss: 0.5879289954900742\n",
      "  batch 2400 loss: 0.601286627650261\n",
      "  batch 2450 loss: 0.5736616915464401\n",
      "  batch 2500 loss: 0.6104408544301987\n",
      "  batch 2550 loss: 0.5910945773124695\n",
      "  batch 2600 loss: 0.5963704586029053\n",
      "  batch 2650 loss: 0.592735840678215\n",
      "  batch 2700 loss: 0.6088889950513839\n",
      "  batch 2750 loss: 0.6000488913059234\n",
      "Epoch 00008: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.60005 valid 0.75213, valid PER 22.73%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.5399199670553207\n",
      "  batch 100 loss: 0.5524868553876877\n",
      "  batch 150 loss: 0.5169765043258667\n",
      "  batch 200 loss: 0.5265405505895615\n",
      "  batch 250 loss: 0.5298130804300308\n",
      "  batch 300 loss: 0.5107944685220719\n",
      "  batch 350 loss: 0.5343338459730148\n",
      "  batch 400 loss: 0.5499812096357346\n",
      "  batch 450 loss: 0.4978063350915909\n",
      "  batch 500 loss: 0.516207765340805\n",
      "  batch 550 loss: 0.5271189683675765\n",
      "  batch 600 loss: 0.520092078447342\n",
      "  batch 650 loss: 0.5259121948480606\n",
      "  batch 700 loss: 0.48677193373441696\n",
      "  batch 750 loss: 0.5183464574813843\n",
      "  batch 800 loss: 0.5053168272972107\n",
      "  batch 850 loss: 0.509226296544075\n",
      "  batch 900 loss: 0.5179818832874298\n",
      "  batch 950 loss: 0.4984793823957443\n",
      "  batch 1000 loss: 0.5061558592319488\n",
      "  batch 1050 loss: 0.48815944194793703\n",
      "  batch 1100 loss: 0.5178953611850738\n",
      "  batch 1150 loss: 0.498134845495224\n",
      "  batch 1200 loss: 0.48721835017204285\n",
      "  batch 1250 loss: 0.5377692312002182\n",
      "  batch 1300 loss: 0.5229251676797867\n",
      "  batch 1350 loss: 0.48884695291519165\n",
      "  batch 1400 loss: 0.5256866067647934\n",
      "  batch 1450 loss: 0.522139521241188\n",
      "  batch 1500 loss: 0.4990019834041595\n",
      "  batch 1550 loss: 0.5067276400327683\n",
      "  batch 1600 loss: 0.4957298445701599\n",
      "  batch 1650 loss: 0.5173371028900147\n",
      "  batch 1700 loss: 0.5092044997215271\n",
      "  batch 1750 loss: 0.4883695673942566\n",
      "  batch 1800 loss: 0.4579200780391693\n",
      "  batch 1850 loss: 0.5364320135116577\n",
      "  batch 1900 loss: 0.5300448149442673\n",
      "  batch 1950 loss: 0.5247652304172515\n",
      "  batch 2000 loss: 0.492734192609787\n",
      "  batch 2050 loss: 0.5038128608465194\n",
      "  batch 2100 loss: 0.5259021645784379\n",
      "  batch 2150 loss: 0.5321944159269333\n",
      "  batch 2200 loss: 0.5215834337472915\n",
      "  batch 2250 loss: 0.5036664283275605\n",
      "  batch 2300 loss: 0.48730690956115724\n",
      "  batch 2350 loss: 0.4791482162475586\n",
      "  batch 2400 loss: 0.5298140555620193\n",
      "  batch 2450 loss: 0.5073603641986847\n",
      "  batch 2500 loss: 0.5011848425865173\n",
      "  batch 2550 loss: 0.509293343424797\n",
      "  batch 2600 loss: 0.49097329676151275\n",
      "  batch 2650 loss: 0.49870789408683774\n",
      "  batch 2700 loss: 0.4769349664449692\n",
      "  batch 2750 loss: 0.5074821627140045\n",
      "LOSS train 0.50748 valid 0.70564, valid PER 21.31%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.4710060423612595\n",
      "  batch 100 loss: 0.4872463393211365\n",
      "  batch 150 loss: 0.502624209523201\n",
      "  batch 200 loss: 0.44804285049438475\n",
      "  batch 250 loss: 0.45872336238622663\n",
      "  batch 300 loss: 0.5128209239244461\n",
      "  batch 350 loss: 0.48103769004344943\n",
      "  batch 400 loss: 0.5183411264419555\n",
      "  batch 450 loss: 0.46767075479030606\n",
      "  batch 500 loss: 0.4789413845539093\n",
      "  batch 550 loss: 0.4787881505489349\n",
      "  batch 600 loss: 0.48136994659900667\n",
      "  batch 650 loss: 0.4834403699636459\n",
      "  batch 700 loss: 0.49481240153312683\n",
      "  batch 750 loss: 0.45579358994960784\n",
      "  batch 800 loss: 0.46381706178188326\n",
      "  batch 850 loss: 0.48156643807888033\n",
      "  batch 900 loss: 0.47838587164878843\n",
      "  batch 950 loss: 0.49901741206645966\n",
      "  batch 1000 loss: 0.4898924964666367\n",
      "  batch 1050 loss: 0.4775929194688797\n",
      "  batch 1100 loss: 0.4485838747024536\n",
      "  batch 1150 loss: 0.46601041257381437\n",
      "  batch 1200 loss: 0.5012889343500138\n",
      "  batch 1250 loss: 0.4887373435497284\n",
      "  batch 1300 loss: 0.4955001598596573\n",
      "  batch 1350 loss: 0.5071105110645294\n",
      "  batch 1400 loss: 0.47894656896591187\n",
      "  batch 1450 loss: 0.4560671681165695\n",
      "  batch 1500 loss: 0.46684477746486663\n",
      "  batch 1550 loss: 0.4732759988307953\n",
      "  batch 1600 loss: 0.46657891482114794\n",
      "  batch 1650 loss: 0.4564738619327545\n",
      "  batch 1700 loss: 0.5084347277879715\n",
      "  batch 1750 loss: 0.4806073445081711\n",
      "  batch 1800 loss: 0.4675065052509308\n",
      "  batch 1850 loss: 0.4887975400686264\n",
      "  batch 1900 loss: 0.4875071239471436\n",
      "  batch 1950 loss: 0.44347316086292266\n",
      "  batch 2000 loss: 0.48920408546924593\n",
      "  batch 2050 loss: 0.5032535988092423\n",
      "  batch 2100 loss: 0.4737867277860641\n",
      "  batch 2150 loss: 0.44940523743629457\n",
      "  batch 2200 loss: 0.47165255814790724\n",
      "  batch 2250 loss: 0.48366755902767183\n",
      "  batch 2300 loss: 0.4809784930944443\n",
      "  batch 2350 loss: 0.4881626531481743\n",
      "  batch 2400 loss: 0.4639408951997757\n",
      "  batch 2450 loss: 0.4471244293451309\n",
      "  batch 2500 loss: 0.4819293838739395\n",
      "  batch 2550 loss: 0.49279169619083407\n",
      "  batch 2600 loss: 0.4626878193020821\n",
      "  batch 2650 loss: 0.46584020495414735\n",
      "  batch 2700 loss: 0.4856343412399292\n",
      "  batch 2750 loss: 0.47887463808059694\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.47887 valid 0.72410, valid PER 21.28%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.43839101761579513\n",
      "  batch 100 loss: 0.4766515129804611\n",
      "  batch 150 loss: 0.4270468765497208\n",
      "  batch 200 loss: 0.44250254452228543\n",
      "  batch 250 loss: 0.43530372619628904\n",
      "  batch 300 loss: 0.46180279791355133\n",
      "  batch 350 loss: 0.45334751665592193\n",
      "  batch 400 loss: 0.44205260813236236\n",
      "  batch 450 loss: 0.45163628458976746\n",
      "  batch 500 loss: 0.4378008568286896\n",
      "  batch 550 loss: 0.45209667682647703\n",
      "  batch 600 loss: 0.4494330710172653\n",
      "  batch 650 loss: 0.4076929581165314\n",
      "  batch 700 loss: 0.41009037017822264\n",
      "  batch 750 loss: 0.4331049174070358\n",
      "  batch 800 loss: 0.4377938535809517\n",
      "  batch 850 loss: 0.44958318531513214\n",
      "  batch 900 loss: 0.42503059446811675\n",
      "  batch 950 loss: 0.4298561453819275\n",
      "  batch 1000 loss: 0.433645726442337\n",
      "  batch 1050 loss: 0.40677577406167986\n",
      "  batch 1100 loss: 0.4165341958403587\n",
      "  batch 1150 loss: 0.42326054275035857\n",
      "  batch 1200 loss: 0.4396224981546402\n",
      "  batch 1250 loss: 0.44976423621177675\n",
      "  batch 1300 loss: 0.43731622815132143\n",
      "  batch 1350 loss: 0.4338085949420929\n",
      "  batch 1400 loss: 0.4120722499489784\n",
      "  batch 1450 loss: 0.43792799234390256\n",
      "  batch 1500 loss: 0.39752130419015885\n",
      "  batch 1550 loss: 0.44399399399757383\n",
      "  batch 1600 loss: 0.41232819855213165\n",
      "  batch 1650 loss: 0.4337896928191185\n",
      "  batch 1700 loss: 0.4165376481413841\n",
      "  batch 1750 loss: 0.43186069905757907\n",
      "  batch 1800 loss: 0.4199364790320396\n",
      "  batch 1850 loss: 0.4197550794482231\n",
      "  batch 1900 loss: 0.44914018243551257\n",
      "  batch 1950 loss: 0.44077132672071456\n",
      "  batch 2000 loss: 0.41463848412036897\n",
      "  batch 2050 loss: 0.42961653918027876\n",
      "  batch 2100 loss: 0.45118392556905745\n",
      "  batch 2150 loss: 0.43724700808525085\n",
      "  batch 2200 loss: 0.4458050936460495\n",
      "  batch 2250 loss: 0.40640765249729155\n",
      "  batch 2300 loss: 0.4402789843082428\n",
      "  batch 2350 loss: 0.40016603350639346\n",
      "  batch 2400 loss: 0.4268996614217758\n",
      "  batch 2450 loss: 0.42370161920785904\n",
      "  batch 2500 loss: 0.44467751294374463\n",
      "  batch 2550 loss: 0.43712425857782367\n",
      "  batch 2600 loss: 0.4487215059995651\n",
      "  batch 2650 loss: 0.460224392414093\n",
      "  batch 2700 loss: 0.43900007724761964\n",
      "  batch 2750 loss: 0.4236017447710037\n",
      "LOSS train 0.42360 valid 0.70350, valid PER 21.06%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.4019638428092003\n",
      "  batch 100 loss: 0.41264233201742173\n",
      "  batch 150 loss: 0.41767164677381513\n",
      "  batch 200 loss: 0.4298574450612068\n",
      "  batch 250 loss: 0.42808672934770586\n",
      "  batch 300 loss: 0.38872855722904204\n",
      "  batch 350 loss: 0.43699280023574827\n",
      "  batch 400 loss: 0.40629289746284486\n",
      "  batch 450 loss: 0.41566522538661954\n",
      "  batch 500 loss: 0.44320934802293777\n",
      "  batch 550 loss: 0.4125175452232361\n",
      "  batch 600 loss: 0.41056996256113054\n",
      "  batch 650 loss: 0.41916838169097903\n",
      "  batch 700 loss: 0.39351576030254365\n",
      "  batch 750 loss: 0.40230567276477813\n",
      "  batch 800 loss: 0.42717056930065156\n",
      "  batch 850 loss: 0.4176084616780281\n",
      "  batch 900 loss: 0.42320871114730835\n",
      "  batch 950 loss: 0.42287144660949705\n",
      "  batch 1000 loss: 0.423272422850132\n",
      "  batch 1050 loss: 0.4095442658662796\n",
      "  batch 1100 loss: 0.3972429022192955\n",
      "  batch 1150 loss: 0.41243237495422364\n",
      "  batch 1200 loss: 0.4149771669507027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1250 loss: 0.4068649035692215\n",
      "  batch 1300 loss: 0.43343608528375627\n",
      "  batch 1350 loss: 0.43632018446922305\n",
      "  batch 1400 loss: 0.42499372124671936\n",
      "  batch 1450 loss: 0.43269784152507784\n",
      "  batch 1500 loss: 0.4307167273759842\n",
      "  batch 1550 loss: 0.418338857293129\n",
      "  batch 1600 loss: 0.41596662759780884\n",
      "  batch 1650 loss: 0.40908138394355775\n",
      "  batch 1700 loss: 0.4234920519590378\n",
      "  batch 1750 loss: 0.41847546517848966\n",
      "  batch 1800 loss: 0.4264876502752304\n",
      "  batch 1850 loss: 0.40391402870416643\n",
      "  batch 1900 loss: 0.3945696699619293\n",
      "  batch 1950 loss: 0.42258731603622435\n",
      "  batch 2000 loss: 0.41524751782417296\n",
      "  batch 2050 loss: 0.4119496649503708\n",
      "  batch 2100 loss: 0.40770502388477325\n",
      "  batch 2150 loss: 0.38744706034660337\n",
      "  batch 2200 loss: 0.4301250886917114\n",
      "  batch 2250 loss: 0.43837101966142655\n",
      "  batch 2300 loss: 0.41147362619638445\n",
      "  batch 2350 loss: 0.41714817106723784\n",
      "  batch 2400 loss: 0.39847392082214356\n",
      "  batch 2450 loss: 0.4038366425037384\n",
      "  batch 2500 loss: 0.4024414786696434\n",
      "  batch 2550 loss: 0.42372335970401764\n",
      "  batch 2600 loss: 0.40594921946525575\n",
      "  batch 2650 loss: 0.41630504101514815\n",
      "  batch 2700 loss: 0.3989165472984314\n",
      "  batch 2750 loss: 0.43855841368436815\n",
      "Epoch 00012: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.43856 valid 0.71020, valid PER 20.77%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.3994018617272377\n",
      "  batch 100 loss: 0.3928294065594673\n",
      "  batch 150 loss: 0.4034584227204323\n",
      "  batch 200 loss: 0.4299372747540474\n",
      "  batch 250 loss: 0.3954983013868332\n",
      "  batch 300 loss: 0.3878655180335045\n",
      "  batch 350 loss: 0.3978291517496109\n",
      "  batch 400 loss: 0.39560117602348327\n",
      "  batch 450 loss: 0.3917186418175697\n",
      "  batch 500 loss: 0.3853994309902191\n",
      "  batch 550 loss: 0.3963289666175842\n",
      "  batch 600 loss: 0.3948074248433113\n",
      "  batch 650 loss: 0.40574356973171233\n",
      "  batch 700 loss: 0.3879706305265427\n",
      "  batch 750 loss: 0.4254553845524788\n",
      "  batch 800 loss: 0.39010620772838595\n",
      "  batch 850 loss: 0.390375417470932\n",
      "  batch 900 loss: 0.4201475617289543\n",
      "  batch 950 loss: 0.3872250202298164\n",
      "  batch 1000 loss: 0.38962789833545686\n",
      "  batch 1050 loss: 0.39053537368774416\n",
      "  batch 1100 loss: 0.40429041534662247\n",
      "  batch 1150 loss: 0.39240551948547364\n",
      "  batch 1200 loss: 0.39033595979213714\n",
      "  batch 1250 loss: 0.39482916057109835\n",
      "  batch 1300 loss: 0.3914415654540062\n",
      "  batch 1350 loss: 0.3774721425771713\n",
      "  batch 1400 loss: 0.4084415897727013\n",
      "  batch 1450 loss: 0.39725870996713636\n",
      "  batch 1500 loss: 0.41575771242380144\n",
      "  batch 1550 loss: 0.3962225443124771\n",
      "  batch 1600 loss: 0.386888855099678\n",
      "  batch 1650 loss: 0.38222842931747436\n",
      "  batch 1700 loss: 0.38745688378810883\n",
      "  batch 1750 loss: 0.38042510926723483\n",
      "  batch 1800 loss: 0.3868454027175903\n",
      "  batch 1850 loss: 0.40629015386104583\n",
      "  batch 1900 loss: 0.3922234085202217\n",
      "  batch 1950 loss: 0.3804162722826004\n",
      "  batch 2000 loss: 0.3958782094717026\n",
      "  batch 2050 loss: 0.3740003854036331\n",
      "  batch 2100 loss: 0.399078528881073\n",
      "  batch 2150 loss: 0.4011745184659958\n",
      "  batch 2200 loss: 0.39583974361419677\n",
      "  batch 2250 loss: 0.3981127414107323\n",
      "  batch 2300 loss: 0.385483972132206\n",
      "  batch 2350 loss: 0.3711481925845146\n",
      "  batch 2400 loss: 0.40072696954011916\n",
      "  batch 2450 loss: 0.38023788332939146\n",
      "  batch 2500 loss: 0.3895237624645233\n",
      "  batch 2550 loss: 0.38204578042030335\n",
      "  batch 2600 loss: 0.37151120603084564\n",
      "  batch 2650 loss: 0.38531576097011566\n",
      "  batch 2700 loss: 0.3811995616555214\n",
      "  batch 2750 loss: 0.3897234943509102\n",
      "Epoch 00013: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.38972 valid 0.71416, valid PER 20.76%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.3819475969672203\n",
      "  batch 100 loss: 0.37393040001392364\n",
      "  batch 150 loss: 0.39574585199356077\n",
      "  batch 200 loss: 0.3787646299600601\n",
      "  batch 250 loss: 0.38145546019077303\n",
      "  batch 300 loss: 0.379658570587635\n",
      "  batch 350 loss: 0.3868301945924759\n",
      "  batch 400 loss: 0.37145007997751234\n",
      "  batch 450 loss: 0.39291021913290025\n",
      "  batch 500 loss: 0.37617316365242004\n",
      "  batch 550 loss: 0.3703593564033508\n",
      "  batch 600 loss: 0.3954417446255684\n",
      "  batch 650 loss: 0.3740034607052803\n",
      "  batch 700 loss: 0.3656292000412941\n",
      "  batch 750 loss: 0.37227536499500274\n",
      "  batch 800 loss: 0.3715510216355324\n",
      "  batch 850 loss: 0.384634322822094\n",
      "  batch 900 loss: 0.37223483562469484\n",
      "  batch 950 loss: 0.3905932551622391\n",
      "  batch 1000 loss: 0.3948423644900322\n",
      "  batch 1050 loss: 0.38138719826936723\n",
      "  batch 1100 loss: 0.37845502108335494\n",
      "  batch 1150 loss: 0.36283340156078336\n",
      "  batch 1200 loss: 0.3584110060334206\n",
      "  batch 1250 loss: 0.38463401556015014\n",
      "  batch 1300 loss: 0.3823641490936279\n",
      "  batch 1350 loss: 0.38191845804452895\n",
      "  batch 1400 loss: 0.38818009734153747\n",
      "  batch 1450 loss: 0.38854395002126696\n",
      "  batch 1500 loss: 0.3623534792661667\n",
      "  batch 1550 loss: 0.3742299109697342\n",
      "  batch 1600 loss: 0.3796713304519653\n",
      "  batch 1650 loss: 0.38973290115594866\n",
      "  batch 1700 loss: 0.3955929073691368\n",
      "  batch 1750 loss: 0.37131138414144516\n",
      "  batch 1800 loss: 0.3781523495912552\n",
      "  batch 1850 loss: 0.39332941085100176\n",
      "  batch 1900 loss: 0.36526212573051453\n",
      "  batch 1950 loss: 0.4003151035308838\n",
      "  batch 2000 loss: 0.35466566354036333\n",
      "  batch 2050 loss: 0.351990065574646\n",
      "  batch 2100 loss: 0.36941944181919095\n",
      "  batch 2150 loss: 0.38669210612773897\n",
      "  batch 2200 loss: 0.3686842468380928\n",
      "  batch 2250 loss: 0.3684628123044968\n",
      "  batch 2300 loss: 0.3733085578680038\n",
      "  batch 2350 loss: 0.37418929755687713\n",
      "  batch 2400 loss: 0.37760293334722517\n",
      "  batch 2450 loss: 0.3810715028643608\n",
      "  batch 2500 loss: 0.3795643621683121\n",
      "  batch 2550 loss: 0.3788531830906868\n",
      "  batch 2600 loss: 0.3634581997990608\n",
      "  batch 2650 loss: 0.3652948904037476\n",
      "  batch 2700 loss: 0.36567029684782026\n",
      "  batch 2750 loss: 0.3952632713317871\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.39526 valid 0.70495, valid PER 20.54%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.3723029601573944\n",
      "  batch 100 loss: 0.36060287863016127\n",
      "  batch 150 loss: 0.38770059078931807\n",
      "  batch 200 loss: 0.3738095885515213\n",
      "  batch 250 loss: 0.3460529386997223\n",
      "  batch 300 loss: 0.36810669004917146\n",
      "  batch 350 loss: 0.35410060435533525\n",
      "  batch 400 loss: 0.36863943576812747\n",
      "  batch 450 loss: 0.36673527598381045\n",
      "  batch 500 loss: 0.38160193264484404\n",
      "  batch 550 loss: 0.3590755060315132\n",
      "  batch 600 loss: 0.3756606522202492\n",
      "  batch 650 loss: 0.37927677273750304\n",
      "  batch 700 loss: 0.3657608437538147\n",
      "  batch 750 loss: 0.36226800590753555\n",
      "  batch 800 loss: 0.36565000385046004\n",
      "  batch 850 loss: 0.36808632194995883\n",
      "  batch 900 loss: 0.3713237562775612\n",
      "  batch 950 loss: 0.3476558783650398\n",
      "  batch 1000 loss: 0.3650433671474457\n",
      "  batch 1050 loss: 0.38101246803998945\n",
      "  batch 1100 loss: 0.3488185626268387\n",
      "  batch 1150 loss: 0.3619860276579857\n",
      "  batch 1200 loss: 0.36462568908929827\n",
      "  batch 1250 loss: 0.3855447313189507\n",
      "  batch 1300 loss: 0.3496526920795441\n",
      "  batch 1350 loss: 0.393009147644043\n",
      "  batch 1400 loss: 0.3775265210866928\n",
      "  batch 1450 loss: 0.37680065393447876\n",
      "  batch 1500 loss: 0.3669604724645615\n",
      "  batch 1550 loss: 0.3798570999503136\n",
      "  batch 1600 loss: 0.396092349588871\n",
      "  batch 1650 loss: 0.3590466511249542\n",
      "  batch 1700 loss: 0.3961364758014679\n",
      "  batch 1750 loss: 0.38307876348495484\n",
      "  batch 1800 loss: 0.36458768784999845\n",
      "  batch 1850 loss: 0.38294335901737214\n",
      "  batch 1900 loss: 0.36599819600582123\n",
      "  batch 1950 loss: 0.36713471114635465\n",
      "  batch 2000 loss: 0.35888216018676755\n",
      "  batch 2050 loss: 0.3589479449391365\n",
      "  batch 2100 loss: 0.3542396005988121\n",
      "  batch 2150 loss: 0.375355723798275\n",
      "  batch 2200 loss: 0.3870719051361084\n",
      "  batch 2250 loss: 0.37452657759189606\n",
      "  batch 2300 loss: 0.3594494342803955\n",
      "  batch 2350 loss: 0.3634144526720047\n",
      "  batch 2400 loss: 0.3788334584236145\n",
      "  batch 2450 loss: 0.37506426990032193\n",
      "  batch 2500 loss: 0.35653818130493165\n",
      "  batch 2550 loss: 0.36721216171979904\n",
      "  batch 2600 loss: 0.3657348564267158\n",
      "  batch 2650 loss: 0.36147145211696624\n",
      "  batch 2700 loss: 0.3782352551817894\n",
      "  batch 2750 loss: 0.3713312402367592\n",
      "Epoch 00015: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.37133 valid 0.70754, valid PER 20.72%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.3692559099197388\n",
      "  batch 100 loss: 0.37543294608592986\n",
      "  batch 150 loss: 0.38280624061822893\n",
      "  batch 200 loss: 0.36394575506448745\n",
      "  batch 250 loss: 0.3677960500121117\n",
      "  batch 300 loss: 0.349173259139061\n",
      "  batch 350 loss: 0.3412670901417732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 400 loss: 0.36928529292345047\n",
      "  batch 450 loss: 0.3622478672862053\n",
      "  batch 500 loss: 0.3770082339644432\n",
      "  batch 550 loss: 0.3708686423301697\n",
      "  batch 600 loss: 0.3504039615392685\n",
      "  batch 650 loss: 0.37198147624731065\n",
      "  batch 700 loss: 0.37444022089242934\n",
      "  batch 750 loss: 0.3711646890640259\n",
      "  batch 800 loss: 0.3421926686167717\n",
      "  batch 850 loss: 0.37175315409898757\n",
      "  batch 900 loss: 0.3494886839389801\n",
      "  batch 950 loss: 0.35920357644557954\n",
      "  batch 1000 loss: 0.3823190712928772\n",
      "  batch 1050 loss: 0.3822011977434158\n",
      "  batch 1100 loss: 0.3579691731929779\n",
      "  batch 1150 loss: 0.3594176024198532\n",
      "  batch 1200 loss: 0.3632941606640816\n",
      "  batch 1250 loss: 0.34833808958530427\n",
      "  batch 1300 loss: 0.38745873838663103\n",
      "  batch 1350 loss: 0.37180380582809447\n",
      "  batch 1400 loss: 0.35058013290166856\n",
      "  batch 1450 loss: 0.3843952560424805\n",
      "  batch 1500 loss: 0.3616779759526253\n",
      "  batch 1550 loss: 0.3609975931048393\n",
      "  batch 1600 loss: 0.3543637138605118\n",
      "  batch 1650 loss: 0.3709777924418449\n",
      "  batch 1700 loss: 0.33185272097587587\n",
      "  batch 1750 loss: 0.3573364275693893\n",
      "  batch 1800 loss: 0.3614380884170532\n",
      "  batch 1850 loss: 0.38295339047908783\n",
      "  batch 1900 loss: 0.3581380370259285\n",
      "  batch 1950 loss: 0.3706059193611145\n",
      "  batch 2000 loss: 0.36553002446889876\n",
      "  batch 2050 loss: 0.349767245054245\n",
      "  batch 2100 loss: 0.3584107619524002\n",
      "  batch 2150 loss: 0.37700256317853925\n",
      "  batch 2200 loss: 0.38746937274932863\n",
      "  batch 2250 loss: 0.36203310281038287\n",
      "  batch 2300 loss: 0.38123052537441254\n",
      "  batch 2350 loss: 0.35779855579137804\n",
      "  batch 2400 loss: 0.3880339762568474\n",
      "  batch 2450 loss: 0.367133791744709\n",
      "  batch 2500 loss: 0.35819742292165757\n",
      "  batch 2550 loss: 0.3712717944383621\n",
      "  batch 2600 loss: 0.36672133445739746\n",
      "  batch 2650 loss: 0.37418618470430376\n",
      "  batch 2700 loss: 0.3725410306453705\n",
      "  batch 2750 loss: 0.35665063083171844\n",
      "Epoch 00016: reducing learning rate of group 0 to 3.9062e-03.\n",
      "LOSS train 0.35665 valid 0.70745, valid PER 20.54%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.36790319919586184\n",
      "  batch 100 loss: 0.3457365635037422\n",
      "  batch 150 loss: 0.37718541771173475\n",
      "  batch 200 loss: 0.3840709048509598\n",
      "  batch 250 loss: 0.36421726644039154\n",
      "  batch 300 loss: 0.3499704423546791\n",
      "  batch 350 loss: 0.3712019595503807\n",
      "  batch 400 loss: 0.35064118772745134\n",
      "  batch 450 loss: 0.3518009912967682\n",
      "  batch 500 loss: 0.37320402354001997\n",
      "  batch 550 loss: 0.36019257992506026\n",
      "  batch 600 loss: 0.3465969324111938\n",
      "  batch 650 loss: 0.3597029793262482\n",
      "  batch 700 loss: 0.3585630151629448\n",
      "  batch 750 loss: 0.3581942814588547\n",
      "  batch 800 loss: 0.3925215655565262\n",
      "  batch 850 loss: 0.3400656634569168\n",
      "  batch 900 loss: 0.35121026515960696\n",
      "  batch 950 loss: 0.3874099546670914\n",
      "  batch 1000 loss: 0.3517071810364723\n",
      "  batch 1050 loss: 0.3699448585510254\n",
      "  batch 1100 loss: 0.37063079118728637\n",
      "  batch 1150 loss: 0.36179555386304857\n",
      "  batch 1200 loss: 0.3662837514281273\n",
      "  batch 1250 loss: 0.36926949083805083\n",
      "  batch 1300 loss: 0.3613155460357666\n",
      "  batch 1350 loss: 0.36319304347038267\n",
      "  batch 1400 loss: 0.3782541161775589\n",
      "  batch 1450 loss: 0.3808326631784439\n",
      "  batch 1500 loss: 0.3516277614235878\n",
      "  batch 1550 loss: 0.3682042482495308\n",
      "  batch 1600 loss: 0.37922718733549116\n",
      "  batch 1650 loss: 0.34828419744968414\n",
      "  batch 1700 loss: 0.38633072286844256\n",
      "  batch 1750 loss: 0.36715912133455275\n",
      "  batch 1800 loss: 0.361618729531765\n",
      "  batch 1850 loss: 0.3727589786052704\n",
      "  batch 1900 loss: 0.3528915849328041\n",
      "  batch 1950 loss: 0.35205049961805346\n",
      "  batch 2000 loss: 0.359609979391098\n",
      "  batch 2050 loss: 0.35585399001836776\n",
      "  batch 2100 loss: 0.34508497893810275\n",
      "  batch 2150 loss: 0.346962109208107\n",
      "  batch 2200 loss: 0.35032386898994444\n",
      "  batch 2250 loss: 0.3674346113204956\n",
      "  batch 2300 loss: 0.39411084532737733\n",
      "  batch 2350 loss: 0.3678476795554161\n",
      "  batch 2400 loss: 0.3630899915099144\n",
      "  batch 2450 loss: 0.34691460072994235\n",
      "  batch 2500 loss: 0.385256467461586\n",
      "  batch 2550 loss: 0.37193492859601973\n",
      "  batch 2600 loss: 0.3742078658938408\n",
      "  batch 2650 loss: 0.38029281377792357\n",
      "  batch 2700 loss: 0.35882547169923784\n",
      "  batch 2750 loss: 0.35807393908500673\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.9531e-03.\n",
      "LOSS train 0.35807 valid 0.70874, valid PER 20.56%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.35610655605793\n",
      "  batch 100 loss: 0.3463581618666649\n",
      "  batch 150 loss: 0.3499894714355469\n",
      "  batch 200 loss: 0.3541227796673775\n",
      "  batch 250 loss: 0.37371271818876267\n",
      "  batch 300 loss: 0.3475754946470261\n",
      "  batch 350 loss: 0.35212414801120756\n",
      "  batch 400 loss: 0.3706974810361862\n",
      "  batch 450 loss: 0.364937963783741\n",
      "  batch 500 loss: 0.39407056152820585\n",
      "  batch 550 loss: 0.37689027696847915\n",
      "  batch 600 loss: 0.36008475244045257\n",
      "  batch 650 loss: 0.37269172102212905\n",
      "  batch 700 loss: 0.3594579583406448\n",
      "  batch 750 loss: 0.38903300613164904\n",
      "  batch 800 loss: 0.3674456638097763\n",
      "  batch 850 loss: 0.3910099068284035\n",
      "  batch 900 loss: 0.33321416795253755\n",
      "  batch 950 loss: 0.3563253080844879\n",
      "  batch 1000 loss: 0.35285637885332105\n",
      "  batch 1050 loss: 0.36628917187452315\n",
      "  batch 1100 loss: 0.3593554171919823\n",
      "  batch 1150 loss: 0.3394021010398865\n",
      "  batch 1200 loss: 0.3723961171507835\n",
      "  batch 1250 loss: 0.3824017968773842\n",
      "  batch 1300 loss: 0.37933229863643647\n",
      "  batch 1350 loss: 0.3573842152953148\n",
      "  batch 1400 loss: 0.370333725810051\n",
      "  batch 1450 loss: 0.36222071051597593\n",
      "  batch 1500 loss: 0.3438762655854225\n",
      "  batch 1550 loss: 0.3473865994811058\n",
      "  batch 1600 loss: 0.3641801366209984\n",
      "  batch 1650 loss: 0.35865411400794983\n",
      "  batch 1700 loss: 0.35593376219272616\n",
      "  batch 1750 loss: 0.3597043192386627\n",
      "  batch 1800 loss: 0.3476252144575119\n",
      "  batch 1850 loss: 0.37190465211868284\n",
      "  batch 1900 loss: 0.3581703144311905\n",
      "  batch 1950 loss: 0.35615337699651717\n",
      "  batch 2000 loss: 0.384948293864727\n",
      "  batch 2050 loss: 0.36419142127037046\n",
      "  batch 2100 loss: 0.37925262868404386\n",
      "  batch 2150 loss: 0.35383536338806154\n",
      "  batch 2200 loss: 0.3574991962313652\n",
      "  batch 2250 loss: 0.36194144159555436\n",
      "  batch 2300 loss: 0.3312650427222252\n",
      "  batch 2350 loss: 0.3612224307656288\n",
      "  batch 2400 loss: 0.36893329828977584\n",
      "  batch 2450 loss: 0.36207305639982224\n",
      "  batch 2500 loss: 0.33961243838071825\n",
      "  batch 2550 loss: 0.3683624064922333\n",
      "  batch 2600 loss: 0.3412408313155174\n",
      "  batch 2650 loss: 0.3815650334954262\n",
      "  batch 2700 loss: 0.3607133394479752\n",
      "  batch 2750 loss: 0.3670362740755081\n",
      "Epoch 00018: reducing learning rate of group 0 to 9.7656e-04.\n",
      "LOSS train 0.36704 valid 0.70724, valid PER 20.55%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.33951395481824875\n",
      "  batch 100 loss: 0.36552852600812913\n",
      "  batch 150 loss: 0.3632690718770027\n",
      "  batch 200 loss: 0.338890528678894\n",
      "  batch 250 loss: 0.36371097654104234\n",
      "  batch 300 loss: 0.3595141616463661\n",
      "  batch 350 loss: 0.33219175040721893\n",
      "  batch 400 loss: 0.3638768798112869\n",
      "  batch 450 loss: 0.36608097702264786\n",
      "  batch 500 loss: 0.36440434187650683\n",
      "  batch 550 loss: 0.3612804988026619\n",
      "  batch 600 loss: 0.37328098952770233\n",
      "  batch 650 loss: 0.3732725352048874\n",
      "  batch 700 loss: 0.3523693558573723\n",
      "  batch 750 loss: 0.37317913442850115\n",
      "  batch 800 loss: 0.356442049741745\n",
      "  batch 850 loss: 0.3499879541993141\n",
      "  batch 900 loss: 0.38123855501413345\n",
      "  batch 950 loss: 0.3495065638422966\n",
      "  batch 1000 loss: 0.3654534351825714\n",
      "  batch 1050 loss: 0.36643499523401263\n",
      "  batch 1100 loss: 0.3564457738399506\n",
      "  batch 1150 loss: 0.3727667617797852\n",
      "  batch 1200 loss: 0.3435239803791046\n",
      "  batch 1250 loss: 0.3660313105583191\n",
      "  batch 1300 loss: 0.35000628292560576\n",
      "  batch 1350 loss: 0.37257428526878356\n",
      "  batch 1400 loss: 0.35755143731832506\n",
      "  batch 1450 loss: 0.3637909182906151\n",
      "  batch 1500 loss: 0.36743925482034684\n",
      "  batch 1550 loss: 0.3516882795095444\n",
      "  batch 1600 loss: 0.36671168088912964\n",
      "  batch 1650 loss: 0.3742501467466354\n",
      "  batch 1700 loss: 0.34541863650083543\n",
      "  batch 1750 loss: 0.35283549010753634\n",
      "  batch 1800 loss: 0.3685406917333603\n",
      "  batch 1850 loss: 0.37326842427253726\n",
      "  batch 1900 loss: 0.36034801095724106\n",
      "  batch 1950 loss: 0.38437272876501083\n",
      "  batch 2000 loss: 0.371782498061657\n",
      "  batch 2050 loss: 0.3605219379067421\n",
      "  batch 2100 loss: 0.3622350180149078\n",
      "  batch 2150 loss: 0.34446172416210175\n",
      "  batch 2200 loss: 0.3726329025626183\n",
      "  batch 2250 loss: 0.35667120426893234\n",
      "  batch 2300 loss: 0.34038307845592497\n",
      "  batch 2350 loss: 0.3436136466264725\n",
      "  batch 2400 loss: 0.3498160824179649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 2450 loss: 0.3561384007334709\n",
      "  batch 2500 loss: 0.36608962804079054\n",
      "  batch 2550 loss: 0.36982079952955244\n",
      "  batch 2600 loss: 0.3769643864035606\n",
      "  batch 2650 loss: 0.37314257591962813\n",
      "  batch 2700 loss: 0.3430856996774673\n",
      "  batch 2750 loss: 0.36600650936365126\n",
      "Epoch 00019: reducing learning rate of group 0 to 4.8828e-04.\n",
      "LOSS train 0.36601 valid 0.70800, valid PER 20.59%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.34897023886442186\n",
      "  batch 100 loss: 0.37523879170417784\n",
      "  batch 150 loss: 0.33800047904253006\n",
      "  batch 200 loss: 0.3840277177095413\n",
      "  batch 250 loss: 0.3739973717927933\n",
      "  batch 300 loss: 0.35727009177207947\n",
      "  batch 350 loss: 0.36684033334255217\n",
      "  batch 400 loss: 0.33930872023105624\n",
      "  batch 450 loss: 0.3568574702739716\n",
      "  batch 500 loss: 0.3443778231739998\n",
      "  batch 550 loss: 0.3566037997603416\n",
      "  batch 600 loss: 0.36193386137485506\n",
      "  batch 650 loss: 0.34851242214441297\n",
      "  batch 700 loss: 0.35787448912858966\n",
      "  batch 750 loss: 0.3591548675298691\n",
      "  batch 800 loss: 0.36566447019577025\n",
      "  batch 850 loss: 0.3524520432949066\n",
      "  batch 900 loss: 0.364142470061779\n",
      "  batch 950 loss: 0.3671610850095749\n",
      "  batch 1000 loss: 0.3566854786872864\n",
      "  batch 1050 loss: 0.3517741513252258\n",
      "  batch 1100 loss: 0.35886427730321885\n",
      "  batch 1150 loss: 0.36601161777973173\n",
      "  batch 1200 loss: 0.3497828343510628\n",
      "  batch 1250 loss: 0.36462493628263476\n",
      "  batch 1300 loss: 0.3625385883450508\n",
      "  batch 1350 loss: 0.3613681349158287\n",
      "  batch 1400 loss: 0.3643326508998871\n",
      "  batch 1450 loss: 0.37239527344703677\n",
      "  batch 1500 loss: 0.3701619300246239\n",
      "  batch 1550 loss: 0.3796452030539513\n",
      "  batch 1600 loss: 0.3595327794551849\n",
      "  batch 1650 loss: 0.3650294554233551\n",
      "  batch 1700 loss: 0.36405067801475527\n",
      "  batch 1750 loss: 0.3664106571674347\n",
      "  batch 1800 loss: 0.3652029013633728\n",
      "  batch 1850 loss: 0.38108538240194323\n",
      "  batch 1900 loss: 0.34406384319067\n",
      "  batch 1950 loss: 0.37591166108846663\n",
      "  batch 2000 loss: 0.3836998662352562\n",
      "  batch 2050 loss: 0.354527000784874\n",
      "  batch 2100 loss: 0.35671403974294663\n",
      "  batch 2150 loss: 0.34124015271663666\n",
      "  batch 2200 loss: 0.37576318591833113\n",
      "  batch 2250 loss: 0.3778602740168571\n",
      "  batch 2300 loss: 0.373356303870678\n",
      "  batch 2350 loss: 0.3892771974205971\n",
      "  batch 2400 loss: 0.37159250855445863\n",
      "  batch 2450 loss: 0.35041445434093477\n",
      "  batch 2500 loss: 0.3479740852117538\n",
      "  batch 2550 loss: 0.37463759273290637\n",
      "  batch 2600 loss: 0.38465496271848676\n",
      "  batch 2650 loss: 0.355975324511528\n",
      "  batch 2700 loss: 0.3501255610585213\n",
      "  batch 2750 loss: 0.3481332120299339\n",
      "Epoch 00020: reducing learning rate of group 0 to 2.4414e-04.\n",
      "LOSS train 0.34813 valid 0.70789, valid PER 20.58%\n",
      "Training finished in 15.0 minutes.\n",
      "Model saved to checkpoints/20231211_145126/model_11\n",
      "Finish SGD_Scheduler optimiser\n",
      "End tuning For 2 Layer LSTM\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout_between_layer\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start dropout tuning For 2 Layer LSTM, with Dropout between layer\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank_augmentation.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank_augmentation.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For 2 Layer LSTM\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3792cf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently using cpu only\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_dataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "# First find the unique phones in train.json, and then\n",
    "# create a file named vocab.txt, each line in this \n",
    "# file is a unique phone, in total there should be \n",
    "# 40 lines\n",
    "\n",
    "vocab = {}\n",
    "phonemes = []\n",
    "with open(\"vocab_39.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab[text.strip()] = id\n",
    "        phonemes.append(text)\n",
    "phonemes = phonemes[1:]\n",
    "from collections import namedtuple\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"currently using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"currently using cpu only\")\n",
    "\n",
    "args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'val_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 128,\n",
    "        'concat': 1,\n",
    "        'lr': 0.5,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "args = namedtuple('x', args)(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e17b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can uncomment the following line and change model path to the model you want to decode\n",
    "model_path=\"checkpoints/20231211_145126/model_11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c71af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_regularisation_dropout_between_layer\n",
    "args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "args = namedtuple('x', args)(**args)\n",
    "model = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f65b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints/20231211_145126/model_11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (lstm): LSTM(23, 128, num_layers=2, bidirectional=True)\n",
       "  (proj): Linear(in_features=256, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print('Loading model from {}'.format(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64e922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB: 13.71%, DEL: 6.27%, INS: 2.78%, COR: 80.02%, PER: 22.76%\n"
     ]
    }
   ],
   "source": [
    "from decoder import decode\n",
    "results = decode(model, args, args.test_json)\n",
    "print(\"SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0fd2b",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227e745",
   "metadata": {},
   "source": [
    "# Droupout rate 0.5\n",
    "# SGD shceduler learning rate 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_dataloader\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44a4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find the unique phones in train.json, and then\n",
    "# create a file named vocab.txt, each line in this \n",
    "# file is a unique phone, in total there should be \n",
    "# 40 lines\n",
    "\n",
    "vocab = {}\n",
    "phonemes = []\n",
    "with open(\"vocab_39.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab[text.strip()] = id\n",
    "        phonemes.append(text)\n",
    "phonemes = phonemes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24eb416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently using cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"currently using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"currently using cpu only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aa56eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0.5]\n",
    "Optimiser = [\"SGD_Scheduler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a516785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dropout tuning For 2 Layer LSTM, with Dropout between layer\n",
      "Currently using SGD_Scheduler optimiser\n",
      "Currently using dropout rate of 0.5\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.609509544372559\n",
      "  batch 100 loss: 3.2777310514450075\n",
      "  batch 150 loss: 3.1288997316360474\n",
      "  batch 200 loss: 2.799676470756531\n",
      "  batch 250 loss: 2.629464559555054\n",
      "  batch 300 loss: 2.448464527130127\n",
      "  batch 350 loss: 2.368602070808411\n",
      "  batch 400 loss: 2.291487956047058\n",
      "  batch 450 loss: 2.162491626739502\n",
      "  batch 500 loss: 2.063810396194458\n",
      "  batch 550 loss: 1.9613248252868651\n",
      "  batch 600 loss: 1.9027792024612427\n",
      "  batch 650 loss: 1.834680082798004\n",
      "  batch 700 loss: 1.7801838207244873\n",
      "  batch 750 loss: 1.7071974110603332\n",
      "  batch 800 loss: 1.6724721002578735\n",
      "  batch 850 loss: 1.6971648359298706\n",
      "  batch 900 loss: 1.6312711763381957\n",
      "  batch 950 loss: 1.613643250465393\n",
      "  batch 1000 loss: 1.583342628479004\n",
      "  batch 1050 loss: 1.5154249572753906\n",
      "  batch 1100 loss: 1.4750522994995117\n",
      "  batch 1150 loss: 1.491740610599518\n",
      "  batch 1200 loss: 1.4630484104156494\n",
      "  batch 1250 loss: 1.5012918043136596\n",
      "  batch 1300 loss: 1.4028314971923828\n",
      "  batch 1350 loss: 1.4083913230895997\n",
      "  batch 1400 loss: 1.404187479019165\n",
      "  batch 1450 loss: 1.3079462909698487\n",
      "  batch 1500 loss: 1.3249528574943543\n",
      "  batch 1550 loss: 1.2959733200073242\n",
      "  batch 1600 loss: 1.316803195476532\n",
      "  batch 1650 loss: 1.2583627271652222\n",
      "  batch 1700 loss: 1.2790554821491242\n",
      "  batch 1750 loss: 1.2406154429912568\n",
      "  batch 1800 loss: 1.2500388383865357\n",
      "  batch 1850 loss: 1.2452439951896668\n",
      "  batch 1900 loss: 1.2113220643997193\n",
      "  batch 1950 loss: 1.235167019367218\n",
      "  batch 2000 loss: 1.2330960202217103\n",
      "  batch 2050 loss: 1.1500956439971923\n",
      "  batch 2100 loss: 1.198404322862625\n",
      "  batch 2150 loss: 1.1655152094364167\n",
      "  batch 2200 loss: 1.1977226114273072\n",
      "  batch 2250 loss: 1.1832479667663574\n",
      "  batch 2300 loss: 1.1858806347846984\n",
      "  batch 2350 loss: 1.168787931203842\n",
      "  batch 2400 loss: 1.1333887994289398\n",
      "  batch 2450 loss: 1.1591587948799134\n",
      "  batch 2500 loss: 1.107270772457123\n",
      "  batch 2550 loss: 1.1042348766326904\n",
      "  batch 2600 loss: 1.0885139524936676\n",
      "  batch 2650 loss: 1.1106525027751923\n",
      "  batch 2700 loss: 1.1390354120731354\n",
      "  batch 2750 loss: 1.1101265919208527\n",
      "LOSS train 1.11013 valid 1.09700, valid PER 33.45%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.02181987285614\n",
      "  batch 100 loss: 1.052468057870865\n",
      "  batch 150 loss: 1.0630679202079774\n",
      "  batch 200 loss: 1.0594503438472749\n",
      "  batch 250 loss: 1.028010926246643\n",
      "  batch 300 loss: 1.0739660727977753\n",
      "  batch 350 loss: 1.0189118242263795\n",
      "  batch 400 loss: 1.0232006132602691\n",
      "  batch 450 loss: 1.0185226798057556\n",
      "  batch 500 loss: 1.0256020760536193\n",
      "  batch 550 loss: 1.0273608720302583\n",
      "  batch 600 loss: 1.034283629655838\n",
      "  batch 650 loss: 1.0496587014198304\n",
      "  batch 700 loss: 1.0210113823413849\n",
      "  batch 750 loss: 1.0167829751968385\n",
      "  batch 800 loss: 1.0058294570446014\n",
      "  batch 850 loss: 0.997413033246994\n",
      "  batch 900 loss: 0.9910232841968536\n",
      "  batch 950 loss: 1.0237039065361022\n",
      "  batch 1000 loss: 0.9938942098617554\n",
      "  batch 1050 loss: 0.9930748975276947\n",
      "  batch 1100 loss: 0.9966530525684356\n",
      "  batch 1150 loss: 0.9777324652671814\n",
      "  batch 1200 loss: 0.9952426934242249\n",
      "  batch 1250 loss: 0.9777723371982574\n",
      "  batch 1300 loss: 1.004870673418045\n",
      "  batch 1350 loss: 0.9710253846645355\n",
      "  batch 1400 loss: 0.9768579423427581\n",
      "  batch 1450 loss: 0.9668366801738739\n",
      "  batch 1500 loss: 0.9597460854053498\n",
      "  batch 1550 loss: 0.9693486201763153\n",
      "  batch 1600 loss: 0.9716300559043884\n",
      "  batch 1650 loss: 0.9508137369155883\n",
      "  batch 1700 loss: 0.9106059348583222\n",
      "  batch 1750 loss: 0.9791761255264282\n",
      "  batch 1800 loss: 0.9601623558998108\n",
      "  batch 1850 loss: 0.9556977117061615\n",
      "  batch 1900 loss: 0.966988331079483\n",
      "  batch 1950 loss: 0.9644177687168122\n",
      "  batch 2000 loss: 0.9113528966903687\n",
      "  batch 2050 loss: 0.9231084287166595\n",
      "  batch 2100 loss: 0.9210025227069855\n",
      "  batch 2150 loss: 0.9382009661197662\n",
      "  batch 2200 loss: 0.9229892599582672\n",
      "  batch 2250 loss: 0.9076440644264221\n",
      "  batch 2300 loss: 0.9244545328617096\n",
      "  batch 2350 loss: 0.9343715047836304\n",
      "  batch 2400 loss: 0.9260622882843017\n",
      "  batch 2450 loss: 0.93169553399086\n",
      "  batch 2500 loss: 0.9268209779262543\n",
      "  batch 2550 loss: 0.9172615420818329\n",
      "  batch 2600 loss: 0.8800278115272522\n",
      "  batch 2650 loss: 0.9172731459140777\n",
      "  batch 2700 loss: 0.9267352271080017\n",
      "  batch 2750 loss: 0.8819997882843018\n",
      "LOSS train 0.88200 valid 0.87725, valid PER 27.46%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 0.8571935164928436\n",
      "  batch 100 loss: 0.862009197473526\n",
      "  batch 150 loss: 0.8633980298042297\n",
      "  batch 200 loss: 0.8551205420494079\n",
      "  batch 250 loss: 0.8668395006656646\n",
      "  batch 300 loss: 0.8340925216674805\n",
      "  batch 350 loss: 0.872632657289505\n",
      "  batch 400 loss: 0.8470791018009186\n",
      "  batch 450 loss: 0.8533851754665375\n",
      "  batch 500 loss: 0.8780264568328857\n",
      "  batch 550 loss: 0.8687025159597397\n",
      "  batch 600 loss: 0.8483964610099792\n",
      "  batch 650 loss: 0.8458451461791993\n",
      "  batch 700 loss: 0.8445145988464355\n",
      "  batch 750 loss: 0.8515200513601303\n",
      "  batch 800 loss: 0.8703003370761871\n",
      "  batch 850 loss: 0.8486832273006439\n",
      "  batch 900 loss: 0.86519850730896\n",
      "  batch 950 loss: 0.8249264937639237\n",
      "  batch 1000 loss: 0.8527449870109558\n",
      "  batch 1050 loss: 0.8300652348995209\n",
      "  batch 1100 loss: 0.8197941792011261\n",
      "  batch 1150 loss: 0.8330668187141419\n",
      "  batch 1200 loss: 0.8178180158138275\n",
      "  batch 1250 loss: 0.8470166945457458\n",
      "  batch 1300 loss: 0.8318066036701203\n",
      "  batch 1350 loss: 0.8449289786815644\n",
      "  batch 1400 loss: 0.8554735863208771\n",
      "  batch 1450 loss: 0.8180477213859558\n",
      "  batch 1500 loss: 0.8534046769142151\n",
      "  batch 1550 loss: 0.8682743501663208\n",
      "  batch 1600 loss: 0.8125823163986206\n",
      "  batch 1650 loss: 0.8337523025274277\n",
      "  batch 1700 loss: 0.8274735569953918\n",
      "  batch 1750 loss: 0.8698528230190277\n",
      "  batch 1800 loss: 0.8406265532970428\n",
      "  batch 1850 loss: 0.8529216504096985\n",
      "  batch 1900 loss: 0.7951271545886993\n",
      "  batch 1950 loss: 0.8064624059200287\n",
      "  batch 2000 loss: 0.8172909557819367\n",
      "  batch 2050 loss: 0.8084269225597381\n",
      "  batch 2100 loss: 0.8380286228656769\n",
      "  batch 2150 loss: 0.7947087311744689\n",
      "  batch 2200 loss: 0.8044574701786041\n",
      "  batch 2250 loss: 0.8347403502464295\n",
      "  batch 2300 loss: 0.8294987344741821\n",
      "  batch 2350 loss: 0.8028243958950043\n",
      "  batch 2400 loss: 0.758725380897522\n",
      "  batch 2450 loss: 0.8165715551376342\n",
      "  batch 2500 loss: 0.7746391183137894\n",
      "  batch 2550 loss: 0.7802738738059998\n",
      "  batch 2600 loss: 0.7994475269317627\n",
      "  batch 2650 loss: 0.7906529068946838\n",
      "  batch 2700 loss: 0.8118510472774506\n",
      "  batch 2750 loss: 0.7464392274618149\n",
      "LOSS train 0.74644 valid 0.84048, valid PER 26.04%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.755718914270401\n",
      "  batch 100 loss: 0.8038717114925384\n",
      "  batch 150 loss: 0.7707852756977082\n",
      "  batch 200 loss: 0.7228434747457504\n",
      "  batch 250 loss: 0.7541400408744812\n",
      "  batch 300 loss: 0.7475747662782669\n",
      "  batch 350 loss: 0.7875903630256653\n",
      "  batch 400 loss: 0.7444280284643173\n",
      "  batch 450 loss: 0.7634753572940827\n",
      "  batch 500 loss: 0.7735880249738694\n",
      "  batch 550 loss: 0.7331344598531723\n",
      "  batch 600 loss: 0.7660328936576843\n",
      "  batch 650 loss: 0.753826711177826\n",
      "  batch 700 loss: 0.7611346518993378\n",
      "  batch 750 loss: 0.7651544934511185\n",
      "  batch 800 loss: 0.7749986714124679\n",
      "  batch 850 loss: 0.7687463313341141\n",
      "  batch 900 loss: 0.7720730757713318\n",
      "  batch 950 loss: 0.7897890079021453\n",
      "  batch 1000 loss: 0.7432732146978378\n",
      "  batch 1050 loss: 0.7525328981876374\n",
      "  batch 1100 loss: 0.7359047001600265\n",
      "  batch 1150 loss: 0.7035895049571991\n",
      "  batch 1200 loss: 0.7750053322315216\n",
      "  batch 1250 loss: 0.8073529458045959\n",
      "  batch 1300 loss: 0.7424623167514801\n",
      "  batch 1350 loss: 0.7276112264394761\n",
      "  batch 1400 loss: 0.7264885503053665\n",
      "  batch 1450 loss: 0.7070344072580338\n",
      "  batch 1500 loss: 0.75688740670681\n",
      "  batch 1550 loss: 0.8015064930915833\n",
      "  batch 1600 loss: 0.7616607558727264\n",
      "  batch 1650 loss: 0.7375698733329773\n",
      "  batch 1700 loss: 0.7136942231655121\n",
      "  batch 1750 loss: 0.7505156099796295\n",
      "  batch 1800 loss: 0.7425570440292358\n",
      "  batch 1850 loss: 0.7401966625452041\n",
      "  batch 1900 loss: 0.7022562289237976\n",
      "  batch 1950 loss: 0.7434845387935638\n",
      "  batch 2000 loss: 0.7591181510686874\n",
      "  batch 2050 loss: 0.779227192401886\n",
      "  batch 2100 loss: 0.739136027097702\n",
      "  batch 2150 loss: 0.7185481810569763\n",
      "  batch 2200 loss: 0.6846376222372055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 2250 loss: 0.7091731494665146\n",
      "  batch 2300 loss: 0.7480642765760421\n",
      "  batch 2350 loss: 0.7344293802976608\n",
      "  batch 2400 loss: 0.731310847401619\n",
      "  batch 2450 loss: 0.7389164853096009\n",
      "  batch 2500 loss: 0.7634649932384491\n",
      "  batch 2550 loss: 0.7452573519945145\n",
      "  batch 2600 loss: 0.7766497772932053\n",
      "  batch 2650 loss: 0.7569308924674988\n",
      "  batch 2700 loss: 0.7217086529731751\n",
      "  batch 2750 loss: 0.7362372517585755\n",
      "LOSS train 0.73624 valid 0.80403, valid PER 24.56%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.7183885109424591\n",
      "  batch 100 loss: 0.7048973381519318\n",
      "  batch 150 loss: 0.7154817104339599\n",
      "  batch 200 loss: 0.7148961740732193\n",
      "  batch 250 loss: 0.692699835896492\n",
      "  batch 300 loss: 0.7222865521907806\n",
      "  batch 350 loss: 0.739528289437294\n",
      "  batch 400 loss: 0.6970810341835022\n",
      "  batch 450 loss: 0.6971015375852585\n",
      "  batch 500 loss: 0.7457836675643921\n",
      "  batch 550 loss: 0.6872825926542282\n",
      "  batch 600 loss: 0.703405739068985\n",
      "  batch 650 loss: 0.6991213995218277\n",
      "  batch 700 loss: 0.7376431119441986\n",
      "  batch 750 loss: 0.7105456054210663\n",
      "  batch 800 loss: 0.711379560828209\n",
      "  batch 850 loss: 0.6677154541015625\n",
      "  batch 900 loss: 0.7139933782815934\n",
      "  batch 950 loss: 0.7278584438562393\n",
      "  batch 1000 loss: 0.6959517574310303\n",
      "  batch 1050 loss: 0.7120356208086014\n",
      "  batch 1100 loss: 0.6895015037059784\n",
      "  batch 1150 loss: 0.6752750903367997\n",
      "  batch 1200 loss: 0.6792161357402802\n",
      "  batch 1250 loss: 0.6907093542814254\n",
      "  batch 1300 loss: 0.6698159509897232\n",
      "  batch 1350 loss: 0.7230060911178589\n",
      "  batch 1400 loss: 0.6713522791862487\n",
      "  batch 1450 loss: 0.6854333883523941\n",
      "  batch 1500 loss: 0.7313513058423996\n",
      "  batch 1550 loss: 0.7076853996515274\n",
      "  batch 1600 loss: 0.6967065191268921\n",
      "  batch 1650 loss: 0.6750976437330246\n",
      "  batch 1700 loss: 0.703108804821968\n",
      "  batch 1750 loss: 0.6865394794940949\n",
      "  batch 1800 loss: 0.6832219153642655\n",
      "  batch 1850 loss: 0.6897481107711791\n",
      "  batch 1900 loss: 0.7104585123062134\n",
      "  batch 1950 loss: 0.6799851757287979\n",
      "  batch 2000 loss: 0.6998440676927566\n",
      "  batch 2050 loss: 0.713067398071289\n",
      "  batch 2100 loss: 0.6996351891756057\n",
      "  batch 2150 loss: 0.6892547190189362\n",
      "  batch 2200 loss: 0.70758833527565\n",
      "  batch 2250 loss: 0.7025964283943176\n",
      "  batch 2300 loss: 0.6781355798244476\n",
      "  batch 2350 loss: 0.6896383804082871\n",
      "  batch 2400 loss: 0.7139572781324387\n",
      "  batch 2450 loss: 0.6669910836219788\n",
      "  batch 2500 loss: 0.6599879544973374\n",
      "  batch 2550 loss: 0.6586016583442688\n",
      "  batch 2600 loss: 0.6875333249568939\n",
      "  batch 2650 loss: 0.6511816209554673\n",
      "  batch 2700 loss: 0.6554827272891999\n",
      "  batch 2750 loss: 0.6690168607234955\n",
      "LOSS train 0.66902 valid 0.79348, valid PER 23.86%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.6474501568078995\n",
      "  batch 100 loss: 0.6606144005060196\n",
      "  batch 150 loss: 0.6283014750480652\n",
      "  batch 200 loss: 0.6318155652284623\n",
      "  batch 250 loss: 0.6626412427425384\n",
      "  batch 300 loss: 0.6523143821954727\n",
      "  batch 350 loss: 0.6610790252685547\n",
      "  batch 400 loss: 0.6418590694665909\n",
      "  batch 450 loss: 0.6691668981313705\n",
      "  batch 500 loss: 0.6491225641965866\n",
      "  batch 550 loss: 0.6006934452056885\n",
      "  batch 600 loss: 0.6404662829637527\n",
      "  batch 650 loss: 0.6885035979747772\n",
      "  batch 700 loss: 0.6459293621778488\n",
      "  batch 750 loss: 0.6343783110380172\n",
      "  batch 800 loss: 0.6503181022405624\n",
      "  batch 850 loss: 0.6383161497116089\n",
      "  batch 900 loss: 0.6511884403228759\n",
      "  batch 950 loss: 0.6755383563041687\n",
      "  batch 1000 loss: 0.65772784948349\n",
      "  batch 1050 loss: 0.6372038662433624\n",
      "  batch 1100 loss: 0.6547941988706589\n",
      "  batch 1150 loss: 0.6255682909488678\n",
      "  batch 1200 loss: 0.654383298754692\n",
      "  batch 1250 loss: 0.6490442496538162\n",
      "  batch 1300 loss: 0.6312351435422897\n",
      "  batch 1350 loss: 0.6830365973711013\n",
      "  batch 1400 loss: 0.6539780306816101\n",
      "  batch 1450 loss: 0.6545523756742477\n",
      "  batch 1500 loss: 0.6407292711734772\n",
      "  batch 1550 loss: 0.647333573102951\n",
      "  batch 1600 loss: 0.6615480089187622\n",
      "  batch 1650 loss: 0.6636850666999817\n",
      "  batch 1700 loss: 0.6316174072027206\n",
      "  batch 1750 loss: 0.655461996793747\n",
      "  batch 1800 loss: 0.6550247430801391\n",
      "  batch 1850 loss: 0.6324540781974792\n",
      "  batch 1900 loss: 0.6487716192007065\n",
      "  batch 1950 loss: 0.6477150756120682\n",
      "  batch 2000 loss: 0.66065893471241\n",
      "  batch 2050 loss: 0.6279252743721009\n",
      "  batch 2100 loss: 0.6625950515270234\n",
      "  batch 2150 loss: 0.6222397011518478\n",
      "  batch 2200 loss: 0.6756358814239501\n",
      "  batch 2250 loss: 0.6297565054893494\n",
      "  batch 2300 loss: 0.6405829745531082\n",
      "  batch 2350 loss: 0.691577627658844\n",
      "  batch 2400 loss: 0.6290239453315735\n",
      "  batch 2450 loss: 0.6275507032871246\n",
      "  batch 2500 loss: 0.623262449502945\n",
      "  batch 2550 loss: 0.6632430917024612\n",
      "  batch 2600 loss: 0.6519468253850937\n",
      "  batch 2650 loss: 0.6678792411088943\n",
      "  batch 2700 loss: 0.6466053014993668\n",
      "  batch 2750 loss: 0.6204509323835373\n",
      "LOSS train 0.62045 valid 0.77372, valid PER 23.30%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.6035303086042404\n",
      "  batch 100 loss: 0.6087315738201141\n",
      "  batch 150 loss: 0.612048727273941\n",
      "  batch 200 loss: 0.6129729217290878\n",
      "  batch 250 loss: 0.6052736485004425\n",
      "  batch 300 loss: 0.5970074093341827\n",
      "  batch 350 loss: 0.6239307790994644\n",
      "  batch 400 loss: 0.5995725893974304\n",
      "  batch 450 loss: 0.5828571408987046\n",
      "  batch 500 loss: 0.6026125073432922\n",
      "  batch 550 loss: 0.6049673891067505\n",
      "  batch 600 loss: 0.5741965508460999\n",
      "  batch 650 loss: 0.6065326607227326\n",
      "  batch 700 loss: 0.6104139685630798\n",
      "  batch 750 loss: 0.6064569610357284\n",
      "  batch 800 loss: 0.6267186921834945\n",
      "  batch 850 loss: 0.6213044768571854\n",
      "  batch 900 loss: 0.6264673107862473\n",
      "  batch 950 loss: 0.6108160120248795\n",
      "  batch 1000 loss: 0.6074810987710952\n",
      "  batch 1050 loss: 0.587899363040924\n",
      "  batch 1100 loss: 0.5878795045614242\n",
      "  batch 1150 loss: 0.6085271883010864\n",
      "  batch 1200 loss: 0.6254249745607376\n",
      "  batch 1250 loss: 0.632106174826622\n",
      "  batch 1300 loss: 0.637046919465065\n",
      "  batch 1350 loss: 0.6006295871734619\n",
      "  batch 1400 loss: 0.5908534836769104\n",
      "  batch 1450 loss: 0.6185944193601608\n",
      "  batch 1500 loss: 0.601718966960907\n",
      "  batch 1550 loss: 0.6141185021400452\n",
      "  batch 1600 loss: 0.6397691738605499\n",
      "  batch 1650 loss: 0.6470784336328507\n",
      "  batch 1700 loss: 0.6420913237333298\n",
      "  batch 1750 loss: 0.6003999280929565\n",
      "  batch 1800 loss: 0.611273638010025\n",
      "  batch 1850 loss: 0.6438368463516235\n",
      "  batch 1900 loss: 0.623126564025879\n",
      "  batch 1950 loss: 0.6549608761072159\n",
      "  batch 2000 loss: 0.6127115297317505\n",
      "  batch 2050 loss: 0.6473883062601089\n",
      "  batch 2100 loss: 0.6108903980255127\n",
      "  batch 2150 loss: 0.6339086842536926\n",
      "  batch 2200 loss: 0.6167166358232499\n",
      "  batch 2250 loss: 0.6336220198869705\n",
      "  batch 2300 loss: 0.6387246692180634\n",
      "  batch 2350 loss: 0.6236807245016098\n",
      "  batch 2400 loss: 0.6149358612298965\n",
      "  batch 2450 loss: 0.6294217580556869\n",
      "  batch 2500 loss: 0.6252766597270966\n",
      "  batch 2550 loss: 0.6027533489465714\n",
      "  batch 2600 loss: 0.6243562698364258\n",
      "  batch 2650 loss: 0.5668412917852401\n",
      "  batch 2700 loss: 0.6168141448497773\n",
      "  batch 2750 loss: 0.6224893820285797\n",
      "LOSS train 0.62249 valid 0.76286, valid PER 22.80%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.572080271244049\n",
      "  batch 100 loss: 0.575404976606369\n",
      "  batch 150 loss: 0.5857699626684189\n",
      "  batch 200 loss: 0.5471862113475799\n",
      "  batch 250 loss: 0.5584952539205551\n",
      "  batch 300 loss: 0.5887370973825454\n",
      "  batch 350 loss: 0.5843080592155456\n",
      "  batch 400 loss: 0.6135468703508377\n",
      "  batch 450 loss: 0.5929002290964127\n",
      "  batch 500 loss: 0.5747267544269562\n",
      "  batch 550 loss: 0.5720551389455796\n",
      "  batch 600 loss: 0.5566795414686203\n",
      "  batch 650 loss: 0.5729492020606994\n",
      "  batch 700 loss: 0.5652724200487137\n",
      "  batch 750 loss: 0.5888256525993347\n",
      "  batch 800 loss: 0.5485957318544388\n",
      "  batch 850 loss: 0.5617656487226487\n",
      "  batch 900 loss: 0.5694666033983231\n",
      "  batch 950 loss: 0.6224107968807221\n",
      "  batch 1000 loss: 0.5940143805742264\n",
      "  batch 1050 loss: 0.5651280760765076\n",
      "  batch 1100 loss: 0.55225710272789\n",
      "  batch 1150 loss: 0.5761468017101288\n",
      "  batch 1200 loss: 0.570849352478981\n",
      "  batch 1250 loss: 0.587637968659401\n",
      "  batch 1300 loss: 0.5856100833415985\n",
      "  batch 1350 loss: 0.5890573960542679\n",
      "  batch 1400 loss: 0.5961118829250336\n",
      "  batch 1450 loss: 0.601048601269722\n",
      "  batch 1500 loss: 0.5663906019926072\n",
      "  batch 1550 loss: 0.6105845475196838\n",
      "  batch 1600 loss: 0.5931122267246246\n",
      "  batch 1650 loss: 0.5877044183015824\n",
      "  batch 1700 loss: 0.5779884386062623\n",
      "  batch 1750 loss: 0.576178013086319\n",
      "  batch 1800 loss: 0.60513518512249\n",
      "  batch 1850 loss: 0.5700159132480621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1900 loss: 0.6102788025140762\n",
      "  batch 1950 loss: 0.5813162809610367\n",
      "  batch 2000 loss: 0.5789337688684464\n",
      "  batch 2050 loss: 0.5748624390363694\n",
      "  batch 2100 loss: 0.5952368766069412\n",
      "  batch 2150 loss: 0.5845299732685089\n",
      "  batch 2200 loss: 0.581552860736847\n",
      "  batch 2250 loss: 0.5976512950658798\n",
      "  batch 2300 loss: 0.583308116197586\n",
      "  batch 2350 loss: 0.5868609386682511\n",
      "  batch 2400 loss: 0.5774581253528595\n",
      "  batch 2450 loss: 0.5500998955965042\n",
      "  batch 2500 loss: 0.5901757258176804\n",
      "  batch 2550 loss: 0.5864786577224731\n",
      "  batch 2600 loss: 0.573754026889801\n",
      "  batch 2650 loss: 0.5658316117525101\n",
      "  batch 2700 loss: 0.6051390582323074\n",
      "  batch 2750 loss: 0.5844399404525756\n",
      "Epoch 00008: reducing learning rate of group 0 to 5.0000e-01.\n",
      "LOSS train 0.58444 valid 0.77965, valid PER 23.18%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.531273792386055\n",
      "  batch 100 loss: 0.5372254604101181\n",
      "  batch 150 loss: 0.505369490981102\n",
      "  batch 200 loss: 0.5084722268581391\n",
      "  batch 250 loss: 0.48896885335445406\n",
      "  batch 300 loss: 0.4953343814611435\n",
      "  batch 350 loss: 0.5042720174789429\n",
      "  batch 400 loss: 0.49393162786960604\n",
      "  batch 450 loss: 0.4800997859239578\n",
      "  batch 500 loss: 0.5030498331785203\n",
      "  batch 550 loss: 0.5023687821626663\n",
      "  batch 600 loss: 0.49539304196834566\n",
      "  batch 650 loss: 0.4810060864686966\n",
      "  batch 700 loss: 0.4436134368181229\n",
      "  batch 750 loss: 0.4741231465339661\n",
      "  batch 800 loss: 0.4800161150097847\n",
      "  batch 850 loss: 0.490084715783596\n",
      "  batch 900 loss: 0.474859294295311\n",
      "  batch 950 loss: 0.4801680052280426\n",
      "  batch 1000 loss: 0.474515061378479\n",
      "  batch 1050 loss: 0.43845146626234055\n",
      "  batch 1100 loss: 0.4901993280649185\n",
      "  batch 1150 loss: 0.456961527466774\n",
      "  batch 1200 loss: 0.45746161818504333\n",
      "  batch 1250 loss: 0.4797081565856934\n",
      "  batch 1300 loss: 0.497540208697319\n",
      "  batch 1350 loss: 0.4520282870531082\n",
      "  batch 1400 loss: 0.4803329762816429\n",
      "  batch 1450 loss: 0.472282789349556\n",
      "  batch 1500 loss: 0.4555820307135582\n",
      "  batch 1550 loss: 0.4771911412477493\n",
      "  batch 1600 loss: 0.42781648099422454\n",
      "  batch 1650 loss: 0.47556741535663605\n",
      "  batch 1700 loss: 0.46688120901584623\n",
      "  batch 1750 loss: 0.46252711772918703\n",
      "  batch 1800 loss: 0.4318874028325081\n",
      "  batch 1850 loss: 0.49425981789827345\n",
      "  batch 1900 loss: 0.47252065062522886\n",
      "  batch 1950 loss: 0.48529920279979705\n",
      "  batch 2000 loss: 0.4690651285648346\n",
      "  batch 2050 loss: 0.49562725603580476\n",
      "  batch 2100 loss: 0.47413118720054626\n",
      "  batch 2150 loss: 0.48193170964717863\n",
      "  batch 2200 loss: 0.4793845081329346\n",
      "  batch 2250 loss: 0.45367029041051865\n",
      "  batch 2300 loss: 0.4352916729450226\n",
      "  batch 2350 loss: 0.4491964855790138\n",
      "  batch 2400 loss: 0.4953378260135651\n",
      "  batch 2450 loss: 0.46959628283977506\n",
      "  batch 2500 loss: 0.4612367114424705\n",
      "  batch 2550 loss: 0.4670878690481186\n",
      "  batch 2600 loss: 0.4508444398641586\n",
      "  batch 2650 loss: 0.4656188929080963\n",
      "  batch 2700 loss: 0.44987561166286466\n",
      "  batch 2750 loss: 0.44961496353149416\n",
      "LOSS train 0.44961 valid 0.72138, valid PER 21.22%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.4385651141405106\n",
      "  batch 100 loss: 0.44098019003868105\n",
      "  batch 150 loss: 0.45421755135059355\n",
      "  batch 200 loss: 0.42700649946928027\n",
      "  batch 250 loss: 0.42012158274650574\n",
      "  batch 300 loss: 0.46701244175434115\n",
      "  batch 350 loss: 0.4249836990237236\n",
      "  batch 400 loss: 0.4751358953118324\n",
      "  batch 450 loss: 0.4357497555017471\n",
      "  batch 500 loss: 0.4419477352499962\n",
      "  batch 550 loss: 0.4338224858045578\n",
      "  batch 600 loss: 0.4247330743074417\n",
      "  batch 650 loss: 0.44176843255758286\n",
      "  batch 700 loss: 0.4583505588769913\n",
      "  batch 750 loss: 0.4142599684000015\n",
      "  batch 800 loss: 0.4280215641856194\n",
      "  batch 850 loss: 0.437231125831604\n",
      "  batch 900 loss: 0.44563654899597166\n",
      "  batch 950 loss: 0.4525972527265549\n",
      "  batch 1000 loss: 0.43738540649414065\n",
      "  batch 1050 loss: 0.43362514972686766\n",
      "  batch 1100 loss: 0.4108952662348747\n",
      "  batch 1150 loss: 0.42351138830184937\n",
      "  batch 1200 loss: 0.4514708757400513\n",
      "  batch 1250 loss: 0.43128355860710144\n",
      "  batch 1300 loss: 0.47342919051647187\n",
      "  batch 1350 loss: 0.4687235680222511\n",
      "  batch 1400 loss: 0.44052778840065004\n",
      "  batch 1450 loss: 0.4060457381606102\n",
      "  batch 1500 loss: 0.4392178928852081\n",
      "  batch 1550 loss: 0.44284180700778963\n",
      "  batch 1600 loss: 0.4445092648267746\n",
      "  batch 1650 loss: 0.42685361742973327\n",
      "  batch 1700 loss: 0.46902553498744964\n",
      "  batch 1750 loss: 0.45501413583755496\n",
      "  batch 1800 loss: 0.42119189620018005\n",
      "  batch 1850 loss: 0.46719950139522554\n",
      "  batch 1900 loss: 0.43749497175216673\n",
      "  batch 1950 loss: 0.41572393357753756\n",
      "  batch 2000 loss: 0.43905558437108994\n",
      "  batch 2050 loss: 0.45813872694969177\n",
      "  batch 2100 loss: 0.43837821841239927\n",
      "  batch 2150 loss: 0.4056222042441368\n",
      "  batch 2200 loss: 0.45458488166332245\n",
      "  batch 2250 loss: 0.4309294056892395\n",
      "  batch 2300 loss: 0.4408828604221344\n",
      "  batch 2350 loss: 0.44593353390693663\n",
      "  batch 2400 loss: 0.4299232345819473\n",
      "  batch 2450 loss: 0.4259378743171692\n",
      "  batch 2500 loss: 0.44067802786827087\n",
      "  batch 2550 loss: 0.4411438226699829\n",
      "  batch 2600 loss: 0.4164428183436394\n",
      "  batch 2650 loss: 0.4273046678304672\n",
      "  batch 2700 loss: 0.44171650648117067\n",
      "  batch 2750 loss: 0.43940950572490695\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.43941 valid 0.73680, valid PER 21.00%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.4001760205626488\n",
      "  batch 100 loss: 0.4217927569150925\n",
      "  batch 150 loss: 0.3795801907777786\n",
      "  batch 200 loss: 0.38656820118427276\n",
      "  batch 250 loss: 0.399102423787117\n",
      "  batch 300 loss: 0.41085501670837404\n",
      "  batch 350 loss: 0.4015872317552567\n",
      "  batch 400 loss: 0.3918844759464264\n",
      "  batch 450 loss: 0.3899825975298882\n",
      "  batch 500 loss: 0.3826492649316788\n",
      "  batch 550 loss: 0.40691480100154875\n",
      "  batch 600 loss: 0.3999854537844658\n",
      "  batch 650 loss: 0.3596129649877548\n",
      "  batch 700 loss: 0.3700017940998077\n",
      "  batch 750 loss: 0.37734637171030044\n",
      "  batch 800 loss: 0.3956328189373016\n",
      "  batch 850 loss: 0.39221164733171465\n",
      "  batch 900 loss: 0.382517549097538\n",
      "  batch 950 loss: 0.3812632745504379\n",
      "  batch 1000 loss: 0.395463407933712\n",
      "  batch 1050 loss: 0.3586800280213356\n",
      "  batch 1100 loss: 0.37676397562026975\n",
      "  batch 1150 loss: 0.3675898227095604\n",
      "  batch 1200 loss: 0.40385358273983\n",
      "  batch 1250 loss: 0.40188288390636445\n",
      "  batch 1300 loss: 0.38454352229833605\n",
      "  batch 1350 loss: 0.3860259512066841\n",
      "  batch 1400 loss: 0.36505872815847396\n",
      "  batch 1450 loss: 0.37456057608127596\n",
      "  batch 1500 loss: 0.37172055423259737\n",
      "  batch 1550 loss: 0.392379067838192\n",
      "  batch 1600 loss: 0.3512908411026001\n",
      "  batch 1650 loss: 0.3964057752490044\n",
      "  batch 1700 loss: 0.3681382492184639\n",
      "  batch 1750 loss: 0.3566388210654259\n",
      "  batch 1800 loss: 0.37437104523181913\n",
      "  batch 1850 loss: 0.37346646666526795\n",
      "  batch 1900 loss: 0.3963208565115929\n",
      "  batch 1950 loss: 0.40022914320230485\n",
      "  batch 2000 loss: 0.36219492942094805\n",
      "  batch 2050 loss: 0.36066608160734176\n",
      "  batch 2100 loss: 0.39640991926193236\n",
      "  batch 2150 loss: 0.3718946218490601\n",
      "  batch 2200 loss: 0.37405963718891144\n",
      "  batch 2250 loss: 0.3629811784625053\n",
      "  batch 2300 loss: 0.38374000251293183\n",
      "  batch 2350 loss: 0.35260317265987395\n",
      "  batch 2400 loss: 0.36461684584617615\n",
      "  batch 2450 loss: 0.36167716920375825\n",
      "  batch 2500 loss: 0.3798163297772408\n",
      "  batch 2550 loss: 0.39744288951158524\n",
      "  batch 2600 loss: 0.401148653626442\n",
      "  batch 2650 loss: 0.39993574291467665\n",
      "  batch 2700 loss: 0.3735958543419838\n",
      "  batch 2750 loss: 0.3798753976821899\n",
      "LOSS train 0.37988 valid 0.71468, valid PER 20.65%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.3449489298462868\n",
      "  batch 100 loss: 0.36139885485172274\n",
      "  batch 150 loss: 0.3633000513911247\n",
      "  batch 200 loss: 0.3724912092089653\n",
      "  batch 250 loss: 0.36724479973316193\n",
      "  batch 300 loss: 0.34501287668943403\n",
      "  batch 350 loss: 0.37778405904769896\n",
      "  batch 400 loss: 0.34816344648599623\n",
      "  batch 450 loss: 0.3613560131192207\n",
      "  batch 500 loss: 0.36685725063085556\n",
      "  batch 550 loss: 0.3641022816300392\n",
      "  batch 600 loss: 0.34187147200107576\n",
      "  batch 650 loss: 0.35132776737213134\n",
      "  batch 700 loss: 0.34163952231407163\n",
      "  batch 750 loss: 0.34361639589071274\n",
      "  batch 800 loss: 0.3823361128568649\n",
      "  batch 850 loss: 0.36524750411510465\n",
      "  batch 900 loss: 0.3788760170340538\n",
      "  batch 950 loss: 0.3462960773706436\n",
      "  batch 1000 loss: 0.35285725504159926\n",
      "  batch 1050 loss: 0.3582326954603195\n",
      "  batch 1100 loss: 0.34974598556756975\n",
      "  batch 1150 loss: 0.3539847204089165\n",
      "  batch 1200 loss: 0.3690492260456085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1250 loss: 0.3650440439581871\n",
      "  batch 1300 loss: 0.35872744739055634\n",
      "  batch 1350 loss: 0.3750202488899231\n",
      "  batch 1400 loss: 0.36659381568431854\n",
      "  batch 1450 loss: 0.37849836975336076\n",
      "  batch 1500 loss: 0.3701036471128464\n",
      "  batch 1550 loss: 0.36179919451475145\n",
      "  batch 1600 loss: 0.3550183042883873\n",
      "  batch 1650 loss: 0.35201298773288725\n",
      "  batch 1700 loss: 0.34569903552532194\n",
      "  batch 1750 loss: 0.353071391582489\n",
      "  batch 1800 loss: 0.359297431409359\n",
      "  batch 1850 loss: 0.3382202282547951\n",
      "  batch 1900 loss: 0.3508001750707626\n",
      "  batch 1950 loss: 0.37884748697280884\n",
      "  batch 2000 loss: 0.3603817707300186\n",
      "  batch 2050 loss: 0.3463579028844833\n",
      "  batch 2100 loss: 0.3604710686206818\n",
      "  batch 2150 loss: 0.3342709872126579\n",
      "  batch 2200 loss: 0.3646461334824562\n",
      "  batch 2250 loss: 0.38544065952301027\n",
      "  batch 2300 loss: 0.3493233835697174\n",
      "  batch 2350 loss: 0.37590689957141876\n",
      "  batch 2400 loss: 0.34440348446369173\n",
      "  batch 2450 loss: 0.37044745445251465\n",
      "  batch 2500 loss: 0.34114893823862075\n",
      "  batch 2550 loss: 0.3810222783684731\n",
      "  batch 2600 loss: 0.35604541391134265\n",
      "  batch 2650 loss: 0.36044516682624816\n",
      "  batch 2700 loss: 0.33929659783840177\n",
      "  batch 2750 loss: 0.36948264241218565\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.36948 valid 0.72816, valid PER 20.52%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.32871744215488435\n",
      "  batch 100 loss: 0.33155561685562135\n",
      "  batch 150 loss: 0.3506921842694283\n",
      "  batch 200 loss: 0.36198235988616945\n",
      "  batch 250 loss: 0.32938910245895386\n",
      "  batch 300 loss: 0.3239798977971077\n",
      "  batch 350 loss: 0.3399020394682884\n",
      "  batch 400 loss: 0.33964011549949646\n",
      "  batch 450 loss: 0.33508271753787994\n",
      "  batch 500 loss: 0.3256592038273811\n",
      "  batch 550 loss: 0.33045202314853667\n",
      "  batch 600 loss: 0.3370291927456856\n",
      "  batch 650 loss: 0.3350581181049347\n",
      "  batch 700 loss: 0.333280523121357\n",
      "  batch 750 loss: 0.3501532590389252\n",
      "  batch 800 loss: 0.31763631612062454\n",
      "  batch 850 loss: 0.32979768335819243\n",
      "  batch 900 loss: 0.3601238054037094\n",
      "  batch 950 loss: 0.3236023163795471\n",
      "  batch 1000 loss: 0.33412164092063906\n",
      "  batch 1050 loss: 0.3465102100372314\n",
      "  batch 1100 loss: 0.34067937433719636\n",
      "  batch 1150 loss: 0.32962620109319685\n",
      "  batch 1200 loss: 0.3239940106868744\n",
      "  batch 1250 loss: 0.329234231710434\n",
      "  batch 1300 loss: 0.3317581781744957\n",
      "  batch 1350 loss: 0.32402723520994187\n",
      "  batch 1400 loss: 0.3460672864317894\n",
      "  batch 1450 loss: 0.33830758452415466\n",
      "  batch 1500 loss: 0.34207237407565116\n",
      "  batch 1550 loss: 0.32995180308818817\n",
      "  batch 1600 loss: 0.3310833477973938\n",
      "  batch 1650 loss: 0.33020291894674303\n",
      "  batch 1700 loss: 0.34562902569770815\n",
      "  batch 1750 loss: 0.3236043158173561\n",
      "  batch 1800 loss: 0.33374405920505523\n",
      "  batch 1850 loss: 0.3298231020569801\n",
      "  batch 1900 loss: 0.33070444971323015\n",
      "  batch 1950 loss: 0.3088585370779037\n",
      "  batch 2000 loss: 0.3336341759562492\n",
      "  batch 2050 loss: 0.31589053362607955\n",
      "  batch 2100 loss: 0.34940490037202837\n",
      "  batch 2150 loss: 0.32496796011924745\n",
      "  batch 2200 loss: 0.31831237554550174\n",
      "  batch 2250 loss: 0.32610376328229906\n",
      "  batch 2300 loss: 0.3294531100988388\n",
      "  batch 2350 loss: 0.31531377136707306\n",
      "  batch 2400 loss: 0.3388858786225319\n",
      "  batch 2450 loss: 0.3212196323275566\n",
      "  batch 2500 loss: 0.31778605937957766\n",
      "  batch 2550 loss: 0.30954040080308914\n",
      "  batch 2600 loss: 0.3218597981333733\n",
      "  batch 2650 loss: 0.33441270917654037\n",
      "  batch 2700 loss: 0.3319428214430809\n",
      "  batch 2750 loss: 0.3427619257569313\n",
      "Epoch 00013: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.34276 valid 0.72830, valid PER 20.33%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.31613546460866926\n",
      "  batch 100 loss: 0.3163193339109421\n",
      "  batch 150 loss: 0.31978786826133726\n",
      "  batch 200 loss: 0.30737979352474215\n",
      "  batch 250 loss: 0.31513787031173707\n",
      "  batch 300 loss: 0.31835918068885805\n",
      "  batch 350 loss: 0.33834425777196886\n",
      "  batch 400 loss: 0.3148066660761833\n",
      "  batch 450 loss: 0.3442722561955452\n",
      "  batch 500 loss: 0.30668746381998063\n",
      "  batch 550 loss: 0.3148975366353989\n",
      "  batch 600 loss: 0.3190967804193497\n",
      "  batch 650 loss: 0.311477407515049\n",
      "  batch 700 loss: 0.3069104501605034\n",
      "  batch 750 loss: 0.3137828269600868\n",
      "  batch 800 loss: 0.32036941677331926\n",
      "  batch 850 loss: 0.3195901855826378\n",
      "  batch 900 loss: 0.3172532996535301\n",
      "  batch 950 loss: 0.32972085148096086\n",
      "  batch 1000 loss: 0.33328257024288177\n",
      "  batch 1050 loss: 0.32367687463760375\n",
      "  batch 1100 loss: 0.3157757906615734\n",
      "  batch 1150 loss: 0.3074322003126144\n",
      "  batch 1200 loss: 0.29492456376552584\n",
      "  batch 1250 loss: 0.3218590927124023\n",
      "  batch 1300 loss: 0.3137238270044327\n",
      "  batch 1350 loss: 0.30951170563697816\n",
      "  batch 1400 loss: 0.3254278174042702\n",
      "  batch 1450 loss: 0.3232183390855789\n",
      "  batch 1500 loss: 0.3033675217628479\n",
      "  batch 1550 loss: 0.3017269682884216\n",
      "  batch 1600 loss: 0.3255824249982834\n",
      "  batch 1650 loss: 0.3245181420445442\n",
      "  batch 1700 loss: 0.33438475549221036\n",
      "  batch 1750 loss: 0.3082787947356701\n",
      "  batch 1800 loss: 0.3116880652308464\n",
      "  batch 1850 loss: 0.3320856948196888\n",
      "  batch 1900 loss: 0.30011421382427217\n",
      "  batch 1950 loss: 0.32321247905492784\n",
      "  batch 2000 loss: 0.29771295994520186\n",
      "  batch 2050 loss: 0.29746774554252625\n",
      "  batch 2100 loss: 0.31011328727006915\n",
      "  batch 2150 loss: 0.3096479314565659\n",
      "  batch 2200 loss: 0.3095171612501144\n",
      "  batch 2250 loss: 0.30591373920440673\n",
      "  batch 2300 loss: 0.3110673502087593\n",
      "  batch 2350 loss: 0.31880578070878984\n",
      "  batch 2400 loss: 0.325928440541029\n",
      "  batch 2450 loss: 0.3133092501759529\n",
      "  batch 2500 loss: 0.31132985919713974\n",
      "  batch 2550 loss: 0.31200838565826416\n",
      "  batch 2600 loss: 0.3057688656449318\n",
      "  batch 2650 loss: 0.31261843055486677\n",
      "  batch 2700 loss: 0.2983432811498642\n",
      "  batch 2750 loss: 0.33784716099500656\n",
      "Epoch 00014: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.33785 valid 0.73110, valid PER 20.17%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.30331990599632264\n",
      "  batch 100 loss: 0.30820575803518296\n",
      "  batch 150 loss: 0.33545711934566497\n",
      "  batch 200 loss: 0.30092798739671706\n",
      "  batch 250 loss: 0.2868770852684975\n",
      "  batch 300 loss: 0.30612059593200686\n",
      "  batch 350 loss: 0.2983920061588287\n",
      "  batch 400 loss: 0.30935451239347456\n",
      "  batch 450 loss: 0.30443923771381376\n",
      "  batch 500 loss: 0.2981484180688858\n",
      "  batch 550 loss: 0.29690556272864344\n",
      "  batch 600 loss: 0.32636289834976195\n",
      "  batch 650 loss: 0.3139032113552094\n",
      "  batch 700 loss: 0.31647590428590777\n",
      "  batch 750 loss: 0.30432030886411665\n",
      "  batch 800 loss: 0.3070032948255539\n",
      "  batch 850 loss: 0.29815523624420165\n",
      "  batch 900 loss: 0.30395046770572665\n",
      "  batch 950 loss: 0.29002880603075026\n",
      "  batch 1000 loss: 0.31031860411167145\n",
      "  batch 1050 loss: 0.31011164337396624\n",
      "  batch 1100 loss: 0.28374353975057603\n",
      "  batch 1150 loss: 0.3000420826673508\n",
      "  batch 1200 loss: 0.3079428485035896\n",
      "  batch 1250 loss: 0.3043974652886391\n",
      "  batch 1300 loss: 0.298796466588974\n",
      "  batch 1350 loss: 0.31314316898584366\n",
      "  batch 1400 loss: 0.31955419391393663\n",
      "  batch 1450 loss: 0.30685214698314667\n",
      "  batch 1500 loss: 0.29688645362854005\n",
      "  batch 1550 loss: 0.3099880051612854\n",
      "  batch 1600 loss: 0.31882486164569857\n",
      "  batch 1650 loss: 0.30138848841190335\n",
      "  batch 1700 loss: 0.3328093200922012\n",
      "  batch 1750 loss: 0.3157158860564232\n",
      "  batch 1800 loss: 0.3034529611468315\n",
      "  batch 1850 loss: 0.3267305183410645\n",
      "  batch 1900 loss: 0.2933206185698509\n",
      "  batch 1950 loss: 0.3074749779701233\n",
      "  batch 2000 loss: 0.28740273863077165\n",
      "  batch 2050 loss: 0.3018124437332153\n",
      "  batch 2100 loss: 0.3098131886124611\n",
      "  batch 2150 loss: 0.3177557209134102\n",
      "  batch 2200 loss: 0.3208402681350708\n",
      "  batch 2250 loss: 0.31265809953212736\n",
      "  batch 2300 loss: 0.30445282012224195\n",
      "  batch 2350 loss: 0.30919304102659223\n",
      "  batch 2400 loss: 0.313100660443306\n",
      "  batch 2450 loss: 0.31678377360105514\n",
      "  batch 2500 loss: 0.3003053268790245\n",
      "  batch 2550 loss: 0.29433046013116837\n",
      "  batch 2600 loss: 0.30825370132923124\n",
      "  batch 2650 loss: 0.29962811917066573\n",
      "  batch 2700 loss: 0.317833571434021\n",
      "  batch 2750 loss: 0.3096484962105751\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.30965 valid 0.73073, valid PER 20.16%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.3008905759453773\n",
      "  batch 100 loss: 0.31875820845365527\n",
      "  batch 150 loss: 0.31086047276854517\n",
      "  batch 200 loss: 0.29951534390449525\n",
      "  batch 250 loss: 0.31334260255098345\n",
      "  batch 300 loss: 0.29992957204580306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 350 loss: 0.2966096645593643\n",
      "  batch 400 loss: 0.312538258433342\n",
      "  batch 450 loss: 0.3097683009505272\n",
      "  batch 500 loss: 0.2989265415072441\n",
      "  batch 550 loss: 0.30231568694114686\n",
      "  batch 600 loss: 0.293539477288723\n",
      "  batch 650 loss: 0.3082249069213867\n",
      "  batch 700 loss: 0.326841002702713\n",
      "  batch 750 loss: 0.3145700228214264\n",
      "  batch 800 loss: 0.2812103289365768\n",
      "  batch 850 loss: 0.30074149429798125\n",
      "  batch 900 loss: 0.3004875096678734\n",
      "  batch 950 loss: 0.2937307956814766\n",
      "  batch 1000 loss: 0.31693636953830717\n",
      "  batch 1050 loss: 0.29798136264085767\n",
      "  batch 1100 loss: 0.3015220034122467\n",
      "  batch 1150 loss: 0.30834407806396485\n",
      "  batch 1200 loss: 0.3042308434844017\n",
      "  batch 1250 loss: 0.27773296505212786\n",
      "  batch 1300 loss: 0.3282755324244499\n",
      "  batch 1350 loss: 0.30770562052726746\n",
      "  batch 1400 loss: 0.2931290239095688\n",
      "  batch 1450 loss: 0.3077496710419655\n",
      "  batch 1500 loss: 0.31754421412944794\n",
      "  batch 1550 loss: 0.2962141454219818\n",
      "  batch 1600 loss: 0.3088872143626213\n",
      "  batch 1650 loss: 0.3052260392904282\n",
      "  batch 1700 loss: 0.271123765707016\n",
      "  batch 1750 loss: 0.3004573222994804\n",
      "  batch 1800 loss: 0.2959734709560871\n",
      "  batch 1850 loss: 0.29281718522310257\n",
      "  batch 1900 loss: 0.29529296964406965\n",
      "  batch 1950 loss: 0.30473426342010496\n",
      "  batch 2000 loss: 0.3030926921963692\n",
      "  batch 2050 loss: 0.29240426629781724\n",
      "  batch 2100 loss: 0.2966936579346657\n",
      "  batch 2150 loss: 0.3020985820889473\n",
      "  batch 2200 loss: 0.31865749359130857\n",
      "  batch 2250 loss: 0.30270020455121993\n",
      "  batch 2300 loss: 0.311349101960659\n",
      "  batch 2350 loss: 0.2918915283679962\n",
      "  batch 2400 loss: 0.32219686001539233\n",
      "  batch 2450 loss: 0.29487688302993775\n",
      "  batch 2500 loss: 0.30038338154554367\n",
      "  batch 2550 loss: 0.30951577812433245\n",
      "  batch 2600 loss: 0.3074319815635681\n",
      "  batch 2650 loss: 0.29792835682630536\n",
      "  batch 2700 loss: 0.31185305416584014\n",
      "  batch 2750 loss: 0.2954079020023346\n",
      "Epoch 00016: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.29541 valid 0.73158, valid PER 20.08%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.308918751180172\n",
      "  batch 100 loss: 0.2902975413203239\n",
      "  batch 150 loss: 0.30638956040143966\n",
      "  batch 200 loss: 0.31486753821372987\n",
      "  batch 250 loss: 0.297768340408802\n",
      "  batch 300 loss: 0.2983840548992157\n",
      "  batch 350 loss: 0.3191099688410759\n",
      "  batch 400 loss: 0.2834421464800835\n",
      "  batch 450 loss: 0.292324595451355\n",
      "  batch 500 loss: 0.30316654801368714\n",
      "  batch 550 loss: 0.3007617709040642\n",
      "  batch 600 loss: 0.2855415037274361\n",
      "  batch 650 loss: 0.2930290211737156\n",
      "  batch 700 loss: 0.298270942568779\n",
      "  batch 750 loss: 0.3156759124994278\n",
      "  batch 800 loss: 0.32077991783618925\n",
      "  batch 850 loss: 0.28012092158198354\n",
      "  batch 900 loss: 0.29852457344532013\n",
      "  batch 950 loss: 0.3029477733373642\n",
      "  batch 1000 loss: 0.29317858785390855\n",
      "  batch 1050 loss: 0.30562889218330386\n",
      "  batch 1100 loss: 0.30507432073354723\n",
      "  batch 1150 loss: 0.2946649357676506\n",
      "  batch 1200 loss: 0.29650716334581373\n",
      "  batch 1250 loss: 0.30560913026332853\n",
      "  batch 1300 loss: 0.30430457055568694\n",
      "  batch 1350 loss: 0.301091146171093\n",
      "  batch 1400 loss: 0.31161040246486665\n",
      "  batch 1450 loss: 0.30622800678014755\n",
      "  batch 1500 loss: 0.2876079371571541\n",
      "  batch 1550 loss: 0.30675574511289594\n",
      "  batch 1600 loss: 0.3262363198399544\n",
      "  batch 1650 loss: 0.28815476208925245\n",
      "  batch 1700 loss: 0.29964060068130494\n",
      "  batch 1750 loss: 0.29679615154862404\n",
      "  batch 1800 loss: 0.29012865900993345\n",
      "  batch 1850 loss: 0.29283732622861863\n",
      "  batch 1900 loss: 0.2840510106086731\n",
      "  batch 1950 loss: 0.2845938149094582\n",
      "  batch 2000 loss: 0.29794634342193604\n",
      "  batch 2050 loss: 0.29830959632992743\n",
      "  batch 2100 loss: 0.2916051572561264\n",
      "  batch 2150 loss: 0.2865625274181366\n",
      "  batch 2200 loss: 0.2892616131901741\n",
      "  batch 2250 loss: 0.3122413834929466\n",
      "  batch 2300 loss: 0.31846980810165404\n",
      "  batch 2350 loss: 0.3114228454232216\n",
      "  batch 2400 loss: 0.30201924592256546\n",
      "  batch 2450 loss: 0.28851107448339464\n",
      "  batch 2500 loss: 0.32065373450517654\n",
      "  batch 2550 loss: 0.3084560218453407\n",
      "  batch 2600 loss: 0.30052880078554156\n",
      "  batch 2650 loss: 0.3176339194178581\n",
      "  batch 2700 loss: 0.29823691457509993\n",
      "  batch 2750 loss: 0.29453055024147035\n",
      "Epoch 00017: reducing learning rate of group 0 to 3.9062e-03.\n",
      "LOSS train 0.29453 valid 0.73283, valid PER 20.08%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.2984152638912201\n",
      "  batch 100 loss: 0.2941910895705223\n",
      "  batch 150 loss: 0.2840533310174942\n",
      "  batch 200 loss: 0.2800545901060104\n",
      "  batch 250 loss: 0.31845005720853803\n",
      "  batch 300 loss: 0.27466890692710877\n",
      "  batch 350 loss: 0.2892533902823925\n",
      "  batch 400 loss: 0.31668207615613936\n",
      "  batch 450 loss: 0.30114671409130095\n",
      "  batch 500 loss: 0.32359420746564865\n",
      "  batch 550 loss: 0.3252766048908234\n",
      "  batch 600 loss: 0.29274925738573077\n",
      "  batch 650 loss: 0.3107471612095833\n",
      "  batch 700 loss: 0.2890604610741139\n",
      "  batch 750 loss: 0.3075645077228546\n",
      "  batch 800 loss: 0.30862723976373674\n",
      "  batch 850 loss: 0.31589215487241745\n",
      "  batch 900 loss: 0.28021652132272723\n",
      "  batch 950 loss: 0.31058255583047867\n",
      "  batch 1000 loss: 0.28282005578279495\n",
      "  batch 1050 loss: 0.3133442041277885\n",
      "  batch 1100 loss: 0.3053101575374603\n",
      "  batch 1150 loss: 0.28631788775324823\n",
      "  batch 1200 loss: 0.3092016831040382\n",
      "  batch 1250 loss: 0.31090194433927537\n",
      "  batch 1300 loss: 0.3103003877401352\n",
      "  batch 1350 loss: 0.3010515159368515\n",
      "  batch 1400 loss: 0.3122343534231186\n",
      "  batch 1450 loss: 0.28896390318870546\n",
      "  batch 1500 loss: 0.2840995919704437\n",
      "  batch 1550 loss: 0.298643646389246\n",
      "  batch 1600 loss: 0.2889143565297127\n",
      "  batch 1650 loss: 0.3003417113423347\n",
      "  batch 1700 loss: 0.30163316190242767\n",
      "  batch 1750 loss: 0.2937033748626709\n",
      "  batch 1800 loss: 0.2967166033387184\n",
      "  batch 1850 loss: 0.30578189343214035\n",
      "  batch 1900 loss: 0.29466787189245225\n",
      "  batch 1950 loss: 0.30481960892677307\n",
      "  batch 2000 loss: 0.31600655883550643\n",
      "  batch 2050 loss: 0.2818572348356247\n",
      "  batch 2100 loss: 0.31323543429374695\n",
      "  batch 2150 loss: 0.28126346111297607\n",
      "  batch 2200 loss: 0.29980411648750305\n",
      "  batch 2250 loss: 0.30033774733543395\n",
      "  batch 2300 loss: 0.2848557832837105\n",
      "  batch 2350 loss: 0.30619976937770843\n",
      "  batch 2400 loss: 0.30570646286010744\n",
      "  batch 2450 loss: 0.29385987013578413\n",
      "  batch 2500 loss: 0.28590519070625303\n",
      "  batch 2550 loss: 0.30523062765598297\n",
      "  batch 2600 loss: 0.27817846447229383\n",
      "  batch 2650 loss: 0.3204545259475708\n",
      "  batch 2700 loss: 0.28807809084653857\n",
      "  batch 2750 loss: 0.308298597484827\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.9531e-03.\n",
      "LOSS train 0.30830 valid 0.73222, valid PER 20.04%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.2926562923192978\n",
      "  batch 100 loss: 0.29907976955175397\n",
      "  batch 150 loss: 0.2917662084102631\n",
      "  batch 200 loss: 0.28855070322751997\n",
      "  batch 250 loss: 0.292535727918148\n",
      "  batch 300 loss: 0.28629029393196104\n",
      "  batch 350 loss: 0.27309831231832504\n",
      "  batch 400 loss: 0.29655961364507677\n",
      "  batch 450 loss: 0.30864319413900376\n",
      "  batch 500 loss: 0.30604040414094924\n",
      "  batch 550 loss: 0.3065809252858162\n",
      "  batch 600 loss: 0.3054676526784897\n",
      "  batch 650 loss: 0.3003744119405746\n",
      "  batch 700 loss: 0.2990976047515869\n",
      "  batch 750 loss: 0.30659452468156817\n",
      "  batch 800 loss: 0.2918299888074398\n",
      "  batch 850 loss: 0.3020923042297363\n",
      "  batch 900 loss: 0.31085242331027985\n",
      "  batch 950 loss: 0.29872444242239\n",
      "  batch 1000 loss: 0.29665005654096605\n",
      "  batch 1050 loss: 0.31115434259176256\n",
      "  batch 1100 loss: 0.2800963453948498\n",
      "  batch 1150 loss: 0.30689832180738447\n",
      "  batch 1200 loss: 0.2932343804836273\n",
      "  batch 1250 loss: 0.301074715256691\n",
      "  batch 1300 loss: 0.29232382118701933\n",
      "  batch 1350 loss: 0.3026795196533203\n",
      "  batch 1400 loss: 0.30079577773809435\n",
      "  batch 1450 loss: 0.30138948768377305\n",
      "  batch 1500 loss: 0.303674781024456\n",
      "  batch 1550 loss: 0.30185666054487226\n",
      "  batch 1600 loss: 0.30167092084884645\n",
      "  batch 1650 loss: 0.29888955056667327\n",
      "  batch 1700 loss: 0.27790464788675306\n",
      "  batch 1750 loss: 0.2993521922826767\n",
      "  batch 1800 loss: 0.3055704066157341\n",
      "  batch 1850 loss: 0.28817511290311815\n",
      "  batch 1900 loss: 0.2954306975007057\n",
      "  batch 1950 loss: 0.3219808229804039\n",
      "  batch 2000 loss: 0.3144821974635124\n",
      "  batch 2050 loss: 0.30056126475334166\n",
      "  batch 2100 loss: 0.30622373342514037\n",
      "  batch 2150 loss: 0.28180322915315625\n",
      "  batch 2200 loss: 0.3013240709900856\n",
      "  batch 2250 loss: 0.2987745088338852\n",
      "  batch 2300 loss: 0.28819048941135406\n",
      "  batch 2350 loss: 0.28420466005802153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 2400 loss: 0.29562187135219575\n",
      "  batch 2450 loss: 0.3056300845742226\n",
      "  batch 2500 loss: 0.3014137315750122\n",
      "  batch 2550 loss: 0.28971683233976364\n",
      "  batch 2600 loss: 0.30888008236885073\n",
      "  batch 2650 loss: 0.2983162260055542\n",
      "  batch 2700 loss: 0.28459420770406724\n",
      "  batch 2750 loss: 0.3071652027964592\n",
      "Epoch 00019: reducing learning rate of group 0 to 9.7656e-04.\n",
      "LOSS train 0.30717 valid 0.73230, valid PER 20.02%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.28951789617538454\n",
      "  batch 100 loss: 0.3122708675265312\n",
      "  batch 150 loss: 0.2747110638022423\n",
      "  batch 200 loss: 0.3168734005093575\n",
      "  batch 250 loss: 0.31890604496002195\n",
      "  batch 300 loss: 0.29542651802301406\n",
      "  batch 350 loss: 0.3150186184048653\n",
      "  batch 400 loss: 0.2852684712409973\n",
      "  batch 450 loss: 0.2809467366337776\n",
      "  batch 500 loss: 0.2909441965818405\n",
      "  batch 550 loss: 0.2859696829319\n",
      "  batch 600 loss: 0.30257520735263826\n",
      "  batch 650 loss: 0.29372184187173844\n",
      "  batch 700 loss: 0.299716919362545\n",
      "  batch 750 loss: 0.28172645062208174\n",
      "  batch 800 loss: 0.3045854711532593\n",
      "  batch 850 loss: 0.28970981150865555\n",
      "  batch 900 loss: 0.29363007605075836\n",
      "  batch 950 loss: 0.30605158358812334\n",
      "  batch 1000 loss: 0.28265454679727553\n",
      "  batch 1050 loss: 0.2832684016227722\n",
      "  batch 1100 loss: 0.29120108783245086\n",
      "  batch 1150 loss: 0.3072322151064873\n",
      "  batch 1200 loss: 0.27768458530306817\n",
      "  batch 1250 loss: 0.3074610349535942\n",
      "  batch 1300 loss: 0.28681310415267947\n",
      "  batch 1350 loss: 0.2954499453306198\n",
      "  batch 1400 loss: 0.30045811489224433\n",
      "  batch 1450 loss: 0.3146855291724205\n",
      "  batch 1500 loss: 0.3000585976243019\n",
      "  batch 1550 loss: 0.3205235931277275\n",
      "  batch 1600 loss: 0.3050529754161835\n",
      "  batch 1650 loss: 0.29742592453956607\n",
      "  batch 1700 loss: 0.3009929606318474\n",
      "  batch 1750 loss: 0.3075329914689064\n",
      "  batch 1800 loss: 0.3045030996203423\n",
      "  batch 1850 loss: 0.32204629004001617\n",
      "  batch 1900 loss: 0.2821018224954605\n",
      "  batch 1950 loss: 0.3040360647439957\n",
      "  batch 2000 loss: 0.31246406972408297\n",
      "  batch 2050 loss: 0.29399935483932493\n",
      "  batch 2100 loss: 0.29233701348304747\n",
      "  batch 2150 loss: 0.29417516916990283\n",
      "  batch 2200 loss: 0.3013176691532135\n",
      "  batch 2250 loss: 0.315543609559536\n",
      "  batch 2300 loss: 0.3219787439703941\n",
      "  batch 2350 loss: 0.305063037276268\n",
      "  batch 2400 loss: 0.30635883420705795\n",
      "  batch 2450 loss: 0.2971556556224823\n",
      "  batch 2500 loss: 0.295534335821867\n",
      "  batch 2550 loss: 0.3054408195614815\n",
      "  batch 2600 loss: 0.3102274537086487\n",
      "  batch 2650 loss: 0.2898581200838089\n",
      "  batch 2700 loss: 0.2915949776768684\n",
      "  batch 2750 loss: 0.2901069077849388\n",
      "Epoch 00020: reducing learning rate of group 0 to 4.8828e-04.\n",
      "LOSS train 0.29011 valid 0.73246, valid PER 20.06%\n",
      "Training finished in 16.0 minutes.\n",
      "Model saved to checkpoints/20231211_154523/model_11\n",
      "Finish SGD_Scheduler optimiser\n",
      "End tuning For 2 Layer LSTM\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout_between_layer\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start dropout tuning For 2 Layer LSTM, with Dropout between layer\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank_augmentation.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank_augmentation.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 1.0,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For 2 Layer LSTM\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0141e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
