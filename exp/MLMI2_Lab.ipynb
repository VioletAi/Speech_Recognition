{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7922c105",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263eab98",
   "metadata": {},
   "source": [
    "## Use `dataloader` to get an utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d135fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_dataloader\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff22c4",
   "metadata": {},
   "source": [
    "# CTC model for ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178724d9",
   "metadata": {},
   "source": [
    "## Obtain phoneme output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b10b7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find the unique phones in train.json, and then\n",
    "# create a file named vocab.txt, each line in this \n",
    "# file is a unique phone, in total there should be \n",
    "# 40 lines\n",
    "\n",
    "vocab = {}\n",
    "phonemes = []\n",
    "with open(\"vocab_39.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab[text.strip()] = id\n",
    "        phonemes.append(text)\n",
    "phonemes = phonemes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2390d43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_': 0,\n",
       " 'uh': 1,\n",
       " 'n': 2,\n",
       " 's': 3,\n",
       " 'th': 4,\n",
       " 'y': 5,\n",
       " 'ih': 6,\n",
       " 'ch': 7,\n",
       " 'aa': 8,\n",
       " 'l': 9,\n",
       " 'uw': 10,\n",
       " 'ah': 11,\n",
       " 'b': 12,\n",
       " 'ow': 13,\n",
       " 'dh': 14,\n",
       " 'd': 15,\n",
       " 'aw': 16,\n",
       " 't': 17,\n",
       " 'r': 18,\n",
       " 'w': 19,\n",
       " 'm': 20,\n",
       " 'v': 21,\n",
       " 'ay': 22,\n",
       " 'f': 23,\n",
       " 'p': 24,\n",
       " 'sh': 25,\n",
       " 'eh': 26,\n",
       " 'oy': 27,\n",
       " 'sil': 28,\n",
       " 'hh': 29,\n",
       " 'dx': 30,\n",
       " 'jh': 31,\n",
       " 'er': 32,\n",
       " 'iy': 33,\n",
       " 'g': 34,\n",
       " 'ae': 35,\n",
       " 'ey': 36,\n",
       " 'z': 37,\n",
       " 'k': 38,\n",
       " 'ng': 39}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b3baa",
   "metadata": {},
   "source": [
    "## Model & training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa93a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently using cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"currently using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"currently using cpu only\")\n",
    "\n",
    "args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'val_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 128,\n",
    "        'concat': 1,\n",
    "        'lr': 0.5,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "args = namedtuple('x', args)(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f83704",
   "metadata": {},
   "source": [
    "# Experiment looking at model dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "522bf613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters is 166952\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "model = models.BiLSTM(\n",
    "    args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab))\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of model parameters is {}'.format(num_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032e1400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "torch.Size([484, 4, 23])\n",
      "torch.Size([484, 4, 256])\n",
      "torch.Size([484, 4, 40])\n",
      "\n",
      "\n",
      "torch.Size([295, 4, 23])\n",
      "torch.Size([295, 4, 256])\n",
      "torch.Size([295, 4, 40])\n",
      "\n",
      "\n",
      "torch.Size([428, 4, 23])\n",
      "torch.Size([428, 4, 256])\n",
      "torch.Size([428, 4, 40])\n",
      "\n",
      "\n",
      "torch.Size([374, 4, 23])\n",
      "torch.Size([374, 4, 256])\n",
      "torch.Size([374, 4, 40])\n",
      "\n",
      "\n",
      "torch.Size([290, 4, 23])\n",
      "torch.Size([290, 4, 256])\n",
      "torch.Size([290, 4, 40])\n",
      "\n",
      "\n",
      "torch.Size([427, 4, 23])\n",
      "torch.Size([427, 4, 256])\n",
      "torch.Size([427, 4, 40])\n",
      "\n",
      "\n",
      "torch.Size([522, 4, 23])\n",
      "torch.Size([522, 4, 256])\n",
      "torch.Size([522, 4, 40])\n",
      "\n",
      "\n",
      "torch.Size([511, 4, 23])\n",
      "torch.Size([511, 4, 256])\n",
      "torch.Size([511, 4, 40])\n",
      "\n",
      "\n",
      "torch.Size([424, 4, 23])\n",
      "torch.Size([424, 4, 256])\n",
      "torch.Size([424, 4, 40])\n",
      "\n",
      "\n",
      "torch.Size([395, 4, 23])\n",
      "torch.Size([395, 4, 256])\n",
      "torch.Size([395, 4, 40])\n",
      "\n",
      "\n",
      "torch.Size([507, 4, 23])\n",
      "torch.Size([507, 4, 256])\n",
      "torch.Size([507, 4, 40])\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 5\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      7\u001b[0m duration \u001b[38;5;241m=\u001b[39m (end \u001b[38;5;241m-\u001b[39m start)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "File \u001b[0;32m/rds/user/wa285/hpc-work/MLMI2/exp/trainer.py:63\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 63\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m running_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n",
      "File \u001b[0;32m/rds/user/wa285/hpc-work/MLMI2/exp/trainer.py:41\u001b[0m, in \u001b[0;36mtrain.<locals>.train_one_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m log_softmax(model(inputs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets, in_lens, out_lens)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#add gradient clipping\u001b[39;00m\n\u001b[1;32m     44\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from trainer import train\n",
    "start = datetime.now()\n",
    "model.to(args.device)\n",
    "model_path = train(model, args)\n",
    "end = datetime.now()\n",
    "duration = (end - start).total_seconds()\n",
    "print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "print('Model saved to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4de747",
   "metadata": {},
   "source": [
    "# RUN ALL ABOVE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec53390d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d308bf0c",
   "metadata": {},
   "source": [
    "#### Study of Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a2595",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'val_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 128,\n",
    "        'concat': 1,\n",
    "        'lr': 0.5,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "args = namedtuple('x', args)(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e0d034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters is 166952\n",
      "EPOCH 1:\n",
      "  batch 50 loss: nan\n",
      "  batch 100 loss: nan\n",
      "  batch 150 loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 14\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     16\u001b[0m duration \u001b[38;5;241m=\u001b[39m (end \u001b[38;5;241m-\u001b[39m start)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "File \u001b[0;32m/rds/user/wa285/hpc-work/MLMI2/exp/trainer_Adam.py:63\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 63\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m running_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n",
      "File \u001b[0;32m/rds/user/wa285/hpc-work/MLMI2/exp/trainer_Adam.py:41\u001b[0m, in \u001b[0;36mtrain.<locals>.train_one_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m log_softmax(model(inputs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets, in_lens, out_lens)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#add gradient clipping\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import models\n",
    "from datetime import datetime\n",
    "from trainer_Adam import train\n",
    "\n",
    "\n",
    "model = models.BiLSTM(\n",
    "    args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab))\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of model parameters is {}'.format(num_params))\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "model.to(args.device)\n",
    "model_path = train(model, args)\n",
    "end = datetime.now()\n",
    "duration = (end - start).total_seconds()\n",
    "print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "print('Model saved to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8373b16a",
   "metadata": {},
   "source": [
    "### Baseline Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ffcb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters is 166952\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "model = models.BiLSTM(\n",
    "    args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab))\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of model parameters is {}'.format(num_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b9c3de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 50 loss: 5.086774172782898\n",
      "  batch 100 loss: 3.283014554977417\n",
      "  batch 150 loss: 3.090360550880432\n",
      "  batch 200 loss: 2.793375668525696\n",
      "  batch 250 loss: 2.614130387306213\n",
      "  batch 300 loss: 2.423339204788208\n",
      "  batch 350 loss: 2.2986184406280517\n",
      "  batch 400 loss: 2.2792709922790526\n",
      "  batch 450 loss: 2.1971713137626647\n",
      "  batch 500 loss: 2.0925161623954773\n",
      "  batch 550 loss: 2.0330736899375914\n",
      "  batch 600 loss: 1.9948135256767272\n",
      "  batch 650 loss: 1.9030866765975951\n",
      "  batch 700 loss: 1.894078896045685\n",
      "  batch 750 loss: 1.8234505438804627\n",
      "  batch 800 loss: 1.8049822735786438\n",
      "  batch 850 loss: 1.7591145157814025\n",
      "  batch 900 loss: 1.7552357149124145\n",
      "LOSS train 1.75524 valid 1.71082, valid PER 62.31%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7059818196296692\n",
      "  batch 100 loss: 1.632240788936615\n",
      "  batch 150 loss: 1.6225002598762512\n",
      "  batch 200 loss: 1.6177998876571655\n",
      "  batch 250 loss: 1.6065989542007446\n",
      "  batch 300 loss: 1.5903397655487062\n",
      "  batch 350 loss: 1.4828792119026184\n",
      "  batch 400 loss: 1.5156202459335326\n",
      "  batch 450 loss: 1.4625972127914428\n",
      "  batch 500 loss: 1.5032314658164978\n",
      "  batch 550 loss: 1.500582160949707\n",
      "  batch 600 loss: 1.4449688816070556\n",
      "  batch 650 loss: 1.4658487963676452\n",
      "  batch 700 loss: 1.439452302455902\n",
      "  batch 750 loss: 1.4065546941757203\n",
      "  batch 800 loss: 1.3600868058204652\n",
      "  batch 850 loss: 1.3554739880561828\n",
      "  batch 900 loss: 1.3894693398475646\n",
      "LOSS train 1.38947 valid 1.35013, valid PER 42.47%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.337108610868454\n",
      "  batch 100 loss: 1.3210371613502503\n",
      "  batch 150 loss: 1.3050436806678771\n",
      "  batch 200 loss: 1.276299810409546\n",
      "  batch 250 loss: 1.2789957487583161\n",
      "  batch 300 loss: 1.2604615473747254\n",
      "  batch 350 loss: 1.3082793593406676\n",
      "  batch 400 loss: 1.2881868755817414\n",
      "  batch 450 loss: 1.2514093601703644\n",
      "  batch 500 loss: 1.2369720697402955\n",
      "  batch 550 loss: 1.2440876471996307\n",
      "  batch 600 loss: 1.213868670463562\n",
      "  batch 650 loss: 1.191460622549057\n",
      "  batch 700 loss: 1.2152143001556397\n",
      "  batch 750 loss: 1.2693943989276886\n",
      "  batch 800 loss: 1.2001871299743652\n",
      "  batch 850 loss: 1.23340886592865\n",
      "  batch 900 loss: 1.155331015586853\n",
      "LOSS train 1.15533 valid 1.22545, valid PER 36.82%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.1461944377422333\n",
      "  batch 100 loss: 1.16784308552742\n",
      "  batch 150 loss: 1.12745343208313\n",
      "  batch 200 loss: 1.1497526144981385\n",
      "  batch 250 loss: 1.173974266052246\n",
      "  batch 300 loss: 1.1633199906349183\n",
      "  batch 350 loss: 1.1037923789024353\n",
      "  batch 400 loss: 1.1299755990505218\n",
      "  batch 450 loss: 1.1152130544185639\n",
      "  batch 500 loss: 1.1066869390010834\n",
      "  batch 550 loss: 1.1226699793338775\n",
      "  batch 600 loss: 1.164214996099472\n",
      "  batch 650 loss: 1.1227041280269623\n",
      "  batch 700 loss: 1.093189798593521\n",
      "  batch 750 loss: 1.0783223068714143\n",
      "  batch 800 loss: 1.0586784505844116\n",
      "  batch 850 loss: 1.1076883697509765\n",
      "  batch 900 loss: 1.1201561677455902\n",
      "LOSS train 1.12016 valid 1.12607, valid PER 34.84%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.0398934304714202\n",
      "  batch 100 loss: 1.0374910151958465\n",
      "  batch 150 loss: 1.0908860301971435\n",
      "  batch 200 loss: 1.01877099275589\n",
      "  batch 250 loss: 1.030315079689026\n",
      "  batch 300 loss: 1.0355705213546753\n",
      "  batch 350 loss: 1.0326206660270691\n",
      "  batch 400 loss: 1.0431273627281188\n",
      "  batch 450 loss: 1.0194332945346831\n",
      "  batch 500 loss: 1.0575924611091614\n",
      "  batch 550 loss: 1.0099442541599273\n",
      "  batch 600 loss: 1.077265615463257\n",
      "  batch 650 loss: 1.026570689678192\n",
      "  batch 700 loss: 1.0583431339263916\n",
      "  batch 750 loss: 0.9964816069602966\n",
      "  batch 800 loss: 1.0331942975521087\n",
      "  batch 850 loss: 1.0295076251029969\n",
      "  batch 900 loss: 1.0309641921520234\n",
      "LOSS train 1.03096 valid 1.08192, valid PER 33.48%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.0145374751091003\n",
      "  batch 100 loss: 0.964639104604721\n",
      "  batch 150 loss: 0.9433189392089844\n",
      "  batch 200 loss: 0.9669739127159118\n",
      "  batch 250 loss: 1.0048378670215607\n",
      "  batch 300 loss: 0.9843430864810944\n",
      "  batch 350 loss: 0.9628606879711151\n",
      "  batch 400 loss: 0.9670757174491882\n",
      "  batch 450 loss: 0.9900656259059906\n",
      "  batch 500 loss: 0.9771029651165009\n",
      "  batch 550 loss: 1.0003348612785339\n",
      "  batch 600 loss: 0.945549488067627\n",
      "  batch 650 loss: 0.9814447331428527\n",
      "  batch 700 loss: 0.9812485706806183\n",
      "  batch 750 loss: 0.9582438790798187\n",
      "  batch 800 loss: 0.9494047331809997\n",
      "  batch 850 loss: 0.9438851428031921\n",
      "  batch 900 loss: 0.9712597632408142\n",
      "LOSS train 0.97126 valid 1.06287, valid PER 32.93%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.9389416229724884\n",
      "  batch 100 loss: 0.926832320690155\n",
      "  batch 150 loss: 0.9279841709136963\n",
      "  batch 200 loss: 0.906079740524292\n",
      "  batch 250 loss: 0.9044059574604034\n",
      "  batch 300 loss: 0.90206094622612\n",
      "  batch 350 loss: 0.9182558608055115\n",
      "  batch 400 loss: 0.9095773363113403\n",
      "  batch 450 loss: 0.9287129604816436\n",
      "  batch 500 loss: 0.9138265585899353\n",
      "  batch 550 loss: 0.9245693707466125\n",
      "  batch 600 loss: 0.9142360889911652\n",
      "  batch 650 loss: 0.9195344817638397\n",
      "  batch 700 loss: 0.9444646489620209\n",
      "  batch 750 loss: 0.9143278563022613\n",
      "  batch 800 loss: 0.9076099896430969\n",
      "  batch 850 loss: 0.9324671041965484\n",
      "  batch 900 loss: 0.9593579661846161\n",
      "LOSS train 0.95936 valid 1.00090, valid PER 31.16%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8693592464923858\n",
      "  batch 100 loss: 0.870538250207901\n",
      "  batch 150 loss: 0.8631743991374969\n",
      "  batch 200 loss: 0.8607201254367829\n",
      "  batch 250 loss: 0.868725517988205\n",
      "  batch 300 loss: 0.827749890089035\n",
      "  batch 350 loss: 0.9035902214050293\n",
      "  batch 400 loss: 0.8624851489067078\n",
      "  batch 450 loss: 0.8887851929664612\n",
      "  batch 500 loss: 0.9100941014289856\n",
      "  batch 550 loss: 0.8406356644630432\n",
      "  batch 600 loss: 0.8958153367042542\n",
      "  batch 650 loss: 0.9192422497272491\n",
      "  batch 700 loss: 0.8640153408050537\n",
      "  batch 750 loss: 0.868216460943222\n",
      "  batch 800 loss: 0.89680619597435\n",
      "  batch 850 loss: 0.8683228302001953\n",
      "  batch 900 loss: 0.867710919380188\n",
      "LOSS train 0.86771 valid 0.98792, valid PER 29.62%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.8002906036376953\n",
      "  batch 100 loss: 0.8392237567901611\n",
      "  batch 150 loss: 0.8454771447181701\n",
      "  batch 200 loss: 0.8167629086971283\n",
      "  batch 250 loss: 0.8403191494941712\n",
      "  batch 300 loss: 0.8367885375022888\n",
      "  batch 350 loss: 0.8560090506076813\n",
      "  batch 400 loss: 0.8426858961582184\n",
      "  batch 450 loss: 0.8438376569747925\n",
      "  batch 500 loss: 0.8070199620723725\n",
      "  batch 550 loss: 0.8585880851745605\n",
      "  batch 600 loss: 0.8575512635707855\n",
      "  batch 650 loss: 0.844754855632782\n",
      "  batch 700 loss: 0.8110990083217621\n",
      "  batch 750 loss: 0.8319711971282959\n",
      "  batch 800 loss: 0.8550473046302796\n",
      "  batch 850 loss: 0.8696510922908783\n",
      "  batch 900 loss: 0.8081085860729218\n",
      "LOSS train 0.80811 valid 0.97826, valid PER 29.69%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7530319368839264\n",
      "  batch 100 loss: 0.7882682406902313\n",
      "  batch 150 loss: 0.8083116245269776\n",
      "  batch 200 loss: 0.8373112428188324\n",
      "  batch 250 loss: 0.8270442271232605\n",
      "  batch 300 loss: 0.7833209264278412\n",
      "  batch 350 loss: 0.8045441615581512\n",
      "  batch 400 loss: 0.7659310066699981\n",
      "  batch 450 loss: 0.7746091330051422\n",
      "  batch 500 loss: 0.8292322111129761\n",
      "  batch 550 loss: 0.837919111251831\n",
      "  batch 600 loss: 0.8083303862810135\n",
      "  batch 650 loss: 0.7863271605968475\n",
      "  batch 700 loss: 0.8059385871887207\n",
      "  batch 750 loss: 0.786233047246933\n",
      "  batch 800 loss: 0.8058851075172424\n",
      "  batch 850 loss: 0.8296556127071381\n",
      "  batch 900 loss: 0.8277507734298706\n",
      "LOSS train 0.82775 valid 0.98429, valid PER 30.49%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.7348445904254913\n",
      "  batch 100 loss: 0.731283575296402\n",
      "  batch 150 loss: 0.7363195484876632\n",
      "  batch 200 loss: 0.7905150663852691\n",
      "  batch 250 loss: 0.7746971553564072\n",
      "  batch 300 loss: 0.7553812605142594\n",
      "  batch 350 loss: 0.7692421269416809\n",
      "  batch 400 loss: 0.7863199090957642\n",
      "  batch 450 loss: 0.7911260056495667\n",
      "  batch 500 loss: 0.7587946403026581\n",
      "  batch 550 loss: 0.7709304022789002\n",
      "  batch 600 loss: 0.7564539313316345\n",
      "  batch 650 loss: 0.8345367741584778\n",
      "  batch 700 loss: 0.7364235866069794\n",
      "  batch 750 loss: 0.7510545778274537\n",
      "  batch 800 loss: 0.8114294910430908\n",
      "  batch 850 loss: 0.8201208901405335\n",
      "  batch 900 loss: 0.797562221288681\n",
      "LOSS train 0.79756 valid 0.96817, valid PER 28.86%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.7342477536201477\n",
      "  batch 100 loss: 0.7272364515066146\n",
      "  batch 150 loss: 0.7151020342111587\n",
      "  batch 200 loss: 0.7388782328367234\n",
      "  batch 250 loss: 0.7387467443943023\n",
      "  batch 300 loss: 0.737351862192154\n",
      "  batch 350 loss: 0.7311704117059707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 400 loss: 0.7491212117671967\n",
      "  batch 450 loss: 0.7511372649669648\n",
      "  batch 500 loss: 0.7659581100940704\n",
      "  batch 550 loss: 0.6965995353460311\n",
      "  batch 600 loss: 0.7450645101070404\n",
      "  batch 650 loss: 0.7636515700817108\n",
      "  batch 700 loss: 0.7618362134695054\n",
      "  batch 750 loss: 0.7375964295864105\n",
      "  batch 800 loss: 0.7501379823684693\n",
      "  batch 850 loss: 0.7929791331291198\n",
      "  batch 900 loss: 0.7944529736042023\n",
      "LOSS train 0.79445 valid 0.95214, valid PER 28.64%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.6863662838935852\n",
      "  batch 100 loss: 0.7173892110586166\n",
      "  batch 150 loss: 0.6900480782985687\n",
      "  batch 200 loss: 0.7261819589138031\n",
      "  batch 250 loss: 0.6869390255212784\n",
      "  batch 300 loss: 0.6909507423639297\n",
      "  batch 350 loss: 0.7214938408136368\n",
      "  batch 400 loss: 0.7088409268856048\n",
      "  batch 450 loss: 0.7219684791564941\n",
      "  batch 500 loss: 0.7039810448884964\n",
      "  batch 550 loss: 0.7466556972265244\n",
      "  batch 600 loss: 0.7109966671466827\n",
      "  batch 650 loss: 0.7457374995946884\n",
      "  batch 700 loss: 0.7470722872018815\n",
      "  batch 750 loss: 0.6979611587524414\n",
      "  batch 800 loss: 0.7114669740200043\n",
      "  batch 850 loss: 0.7507269757986069\n",
      "  batch 900 loss: 0.7450120270252227\n",
      "LOSS train 0.74501 valid 0.95768, valid PER 28.54%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.6812797462940217\n",
      "  batch 100 loss: 0.6708585029840469\n",
      "  batch 150 loss: 0.6731629592180252\n",
      "  batch 200 loss: 0.681263530254364\n",
      "  batch 250 loss: 0.6875471591949462\n",
      "  batch 300 loss: 0.7306889617443084\n",
      "  batch 350 loss: 0.670294599533081\n",
      "  batch 400 loss: 0.6876282554864883\n",
      "  batch 450 loss: 0.6798035699129105\n",
      "  batch 500 loss: 0.7140411317348481\n",
      "  batch 550 loss: 0.7148218703269958\n",
      "  batch 600 loss: 0.6714939469099045\n",
      "  batch 650 loss: 0.7140835821628571\n",
      "  batch 700 loss: 0.7481703507900238\n",
      "  batch 750 loss: 0.691406530737877\n",
      "  batch 800 loss: 0.6777428728342056\n",
      "  batch 850 loss: 0.7245912444591522\n",
      "  batch 900 loss: 0.7189444851875305\n",
      "LOSS train 0.71894 valid 0.96294, valid PER 28.76%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6409526389837265\n",
      "  batch 100 loss: 0.635938326716423\n",
      "  batch 150 loss: 0.6620517575740814\n",
      "  batch 200 loss: 0.685852422118187\n",
      "  batch 250 loss: 0.6880056929588317\n",
      "  batch 300 loss: 0.6547611689567566\n",
      "  batch 350 loss: 0.6668742144107819\n",
      "  batch 400 loss: 0.6580085545778275\n",
      "  batch 450 loss: 0.6556956750154496\n",
      "  batch 500 loss: 0.6306589061021804\n",
      "  batch 550 loss: 0.6788219112157822\n",
      "  batch 600 loss: 0.7025883227586747\n",
      "  batch 650 loss: 0.7040995979309081\n",
      "  batch 700 loss: 0.7118571215867996\n",
      "  batch 750 loss: 0.6965030181407929\n",
      "  batch 800 loss: 0.6752855509519577\n",
      "  batch 850 loss: 0.6547383362054825\n",
      "  batch 900 loss: 0.6878400295972824\n",
      "LOSS train 0.68784 valid 0.96177, valid PER 28.67%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.6496979999542236\n",
      "  batch 100 loss: 0.6226589798927307\n",
      "  batch 150 loss: 0.6355706316232681\n",
      "  batch 200 loss: 0.6326492649316787\n",
      "  batch 250 loss: 0.6650484329462052\n",
      "  batch 300 loss: 0.6435586583614349\n",
      "  batch 350 loss: 0.6570881831645966\n",
      "  batch 400 loss: 0.6565283811092377\n",
      "  batch 450 loss: 0.6789886897802353\n",
      "  batch 500 loss: 0.6195938378572464\n",
      "  batch 550 loss: 0.6576669013500214\n",
      "  batch 600 loss: 0.6445552670955658\n",
      "  batch 650 loss: 0.6669662177562714\n",
      "  batch 700 loss: 0.6500789159536362\n",
      "  batch 750 loss: 0.6662734985351563\n",
      "  batch 800 loss: 0.669262974858284\n",
      "  batch 850 loss: 0.6576247650384903\n",
      "  batch 900 loss: 0.6677672535181045\n",
      "LOSS train 0.66777 valid 0.96753, valid PER 27.62%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.6186375969648361\n",
      "  batch 100 loss: 0.6350529778003693\n",
      "  batch 150 loss: 0.6166372501850128\n",
      "  batch 200 loss: 0.6105012470483779\n",
      "  batch 250 loss: 0.6548776048421859\n",
      "  batch 300 loss: 0.6315918278694153\n",
      "  batch 350 loss: 0.6130149245262146\n",
      "  batch 400 loss: 0.6495472025871277\n",
      "  batch 450 loss: 0.6490988940000534\n",
      "  batch 500 loss: 0.6068997859954834\n",
      "  batch 550 loss: 0.6339905476570129\n",
      "  batch 600 loss: 0.662586675286293\n",
      "  batch 650 loss: 0.6287691378593445\n",
      "  batch 700 loss: 0.6364338171482086\n",
      "  batch 750 loss: 0.625658038854599\n",
      "  batch 800 loss: 0.6277098244428635\n",
      "  batch 850 loss: 0.6445474141836166\n",
      "  batch 900 loss: 0.6244815719127655\n",
      "LOSS train 0.62448 valid 0.96776, valid PER 27.62%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.5797555482387543\n",
      "  batch 100 loss: 0.6081847834587097\n",
      "  batch 150 loss: 0.6243676513433456\n",
      "  batch 200 loss: 0.5944636768102646\n",
      "  batch 250 loss: 0.6069986492395401\n",
      "  batch 300 loss: 0.599713483452797\n",
      "  batch 350 loss: 0.6111825227737426\n",
      "  batch 400 loss: 0.5938448572158813\n",
      "  batch 450 loss: 0.6326888674497604\n",
      "  batch 500 loss: 0.6227220207452774\n",
      "  batch 550 loss: 0.6298976922035218\n",
      "  batch 600 loss: 0.6075549060106278\n",
      "  batch 650 loss: 0.6118370187282562\n",
      "  batch 700 loss: 0.6310928988456727\n",
      "  batch 750 loss: 0.6148734444379806\n",
      "  batch 800 loss: 0.6120074623823166\n",
      "  batch 850 loss: 0.6163017010688782\n",
      "  batch 900 loss: 0.6431470370292663\n",
      "LOSS train 0.64315 valid 0.96613, valid PER 28.38%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5486046546697616\n",
      "  batch 100 loss: 0.5499483060836792\n",
      "  batch 150 loss: 0.5724177926778793\n",
      "  batch 200 loss: 0.5900583225488663\n",
      "  batch 250 loss: 0.5890857410430909\n",
      "  batch 300 loss: 0.6071023231744767\n",
      "  batch 350 loss: 0.5871704542636871\n",
      "  batch 400 loss: 0.6115367710590363\n",
      "  batch 450 loss: 0.5810096991062165\n",
      "  batch 500 loss: 0.59089470744133\n",
      "  batch 550 loss: 0.5890891063213348\n",
      "  batch 600 loss: 0.6030006164312363\n",
      "  batch 650 loss: 0.6399834424257278\n",
      "  batch 700 loss: 0.5978425401449203\n",
      "  batch 750 loss: 0.581774868965149\n",
      "  batch 800 loss: 0.6162600642442704\n",
      "  batch 850 loss: 0.6273188215494155\n",
      "  batch 900 loss: 0.6122510623931885\n",
      "LOSS train 0.61225 valid 0.98997, valid PER 28.42%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.5471829837560653\n",
      "  batch 100 loss: 0.5334948843717575\n",
      "  batch 150 loss: 0.5353400200605393\n",
      "  batch 200 loss: 0.5641018635034561\n",
      "  batch 250 loss: 0.5498881846666336\n",
      "  batch 300 loss: 0.580160979628563\n",
      "  batch 350 loss: 0.555381880402565\n",
      "  batch 400 loss: 0.5793261444568634\n",
      "  batch 450 loss: 0.586473999619484\n",
      "  batch 500 loss: 0.546572345495224\n",
      "  batch 550 loss: 0.6057978469133377\n",
      "  batch 600 loss: 0.5675161343812942\n",
      "  batch 650 loss: 0.5935176938772202\n",
      "  batch 700 loss: 0.6025082355737686\n",
      "  batch 750 loss: 0.565866311788559\n",
      "  batch 800 loss: 0.6154355722665786\n",
      "  batch 850 loss: 0.6115247112512588\n",
      "  batch 900 loss: 0.5963181930780411\n",
      "LOSS train 0.59632 valid 0.98575, valid PER 28.12%\n",
      "Training finished in 5.0 minutes.\n",
      "Model saved to checkpoints/20231204_162311/model_12\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from trainer import train\n",
    "start = datetime.now()\n",
    "model.to(args.device)\n",
    "model_path = train(model, args)\n",
    "end = datetime.now()\n",
    "duration = (end - start).total_seconds()\n",
    "print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "print('Model saved to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935f8ff",
   "metadata": {},
   "source": [
    "### DropOut Study here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a794f9",
   "metadata": {},
   "source": [
    "#### Dropout Study for baseline model is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cef2962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dropout tuning\n",
      "Total number of model parameters is 166952\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.890107951164246\n",
      "  batch 100 loss: 3.123832583427429\n",
      "  batch 150 loss: 2.9612038803100584\n",
      "  batch 200 loss: 2.837465834617615\n",
      "  batch 250 loss: 2.681118760108948\n",
      "  batch 300 loss: 2.47402494430542\n",
      "  batch 350 loss: 2.3582753562927246\n",
      "  batch 400 loss: 2.306531598567963\n",
      "  batch 450 loss: 2.236013696193695\n",
      "  batch 500 loss: 2.1494071006774904\n",
      "  batch 550 loss: 2.0773372983932497\n",
      "  batch 600 loss: 2.029969322681427\n",
      "  batch 650 loss: 1.9386557745933533\n",
      "  batch 700 loss: 1.9379531598091126\n",
      "  batch 750 loss: 1.8681982254981995\n",
      "  batch 800 loss: 1.850289089679718\n",
      "  batch 850 loss: 1.8001644897460938\n",
      "  batch 900 loss: 1.7743332695960998\n",
      "LOSS train 1.77433 valid 1.70116, valid PER 64.41%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7266140460968018\n",
      "  batch 100 loss: 1.683559033870697\n",
      "  batch 150 loss: 1.649568510055542\n",
      "  batch 200 loss: 1.6512782955169678\n",
      "  batch 250 loss: 1.6526165342330932\n",
      "  batch 300 loss: 1.6292884159088135\n",
      "  batch 350 loss: 1.5430630350112915\n",
      "  batch 400 loss: 1.5472568893432617\n",
      "  batch 450 loss: 1.4891640186309814\n",
      "  batch 500 loss: 1.5250274157524109\n",
      "  batch 550 loss: 1.534726746082306\n",
      "  batch 600 loss: 1.4708243918418884\n",
      "  batch 650 loss: 1.4903717684745788\n",
      "  batch 700 loss: 1.465440559387207\n",
      "  batch 750 loss: 1.4403750133514404\n",
      "  batch 800 loss: 1.3718605136871338\n",
      "  batch 850 loss: 1.3843205904960632\n",
      "  batch 900 loss: 1.4053690457344055\n",
      "LOSS train 1.40537 valid 1.34041, valid PER 42.95%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.3662464356422424\n",
      "  batch 100 loss: 1.3414109337329865\n",
      "  batch 150 loss: 1.3425548434257508\n",
      "  batch 200 loss: 1.2954026532173157\n",
      "  batch 250 loss: 1.2930319797992706\n",
      "  batch 300 loss: 1.2899691903591155\n",
      "  batch 350 loss: 1.3260529685020446\n",
      "  batch 400 loss: 1.2963419830799103\n",
      "  batch 450 loss: 1.2673703980445863\n",
      "  batch 500 loss: 1.2727383828163148\n",
      "  batch 550 loss: 1.2768330049514771\n",
      "  batch 600 loss: 1.2691479480266572\n",
      "  batch 650 loss: 1.2193198156356813\n",
      "  batch 700 loss: 1.2566464304924012\n",
      "  batch 750 loss: 1.3112242543697357\n",
      "  batch 800 loss: 1.2236307215690614\n",
      "  batch 850 loss: 1.2454088830947876\n",
      "  batch 900 loss: 1.2031384479999543\n",
      "LOSS train 1.20314 valid 1.21994, valid PER 37.93%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.1783699941635133\n",
      "  batch 100 loss: 1.2091679561138153\n",
      "  batch 150 loss: 1.163264226913452\n",
      "  batch 200 loss: 1.1960920155048371\n",
      "  batch 250 loss: 1.1968868732452393\n",
      "  batch 300 loss: 1.214454960823059\n",
      "  batch 350 loss: 1.1441186571121216\n",
      "  batch 400 loss: 1.1797354590892792\n",
      "  batch 450 loss: 1.1660816633701325\n",
      "  batch 500 loss: 1.1607106816768646\n",
      "  batch 550 loss: 1.1768478727340699\n",
      "  batch 600 loss: 1.2132455599308014\n",
      "  batch 650 loss: 1.1687973976135253\n",
      "  batch 700 loss: 1.145246102809906\n",
      "  batch 750 loss: 1.1393180561065674\n",
      "  batch 800 loss: 1.100846083164215\n",
      "  batch 850 loss: 1.1555378913879395\n",
      "  batch 900 loss: 1.1769747424125672\n",
      "LOSS train 1.17697 valid 1.13982, valid PER 35.40%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.094754844903946\n",
      "  batch 100 loss: 1.106348158121109\n",
      "  batch 150 loss: 1.1427917563915253\n",
      "  batch 200 loss: 1.0834794485569\n",
      "  batch 250 loss: 1.0808015048503876\n",
      "  batch 300 loss: 1.1042151880264282\n",
      "  batch 350 loss: 1.0931479358673095\n",
      "  batch 400 loss: 1.102407491207123\n",
      "  batch 450 loss: 1.0868338227272034\n",
      "  batch 500 loss: 1.0983928990364076\n",
      "  batch 550 loss: 1.0649818539619447\n",
      "  batch 600 loss: 1.1238090252876283\n",
      "  batch 650 loss: 1.0880995655059815\n",
      "  batch 700 loss: 1.1224791049957275\n",
      "  batch 750 loss: 1.0522576081752777\n",
      "  batch 800 loss: 1.07920107960701\n",
      "  batch 850 loss: 1.1037063264846803\n",
      "  batch 900 loss: 1.0940393149852752\n",
      "LOSS train 1.09404 valid 1.09622, valid PER 34.08%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.0889144384860991\n",
      "  batch 100 loss: 1.034007202386856\n",
      "  batch 150 loss: 1.02709477186203\n",
      "  batch 200 loss: 1.0438874113559722\n",
      "  batch 250 loss: 1.0667547535896302\n",
      "  batch 300 loss: 1.0529281866550446\n",
      "  batch 350 loss: 1.0401423788070678\n",
      "  batch 400 loss: 1.0369190347194672\n",
      "  batch 450 loss: 1.0767484951019286\n",
      "  batch 500 loss: 1.0566209769248962\n",
      "  batch 550 loss: 1.071710673570633\n",
      "  batch 600 loss: 1.0437991869449617\n",
      "  batch 650 loss: 1.030102597475052\n",
      "  batch 700 loss: 1.046658637523651\n",
      "  batch 750 loss: 1.0224214577674866\n",
      "  batch 800 loss: 1.0262987291812897\n",
      "  batch 850 loss: 1.0099538457393646\n",
      "  batch 900 loss: 1.0542647981643676\n",
      "LOSS train 1.05426 valid 1.05520, valid PER 33.67%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.0259234941005706\n",
      "  batch 100 loss: 1.026626431941986\n",
      "  batch 150 loss: 1.0188097190856933\n",
      "  batch 200 loss: 0.9985960805416108\n",
      "  batch 250 loss: 0.994280149936676\n",
      "  batch 300 loss: 0.9814891803264618\n",
      "  batch 350 loss: 0.9773880350589752\n",
      "  batch 400 loss: 1.0084350717067718\n",
      "  batch 450 loss: 1.0069153332710266\n",
      "  batch 500 loss: 0.9790928936004639\n",
      "  batch 550 loss: 1.0136120021343231\n",
      "  batch 600 loss: 0.9880113458633423\n",
      "  batch 650 loss: 1.0024557495117188\n",
      "  batch 700 loss: 1.0165152394771575\n",
      "  batch 750 loss: 0.9823886799812317\n",
      "  batch 800 loss: 0.97738689661026\n",
      "  batch 850 loss: 1.0260731673240662\n",
      "  batch 900 loss: 1.0410805809497834\n",
      "LOSS train 1.04108 valid 1.03559, valid PER 33.12%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.9626719152927399\n",
      "  batch 100 loss: 0.9600415027141571\n",
      "  batch 150 loss: 0.9633189821243286\n",
      "  batch 200 loss: 0.9447251844406128\n",
      "  batch 250 loss: 0.9778933262825013\n",
      "  batch 300 loss: 0.9037618279457093\n",
      "  batch 350 loss: 0.9706202137470246\n",
      "  batch 400 loss: 0.9509328269958496\n",
      "  batch 450 loss: 0.9777150321006775\n",
      "  batch 500 loss: 1.0114691150188446\n",
      "  batch 550 loss: 0.9366719543933868\n",
      "  batch 600 loss: 0.9750475454330444\n",
      "  batch 650 loss: 0.9973839449882508\n",
      "  batch 700 loss: 0.9490672314167022\n",
      "  batch 750 loss: 0.9675363695621491\n",
      "  batch 800 loss: 0.9822844803333283\n",
      "  batch 850 loss: 0.9711802434921265\n",
      "  batch 900 loss: 0.9593174064159393\n",
      "LOSS train 0.95932 valid 1.01760, valid PER 32.37%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.9293974494934082\n",
      "  batch 100 loss: 0.9442276763916015\n",
      "  batch 150 loss: 0.9341436517238617\n",
      "  batch 200 loss: 0.913530969619751\n",
      "  batch 250 loss: 0.9446100652217865\n",
      "  batch 300 loss: 0.945527263879776\n",
      "  batch 350 loss: 0.977331190109253\n",
      "  batch 400 loss: 0.9582402908802032\n",
      "  batch 450 loss: 0.9480319654941559\n",
      "  batch 500 loss: 0.9134587848186493\n",
      "  batch 550 loss: 0.9584154045581817\n",
      "  batch 600 loss: 0.9633473610877991\n",
      "  batch 650 loss: 0.9400523722171783\n",
      "  batch 700 loss: 0.9502769660949707\n",
      "  batch 750 loss: 0.9598099291324615\n",
      "  batch 800 loss: 0.9643144500255585\n",
      "  batch 850 loss: 0.9685314667224884\n",
      "  batch 900 loss: 0.9158102977275848\n",
      "LOSS train 0.91581 valid 0.98423, valid PER 30.82%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.8775173103809357\n",
      "  batch 100 loss: 0.913598061800003\n",
      "  batch 150 loss: 0.921885508298874\n",
      "  batch 200 loss: 0.9263349854946137\n",
      "  batch 250 loss: 0.9163955247402191\n",
      "  batch 300 loss: 0.88387033700943\n",
      "  batch 350 loss: 0.930627875328064\n",
      "  batch 400 loss: 0.8877560091018677\n",
      "  batch 450 loss: 0.8735936224460602\n",
      "  batch 500 loss: 0.9363296484947204\n",
      "  batch 550 loss: 0.9217387497425079\n",
      "  batch 600 loss: 0.8980883586406708\n",
      "  batch 650 loss: 0.9075719082355499\n",
      "  batch 700 loss: 0.9172575032711029\n",
      "  batch 750 loss: 0.887300786972046\n",
      "  batch 800 loss: 0.9157607638835907\n",
      "  batch 850 loss: 0.9319009399414062\n",
      "  batch 900 loss: 0.9184991812705994\n",
      "LOSS train 0.91850 valid 0.99652, valid PER 32.38%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.8625393903255463\n",
      "  batch 100 loss: 0.839671196937561\n",
      "  batch 150 loss: 0.8634918904304505\n",
      "  batch 200 loss: 0.9081587529182434\n",
      "  batch 250 loss: 0.8904229617118835\n",
      "  batch 300 loss: 0.8364888668060303\n",
      "  batch 350 loss: 0.8526960802078247\n",
      "  batch 400 loss: 0.8938845813274383\n",
      "  batch 450 loss: 0.8906530976295471\n",
      "  batch 500 loss: 0.8971701776981353\n",
      "  batch 550 loss: 0.8908345210552215\n",
      "  batch 600 loss: 0.8891965687274933\n",
      "  batch 650 loss: 0.9354985320568084\n",
      "  batch 700 loss: 0.8570788061618805\n",
      "  batch 750 loss: 0.8767597806453705\n",
      "  batch 800 loss: 0.9148175895214081\n",
      "  batch 850 loss: 0.924563307762146\n",
      "  batch 900 loss: 0.9289251303672791\n",
      "LOSS train 0.92893 valid 0.97314, valid PER 30.74%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.8768631744384766\n",
      "  batch 100 loss: 0.8400574004650116\n",
      "  batch 150 loss: 0.8360664319992065\n",
      "  batch 200 loss: 0.8544304025173187\n",
      "  batch 250 loss: 0.8661251091957092\n",
      "  batch 300 loss: 0.8572370064258575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 350 loss: 0.8611113953590394\n",
      "  batch 400 loss: 0.8878334987163544\n",
      "  batch 450 loss: 0.8760842978954315\n",
      "  batch 500 loss: 0.9018292856216431\n",
      "  batch 550 loss: 0.8373200178146363\n",
      "  batch 600 loss: 0.8450618469715119\n",
      "  batch 650 loss: 0.8954958248138428\n",
      "  batch 700 loss: 0.8742932832241058\n",
      "  batch 750 loss: 0.8496621632575989\n",
      "  batch 800 loss: 0.8632363307476044\n",
      "  batch 850 loss: 0.9055159294605255\n",
      "  batch 900 loss: 0.8848416638374329\n",
      "LOSS train 0.88484 valid 0.94071, valid PER 29.80%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.8132411193847656\n",
      "  batch 100 loss: 0.8469280171394348\n",
      "  batch 150 loss: 0.8175043714046478\n",
      "  batch 200 loss: 0.8333542454242706\n",
      "  batch 250 loss: 0.8340844535827636\n",
      "  batch 300 loss: 0.8193296492099762\n",
      "  batch 350 loss: 0.8398231458663941\n",
      "  batch 400 loss: 0.8458323919773102\n",
      "  batch 450 loss: 0.8519457268714905\n",
      "  batch 500 loss: 0.821226360797882\n",
      "  batch 550 loss: 0.8468969118595123\n",
      "  batch 600 loss: 0.8427791285514832\n",
      "  batch 650 loss: 0.8556023001670837\n",
      "  batch 700 loss: 0.8603373467922211\n",
      "  batch 750 loss: 0.8132514822483062\n",
      "  batch 800 loss: 0.8307002878189087\n",
      "  batch 850 loss: 0.8747296869754791\n",
      "  batch 900 loss: 0.8596492695808411\n",
      "LOSS train 0.85965 valid 0.94888, valid PER 29.50%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.8014659869670868\n",
      "  batch 100 loss: 0.8323886847496033\n",
      "  batch 150 loss: 0.8146498727798462\n",
      "  batch 200 loss: 0.8385422122478485\n",
      "  batch 250 loss: 0.8347208058834076\n",
      "  batch 300 loss: 0.8555596530437469\n",
      "  batch 350 loss: 0.8203481876850128\n",
      "  batch 400 loss: 0.8159799253940583\n",
      "  batch 450 loss: 0.8183926260471344\n",
      "  batch 500 loss: 0.8323267960548401\n",
      "  batch 550 loss: 0.8360108470916748\n",
      "  batch 600 loss: 0.8043940103054047\n",
      "  batch 650 loss: 0.827293426990509\n",
      "  batch 700 loss: 0.8585725343227386\n",
      "  batch 750 loss: 0.8205786550045013\n",
      "  batch 800 loss: 0.7944967067241668\n",
      "  batch 850 loss: 0.8634980881214142\n",
      "  batch 900 loss: 0.8499953055381775\n",
      "LOSS train 0.85000 valid 0.94041, valid PER 29.40%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.7972995150089264\n",
      "  batch 100 loss: 0.775305882692337\n",
      "  batch 150 loss: 0.7987935161590576\n",
      "  batch 200 loss: 0.8366833591461181\n",
      "  batch 250 loss: 0.8347377026081085\n",
      "  batch 300 loss: 0.8070062577724457\n",
      "  batch 350 loss: 0.7979271578788757\n",
      "  batch 400 loss: 0.8027207970619201\n",
      "  batch 450 loss: 0.7910550880432129\n",
      "  batch 500 loss: 0.7766602814197541\n",
      "  batch 550 loss: 0.8026414132118225\n",
      "  batch 600 loss: 0.838420068025589\n",
      "  batch 650 loss: 0.8151787054538727\n",
      "  batch 700 loss: 0.8268147349357605\n",
      "  batch 750 loss: 0.8050629955530166\n",
      "  batch 800 loss: 0.8061955809593201\n",
      "  batch 850 loss: 0.7842384421825409\n",
      "  batch 900 loss: 0.8084138822555542\n",
      "LOSS train 0.80841 valid 0.93980, valid PER 29.52%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.7986081254482269\n",
      "  batch 100 loss: 0.7701155591011047\n",
      "  batch 150 loss: 0.7930908048152924\n",
      "  batch 200 loss: 0.7773010790348053\n",
      "  batch 250 loss: 0.8101115870475769\n",
      "  batch 300 loss: 0.7706248033046722\n",
      "  batch 350 loss: 0.8122784316539764\n",
      "  batch 400 loss: 0.7971975564956665\n",
      "  batch 450 loss: 0.8139127647876739\n",
      "  batch 500 loss: 0.7668457961082459\n",
      "  batch 550 loss: 0.8230286145210266\n",
      "  batch 600 loss: 0.7921024811267853\n",
      "  batch 650 loss: 0.7953870487213135\n",
      "  batch 700 loss: 0.7625765585899353\n",
      "  batch 750 loss: 0.7916645407676697\n",
      "  batch 800 loss: 0.7994756317138672\n",
      "  batch 850 loss: 0.7892766630649567\n",
      "  batch 900 loss: 0.7728256762027741\n",
      "LOSS train 0.77283 valid 0.93634, valid PER 28.92%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.773362917304039\n",
      "  batch 100 loss: 0.7822230046987534\n",
      "  batch 150 loss: 0.7558056938648224\n",
      "  batch 200 loss: 0.7646186351776123\n",
      "  batch 250 loss: 0.7718977415561676\n",
      "  batch 300 loss: 0.7803913962841034\n",
      "  batch 350 loss: 0.7317826855182648\n",
      "  batch 400 loss: 0.8031967675685883\n",
      "  batch 450 loss: 0.7998388648033142\n",
      "  batch 500 loss: 0.7664595091342926\n",
      "  batch 550 loss: 0.7786786830425263\n",
      "  batch 600 loss: 0.8160980057716369\n",
      "  batch 650 loss: 0.7808106768131257\n",
      "  batch 700 loss: 0.7833675193786621\n",
      "  batch 750 loss: 0.748669947385788\n",
      "  batch 800 loss: 0.7613657540082932\n",
      "  batch 850 loss: 0.7749056375026703\n",
      "  batch 900 loss: 0.7615512853860855\n",
      "LOSS train 0.76155 valid 0.93188, valid PER 28.45%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.731972541809082\n",
      "  batch 100 loss: 0.7787838661670685\n",
      "  batch 150 loss: 0.7799762797355652\n",
      "  batch 200 loss: 0.7658025509119034\n",
      "  batch 250 loss: 0.7728963112831115\n",
      "  batch 300 loss: 0.7313384890556336\n",
      "  batch 350 loss: 0.7700940257310868\n",
      "  batch 400 loss: 0.7462205588817596\n",
      "  batch 450 loss: 0.7867320561408997\n",
      "  batch 500 loss: 0.7552882188558578\n",
      "  batch 550 loss: 0.756937313079834\n",
      "  batch 600 loss: 0.7483385467529297\n",
      "  batch 650 loss: 0.7422191113233566\n",
      "  batch 700 loss: 0.7826294088363648\n",
      "  batch 750 loss: 0.7394195711612701\n",
      "  batch 800 loss: 0.7401522541046143\n",
      "  batch 850 loss: 0.7521633726358413\n",
      "  batch 900 loss: 0.7921777629852295\n",
      "LOSS train 0.79218 valid 0.93007, valid PER 28.44%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.6962440407276154\n",
      "  batch 100 loss: 0.7115123009681702\n",
      "  batch 150 loss: 0.740871309041977\n",
      "  batch 200 loss: 0.7297595822811127\n",
      "  batch 250 loss: 0.7593446993827819\n",
      "  batch 300 loss: 0.7617752969264984\n",
      "  batch 350 loss: 0.7291662752628326\n",
      "  batch 400 loss: 0.7483492326736451\n",
      "  batch 450 loss: 0.7530409324169159\n",
      "  batch 500 loss: 0.755108014345169\n",
      "  batch 550 loss: 0.7457600736618042\n",
      "  batch 600 loss: 0.7282477760314942\n",
      "  batch 650 loss: 0.8119415783882141\n",
      "  batch 700 loss: 0.7248646199703217\n",
      "  batch 750 loss: 0.7262143421173096\n",
      "  batch 800 loss: 0.769889019727707\n",
      "  batch 850 loss: 0.7878655028343201\n",
      "  batch 900 loss: 0.7663096797466278\n",
      "LOSS train 0.76631 valid 0.92845, valid PER 28.34%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.7066540068387985\n",
      "  batch 100 loss: 0.697156388759613\n",
      "  batch 150 loss: 0.71040660738945\n",
      "  batch 200 loss: 0.7286454308032989\n",
      "  batch 250 loss: 0.728281729221344\n",
      "  batch 300 loss: 0.7453013181686401\n",
      "  batch 350 loss: 0.7206887340545655\n",
      "  batch 400 loss: 0.7243891477584838\n",
      "  batch 450 loss: 0.7498035311698914\n",
      "  batch 500 loss: 0.7142380917072296\n",
      "  batch 550 loss: 0.7896953588724136\n",
      "  batch 600 loss: 0.7132754564285279\n",
      "  batch 650 loss: 0.7406236809492112\n",
      "  batch 700 loss: 0.7457683277130127\n",
      "  batch 750 loss: 0.7288996404409409\n",
      "  batch 800 loss: 0.7623307037353516\n",
      "  batch 850 loss: 0.7456953036785126\n",
      "  batch 900 loss: 0.754433496594429\n",
      "LOSS train 0.75443 valid 0.91062, valid PER 28.05%\n",
      "Training finished in 4.0 minutes.\n",
      "Model saved to checkpoints/20231205_142146/model_20\n",
      "Loading model from checkpoints/20231205_142146/model_20\n",
      "For dropout rate 0.1 the best model has SUB: 16.39%, DEL: 11.33%, INS: 2.41%, COR: 72.28%, PER: 30.13%\n",
      "Total number of model parameters is 166952\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.120614500045776\n",
      "  batch 100 loss: 3.3000460195541383\n",
      "  batch 150 loss: 3.124601435661316\n",
      "  batch 200 loss: 2.858131489753723\n",
      "  batch 250 loss: 2.715221242904663\n",
      "  batch 300 loss: 2.551434235572815\n",
      "  batch 350 loss: 2.4381463718414307\n",
      "  batch 400 loss: 2.3896812629699706\n",
      "  batch 450 loss: 2.319038777351379\n",
      "  batch 500 loss: 2.2150794744491575\n",
      "  batch 550 loss: 2.1553579139709473\n",
      "  batch 600 loss: 2.094276638031006\n",
      "  batch 650 loss: 2.0233914113044738\n",
      "  batch 700 loss: 2.019061732292175\n",
      "  batch 750 loss: 1.9698003458976745\n",
      "  batch 800 loss: 1.9542832708358764\n",
      "  batch 850 loss: 1.8837035250663758\n",
      "  batch 900 loss: 1.8658031249046325\n",
      "LOSS train 1.86580 valid 1.77041, valid PER 67.91%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.8048212027549744\n",
      "  batch 100 loss: 1.7539086651802063\n",
      "  batch 150 loss: 1.7419158434867859\n",
      "  batch 200 loss: 1.746788787841797\n",
      "  batch 250 loss: 1.75826486825943\n",
      "  batch 300 loss: 1.713346996307373\n",
      "  batch 350 loss: 1.6345680785179137\n",
      "  batch 400 loss: 1.6466498279571533\n",
      "  batch 450 loss: 1.6175204944610595\n",
      "  batch 500 loss: 1.6296323537826538\n",
      "  batch 550 loss: 1.6456230878829956\n",
      "  batch 600 loss: 1.590837640762329\n",
      "  batch 650 loss: 1.6346007704734802\n",
      "  batch 700 loss: 1.5891929936408997\n",
      "  batch 750 loss: 1.570399432182312\n",
      "  batch 800 loss: 1.5076803255081177\n",
      "  batch 850 loss: 1.5124828147888183\n",
      "  batch 900 loss: 1.5539766240119934\n",
      "LOSS train 1.55398 valid 1.45034, valid PER 52.83%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.5099555087089538\n",
      "  batch 100 loss: 1.4735266518592836\n",
      "  batch 150 loss: 1.4698436665534973\n",
      "  batch 200 loss: 1.460233371257782\n",
      "  batch 250 loss: 1.4521728992462157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 1.4449453711509705\n",
      "  batch 350 loss: 1.4997631525993347\n",
      "  batch 400 loss: 1.463903205394745\n",
      "  batch 450 loss: 1.444760868549347\n",
      "  batch 500 loss: 1.4349454879760741\n",
      "  batch 550 loss: 1.4261809849739076\n",
      "  batch 600 loss: 1.403098659515381\n",
      "  batch 650 loss: 1.3790776658058166\n",
      "  batch 700 loss: 1.4032788324356078\n",
      "  batch 750 loss: 1.4609530544281006\n",
      "  batch 800 loss: 1.3834053802490234\n",
      "  batch 850 loss: 1.4100740432739258\n",
      "  batch 900 loss: 1.3578569269180298\n",
      "LOSS train 1.35786 valid 1.34689, valid PER 46.03%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.3459925580024719\n",
      "  batch 100 loss: 1.3574698305130004\n",
      "  batch 150 loss: 1.3052812790870667\n",
      "  batch 200 loss: 1.3688257503509522\n",
      "  batch 250 loss: 1.3544222950935363\n",
      "  batch 300 loss: 1.367674810886383\n",
      "  batch 350 loss: 1.2633470797538757\n",
      "  batch 400 loss: 1.3262374711036682\n",
      "  batch 450 loss: 1.3091137957572938\n",
      "  batch 500 loss: 1.3010404324531555\n",
      "  batch 550 loss: 1.3079396271705628\n",
      "  batch 600 loss: 1.3253489208221436\n",
      "  batch 650 loss: 1.3196764862537385\n",
      "  batch 700 loss: 1.275055023431778\n",
      "  batch 750 loss: 1.245316447019577\n",
      "  batch 800 loss: 1.230403138399124\n",
      "  batch 850 loss: 1.2743750619888305\n",
      "  batch 900 loss: 1.2851456809043884\n",
      "LOSS train 1.28515 valid 1.20374, valid PER 38.57%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.2215276968479156\n",
      "  batch 100 loss: 1.2301368927955627\n",
      "  batch 150 loss: 1.2672670447826386\n",
      "  batch 200 loss: 1.1982563066482543\n",
      "  batch 250 loss: 1.213325251340866\n",
      "  batch 300 loss: 1.2292877280712127\n",
      "  batch 350 loss: 1.2274421095848083\n",
      "  batch 400 loss: 1.2328624796867371\n",
      "  batch 450 loss: 1.2045912766456603\n",
      "  batch 500 loss: 1.2379851198196412\n",
      "  batch 550 loss: 1.182734373807907\n",
      "  batch 600 loss: 1.2349057364463807\n",
      "  batch 650 loss: 1.1866609537601471\n",
      "  batch 700 loss: 1.2288083505630494\n",
      "  batch 750 loss: 1.1642203795909882\n",
      "  batch 800 loss: 1.1985673689842224\n",
      "  batch 850 loss: 1.2145536506175996\n",
      "  batch 900 loss: 1.185974270105362\n",
      "LOSS train 1.18597 valid 1.13978, valid PER 37.95%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.186157957315445\n",
      "  batch 100 loss: 1.151495635509491\n",
      "  batch 150 loss: 1.1537120926380158\n",
      "  batch 200 loss: 1.169943369626999\n",
      "  batch 250 loss: 1.1580567133426667\n",
      "  batch 300 loss: 1.1516587507724763\n",
      "  batch 350 loss: 1.150582456588745\n",
      "  batch 400 loss: 1.129864408969879\n",
      "  batch 450 loss: 1.168559616804123\n",
      "  batch 500 loss: 1.163721762895584\n",
      "  batch 550 loss: 1.1771622562408448\n",
      "  batch 600 loss: 1.1340140318870544\n",
      "  batch 650 loss: 1.1497012996673583\n",
      "  batch 700 loss: 1.144165358543396\n",
      "  batch 750 loss: 1.111202108860016\n",
      "  batch 800 loss: 1.121139041185379\n",
      "  batch 850 loss: 1.1278620624542237\n",
      "  batch 900 loss: 1.1351545023918153\n",
      "LOSS train 1.13515 valid 1.09791, valid PER 35.99%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.135943489074707\n",
      "  batch 100 loss: 1.1441657793521882\n",
      "  batch 150 loss: 1.1195041024684906\n",
      "  batch 200 loss: 1.0976552712917327\n",
      "  batch 250 loss: 1.1091918230056763\n",
      "  batch 300 loss: 1.0812500262260436\n",
      "  batch 350 loss: 1.0938219678401948\n",
      "  batch 400 loss: 1.103388261795044\n",
      "  batch 450 loss: 1.1088092482089997\n",
      "  batch 500 loss: 1.0910662853717803\n",
      "  batch 550 loss: 1.0837358403205872\n",
      "  batch 600 loss: 1.086196163892746\n",
      "  batch 650 loss: 1.0817163980007172\n",
      "  batch 700 loss: 1.1110864961147309\n",
      "  batch 750 loss: 1.0774617493152618\n",
      "  batch 800 loss: 1.0759668254852295\n",
      "  batch 850 loss: 1.1078664672374725\n",
      "  batch 900 loss: 1.1543987858295441\n",
      "LOSS train 1.15440 valid 1.09576, valid PER 35.98%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.0745934200286866\n",
      "  batch 100 loss: 1.0719901156425475\n",
      "  batch 150 loss: 1.0748375964164734\n",
      "  batch 200 loss: 1.0395361924171447\n",
      "  batch 250 loss: 1.0712410140037536\n",
      "  batch 300 loss: 1.0197840857505798\n",
      "  batch 350 loss: 1.090737270116806\n",
      "  batch 400 loss: 1.0448904716968537\n",
      "  batch 450 loss: 1.0715847527980804\n",
      "  batch 500 loss: 1.096822041273117\n",
      "  batch 550 loss: 1.0571181571483612\n",
      "  batch 600 loss: 1.0647755408287047\n",
      "  batch 650 loss: 1.1098665511608123\n",
      "  batch 700 loss: 1.022828713655472\n",
      "  batch 750 loss: 1.0628142499923705\n",
      "  batch 800 loss: 1.0635030317306517\n",
      "  batch 850 loss: 1.0686325299739838\n",
      "  batch 900 loss: 1.0484901416301726\n",
      "LOSS train 1.04849 valid 1.06385, valid PER 34.38%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 1.005320588350296\n",
      "  batch 100 loss: 1.049407378435135\n",
      "  batch 150 loss: 1.0382671320438386\n",
      "  batch 200 loss: 1.0034112226963043\n",
      "  batch 250 loss: 1.0456546092033385\n",
      "  batch 300 loss: 1.0384497940540314\n",
      "  batch 350 loss: 1.0461057257652282\n",
      "  batch 400 loss: 1.0647077107429503\n",
      "  batch 450 loss: 1.0363554871082306\n",
      "  batch 500 loss: 1.0074115467071534\n",
      "  batch 550 loss: 1.0462544095516204\n",
      "  batch 600 loss: 1.0425252211093903\n",
      "  batch 650 loss: 1.0169079756736756\n",
      "  batch 700 loss: 1.0312034785747528\n",
      "  batch 750 loss: 1.0302592062950133\n",
      "  batch 800 loss: 1.0716106808185577\n",
      "  batch 850 loss: 1.063189947605133\n",
      "  batch 900 loss: 1.0118159449100494\n",
      "LOSS train 1.01182 valid 1.02929, valid PER 33.89%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.977198201417923\n",
      "  batch 100 loss: 1.0042249929904938\n",
      "  batch 150 loss: 1.023333865404129\n",
      "  batch 200 loss: 1.0220734429359437\n",
      "  batch 250 loss: 1.0123943591117859\n",
      "  batch 300 loss: 0.9729908323287964\n",
      "  batch 350 loss: 1.0233119094371796\n",
      "  batch 400 loss: 0.9724419462680817\n",
      "  batch 450 loss: 0.9741350901126862\n",
      "  batch 500 loss: 1.0307322859764099\n",
      "  batch 550 loss: 1.0384782195091247\n",
      "  batch 600 loss: 1.0203086984157563\n",
      "  batch 650 loss: 0.9932240855693817\n",
      "  batch 700 loss: 1.0121105372905732\n",
      "  batch 750 loss: 1.008101797103882\n",
      "  batch 800 loss: 1.0270920562744141\n",
      "  batch 850 loss: 1.046397839784622\n",
      "  batch 900 loss: 1.0210570538043975\n",
      "LOSS train 1.02106 valid 1.00691, valid PER 32.92%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.9564932024478913\n",
      "  batch 100 loss: 0.936426705121994\n",
      "  batch 150 loss: 0.9545425343513488\n",
      "  batch 200 loss: 1.018390132188797\n",
      "  batch 250 loss: 0.9850155282020568\n",
      "  batch 300 loss: 0.9590071713924408\n",
      "  batch 350 loss: 0.9755381906032562\n",
      "  batch 400 loss: 1.0204986011981965\n",
      "  batch 450 loss: 0.9905347728729248\n",
      "  batch 500 loss: 0.9679548227787018\n",
      "  batch 550 loss: 0.9821150612831115\n",
      "  batch 600 loss: 0.9498224794864655\n",
      "  batch 650 loss: 1.0230351424217223\n",
      "  batch 700 loss: 0.9571005141735077\n",
      "  batch 750 loss: 0.9693765783309937\n",
      "  batch 800 loss: 0.9937552630901336\n",
      "  batch 850 loss: 1.0172412669658661\n",
      "  batch 900 loss: 1.014075733423233\n",
      "LOSS train 1.01408 valid 1.00159, valid PER 32.35%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.9963335514068603\n",
      "  batch 100 loss: 0.9461266815662384\n",
      "  batch 150 loss: 0.9410050344467163\n",
      "  batch 200 loss: 0.9800203216075897\n",
      "  batch 250 loss: 0.9900774598121643\n",
      "  batch 300 loss: 0.9512972700595855\n",
      "  batch 350 loss: 0.9522692430019378\n",
      "  batch 400 loss: 0.978876085281372\n",
      "  batch 450 loss: 0.9760887622833252\n",
      "  batch 500 loss: 0.993694143295288\n",
      "  batch 550 loss: 0.9193350052833558\n",
      "  batch 600 loss: 0.9261996150016785\n",
      "  batch 650 loss: 0.9833146178722382\n",
      "  batch 700 loss: 0.982774670124054\n",
      "  batch 750 loss: 0.9218987810611725\n",
      "  batch 800 loss: 0.9554567444324493\n",
      "  batch 850 loss: 0.9906582665443421\n",
      "  batch 900 loss: 0.9936642706394195\n",
      "LOSS train 0.99366 valid 1.00407, valid PER 32.43%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.9189270782470703\n",
      "  batch 100 loss: 0.9373662769794464\n",
      "  batch 150 loss: 0.9172213470935822\n",
      "  batch 200 loss: 0.9362669503688812\n",
      "  batch 250 loss: 0.9432704877853394\n",
      "  batch 300 loss: 0.9284270906448364\n",
      "  batch 350 loss: 0.9336843740940094\n",
      "  batch 400 loss: 0.9428763616085053\n",
      "  batch 450 loss: 0.9681419599056243\n",
      "  batch 500 loss: 0.9145668470859527\n",
      "  batch 550 loss: 0.9282093334197998\n",
      "  batch 600 loss: 0.9303999841213226\n",
      "  batch 650 loss: 0.9433355128765106\n",
      "  batch 700 loss: 0.94961629986763\n",
      "  batch 750 loss: 0.9008127772808074\n",
      "  batch 800 loss: 0.9442488408088684\n",
      "  batch 850 loss: 0.973256094455719\n",
      "  batch 900 loss: 0.96206014752388\n",
      "LOSS train 0.96206 valid 0.98450, valid PER 31.85%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.9223851096630097\n",
      "  batch 100 loss: 0.9205775654315949\n",
      "  batch 150 loss: 0.9219070649147034\n",
      "  batch 200 loss: 0.9258728146553039\n",
      "  batch 250 loss: 0.9072824108600617\n",
      "  batch 300 loss: 0.9533568394184112\n",
      "  batch 350 loss: 0.8924210464954376\n",
      "  batch 400 loss: 0.9062508141994476\n",
      "  batch 450 loss: 0.9094022846221924\n",
      "  batch 500 loss: 0.9093809342384338\n",
      "  batch 550 loss: 0.9449470448493957\n",
      "  batch 600 loss: 0.9067639482021331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 650 loss: 0.9395581877231598\n",
      "  batch 700 loss: 0.9653921222686768\n",
      "  batch 750 loss: 0.9124148428440094\n",
      "  batch 800 loss: 0.9002695143222809\n",
      "  batch 850 loss: 0.9361945915222168\n",
      "  batch 900 loss: 0.9338679730892181\n",
      "LOSS train 0.93387 valid 0.97972, valid PER 31.15%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.9046013760566711\n",
      "  batch 100 loss: 0.8899940848350525\n",
      "  batch 150 loss: 0.9034054362773896\n",
      "  batch 200 loss: 0.9487513303756714\n",
      "  batch 250 loss: 0.9294521808624268\n",
      "  batch 300 loss: 0.89101478099823\n",
      "  batch 350 loss: 0.8995310854911804\n",
      "  batch 400 loss: 0.896698739528656\n",
      "  batch 450 loss: 0.9088983130455017\n",
      "  batch 500 loss: 0.8744363379478455\n",
      "  batch 550 loss: 0.9166146326065063\n",
      "  batch 600 loss: 0.9526425337791443\n",
      "  batch 650 loss: 0.9376431119441986\n",
      "  batch 700 loss: 0.9414861524105071\n",
      "  batch 750 loss: 0.9244101524353028\n",
      "  batch 800 loss: 0.9065834474563599\n",
      "  batch 850 loss: 0.9027424621582031\n",
      "  batch 900 loss: 0.9141540694236755\n",
      "LOSS train 0.91415 valid 0.97418, valid PER 30.84%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.9130405569076538\n",
      "  batch 100 loss: 0.870883332490921\n",
      "  batch 150 loss: 0.8964839613437653\n",
      "  batch 200 loss: 0.9152531218528748\n",
      "  batch 250 loss: 0.9116529023647308\n",
      "  batch 300 loss: 0.8912066435813903\n",
      "  batch 350 loss: 0.912507348060608\n",
      "  batch 400 loss: 0.9098565554618836\n",
      "  batch 450 loss: 0.9243307161331177\n",
      "  batch 500 loss: 0.857400095462799\n",
      "  batch 550 loss: 0.8987109255790711\n",
      "  batch 600 loss: 0.8851453506946564\n",
      "  batch 650 loss: 0.9033209013938904\n",
      "  batch 700 loss: 0.8782714653015137\n",
      "  batch 750 loss: 0.8918269002437591\n",
      "  batch 800 loss: 0.9143328070640564\n",
      "  batch 850 loss: 0.8916151225566864\n",
      "  batch 900 loss: 0.900327365398407\n",
      "LOSS train 0.90033 valid 0.96213, valid PER 30.80%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.8767118716239929\n",
      "  batch 100 loss: 0.886063768863678\n",
      "  batch 150 loss: 0.8483995258808136\n",
      "  batch 200 loss: 0.8625687599182129\n",
      "  batch 250 loss: 0.8882120263576507\n",
      "  batch 300 loss: 0.8886216187477112\n",
      "  batch 350 loss: 0.861457347869873\n",
      "  batch 400 loss: 0.9169968366622925\n",
      "  batch 450 loss: 0.912857369184494\n",
      "  batch 500 loss: 0.8543757164478302\n",
      "  batch 550 loss: 0.892466037273407\n",
      "  batch 600 loss: 0.9235541212558747\n",
      "  batch 650 loss: 0.8865994322299957\n",
      "  batch 700 loss: 0.8671466445922852\n",
      "  batch 750 loss: 0.8564491593837738\n",
      "  batch 800 loss: 0.8716144716739654\n",
      "  batch 850 loss: 0.8945591616630554\n",
      "  batch 900 loss: 0.8341677010059356\n",
      "LOSS train 0.83417 valid 0.94292, valid PER 29.72%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.8771552574634552\n",
      "  batch 100 loss: 0.8737812876701355\n",
      "  batch 150 loss: 0.8884453117847443\n",
      "  batch 200 loss: 0.8608335280418395\n",
      "  batch 250 loss: 0.8547910940647125\n",
      "  batch 300 loss: 0.8423548340797424\n",
      "  batch 350 loss: 0.8693634390830993\n",
      "  batch 400 loss: 0.8457250463962555\n",
      "  batch 450 loss: 0.9121624720096588\n",
      "  batch 500 loss: 0.8846622347831726\n",
      "  batch 550 loss: 0.8781128025054932\n",
      "  batch 600 loss: 0.8757049882411957\n",
      "  batch 650 loss: 0.862754430770874\n",
      "  batch 700 loss: 0.8917391347885132\n",
      "  batch 750 loss: 0.8418434083461761\n",
      "  batch 800 loss: 0.849913147687912\n",
      "  batch 850 loss: 0.8557979500293732\n",
      "  batch 900 loss: 0.8847065329551697\n",
      "LOSS train 0.88471 valid 0.98452, valid PER 31.44%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.8217922270298004\n",
      "  batch 100 loss: 0.8210309565067291\n",
      "  batch 150 loss: 0.8451923847198486\n",
      "  batch 200 loss: 0.8649770927429199\n",
      "  batch 250 loss: 0.9052310347557068\n",
      "  batch 300 loss: 0.8622901737689972\n",
      "  batch 350 loss: 0.8262952888011932\n",
      "  batch 400 loss: 0.8429688096046448\n",
      "  batch 450 loss: 0.8623302674293518\n",
      "  batch 500 loss: 0.8920584213733673\n",
      "  batch 550 loss: 0.8549021446704864\n",
      "  batch 600 loss: 0.8596446573734283\n",
      "  batch 650 loss: 0.9132473850250244\n",
      "  batch 700 loss: 0.8465803265571594\n",
      "  batch 750 loss: 0.8288228297233582\n",
      "  batch 800 loss: 0.8951899623870849\n",
      "  batch 850 loss: 0.8601623332500458\n",
      "  batch 900 loss: 0.8365276563167572\n",
      "LOSS train 0.83653 valid 0.95191, valid PER 29.80%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.8336544728279114\n",
      "  batch 100 loss: 0.8282880365848542\n",
      "  batch 150 loss: 0.8095559716224671\n",
      "  batch 200 loss: 0.8504553472995758\n",
      "  batch 250 loss: 0.840456737279892\n",
      "  batch 300 loss: 0.8371189522743225\n",
      "  batch 350 loss: 0.8300205743312836\n",
      "  batch 400 loss: 0.8556610774993897\n",
      "  batch 450 loss: 0.8382148730754853\n",
      "  batch 500 loss: 0.823244571685791\n",
      "  batch 550 loss: 0.8819130277633667\n",
      "  batch 600 loss: 0.8344896614551545\n",
      "  batch 650 loss: 0.8885328888893127\n",
      "  batch 700 loss: 0.8616962647438049\n",
      "  batch 750 loss: 0.8371648669242859\n",
      "  batch 800 loss: 0.8674851655960083\n",
      "  batch 850 loss: 0.867018095254898\n",
      "  batch 900 loss: 0.8554747927188874\n",
      "LOSS train 0.85547 valid 0.94166, valid PER 29.66%\n",
      "Training finished in 3.0 minutes.\n",
      "Model saved to checkpoints/20231205_142615/model_20\n",
      "Loading model from checkpoints/20231205_142615/model_20\n",
      "For dropout rate 0.3 the best model has SUB: 15.63%, DEL: 14.83%, INS: 1.29%, COR: 69.54%, PER: 31.75%\n",
      "Total number of model parameters is 166952\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.118948860168457\n",
      "  batch 100 loss: 3.297961459159851\n",
      "  batch 150 loss: 3.1246758460998536\n",
      "  batch 200 loss: 2.8762296772003175\n",
      "  batch 250 loss: 2.718871483802795\n",
      "  batch 300 loss: 2.553984441757202\n",
      "  batch 350 loss: 2.4586208486557006\n",
      "  batch 400 loss: 2.4237012910842894\n",
      "  batch 450 loss: 2.358083701133728\n",
      "  batch 500 loss: 2.2731155014038085\n",
      "  batch 550 loss: 2.1994093751907347\n",
      "  batch 600 loss: 2.153946557044983\n",
      "  batch 650 loss: 2.086991775035858\n",
      "  batch 700 loss: 2.077454767227173\n",
      "  batch 750 loss: 2.032920501232147\n",
      "  batch 800 loss: 2.009036681652069\n",
      "  batch 850 loss: 1.9509474420547486\n",
      "  batch 900 loss: 1.9561198878288268\n",
      "LOSS train 1.95612 valid 1.83756, valid PER 74.24%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.8953921270370484\n",
      "  batch 100 loss: 1.8392416501045228\n",
      "  batch 150 loss: 1.8313003039360047\n",
      "  batch 200 loss: 1.8378636384010314\n",
      "  batch 250 loss: 1.8459821462631225\n",
      "  batch 300 loss: 1.801546678543091\n",
      "  batch 350 loss: 1.7311711144447326\n",
      "  batch 400 loss: 1.7432802605628968\n",
      "  batch 450 loss: 1.7038072395324706\n",
      "  batch 500 loss: 1.728890130519867\n",
      "  batch 550 loss: 1.7317760682106018\n",
      "  batch 600 loss: 1.689454483985901\n",
      "  batch 650 loss: 1.731104097366333\n",
      "  batch 700 loss: 1.6897924518585206\n",
      "  batch 750 loss: 1.6736325097084046\n",
      "  batch 800 loss: 1.6027063703536988\n",
      "  batch 850 loss: 1.6226115345954895\n",
      "  batch 900 loss: 1.6503333854675293\n",
      "LOSS train 1.65033 valid 1.52728, valid PER 60.00%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.6183930158615112\n",
      "  batch 100 loss: 1.5744520449638366\n",
      "  batch 150 loss: 1.577164580821991\n",
      "  batch 200 loss: 1.568639268875122\n",
      "  batch 250 loss: 1.5598477602005005\n",
      "  batch 300 loss: 1.554230558872223\n",
      "  batch 350 loss: 1.5875686073303223\n",
      "  batch 400 loss: 1.5658883953094482\n",
      "  batch 450 loss: 1.5466305923461914\n",
      "  batch 500 loss: 1.5282957363128662\n",
      "  batch 550 loss: 1.520930745601654\n",
      "  batch 600 loss: 1.5076969027519227\n",
      "  batch 650 loss: 1.4870125603675843\n",
      "  batch 700 loss: 1.5107554984092713\n",
      "  batch 750 loss: 1.5732583498954773\n",
      "  batch 800 loss: 1.491846661567688\n",
      "  batch 850 loss: 1.5183656334877014\n",
      "  batch 900 loss: 1.4507115483283997\n",
      "LOSS train 1.45071 valid 1.43251, valid PER 53.13%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.469339075088501\n",
      "  batch 100 loss: 1.4762518882751465\n",
      "  batch 150 loss: 1.4204707169532775\n",
      "  batch 200 loss: 1.477968032360077\n",
      "  batch 250 loss: 1.4726079535484313\n",
      "  batch 300 loss: 1.471742374897003\n",
      "  batch 350 loss: 1.4011637449264527\n",
      "  batch 400 loss: 1.4407696008682251\n",
      "  batch 450 loss: 1.440278091430664\n",
      "  batch 500 loss: 1.4126926493644714\n",
      "  batch 550 loss: 1.436172423362732\n",
      "  batch 600 loss: 1.4537531638145447\n",
      "  batch 650 loss: 1.4323176169395446\n",
      "  batch 700 loss: 1.4022157454490662\n",
      "  batch 750 loss: 1.3959686636924744\n",
      "  batch 800 loss: 1.3513557600975037\n",
      "  batch 850 loss: 1.3949056720733644\n",
      "  batch 900 loss: 1.4271758317947387\n",
      "LOSS train 1.42718 valid 1.32692, valid PER 48.37%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.369853663444519\n",
      "  batch 100 loss: 1.36995698928833\n",
      "  batch 150 loss: 1.4198426032066345\n",
      "  batch 200 loss: 1.3282460629940034\n",
      "  batch 250 loss: 1.3559516549110413\n",
      "  batch 300 loss: 1.357889063358307\n",
      "  batch 350 loss: 1.367707498073578\n",
      "  batch 400 loss: 1.3634373044967651\n",
      "  batch 450 loss: 1.3450072026252746\n",
      "  batch 500 loss: 1.3564003777503968\n",
      "  batch 550 loss: 1.319064928293228\n",
      "  batch 600 loss: 1.3741400957107544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 650 loss: 1.3292195630073547\n",
      "  batch 700 loss: 1.3736177134513854\n",
      "  batch 750 loss: 1.3042390489578246\n",
      "  batch 800 loss: 1.3427158617973327\n",
      "  batch 850 loss: 1.3480320596694946\n",
      "  batch 900 loss: 1.3546583557128906\n",
      "LOSS train 1.35466 valid 1.26431, valid PER 43.73%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.330161874294281\n",
      "  batch 100 loss: 1.2745098960399628\n",
      "  batch 150 loss: 1.283777447938919\n",
      "  batch 200 loss: 1.2954178428649903\n",
      "  batch 250 loss: 1.320525608062744\n",
      "  batch 300 loss: 1.2892670106887818\n",
      "  batch 350 loss: 1.300743155479431\n",
      "  batch 400 loss: 1.2775436902046204\n",
      "  batch 450 loss: 1.3058509886264802\n",
      "  batch 500 loss: 1.2866346991062165\n",
      "  batch 550 loss: 1.3192792558670043\n",
      "  batch 600 loss: 1.2683533573150634\n",
      "  batch 650 loss: 1.2947034525871277\n",
      "  batch 700 loss: 1.2748800683021546\n",
      "  batch 750 loss: 1.2526218175888062\n",
      "  batch 800 loss: 1.2498886799812317\n",
      "  batch 850 loss: 1.2526446998119354\n",
      "  batch 900 loss: 1.2733447825908661\n",
      "LOSS train 1.27334 valid 1.19146, valid PER 40.92%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.2413323307037354\n",
      "  batch 100 loss: 1.2699664688110353\n",
      "  batch 150 loss: 1.242513736486435\n",
      "  batch 200 loss: 1.228369461297989\n",
      "  batch 250 loss: 1.2220222103595733\n",
      "  batch 300 loss: 1.20248109459877\n",
      "  batch 350 loss: 1.212457299232483\n",
      "  batch 400 loss: 1.2280672574043274\n",
      "  batch 450 loss: 1.2168922019004822\n",
      "  batch 500 loss: 1.2341079127788543\n",
      "  batch 550 loss: 1.2059121251106262\n",
      "  batch 600 loss: 1.227170959711075\n",
      "  batch 650 loss: 1.1993056797981263\n",
      "  batch 700 loss: 1.2392175471782685\n",
      "  batch 750 loss: 1.1914870870113372\n",
      "  batch 800 loss: 1.2134136211872102\n",
      "  batch 850 loss: 1.2195866429805755\n",
      "  batch 900 loss: 1.2819155383110046\n",
      "LOSS train 1.28192 valid 1.14384, valid PER 38.17%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.2098490035533904\n",
      "  batch 100 loss: 1.1851897382736205\n",
      "  batch 150 loss: 1.1683724510669709\n",
      "  batch 200 loss: 1.1633862948417664\n",
      "  batch 250 loss: 1.1918345546722413\n",
      "  batch 300 loss: 1.1148759365081786\n",
      "  batch 350 loss: 1.218733741044998\n",
      "  batch 400 loss: 1.172246391773224\n",
      "  batch 450 loss: 1.1968179059028625\n",
      "  batch 500 loss: 1.222965146303177\n",
      "  batch 550 loss: 1.1524651062488556\n",
      "  batch 600 loss: 1.1892919290065764\n",
      "  batch 650 loss: 1.2245773148536683\n",
      "  batch 700 loss: 1.146343024969101\n",
      "  batch 750 loss: 1.1591064202785493\n",
      "  batch 800 loss: 1.1907350242137908\n",
      "  batch 850 loss: 1.1857887053489684\n",
      "  batch 900 loss: 1.167355409860611\n",
      "LOSS train 1.16736 valid 1.10281, valid PER 36.82%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 1.134043734073639\n",
      "  batch 100 loss: 1.1714467227458953\n",
      "  batch 150 loss: 1.146565259695053\n",
      "  batch 200 loss: 1.1107511734962463\n",
      "  batch 250 loss: 1.146655455827713\n",
      "  batch 300 loss: 1.1532543349266051\n",
      "  batch 350 loss: 1.1609103548526765\n",
      "  batch 400 loss: 1.1602192211151123\n",
      "  batch 450 loss: 1.149060662984848\n",
      "  batch 500 loss: 1.1347567093372346\n",
      "  batch 550 loss: 1.1693065011501311\n",
      "  batch 600 loss: 1.174647400379181\n",
      "  batch 650 loss: 1.1306063508987427\n",
      "  batch 700 loss: 1.1535667407512664\n",
      "  batch 750 loss: 1.1447451198101044\n",
      "  batch 800 loss: 1.1547102630138397\n",
      "  batch 850 loss: 1.192349922657013\n",
      "  batch 900 loss: 1.144285522699356\n",
      "LOSS train 1.14429 valid 1.08406, valid PER 35.88%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 1.0942558312416077\n",
      "  batch 100 loss: 1.1185176730155946\n",
      "  batch 150 loss: 1.1178681778907775\n",
      "  batch 200 loss: 1.1483396577835083\n",
      "  batch 250 loss: 1.131190459728241\n",
      "  batch 300 loss: 1.0789769530296325\n",
      "  batch 350 loss: 1.123005095720291\n",
      "  batch 400 loss: 1.1008726668357849\n",
      "  batch 450 loss: 1.0892326128482819\n",
      "  batch 500 loss: 1.1212266421318053\n",
      "  batch 550 loss: 1.1582954156398773\n",
      "  batch 600 loss: 1.1357409417629243\n",
      "  batch 650 loss: 1.1168115425109864\n",
      "  batch 700 loss: 1.1265869772434234\n",
      "  batch 750 loss: 1.1183190381526946\n",
      "  batch 800 loss: 1.1214157795906068\n",
      "  batch 850 loss: 1.135741227865219\n",
      "  batch 900 loss: 1.1427541148662568\n",
      "LOSS train 1.14275 valid 1.06436, valid PER 35.82%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 1.0864974570274353\n",
      "  batch 100 loss: 1.0714511215686797\n",
      "  batch 150 loss: 1.08482861995697\n",
      "  batch 200 loss: 1.1094125056266784\n",
      "  batch 250 loss: 1.089536918401718\n",
      "  batch 300 loss: 1.0684504950046538\n",
      "  batch 350 loss: 1.0922044384479523\n",
      "  batch 400 loss: 1.1122174680233001\n",
      "  batch 450 loss: 1.111831201314926\n",
      "  batch 500 loss: 1.0774058437347411\n",
      "  batch 550 loss: 1.082356573343277\n",
      "  batch 600 loss: 1.056705468893051\n",
      "  batch 650 loss: 1.134032119512558\n",
      "  batch 700 loss: 1.0633862125873565\n",
      "  batch 750 loss: 1.0753179776668549\n",
      "  batch 800 loss: 1.1138284862041474\n",
      "  batch 850 loss: 1.1255690693855285\n",
      "  batch 900 loss: 1.1054975140094756\n",
      "LOSS train 1.10550 valid 1.05162, valid PER 35.61%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 1.0827139234542846\n",
      "  batch 100 loss: 1.0618105256557464\n",
      "  batch 150 loss: 1.0417972993850708\n",
      "  batch 200 loss: 1.0583286082744598\n",
      "  batch 250 loss: 1.083249844312668\n",
      "  batch 300 loss: 1.0941974341869354\n",
      "  batch 350 loss: 1.0658267366886138\n",
      "  batch 400 loss: 1.0777462196350098\n",
      "  batch 450 loss: 1.0894116365909576\n",
      "  batch 500 loss: 1.1030082666873933\n",
      "  batch 550 loss: 1.0093308079242707\n",
      "  batch 600 loss: 1.0477242839336396\n",
      "  batch 650 loss: 1.1046294045448304\n",
      "  batch 700 loss: 1.0983250498771668\n",
      "  batch 750 loss: 1.04898064494133\n",
      "  batch 800 loss: 1.068757711648941\n",
      "  batch 850 loss: 1.0984423673152923\n",
      "  batch 900 loss: 1.1291278862953187\n",
      "LOSS train 1.12913 valid 1.01713, valid PER 34.22%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 1.0314054262638093\n",
      "  batch 100 loss: 1.053471670150757\n",
      "  batch 150 loss: 1.0418914771080017\n",
      "  batch 200 loss: 1.0821744310855865\n",
      "  batch 250 loss: 1.069894403219223\n",
      "  batch 300 loss: 1.0374945306777954\n",
      "  batch 350 loss: 1.057798810005188\n",
      "  batch 400 loss: 1.057416377067566\n",
      "  batch 450 loss: 1.0803458821773528\n",
      "  batch 500 loss: 1.0209180235862731\n",
      "  batch 550 loss: 1.0486896872520446\n",
      "  batch 600 loss: 1.0580621075630188\n",
      "  batch 650 loss: 1.0495689356327056\n",
      "  batch 700 loss: 1.0551234591007232\n",
      "  batch 750 loss: 1.0033120107650757\n",
      "  batch 800 loss: 1.0273631536960601\n",
      "  batch 850 loss: 1.0797272753715514\n",
      "  batch 900 loss: 1.0618314278125762\n",
      "LOSS train 1.06183 valid 1.03335, valid PER 33.56%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 1.0355857193470002\n",
      "  batch 100 loss: 1.0316639399528504\n",
      "  batch 150 loss: 1.0472844898700715\n",
      "  batch 200 loss: 1.0527881848812104\n",
      "  batch 250 loss: 1.0255061566829682\n",
      "  batch 300 loss: 1.0749009668827056\n",
      "  batch 350 loss: 1.024700986146927\n",
      "  batch 400 loss: 1.031693549156189\n",
      "  batch 450 loss: 1.0289297103881836\n",
      "  batch 500 loss: 1.046653176546097\n",
      "  batch 550 loss: 1.0464257299900055\n",
      "  batch 600 loss: 1.0011478888988494\n",
      "  batch 650 loss: 1.0391375231742859\n",
      "  batch 700 loss: 1.0798309767246246\n",
      "  batch 750 loss: 1.0163476729393006\n",
      "  batch 800 loss: 1.0158476662635803\n",
      "  batch 850 loss: 1.0411784279346465\n",
      "  batch 900 loss: 1.041883281469345\n",
      "LOSS train 1.04188 valid 1.01932, valid PER 33.69%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 1.018726509809494\n",
      "  batch 100 loss: 0.986447606086731\n",
      "  batch 150 loss: 1.0086023366451264\n",
      "  batch 200 loss: 1.048911610841751\n",
      "  batch 250 loss: 1.0390639317035675\n",
      "  batch 300 loss: 0.9881329691410065\n",
      "  batch 350 loss: 1.014093633890152\n",
      "  batch 400 loss: 1.0071798264980316\n",
      "  batch 450 loss: 1.011828167438507\n",
      "  batch 500 loss: 0.9856778907775879\n",
      "  batch 550 loss: 1.0303430044651032\n",
      "  batch 600 loss: 1.052267507314682\n",
      "  batch 650 loss: 1.0294695460796357\n",
      "  batch 700 loss: 1.0312824416160584\n",
      "  batch 750 loss: 1.0368051767349242\n",
      "  batch 800 loss: 1.011153243780136\n",
      "  batch 850 loss: 1.0100219762325287\n",
      "  batch 900 loss: 1.0319213962554932\n",
      "LOSS train 1.03192 valid 1.01406, valid PER 33.02%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 1.0273600792884827\n",
      "  batch 100 loss: 0.9819635128974915\n",
      "  batch 150 loss: 1.0115418314933777\n",
      "  batch 200 loss: 1.018312680721283\n",
      "  batch 250 loss: 1.0452317202091217\n",
      "  batch 300 loss: 1.0151813995838166\n",
      "  batch 350 loss: 1.0363932311534882\n",
      "  batch 400 loss: 1.0292083394527436\n",
      "  batch 450 loss: 1.0296496415138245\n",
      "  batch 500 loss: 0.9945372045040131\n",
      "  batch 550 loss: 1.0302494168281555\n",
      "  batch 600 loss: 1.004073792695999\n",
      "  batch 650 loss: 1.0327426266670228\n",
      "  batch 700 loss: 1.0021057260036468\n",
      "  batch 750 loss: 0.9940374541282654\n",
      "  batch 800 loss: 1.0159067296981812\n",
      "  batch 850 loss: 0.9794527518749238\n",
      "  batch 900 loss: 1.0140600275993348\n",
      "LOSS train 1.01406 valid 1.00836, valid PER 32.78%\n",
      "EPOCH 17:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 50 loss: 0.9936682057380676\n",
      "  batch 100 loss: 0.992582665681839\n",
      "  batch 150 loss: 0.9895037305355072\n",
      "  batch 200 loss: 0.9792919480800628\n",
      "  batch 250 loss: 0.9966071152687073\n",
      "  batch 300 loss: 0.9792253005504609\n",
      "  batch 350 loss: 0.9785455143451691\n",
      "  batch 400 loss: 1.0347241938114167\n",
      "  batch 450 loss: 1.0235244929790497\n",
      "  batch 500 loss: 0.9943523812294006\n",
      "  batch 550 loss: 1.0067520797252656\n",
      "  batch 600 loss: 1.0146877717971803\n",
      "  batch 650 loss: 1.0043502390384673\n",
      "  batch 700 loss: 0.9991615414619446\n",
      "  batch 750 loss: 0.97656534075737\n",
      "  batch 800 loss: 0.9910345017910004\n",
      "  batch 850 loss: 0.9893876528739929\n",
      "  batch 900 loss: 0.9616194927692413\n",
      "LOSS train 0.96162 valid 0.98419, valid PER 32.26%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.9784866058826447\n",
      "  batch 100 loss: 0.9820462501049042\n",
      "  batch 150 loss: 0.99570272564888\n",
      "  batch 200 loss: 1.0098820960521697\n",
      "  batch 250 loss: 0.9774351942539216\n",
      "  batch 300 loss: 0.9623030960559845\n",
      "  batch 350 loss: 1.0032907402515412\n",
      "  batch 400 loss: 0.9564145147800446\n",
      "  batch 450 loss: 1.0047502851486205\n",
      "  batch 500 loss: 0.9852379453182221\n",
      "  batch 550 loss: 0.9776895892620087\n",
      "  batch 600 loss: 0.9599750220775605\n",
      "  batch 650 loss: 0.9575279104709625\n",
      "  batch 700 loss: 1.040428376197815\n",
      "  batch 750 loss: 0.9781134438514709\n",
      "  batch 800 loss: 0.9870634603500367\n",
      "  batch 850 loss: 0.9852942788600921\n",
      "  batch 900 loss: 1.002040982246399\n",
      "LOSS train 1.00204 valid 0.99917, valid PER 33.26%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.9096757340431213\n",
      "  batch 100 loss: 0.9408627843856812\n",
      "  batch 150 loss: 0.9684953272342682\n",
      "  batch 200 loss: 0.9742604053020477\n",
      "  batch 250 loss: 0.9969636619091033\n",
      "  batch 300 loss: 0.980564855337143\n",
      "  batch 350 loss: 0.933870381116867\n",
      "  batch 400 loss: 0.9655877137184143\n",
      "  batch 450 loss: 0.967582391500473\n",
      "  batch 500 loss: 0.9777765798568726\n",
      "  batch 550 loss: 0.9481277692317963\n",
      "  batch 600 loss: 0.963584588766098\n",
      "  batch 650 loss: 1.0311808431148528\n",
      "  batch 700 loss: 0.9666598200798034\n",
      "  batch 750 loss: 0.9502511012554169\n",
      "  batch 800 loss: 0.9886382126808166\n",
      "  batch 850 loss: 0.9872216892242431\n",
      "  batch 900 loss: 0.9569243502616882\n",
      "LOSS train 0.95692 valid 0.98982, valid PER 32.22%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.937161592245102\n",
      "  batch 100 loss: 0.9435813701152802\n",
      "  batch 150 loss: 0.9506485259532929\n",
      "  batch 200 loss: 0.9665754568576813\n",
      "  batch 250 loss: 0.9541255474090576\n",
      "  batch 300 loss: 0.9753498005867004\n",
      "  batch 350 loss: 0.9444129049777985\n",
      "  batch 400 loss: 0.9452061831951142\n",
      "  batch 450 loss: 0.9713260161876679\n",
      "  batch 500 loss: 0.9382169651985168\n",
      "  batch 550 loss: 0.993319890499115\n",
      "  batch 600 loss: 0.940054122209549\n",
      "  batch 650 loss: 0.9841331028938294\n",
      "  batch 700 loss: 0.9533889484405518\n",
      "  batch 750 loss: 0.9623555529117584\n",
      "  batch 800 loss: 0.9698515009880065\n",
      "  batch 850 loss: 0.9755300998687744\n",
      "  batch 900 loss: 0.9888857209682465\n",
      "LOSS train 0.98889 valid 0.97777, valid PER 32.13%\n",
      "Training finished in 3.0 minutes.\n",
      "Model saved to checkpoints/20231205_142936/model_20\n",
      "Loading model from checkpoints/20231205_142936/model_20\n",
      "For dropout rate 0.5 the best model has SUB: 15.74%, DEL: 16.67%, INS: 1.15%, COR: 67.59%, PER: 33.56%\n",
      "End dropout tuning\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout\n",
    "from datetime import datetime\n",
    "from trainer import train\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start dropout tuning\")\n",
    "\n",
    "dropout_rates=[0.1, 0.3, 0.5]\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    model_with_dropout = model_regularisation_dropout.BiLSTM(args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "    num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "    print('Total number of model parameters is {}'.format(num_params))\n",
    "    start = datetime.now()\n",
    "    model_with_dropout.to(args.device)\n",
    "    model_path = train(model_with_dropout, args)\n",
    "    end = datetime.now()\n",
    "    duration = (end - start).total_seconds()\n",
    "    print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "    print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print('Loading model from {}'.format(model_path))\n",
    "    model_with_dropout.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model_with_dropout.eval()\n",
    "    results = decode(model_with_dropout, args, args.test_json)\n",
    "    print(\"For dropout rate \"+str(dropout_rate)+\" the best model has \"+\"SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%\".format(*results))\n",
    "\n",
    "print(\"End dropout tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13669c21",
   "metadata": {},
   "source": [
    "#### Dropout Study for 2 layer LSTM is here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74023e66",
   "metadata": {},
   "source": [
    "##### run baseline first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ad9fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.061193742752075\n",
      "  batch 100 loss: 3.3865881252288816\n",
      "  batch 150 loss: 3.293755221366882\n",
      "  batch 200 loss: 3.175789179801941\n",
      "  batch 250 loss: 3.0672779417037965\n",
      "  batch 300 loss: 2.830714225769043\n",
      "  batch 350 loss: 2.682212929725647\n",
      "  batch 400 loss: 2.5671300077438355\n",
      "  batch 450 loss: 2.472991647720337\n",
      "  batch 500 loss: 2.35486225605011\n",
      "  batch 550 loss: 2.2587716722488405\n",
      "  batch 600 loss: 2.1914604473114014\n",
      "  batch 650 loss: 2.0849543738365175\n",
      "  batch 700 loss: 2.074123406410217\n",
      "  batch 750 loss: 1.9836278295516967\n",
      "  batch 800 loss: 1.9454709339141845\n",
      "  batch 850 loss: 1.8785255074501037\n",
      "  batch 900 loss: 1.837413592338562\n",
      "LOSS train 1.83741 valid 1.74688, valid PER 67.77%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7467482495307922\n",
      "  batch 100 loss: 1.6714426279067993\n",
      "  batch 150 loss: 1.625415678024292\n",
      "  batch 200 loss: 1.6248766922950744\n",
      "  batch 250 loss: 1.6164043831825257\n",
      "  batch 300 loss: 1.5586814308166503\n",
      "  batch 350 loss: 1.4702653574943543\n",
      "  batch 400 loss: 1.4709549474716186\n",
      "  batch 450 loss: 1.4144835734367371\n",
      "  batch 500 loss: 1.423290023803711\n",
      "  batch 550 loss: 1.4178285193443299\n",
      "  batch 600 loss: 1.342665560245514\n",
      "  batch 650 loss: 1.3766816711425782\n",
      "  batch 700 loss: 1.3184044075012207\n",
      "  batch 750 loss: 1.3171029949188233\n",
      "  batch 800 loss: 1.2336521184444427\n",
      "  batch 850 loss: 1.249532836675644\n",
      "  batch 900 loss: 1.2623320150375366\n",
      "LOSS train 1.26233 valid 1.22971, valid PER 37.84%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.214208847284317\n",
      "  batch 100 loss: 1.195261045694351\n",
      "  batch 150 loss: 1.172592054605484\n",
      "  batch 200 loss: 1.1622734248638154\n",
      "  batch 250 loss: 1.1572011852264403\n",
      "  batch 300 loss: 1.1429054760932922\n",
      "  batch 350 loss: 1.1837054622173309\n",
      "  batch 400 loss: 1.1549097764492036\n",
      "  batch 450 loss: 1.1260046255588532\n",
      "  batch 500 loss: 1.1166860008239745\n",
      "  batch 550 loss: 1.1154245138168335\n",
      "  batch 600 loss: 1.0919878566265107\n",
      "  batch 650 loss: 1.059108523130417\n",
      "  batch 700 loss: 1.0963352751731872\n",
      "  batch 750 loss: 1.1329327094554902\n",
      "  batch 800 loss: 1.0759277582168578\n",
      "  batch 850 loss: 1.098340095281601\n",
      "  batch 900 loss: 1.0310399436950684\n",
      "LOSS train 1.03104 valid 1.07430, valid PER 32.90%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0239323210716247\n",
      "  batch 100 loss: 1.0282196962833405\n",
      "  batch 150 loss: 1.004161797761917\n",
      "  batch 200 loss: 1.0241806542873382\n",
      "  batch 250 loss: 1.0394918060302734\n",
      "  batch 300 loss: 1.0299662721157075\n",
      "  batch 350 loss: 0.9721579837799073\n",
      "  batch 400 loss: 1.0046565079689025\n",
      "  batch 450 loss: 0.9832124304771424\n",
      "  batch 500 loss: 0.9751379513740539\n",
      "  batch 550 loss: 1.00085782289505\n",
      "  batch 600 loss: 1.018567258119583\n",
      "  batch 650 loss: 0.9983161985874176\n",
      "  batch 700 loss: 0.9630227530002594\n",
      "  batch 750 loss: 0.9617591667175293\n",
      "  batch 800 loss: 0.92877032995224\n",
      "  batch 850 loss: 0.9528133177757263\n",
      "  batch 900 loss: 1.009161458015442\n",
      "LOSS train 1.00916 valid 0.97725, valid PER 30.38%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9180868685245513\n",
      "  batch 100 loss: 0.9260568332672119\n",
      "  batch 150 loss: 0.9642847776412964\n",
      "  batch 200 loss: 0.8843211090564728\n",
      "  batch 250 loss: 0.9052803933620452\n",
      "  batch 300 loss: 0.9143050026893615\n",
      "  batch 350 loss: 0.9034741508960724\n",
      "  batch 400 loss: 0.9251833832263947\n",
      "  batch 450 loss: 0.8981532216072082\n",
      "  batch 500 loss: 0.9483284449577332\n",
      "  batch 550 loss: 0.8791597628593445\n",
      "  batch 600 loss: 0.9366281604766846\n",
      "  batch 650 loss: 0.9048696780204772\n",
      "  batch 700 loss: 0.9536407840251923\n",
      "  batch 750 loss: 0.8705822944641113\n",
      "  batch 800 loss: 0.9024621844291687\n",
      "  batch 850 loss: 0.8952609646320343\n",
      "  batch 900 loss: 0.8962230575084686\n",
      "LOSS train 0.89622 valid 0.94242, valid PER 29.12%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8891383028030395\n",
      "  batch 100 loss: 0.8385858869552613\n",
      "  batch 150 loss: 0.8355695545673371\n",
      "  batch 200 loss: 0.8474305826425552\n",
      "  batch 250 loss: 0.8911713278293609\n",
      "  batch 300 loss: 0.872633023262024\n",
      "  batch 350 loss: 0.8567809426784515\n",
      "  batch 400 loss: 0.8302190124988555\n",
      "  batch 450 loss: 0.8675538545846939\n",
      "  batch 500 loss: 0.8463104701042176\n",
      "  batch 550 loss: 0.8741362667083741\n",
      "  batch 600 loss: 0.8552927005290986\n",
      "  batch 650 loss: 0.8455573844909668\n",
      "  batch 700 loss: 0.8428250348567963\n",
      "  batch 750 loss: 0.838928644657135\n",
      "  batch 800 loss: 0.8357394099235534\n",
      "  batch 850 loss: 0.8161771881580353\n",
      "  batch 900 loss: 0.8269148111343384\n",
      "LOSS train 0.82691 valid 0.90652, valid PER 27.94%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.807637619972229\n",
      "  batch 100 loss: 0.8157339715957641\n",
      "  batch 150 loss: 0.7815066230297089\n",
      "  batch 200 loss: 0.786224039196968\n",
      "  batch 250 loss: 0.7946733283996582\n",
      "  batch 300 loss: 0.7698791646957397\n",
      "  batch 350 loss: 0.7940207290649414\n",
      "  batch 400 loss: 0.7788420546054841\n",
      "  batch 450 loss: 0.7748182678222656\n",
      "  batch 500 loss: 0.7901590222120285\n",
      "  batch 550 loss: 0.7592356967926025\n",
      "  batch 600 loss: 0.8069173300266266\n",
      "  batch 650 loss: 0.7763202345371246\n",
      "  batch 700 loss: 0.8111855131387711\n",
      "  batch 750 loss: 0.7923351490497589\n",
      "  batch 800 loss: 0.7762599623203278\n",
      "  batch 850 loss: 0.780282951593399\n",
      "  batch 900 loss: 0.8349350619316102\n",
      "LOSS train 0.83494 valid 0.87576, valid PER 27.16%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.7372149682044983\n",
      "  batch 100 loss: 0.7309159260988235\n",
      "  batch 150 loss: 0.7245810282230377\n",
      "  batch 200 loss: 0.7356967115402222\n",
      "  batch 250 loss: 0.7441107219457627\n",
      "  batch 300 loss: 0.7016502779722213\n",
      "  batch 350 loss: 0.7671323525905609\n",
      "  batch 400 loss: 0.7218638920783996\n",
      "  batch 450 loss: 0.74728730738163\n",
      "  batch 500 loss: 0.77357177734375\n",
      "  batch 550 loss: 0.7157224047183991\n",
      "  batch 600 loss: 0.7658214116096497\n",
      "  batch 650 loss: 0.7621596777439117\n",
      "  batch 700 loss: 0.7357078659534454\n",
      "  batch 750 loss: 0.72649258852005\n",
      "  batch 800 loss: 0.7454624301195145\n",
      "  batch 850 loss: 0.740326252579689\n",
      "  batch 900 loss: 0.7595517069101334\n",
      "LOSS train 0.75955 valid 0.84233, valid PER 26.10%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.6794312608242035\n",
      "  batch 100 loss: 0.7015932011604309\n",
      "  batch 150 loss: 0.7131253486871719\n",
      "  batch 200 loss: 0.6843348318338394\n",
      "  batch 250 loss: 0.7023911297321319\n",
      "  batch 300 loss: 0.7008917105197906\n",
      "  batch 350 loss: 0.7383225691318512\n",
      "  batch 400 loss: 0.7053570920228958\n",
      "  batch 450 loss: 0.714630382657051\n",
      "  batch 500 loss: 0.6866234171390534\n",
      "  batch 550 loss: 0.714216200709343\n",
      "  batch 600 loss: 0.7322727966308594\n",
      "  batch 650 loss: 0.7078194499015809\n",
      "  batch 700 loss: 0.6953322207927704\n",
      "  batch 750 loss: 0.6884033441543579\n",
      "  batch 800 loss: 0.7106656497716903\n",
      "  batch 850 loss: 0.7370320248603821\n",
      "  batch 900 loss: 0.6982107418775558\n",
      "LOSS train 0.69821 valid 0.83110, valid PER 25.92%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6442093735933304\n",
      "  batch 100 loss: 0.6507316213846207\n",
      "  batch 150 loss: 0.6574754601716996\n",
      "  batch 200 loss: 0.6847720265388488\n",
      "  batch 250 loss: 0.6711735856533051\n",
      "  batch 300 loss: 0.6566752421855927\n",
      "  batch 350 loss: 0.6791417449712753\n",
      "  batch 400 loss: 0.6394385397434235\n",
      "  batch 450 loss: 0.6502879542112351\n",
      "  batch 500 loss: 0.6672678560018539\n",
      "  batch 550 loss: 0.6937494522333145\n",
      "  batch 600 loss: 0.6749624675512313\n",
      "  batch 650 loss: 0.6655587553977966\n",
      "  batch 700 loss: 0.6958885985612869\n",
      "  batch 750 loss: 0.664905896782875\n",
      "  batch 800 loss: 0.6922437691688538\n",
      "  batch 850 loss: 0.6828690564632416\n",
      "  batch 900 loss: 0.690402272939682\n",
      "LOSS train 0.69040 valid 0.83189, valid PER 25.90%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6084227323532104\n",
      "  batch 100 loss: 0.5822207587957382\n",
      "  batch 150 loss: 0.6166433465480804\n",
      "  batch 200 loss: 0.6634655445814133\n",
      "  batch 250 loss: 0.6498503226041794\n",
      "  batch 300 loss: 0.615867230296135\n",
      "  batch 350 loss: 0.6392913508415222\n",
      "  batch 400 loss: 0.6554463732242585\n",
      "  batch 450 loss: 0.6372477495670319\n",
      "  batch 500 loss: 0.6173075455427169\n",
      "  batch 550 loss: 0.6420679676532746\n",
      "  batch 600 loss: 0.6229739570617676\n",
      "  batch 650 loss: 0.688008199930191\n",
      "  batch 700 loss: 0.615008225440979\n",
      "  batch 750 loss: 0.6186051833629608\n",
      "  batch 800 loss: 0.6554731494188308\n",
      "  batch 850 loss: 0.6490075635910034\n",
      "  batch 900 loss: 0.6717963361740112\n",
      "LOSS train 0.67180 valid 0.81111, valid PER 24.65%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.5994827783107758\n",
      "  batch 100 loss: 0.5889110010862351\n",
      "  batch 150 loss: 0.5623543030023574\n",
      "  batch 200 loss: 0.5866096836328506\n",
      "  batch 250 loss: 0.6150074356794357\n",
      "  batch 300 loss: 0.5922254252433777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 350 loss: 0.6080732196569443\n",
      "  batch 400 loss: 0.6391519087553025\n",
      "  batch 450 loss: 0.6158985376358033\n",
      "  batch 500 loss: 0.6238083720207215\n",
      "  batch 550 loss: 0.5855730122327805\n",
      "  batch 600 loss: 0.5930833226442337\n",
      "  batch 650 loss: 0.6341790676116943\n",
      "  batch 700 loss: 0.6083855080604553\n",
      "  batch 750 loss: 0.6148589742183685\n",
      "  batch 800 loss: 0.6039435869455337\n",
      "  batch 850 loss: 0.6571168112754822\n",
      "  batch 900 loss: 0.6443538123369217\n",
      "LOSS train 0.64435 valid 0.79932, valid PER 24.45%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.5453387981653214\n",
      "  batch 100 loss: 0.5575257384777069\n",
      "  batch 150 loss: 0.5445376896858215\n",
      "  batch 200 loss: 0.5853622877597808\n",
      "  batch 250 loss: 0.5634567672014237\n",
      "  batch 300 loss: 0.555397053360939\n",
      "  batch 350 loss: 0.5717096066474915\n",
      "  batch 400 loss: 0.5824231278896331\n",
      "  batch 450 loss: 0.5789652889966965\n",
      "  batch 500 loss: 0.5449232518672943\n",
      "  batch 550 loss: 0.6015948045253754\n",
      "  batch 600 loss: 0.5736577785015107\n",
      "  batch 650 loss: 0.6035661166906356\n",
      "  batch 700 loss: 0.5936141604185105\n",
      "  batch 750 loss: 0.5493185305595398\n",
      "  batch 800 loss: 0.5789596414566041\n",
      "  batch 850 loss: 0.6100441509485245\n",
      "  batch 900 loss: 0.5796628445386887\n",
      "LOSS train 0.57966 valid 0.82279, valid PER 24.47%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.5163505524396896\n",
      "  batch 100 loss: 0.5272114008665085\n",
      "  batch 150 loss: 0.52720967233181\n",
      "  batch 200 loss: 0.5291372936964035\n",
      "  batch 250 loss: 0.5484788942337037\n",
      "  batch 300 loss: 0.5540969568490982\n",
      "  batch 350 loss: 0.5321626806259155\n",
      "  batch 400 loss: 0.5378915131092071\n",
      "  batch 450 loss: 0.5425549829006195\n",
      "  batch 500 loss: 0.5622756093740463\n",
      "  batch 550 loss: 0.5564224326610565\n",
      "  batch 600 loss: 0.5340262204408646\n",
      "  batch 650 loss: 0.5639948213100433\n",
      "  batch 700 loss: 0.5856736046075821\n",
      "  batch 750 loss: 0.5421415275335312\n",
      "  batch 800 loss: 0.524578161239624\n",
      "  batch 850 loss: 0.5660522812604905\n",
      "  batch 900 loss: 0.560798749923706\n",
      "LOSS train 0.56080 valid 0.81397, valid PER 24.56%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.48984003484249117\n",
      "  batch 100 loss: 0.49868094563484194\n",
      "  batch 150 loss: 0.5127694064378738\n",
      "  batch 200 loss: 0.5168709647655487\n",
      "  batch 250 loss: 0.5326341772079468\n",
      "  batch 300 loss: 0.502594901919365\n",
      "  batch 350 loss: 0.5044869029521942\n",
      "  batch 400 loss: 0.5168407827615737\n",
      "  batch 450 loss: 0.5086388540267944\n",
      "  batch 500 loss: 0.48964326083660126\n",
      "  batch 550 loss: 0.5163855075836181\n",
      "  batch 600 loss: 0.5318146693706512\n",
      "  batch 650 loss: 0.535772117972374\n",
      "  batch 700 loss: 0.5183378303050995\n",
      "  batch 750 loss: 0.5456710702180863\n",
      "  batch 800 loss: 0.5281819605827331\n",
      "  batch 850 loss: 0.5012145334482193\n",
      "  batch 900 loss: 0.5168119114637375\n",
      "LOSS train 0.51681 valid 0.81913, valid PER 24.54%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.4824519336223602\n",
      "  batch 100 loss: 0.4793822181224823\n",
      "  batch 150 loss: 0.46632942676544187\n",
      "  batch 200 loss: 0.4764863187074661\n",
      "  batch 250 loss: 0.4825226771831512\n",
      "  batch 300 loss: 0.4781539183855057\n",
      "  batch 350 loss: 0.49418478906154634\n",
      "  batch 400 loss: 0.5091792786121369\n",
      "  batch 450 loss: 0.49297056555747987\n",
      "  batch 500 loss: 0.4669963228702545\n",
      "  batch 550 loss: 0.4835474330186844\n",
      "  batch 600 loss: 0.47443865835666654\n",
      "  batch 650 loss: 0.5163311904668808\n",
      "  batch 700 loss: 0.4866557759046555\n",
      "  batch 750 loss: 0.4957499438524246\n",
      "  batch 800 loss: 0.49934683442115785\n",
      "  batch 850 loss: 0.5043422842025757\n",
      "  batch 900 loss: 0.4994831919670105\n",
      "LOSS train 0.49948 valid 0.80841, valid PER 23.52%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.4439354383945465\n",
      "  batch 100 loss: 0.45938228607177733\n",
      "  batch 150 loss: 0.44104254961013795\n",
      "  batch 200 loss: 0.44723804414272306\n",
      "  batch 250 loss: 0.46033682465553283\n",
      "  batch 300 loss: 0.461053763628006\n",
      "  batch 350 loss: 0.44404935240745547\n",
      "  batch 400 loss: 0.49285746812820436\n",
      "  batch 450 loss: 0.4826195567846298\n",
      "  batch 500 loss: 0.45808195769786836\n",
      "  batch 550 loss: 0.4531992256641388\n",
      "  batch 600 loss: 0.4775106531381607\n",
      "  batch 650 loss: 0.47669284999370576\n",
      "  batch 700 loss: 0.4652164399623871\n",
      "  batch 750 loss: 0.4445856007933617\n",
      "  batch 800 loss: 0.45425005197525026\n",
      "  batch 850 loss: 0.48910592406988146\n",
      "  batch 900 loss: 0.46702281951904295\n",
      "LOSS train 0.46702 valid 0.81814, valid PER 23.48%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.4102951940894127\n",
      "  batch 100 loss: 0.42613764882087707\n",
      "  batch 150 loss: 0.439459348320961\n",
      "  batch 200 loss: 0.43638495802879335\n",
      "  batch 250 loss: 0.4470665103197098\n",
      "  batch 300 loss: 0.4260507148504257\n",
      "  batch 350 loss: 0.4458563920855522\n",
      "  batch 400 loss: 0.431195025742054\n",
      "  batch 450 loss: 0.46799118459224703\n",
      "  batch 500 loss: 0.43590885370969773\n",
      "  batch 550 loss: 0.47436297237873076\n",
      "  batch 600 loss: 0.4142638885974884\n",
      "  batch 650 loss: 0.4432734024524689\n",
      "  batch 700 loss: 0.47377117574214933\n",
      "  batch 750 loss: 0.4315237057209015\n",
      "  batch 800 loss: 0.43754760146141053\n",
      "  batch 850 loss: 0.44199482560157777\n",
      "  batch 900 loss: 0.462989394068718\n",
      "LOSS train 0.46299 valid 0.82266, valid PER 23.90%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.3767018550634384\n",
      "  batch 100 loss: 0.38491726517677305\n",
      "  batch 150 loss: 0.38014208555221557\n",
      "  batch 200 loss: 0.4009065517783165\n",
      "  batch 250 loss: 0.40485488712787626\n",
      "  batch 300 loss: 0.4119875630736351\n",
      "  batch 350 loss: 0.4031640815734863\n",
      "  batch 400 loss: 0.41795950174331664\n",
      "  batch 450 loss: 0.43388198912143705\n",
      "  batch 500 loss: 0.42248269736766814\n",
      "  batch 550 loss: 0.40043672531843183\n",
      "  batch 600 loss: 0.414306997358799\n",
      "  batch 650 loss: 0.45720151364803313\n",
      "  batch 700 loss: 0.3995623016357422\n",
      "  batch 750 loss: 0.40824020743370054\n",
      "  batch 800 loss: 0.43755801737308503\n",
      "  batch 850 loss: 0.42731866121292117\n",
      "  batch 900 loss: 0.4375898540019989\n",
      "LOSS train 0.43759 valid 0.84383, valid PER 24.08%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.3692528918385506\n",
      "  batch 100 loss: 0.3629370191693306\n",
      "  batch 150 loss: 0.3630901476740837\n",
      "  batch 200 loss: 0.392979736328125\n",
      "  batch 250 loss: 0.37798539102077483\n",
      "  batch 300 loss: 0.3935583919286728\n",
      "  batch 350 loss: 0.3679445099830627\n",
      "  batch 400 loss: 0.3863806939125061\n",
      "  batch 450 loss: 0.3883060824871063\n",
      "  batch 500 loss: 0.3648967498540878\n",
      "  batch 550 loss: 0.42291117668151856\n",
      "  batch 600 loss: 0.37531455487012866\n",
      "  batch 650 loss: 0.4114463809132576\n",
      "  batch 700 loss: 0.3999723929166794\n",
      "  batch 750 loss: 0.3784162163734436\n",
      "  batch 800 loss: 0.42661190271377564\n",
      "  batch 850 loss: 0.4181522661447525\n",
      "  batch 900 loss: 0.42099318385124207\n",
      "LOSS train 0.42099 valid 0.85210, valid PER 23.69%\n",
      "Training finished in 5.0 minutes.\n",
      "Model saved to checkpoints/20231205_121351/model_12\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "model = models.BiLSTM(\n",
    "    2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab))\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of model parameters is {}'.format(num_params))\n",
    "\n",
    "from datetime import datetime\n",
    "from trainer import train\n",
    "start = datetime.now()\n",
    "model.to(args.device)\n",
    "model_path = train(model, args)\n",
    "end = datetime.now()\n",
    "duration = (end - start).total_seconds()\n",
    "print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "print('Model saved to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00989d",
   "metadata": {},
   "source": [
    "##### Expriment for dropout in the feed forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c8f0193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dropout tuning For 2 Layer LSTM\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.060188307762146\n",
      "  batch 100 loss: 3.3858893871307374\n",
      "  batch 150 loss: 3.2872353744506837\n",
      "  batch 200 loss: 3.1795988035202027\n",
      "  batch 250 loss: 3.067919526100159\n",
      "  batch 300 loss: 2.8352953433990478\n",
      "  batch 350 loss: 2.6856843566894533\n",
      "  batch 400 loss: 2.571924910545349\n",
      "  batch 450 loss: 2.4816431427001953\n",
      "  batch 500 loss: 2.3627015018463133\n",
      "  batch 550 loss: 2.2688104462623597\n",
      "  batch 600 loss: 2.207112829685211\n",
      "  batch 650 loss: 2.1014141702651976\n",
      "  batch 700 loss: 2.0844292569160463\n",
      "  batch 750 loss: 1.9996450185775756\n",
      "  batch 800 loss: 1.9671517157554625\n",
      "  batch 850 loss: 1.8950777316093446\n",
      "  batch 900 loss: 1.858528697490692\n",
      "LOSS train 1.85853 valid 1.75822, valid PER 68.83%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.771159212589264\n",
      "  batch 100 loss: 1.6993750786781312\n",
      "  batch 150 loss: 1.6493911504745484\n",
      "  batch 200 loss: 1.6494285464286804\n",
      "  batch 250 loss: 1.6486912989616394\n",
      "  batch 300 loss: 1.5954799008369447\n",
      "  batch 350 loss: 1.507632670402527\n",
      "  batch 400 loss: 1.5062076783180236\n",
      "  batch 450 loss: 1.4553094625473022\n",
      "  batch 500 loss: 1.4698145532608031\n",
      "  batch 550 loss: 1.466945984363556\n",
      "  batch 600 loss: 1.395002202987671\n",
      "  batch 650 loss: 1.416783528327942\n",
      "  batch 700 loss: 1.3759632015228271\n",
      "  batch 750 loss: 1.3643364441394805\n",
      "  batch 800 loss: 1.2849961829185486\n",
      "  batch 850 loss: 1.2898043727874755\n",
      "  batch 900 loss: 1.3083481526374816\n",
      "LOSS train 1.30835 valid 1.25913, valid PER 39.97%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.25835679769516\n",
      "  batch 100 loss: 1.2336172950267792\n",
      "  batch 150 loss: 1.2223547351360322\n",
      "  batch 200 loss: 1.2052419126033782\n",
      "  batch 250 loss: 1.1930539393424988\n",
      "  batch 300 loss: 1.1875369799137117\n",
      "  batch 350 loss: 1.226012464761734\n",
      "  batch 400 loss: 1.1987983465194703\n",
      "  batch 450 loss: 1.16534952044487\n",
      "  batch 500 loss: 1.1602652275562286\n",
      "  batch 550 loss: 1.153755123615265\n",
      "  batch 600 loss: 1.1506252956390381\n",
      "  batch 650 loss: 1.105399180650711\n",
      "  batch 700 loss: 1.1357111740112305\n",
      "  batch 750 loss: 1.18050537109375\n",
      "  batch 800 loss: 1.1139285504817962\n",
      "  batch 850 loss: 1.1416112720966338\n",
      "  batch 900 loss: 1.0660168099403382\n",
      "LOSS train 1.06602 valid 1.08887, valid PER 33.30%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0564081692695617\n",
      "  batch 100 loss: 1.0662915802001953\n",
      "  batch 150 loss: 1.0410741877555847\n",
      "  batch 200 loss: 1.0540039324760437\n",
      "  batch 250 loss: 1.071194280385971\n",
      "  batch 300 loss: 1.0821036982536316\n",
      "  batch 350 loss: 1.0075091755390166\n",
      "  batch 400 loss: 1.037229915857315\n",
      "  batch 450 loss: 1.03344052195549\n",
      "  batch 500 loss: 1.0074503910541535\n",
      "  batch 550 loss: 1.0362443113327027\n",
      "  batch 600 loss: 1.04860741853714\n",
      "  batch 650 loss: 1.0406869089603423\n",
      "  batch 700 loss: 1.010628422498703\n",
      "  batch 750 loss: 0.9927913522720337\n",
      "  batch 800 loss: 0.965489559173584\n",
      "  batch 850 loss: 0.9907783484458923\n",
      "  batch 900 loss: 1.038262013196945\n",
      "LOSS train 1.03826 valid 0.98913, valid PER 31.28%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9591709089279175\n",
      "  batch 100 loss: 0.9533799815177918\n",
      "  batch 150 loss: 1.0098508894443512\n",
      "  batch 200 loss: 0.934907180070877\n",
      "  batch 250 loss: 0.946542147397995\n",
      "  batch 300 loss: 0.9614762961864471\n",
      "  batch 350 loss: 0.9425803875923157\n",
      "  batch 400 loss: 0.9591775226593018\n",
      "  batch 450 loss: 0.9384827291965485\n",
      "  batch 500 loss: 0.9671417295932769\n",
      "  batch 550 loss: 0.9101091086864471\n",
      "  batch 600 loss: 0.9680600810050964\n",
      "  batch 650 loss: 0.9228988003730774\n",
      "  batch 700 loss: 0.9682310235500335\n",
      "  batch 750 loss: 0.9156794977188111\n",
      "  batch 800 loss: 0.9349529874324799\n",
      "  batch 850 loss: 0.9407130944728851\n",
      "  batch 900 loss: 0.9347854447364807\n",
      "LOSS train 0.93479 valid 0.96277, valid PER 29.84%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9276396822929383\n",
      "  batch 100 loss: 0.8823659908771515\n",
      "  batch 150 loss: 0.883533650636673\n",
      "  batch 200 loss: 0.8921567893028259\n",
      "  batch 250 loss: 0.9355257666110992\n",
      "  batch 300 loss: 0.8999230909347534\n",
      "  batch 350 loss: 0.8999258410930634\n",
      "  batch 400 loss: 0.8714037096500397\n",
      "  batch 450 loss: 0.8938620817661286\n",
      "  batch 500 loss: 0.8762922883033752\n",
      "  batch 550 loss: 0.9122137582302093\n",
      "  batch 600 loss: 0.891950546503067\n",
      "  batch 650 loss: 0.8847449946403504\n",
      "  batch 700 loss: 0.8743011140823365\n",
      "  batch 750 loss: 0.8731575012207031\n",
      "  batch 800 loss: 0.8850069582462311\n",
      "  batch 850 loss: 0.853036721944809\n",
      "  batch 900 loss: 0.8700132739543914\n",
      "LOSS train 0.87001 valid 0.91161, valid PER 28.08%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8569183039665222\n",
      "  batch 100 loss: 0.8719577193260193\n",
      "  batch 150 loss: 0.8312638354301453\n",
      "  batch 200 loss: 0.8486536693572998\n",
      "  batch 250 loss: 0.8287724047899246\n",
      "  batch 300 loss: 0.8264388597011566\n",
      "  batch 350 loss: 0.8249680292606354\n",
      "  batch 400 loss: 0.8490409803390503\n",
      "  batch 450 loss: 0.8288896346092224\n",
      "  batch 500 loss: 0.8306880784034729\n",
      "  batch 550 loss: 0.8170049905776977\n",
      "  batch 600 loss: 0.8551669442653655\n",
      "  batch 650 loss: 0.8335542261600495\n",
      "  batch 700 loss: 0.8467308592796325\n",
      "  batch 750 loss: 0.8259942388534546\n",
      "  batch 800 loss: 0.8112263917922974\n",
      "  batch 850 loss: 0.8205950963497162\n",
      "  batch 900 loss: 0.874913524389267\n",
      "LOSS train 0.87491 valid 0.87683, valid PER 27.40%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8031516981124878\n",
      "  batch 100 loss: 0.7783747851848603\n",
      "  batch 150 loss: 0.7812040781974793\n",
      "  batch 200 loss: 0.7700981712341308\n",
      "  batch 250 loss: 0.7945882201194763\n",
      "  batch 300 loss: 0.7557815659046173\n",
      "  batch 350 loss: 0.824334214925766\n",
      "  batch 400 loss: 0.7696602195501328\n",
      "  batch 450 loss: 0.7848881912231446\n",
      "  batch 500 loss: 0.8227200520038604\n",
      "  batch 550 loss: 0.7754135036468506\n",
      "  batch 600 loss: 0.8051702404022216\n",
      "  batch 650 loss: 0.8204910516738891\n",
      "  batch 700 loss: 0.7844992923736572\n",
      "  batch 750 loss: 0.795002977848053\n",
      "  batch 800 loss: 0.8040626162290573\n",
      "  batch 850 loss: 0.784092229604721\n",
      "  batch 900 loss: 0.819341516494751\n",
      "LOSS train 0.81934 valid 0.86601, valid PER 26.86%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7362462621927262\n",
      "  batch 100 loss: 0.749366849064827\n",
      "  batch 150 loss: 0.7628616607189178\n",
      "  batch 200 loss: 0.7239627420902253\n",
      "  batch 250 loss: 0.7634852659702301\n",
      "  batch 300 loss: 0.7455023109912873\n",
      "  batch 350 loss: 0.7815647268295288\n",
      "  batch 400 loss: 0.7527273225784302\n",
      "  batch 450 loss: 0.7517929500341416\n",
      "  batch 500 loss: 0.7252522504329681\n",
      "  batch 550 loss: 0.7512148517370224\n",
      "  batch 600 loss: 0.7713400542736053\n",
      "  batch 650 loss: 0.743885989189148\n",
      "  batch 700 loss: 0.7227435678243637\n",
      "  batch 750 loss: 0.7569229459762573\n",
      "  batch 800 loss: 0.7689728379249573\n",
      "  batch 850 loss: 0.7927756190299988\n",
      "  batch 900 loss: 0.7247444462776184\n",
      "LOSS train 0.72474 valid 0.82759, valid PER 25.94%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6734069168567658\n",
      "  batch 100 loss: 0.6899549645185471\n",
      "  batch 150 loss: 0.7118229603767395\n",
      "  batch 200 loss: 0.7288317143917084\n",
      "  batch 250 loss: 0.7327454310655593\n",
      "  batch 300 loss: 0.6964736360311509\n",
      "  batch 350 loss: 0.7411483681201935\n",
      "  batch 400 loss: 0.6834751039743423\n",
      "  batch 450 loss: 0.6881091636419296\n",
      "  batch 500 loss: 0.728052933216095\n",
      "  batch 550 loss: 0.7533558624982833\n",
      "  batch 600 loss: 0.7178743028640747\n",
      "  batch 650 loss: 0.7088146501779556\n",
      "  batch 700 loss: 0.7221619391441345\n",
      "  batch 750 loss: 0.7113574594259262\n",
      "  batch 800 loss: 0.7252297455072403\n",
      "  batch 850 loss: 0.7203830087184906\n",
      "  batch 900 loss: 0.7457357454299927\n",
      "LOSS train 0.74574 valid 0.82626, valid PER 25.74%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6651318126916885\n",
      "  batch 100 loss: 0.6298497205972672\n",
      "  batch 150 loss: 0.660174652338028\n",
      "  batch 200 loss: 0.7064257109165192\n",
      "  batch 250 loss: 0.6944493663311004\n",
      "  batch 300 loss: 0.6630162477493287\n",
      "  batch 350 loss: 0.6866816657781601\n",
      "  batch 400 loss: 0.7014630627632141\n",
      "  batch 450 loss: 0.691537870168686\n",
      "  batch 500 loss: 0.6589685440063476\n",
      "  batch 550 loss: 0.684692622423172\n",
      "  batch 600 loss: 0.6715379738807679\n",
      "  batch 650 loss: 0.7466013824939728\n",
      "  batch 700 loss: 0.6697843533754348\n",
      "  batch 750 loss: 0.6496179157495499\n",
      "  batch 800 loss: 0.7104012894630433\n",
      "  batch 850 loss: 0.7202341473102569\n",
      "  batch 900 loss: 0.7198589670658112\n",
      "LOSS train 0.71986 valid 0.80651, valid PER 24.50%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.644150179028511\n",
      "  batch 100 loss: 0.6443686360120773\n",
      "  batch 150 loss: 0.6054836761951446\n",
      "  batch 200 loss: 0.6359477633237839\n",
      "  batch 250 loss: 0.6571795165538787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 0.6378621757030487\n",
      "  batch 350 loss: 0.6293581295013427\n",
      "  batch 400 loss: 0.657848391532898\n",
      "  batch 450 loss: 0.6672449773550033\n",
      "  batch 500 loss: 0.6682946443557739\n",
      "  batch 550 loss: 0.6269550216197968\n",
      "  batch 600 loss: 0.6290220689773559\n",
      "  batch 650 loss: 0.6826324135065078\n",
      "  batch 700 loss: 0.6652283596992493\n",
      "  batch 750 loss: 0.6575614804029465\n",
      "  batch 800 loss: 0.6290613812208176\n",
      "  batch 850 loss: 0.6931658500432968\n",
      "  batch 900 loss: 0.6664961874485016\n",
      "LOSS train 0.66650 valid 0.79776, valid PER 24.34%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.6145913648605347\n",
      "  batch 100 loss: 0.6273281401395798\n",
      "  batch 150 loss: 0.5997854423522949\n",
      "  batch 200 loss: 0.6248656010627747\n",
      "  batch 250 loss: 0.6095538872480393\n",
      "  batch 300 loss: 0.5999653202295303\n",
      "  batch 350 loss: 0.6087235271930694\n",
      "  batch 400 loss: 0.6314075326919556\n",
      "  batch 450 loss: 0.6423661768436432\n",
      "  batch 500 loss: 0.5914792603254319\n",
      "  batch 550 loss: 0.6529319715499878\n",
      "  batch 600 loss: 0.6179822385311127\n",
      "  batch 650 loss: 0.6493036323785781\n",
      "  batch 700 loss: 0.6313871258497238\n",
      "  batch 750 loss: 0.6011878848075867\n",
      "  batch 800 loss: 0.6052805590629577\n",
      "  batch 850 loss: 0.6513533848524093\n",
      "  batch 900 loss: 0.6415346574783325\n",
      "LOSS train 0.64153 valid 0.79275, valid PER 23.94%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.5828240489959717\n",
      "  batch 100 loss: 0.5827740824222565\n",
      "  batch 150 loss: 0.5719766187667846\n",
      "  batch 200 loss: 0.5750532013177871\n",
      "  batch 250 loss: 0.5837068736553193\n",
      "  batch 300 loss: 0.6139465659856796\n",
      "  batch 350 loss: 0.57852987408638\n",
      "  batch 400 loss: 0.5846058869361878\n",
      "  batch 450 loss: 0.6025421541929245\n",
      "  batch 500 loss: 0.620644371509552\n",
      "  batch 550 loss: 0.6139739608764648\n",
      "  batch 600 loss: 0.590693033337593\n",
      "  batch 650 loss: 0.5962520962953568\n",
      "  batch 700 loss: 0.6514581257104873\n",
      "  batch 750 loss: 0.6068620526790619\n",
      "  batch 800 loss: 0.5773176145553589\n",
      "  batch 850 loss: 0.6126873135566712\n",
      "  batch 900 loss: 0.610375919342041\n",
      "LOSS train 0.61038 valid 0.78598, valid PER 23.51%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.5519093263149262\n",
      "  batch 100 loss: 0.5585549128055572\n",
      "  batch 150 loss: 0.5512062901258469\n",
      "  batch 200 loss: 0.5742054504156112\n",
      "  batch 250 loss: 0.5855896180868149\n",
      "  batch 300 loss: 0.5519464403390885\n",
      "  batch 350 loss: 0.561915147304535\n",
      "  batch 400 loss: 0.5654280215501786\n",
      "  batch 450 loss: 0.5610109889507293\n",
      "  batch 500 loss: 0.5376424711942672\n",
      "  batch 550 loss: 0.561347889304161\n",
      "  batch 600 loss: 0.5776801711320877\n",
      "  batch 650 loss: 0.5889531642198562\n",
      "  batch 700 loss: 0.5842707526683807\n",
      "  batch 750 loss: 0.5918998104333878\n",
      "  batch 800 loss: 0.5722639405727387\n",
      "  batch 850 loss: 0.5556915444135666\n",
      "  batch 900 loss: 0.5752083802223206\n",
      "LOSS train 0.57521 valid 0.78620, valid PER 23.30%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.5384130436182022\n",
      "  batch 100 loss: 0.5126630872488022\n",
      "  batch 150 loss: 0.5261776673793793\n",
      "  batch 200 loss: 0.5309298855066299\n",
      "  batch 250 loss: 0.5466359347105026\n",
      "  batch 300 loss: 0.5387773370742798\n",
      "  batch 350 loss: 0.5550443142652511\n",
      "  batch 400 loss: 0.5490948802232742\n",
      "  batch 450 loss: 0.5501628118753433\n",
      "  batch 500 loss: 0.5136238670349121\n",
      "  batch 550 loss: 0.5501316851377487\n",
      "  batch 600 loss: 0.5404886662960052\n",
      "  batch 650 loss: 0.5640816420316697\n",
      "  batch 700 loss: 0.5411945992708206\n",
      "  batch 750 loss: 0.5604022926092148\n",
      "  batch 800 loss: 0.5584979009628296\n",
      "  batch 850 loss: 0.5468327605724335\n",
      "  batch 900 loss: 0.5520869386196137\n",
      "LOSS train 0.55209 valid 0.78779, valid PER 23.29%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.5170502489805222\n",
      "  batch 100 loss: 0.5211406582593918\n",
      "  batch 150 loss: 0.5040517085790635\n",
      "  batch 200 loss: 0.5072329348325729\n",
      "  batch 250 loss: 0.5232132703065873\n",
      "  batch 300 loss: 0.5247439980506897\n",
      "  batch 350 loss: 0.5048763138055802\n",
      "  batch 400 loss: 0.5367278695106507\n",
      "  batch 450 loss: 0.5310017293691636\n",
      "  batch 500 loss: 0.5182323133945466\n",
      "  batch 550 loss: 0.5227222400903702\n",
      "  batch 600 loss: 0.5392025578022003\n",
      "  batch 650 loss: 0.5186126625537872\n",
      "  batch 700 loss: 0.5118745988607407\n",
      "  batch 750 loss: 0.513072247505188\n",
      "  batch 800 loss: 0.5248583179712295\n",
      "  batch 850 loss: 0.5479079419374466\n",
      "  batch 900 loss: 0.5139381241798401\n",
      "LOSS train 0.51394 valid 0.80351, valid PER 23.24%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.48191447257995607\n",
      "  batch 100 loss: 0.4850971859693527\n",
      "  batch 150 loss: 0.5178724658489228\n",
      "  batch 200 loss: 0.49058140635490416\n",
      "  batch 250 loss: 0.5038618624210358\n",
      "  batch 300 loss: 0.48312063336372374\n",
      "  batch 350 loss: 0.5001691496372223\n",
      "  batch 400 loss: 0.4769683402776718\n",
      "  batch 450 loss: 0.5246594685316086\n",
      "  batch 500 loss: 0.5086158233880996\n",
      "  batch 550 loss: 0.5171098482608795\n",
      "  batch 600 loss: 0.47080123245716093\n",
      "  batch 650 loss: 0.5001014018058777\n",
      "  batch 700 loss: 0.5258039289712906\n",
      "  batch 750 loss: 0.4968939408659935\n",
      "  batch 800 loss: 0.4898889684677124\n",
      "  batch 850 loss: 0.5121655121445656\n",
      "  batch 900 loss: 0.529642396569252\n",
      "LOSS train 0.52964 valid 0.79612, valid PER 23.38%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.4433078646659851\n",
      "  batch 100 loss: 0.4435762998461723\n",
      "  batch 150 loss: 0.45109511613845826\n",
      "  batch 200 loss: 0.4550425410270691\n",
      "  batch 250 loss: 0.45937653303146364\n",
      "  batch 300 loss: 0.46801373541355135\n",
      "  batch 350 loss: 0.4683621591329575\n",
      "  batch 400 loss: 0.48161198526620863\n",
      "  batch 450 loss: 0.4880222073197365\n",
      "  batch 500 loss: 0.48852607250213625\n",
      "  batch 550 loss: 0.4664109754562378\n",
      "  batch 600 loss: 0.46100028455257414\n",
      "  batch 650 loss: 0.5163515985012055\n",
      "  batch 700 loss: 0.4596323925256729\n",
      "  batch 750 loss: 0.4650007113814354\n",
      "  batch 800 loss: 0.5029957145452499\n",
      "  batch 850 loss: 0.4881881272792816\n",
      "  batch 900 loss: 0.5109472608566284\n",
      "LOSS train 0.51095 valid 0.80442, valid PER 23.15%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.4255486822128296\n",
      "  batch 100 loss: 0.4151411184668541\n",
      "  batch 150 loss: 0.42135911017656325\n",
      "  batch 200 loss: 0.45006381571292875\n",
      "  batch 250 loss: 0.4418302288651466\n",
      "  batch 300 loss: 0.4515959298610687\n",
      "  batch 350 loss: 0.43395768880844116\n",
      "  batch 400 loss: 0.4627483469247818\n",
      "  batch 450 loss: 0.4711558973789215\n",
      "  batch 500 loss: 0.4317104923725128\n",
      "  batch 550 loss: 0.5036460816860199\n",
      "  batch 600 loss: 0.4432645472884178\n",
      "  batch 650 loss: 0.4527766275405884\n",
      "  batch 700 loss: 0.4759663677215576\n",
      "  batch 750 loss: 0.4353660586476326\n",
      "  batch 800 loss: 0.48575391352176667\n",
      "  batch 850 loss: 0.49003146052360536\n",
      "  batch 900 loss: 0.4859844174981117\n",
      "LOSS train 0.48598 valid 0.81065, valid PER 23.12%\n",
      "Training finished in 4.0 minutes.\n",
      "Model saved to checkpoints/20231205_143304/model_14\n",
      "Loading model from checkpoints/20231205_143304/model_14\n",
      "For dropout rate 0.1 the best model has SUB: 15.26%, DEL: 8.13%, INS: 1.95%, COR: 76.60%, PER: 25.34%\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.059004940986633\n",
      "  batch 100 loss: 3.3843990468978884\n",
      "  batch 150 loss: 3.2912643432617186\n",
      "  batch 200 loss: 3.180320134162903\n",
      "  batch 250 loss: 3.0679417705535887\n",
      "  batch 300 loss: 2.8396772289276124\n",
      "  batch 350 loss: 2.689205288887024\n",
      "  batch 400 loss: 2.5731665229797365\n",
      "  batch 450 loss: 2.488823175430298\n",
      "  batch 500 loss: 2.3677814960479737\n",
      "  batch 550 loss: 2.2851121187210084\n",
      "  batch 600 loss: 2.2183781671524048\n",
      "  batch 650 loss: 2.115030014514923\n",
      "  batch 700 loss: 2.0982160139083863\n",
      "  batch 750 loss: 2.0144899916648864\n",
      "  batch 800 loss: 1.9791600060462953\n",
      "  batch 850 loss: 1.913312211036682\n",
      "  batch 900 loss: 1.8728599429130555\n",
      "LOSS train 1.87286 valid 1.77015, valid PER 69.50%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.791907877922058\n",
      "  batch 100 loss: 1.725692903995514\n",
      "  batch 150 loss: 1.6744883203506469\n",
      "  batch 200 loss: 1.6736879897117616\n",
      "  batch 250 loss: 1.6707185173034669\n",
      "  batch 300 loss: 1.6242363929748536\n",
      "  batch 350 loss: 1.5345028114318848\n",
      "  batch 400 loss: 1.5429300093650817\n",
      "  batch 450 loss: 1.494234163761139\n",
      "  batch 500 loss: 1.5032862257957458\n",
      "  batch 550 loss: 1.5094059371948243\n",
      "  batch 600 loss: 1.447042257785797\n",
      "  batch 650 loss: 1.4686670446395873\n",
      "  batch 700 loss: 1.4104131960868835\n",
      "  batch 750 loss: 1.4071700263023377\n",
      "  batch 800 loss: 1.327821216583252\n",
      "  batch 850 loss: 1.3361555767059325\n",
      "  batch 900 loss: 1.344155685901642\n",
      "LOSS train 1.34416 valid 1.27566, valid PER 40.88%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.2958643078804015\n",
      "  batch 100 loss: 1.2681945657730103\n",
      "  batch 150 loss: 1.2674946427345275\n",
      "  batch 200 loss: 1.2407220435142516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 250 loss: 1.2253919792175294\n",
      "  batch 300 loss: 1.2173623478412627\n",
      "  batch 350 loss: 1.2595860767364502\n",
      "  batch 400 loss: 1.2377088379859924\n",
      "  batch 450 loss: 1.2120272827148437\n",
      "  batch 500 loss: 1.185941617488861\n",
      "  batch 550 loss: 1.1956944096088409\n",
      "  batch 600 loss: 1.176425689458847\n",
      "  batch 650 loss: 1.1383632922172546\n",
      "  batch 700 loss: 1.1559358644485473\n",
      "  batch 750 loss: 1.204849693775177\n",
      "  batch 800 loss: 1.1424061834812165\n",
      "  batch 850 loss: 1.1770251297950745\n",
      "  batch 900 loss: 1.1132957673072814\n",
      "LOSS train 1.11330 valid 1.10549, valid PER 34.28%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0894754946231842\n",
      "  batch 100 loss: 1.1174729120731355\n",
      "  batch 150 loss: 1.0627294540405274\n",
      "  batch 200 loss: 1.093819923400879\n",
      "  batch 250 loss: 1.1063812112808227\n",
      "  batch 300 loss: 1.1057107102870942\n",
      "  batch 350 loss: 1.0285395288467407\n",
      "  batch 400 loss: 1.0673136389255524\n",
      "  batch 450 loss: 1.0644252824783325\n",
      "  batch 500 loss: 1.050697650909424\n",
      "  batch 550 loss: 1.0643089628219604\n",
      "  batch 600 loss: 1.0783718204498292\n",
      "  batch 650 loss: 1.061499572992325\n",
      "  batch 700 loss: 1.0287393248081207\n",
      "  batch 750 loss: 1.0279140746593476\n",
      "  batch 800 loss: 0.9933787834644318\n",
      "  batch 850 loss: 1.0231274330615998\n",
      "  batch 900 loss: 1.076343903541565\n",
      "LOSS train 1.07634 valid 1.02504, valid PER 32.29%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9999358963966369\n",
      "  batch 100 loss: 0.9782011032104492\n",
      "  batch 150 loss: 1.034299967288971\n",
      "  batch 200 loss: 0.970495502948761\n",
      "  batch 250 loss: 0.9853840494155883\n",
      "  batch 300 loss: 0.9989755308628082\n",
      "  batch 350 loss: 0.9882919466495514\n",
      "  batch 400 loss: 0.9950869596004486\n",
      "  batch 450 loss: 0.9620528650283814\n",
      "  batch 500 loss: 0.9949938297271729\n",
      "  batch 550 loss: 0.9321685349941253\n",
      "  batch 600 loss: 1.004496876001358\n",
      "  batch 650 loss: 0.9617185282707215\n",
      "  batch 700 loss: 0.9857383942604065\n",
      "  batch 750 loss: 0.9423183321952819\n",
      "  batch 800 loss: 0.9769934582710266\n",
      "  batch 850 loss: 0.9850954008102417\n",
      "  batch 900 loss: 0.9587971246242524\n",
      "LOSS train 0.95880 valid 0.95511, valid PER 29.84%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9583262026309967\n",
      "  batch 100 loss: 0.9102289795875549\n",
      "  batch 150 loss: 0.9057479512691498\n",
      "  batch 200 loss: 0.9109507441520691\n",
      "  batch 250 loss: 0.9475143301486969\n",
      "  batch 300 loss: 0.915019097328186\n",
      "  batch 350 loss: 0.9345204615592957\n",
      "  batch 400 loss: 0.9001687896251679\n",
      "  batch 450 loss: 0.9245232927799225\n",
      "  batch 500 loss: 0.906004353761673\n",
      "  batch 550 loss: 0.9193899476528168\n",
      "  batch 600 loss: 0.9108883571624756\n",
      "  batch 650 loss: 0.9086448359489441\n",
      "  batch 700 loss: 0.9107826054096222\n",
      "  batch 750 loss: 0.8986004757881164\n",
      "  batch 800 loss: 0.8852786755561829\n",
      "  batch 850 loss: 0.8933817851543426\n",
      "  batch 900 loss: 0.9046910798549652\n",
      "LOSS train 0.90469 valid 0.91794, valid PER 28.46%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8751756191253662\n",
      "  batch 100 loss: 0.877997055053711\n",
      "  batch 150 loss: 0.8554542219638824\n",
      "  batch 200 loss: 0.8601598054170608\n",
      "  batch 250 loss: 0.8494706559181213\n",
      "  batch 300 loss: 0.8414313685894013\n",
      "  batch 350 loss: 0.8536656427383423\n",
      "  batch 400 loss: 0.8480089175701141\n",
      "  batch 450 loss: 0.8687382298707962\n",
      "  batch 500 loss: 0.8560002195835114\n",
      "  batch 550 loss: 0.8499951362609863\n",
      "  batch 600 loss: 0.8603910005092621\n",
      "  batch 650 loss: 0.8454962503910065\n",
      "  batch 700 loss: 0.8756204605102539\n",
      "  batch 750 loss: 0.8473872721195221\n",
      "  batch 800 loss: 0.8358932292461395\n",
      "  batch 850 loss: 0.8435553967952728\n",
      "  batch 900 loss: 0.9031197583675384\n",
      "LOSS train 0.90312 valid 0.89563, valid PER 28.54%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8296001899242401\n",
      "  batch 100 loss: 0.7977600729465485\n",
      "  batch 150 loss: 0.8018591600656509\n",
      "  batch 200 loss: 0.8101068222522736\n",
      "  batch 250 loss: 0.8175292050838471\n",
      "  batch 300 loss: 0.7708097922801972\n",
      "  batch 350 loss: 0.8488768053054809\n",
      "  batch 400 loss: 0.7937206494808197\n",
      "  batch 450 loss: 0.8186888170242309\n",
      "  batch 500 loss: 0.8397235429286957\n",
      "  batch 550 loss: 0.7704155099391937\n",
      "  batch 600 loss: 0.8119493472576141\n",
      "  batch 650 loss: 0.8437756550312042\n",
      "  batch 700 loss: 0.7982441639900207\n",
      "  batch 750 loss: 0.8068497753143311\n",
      "  batch 800 loss: 0.806656414270401\n",
      "  batch 850 loss: 0.7974304330348968\n",
      "  batch 900 loss: 0.833315122127533\n",
      "LOSS train 0.83332 valid 0.87290, valid PER 27.03%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7511118042469025\n",
      "  batch 100 loss: 0.7790754091739654\n",
      "  batch 150 loss: 0.7788338482379913\n",
      "  batch 200 loss: 0.754731936454773\n",
      "  batch 250 loss: 0.7942941868305207\n",
      "  batch 300 loss: 0.7865530848503113\n",
      "  batch 350 loss: 0.8144407474994659\n",
      "  batch 400 loss: 0.7790932178497314\n",
      "  batch 450 loss: 0.7801015496253967\n",
      "  batch 500 loss: 0.7652881854772567\n",
      "  batch 550 loss: 0.7736427748203277\n",
      "  batch 600 loss: 0.7864184445142746\n",
      "  batch 650 loss: 0.7630309820175171\n",
      "  batch 700 loss: 0.756212842464447\n",
      "  batch 750 loss: 0.7574486958980561\n",
      "  batch 800 loss: 0.7779777097702026\n",
      "  batch 850 loss: 0.7996722400188446\n",
      "  batch 900 loss: 0.7507310855388641\n",
      "LOSS train 0.75073 valid 0.83936, valid PER 26.20%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7062873375415802\n",
      "  batch 100 loss: 0.7120824211835861\n",
      "  batch 150 loss: 0.7299413084983826\n",
      "  batch 200 loss: 0.7536506301164627\n",
      "  batch 250 loss: 0.7596283221244812\n",
      "  batch 300 loss: 0.7157177782058716\n",
      "  batch 350 loss: 0.7398569774627686\n",
      "  batch 400 loss: 0.7074601638317108\n",
      "  batch 450 loss: 0.7183818727731704\n",
      "  batch 500 loss: 0.7500293815135955\n",
      "  batch 550 loss: 0.7563563191890716\n",
      "  batch 600 loss: 0.74286117374897\n",
      "  batch 650 loss: 0.7214595603942872\n",
      "  batch 700 loss: 0.7622966027259827\n",
      "  batch 750 loss: 0.7463645827770233\n",
      "  batch 800 loss: 0.762217891216278\n",
      "  batch 850 loss: 0.7467194241285324\n",
      "  batch 900 loss: 0.7518393528461457\n",
      "LOSS train 0.75184 valid 0.86061, valid PER 26.84%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6914865696430206\n",
      "  batch 100 loss: 0.6514009797573089\n",
      "  batch 150 loss: 0.6826395237445831\n",
      "  batch 200 loss: 0.7234396028518677\n",
      "  batch 250 loss: 0.7153416430950165\n",
      "  batch 300 loss: 0.677262282371521\n",
      "  batch 350 loss: 0.6861955344676971\n",
      "  batch 400 loss: 0.7153965568542481\n",
      "  batch 450 loss: 0.7142418068647385\n",
      "  batch 500 loss: 0.6797921282052993\n",
      "  batch 550 loss: 0.7168210369348526\n",
      "  batch 600 loss: 0.6817980706691742\n",
      "  batch 650 loss: 0.7434741985797882\n",
      "  batch 700 loss: 0.6874020963907241\n",
      "  batch 750 loss: 0.6797787076234818\n",
      "  batch 800 loss: 0.7286160045862198\n",
      "  batch 850 loss: 0.731581956744194\n",
      "  batch 900 loss: 0.7436182498931885\n",
      "LOSS train 0.74362 valid 0.81921, valid PER 25.14%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.6781422424316407\n",
      "  batch 100 loss: 0.6723624473810196\n",
      "  batch 150 loss: 0.6225927263498306\n",
      "  batch 200 loss: 0.6561164689064026\n",
      "  batch 250 loss: 0.6707573056221008\n",
      "  batch 300 loss: 0.6679308754205704\n",
      "  batch 350 loss: 0.6500214016437531\n",
      "  batch 400 loss: 0.6919384396076202\n",
      "  batch 450 loss: 0.6932520717382431\n",
      "  batch 500 loss: 0.7089422780275345\n",
      "  batch 550 loss: 0.6499798130989075\n",
      "  batch 600 loss: 0.6616006034612656\n",
      "  batch 650 loss: 0.7050856554508209\n",
      "  batch 700 loss: 0.6811419796943664\n",
      "  batch 750 loss: 0.6621378934383393\n",
      "  batch 800 loss: 0.657513153553009\n",
      "  batch 850 loss: 0.7129296386241912\n",
      "  batch 900 loss: 0.7045147997140885\n",
      "LOSS train 0.70451 valid 0.79568, valid PER 24.63%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.6290862792730332\n",
      "  batch 100 loss: 0.6345839458703995\n",
      "  batch 150 loss: 0.6214997446537018\n",
      "  batch 200 loss: 0.6536507338285447\n",
      "  batch 250 loss: 0.6412887257337571\n",
      "  batch 300 loss: 0.6425611627101898\n",
      "  batch 350 loss: 0.6313177585601807\n",
      "  batch 400 loss: 0.6627191853523254\n",
      "  batch 450 loss: 0.6546011513471603\n",
      "  batch 500 loss: 0.6178720569610596\n",
      "  batch 550 loss: 0.6866710156202316\n",
      "  batch 600 loss: 0.6424838852882385\n",
      "  batch 650 loss: 0.6760932385921479\n",
      "  batch 700 loss: 0.654301266670227\n",
      "  batch 750 loss: 0.6217628026008606\n",
      "  batch 800 loss: 0.6288111835718155\n",
      "  batch 850 loss: 0.6851308071613311\n",
      "  batch 900 loss: 0.6634860676527023\n",
      "LOSS train 0.66349 valid 0.79878, valid PER 24.76%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.5958906936645508\n",
      "  batch 100 loss: 0.6131570547819137\n",
      "  batch 150 loss: 0.6119706779718399\n",
      "  batch 200 loss: 0.5999330973625183\n",
      "  batch 250 loss: 0.6036799710988998\n",
      "  batch 300 loss: 0.6295984876155853\n",
      "  batch 350 loss: 0.602291363477707\n",
      "  batch 400 loss: 0.6026623666286468\n",
      "  batch 450 loss: 0.6248960375785828\n",
      "  batch 500 loss: 0.6238565808534622\n",
      "  batch 550 loss: 0.6471309787034989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 600 loss: 0.6053373885154724\n",
      "  batch 650 loss: 0.6350982815027237\n",
      "  batch 700 loss: 0.6586503267288208\n",
      "  batch 750 loss: 0.6282279473543168\n",
      "  batch 800 loss: 0.6025258708000183\n",
      "  batch 850 loss: 0.6350004637241363\n",
      "  batch 900 loss: 0.6452437686920166\n",
      "LOSS train 0.64524 valid 0.78825, valid PER 24.21%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.5861858868598938\n",
      "  batch 100 loss: 0.5999251955747604\n",
      "  batch 150 loss: 0.5993826723098755\n",
      "  batch 200 loss: 0.6458119481801987\n",
      "  batch 250 loss: 0.6181056714057922\n",
      "  batch 300 loss: 0.6017287302017212\n",
      "  batch 350 loss: 0.593247184753418\n",
      "  batch 400 loss: 0.6009963518381118\n",
      "  batch 450 loss: 0.589186400771141\n",
      "  batch 500 loss: 0.5717826420068741\n",
      "  batch 550 loss: 0.588403902053833\n",
      "  batch 600 loss: 0.6028299361467362\n",
      "  batch 650 loss: 0.6066710674762725\n",
      "  batch 700 loss: 0.6249493777751922\n",
      "  batch 750 loss: 0.6228002494573593\n",
      "  batch 800 loss: 0.6010503679513931\n",
      "  batch 850 loss: 0.5779400831460952\n",
      "  batch 900 loss: 0.5966325944662094\n",
      "LOSS train 0.59663 valid 0.80405, valid PER 24.42%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.5737743270397186\n",
      "  batch 100 loss: 0.5350573408603668\n",
      "  batch 150 loss: 0.5436194568872452\n",
      "  batch 200 loss: 0.5435697883367538\n",
      "  batch 250 loss: 0.5629680693149567\n",
      "  batch 300 loss: 0.5629774171113968\n",
      "  batch 350 loss: 0.5722158002853394\n",
      "  batch 400 loss: 0.5915225392580032\n",
      "  batch 450 loss: 0.590921123623848\n",
      "  batch 500 loss: 0.5570908600091934\n",
      "  batch 550 loss: 0.5823507642745972\n",
      "  batch 600 loss: 0.5734999179840088\n",
      "  batch 650 loss: 0.5945126277208328\n",
      "  batch 700 loss: 0.5493633246421814\n",
      "  batch 750 loss: 0.5720670771598816\n",
      "  batch 800 loss: 0.5972093522548676\n",
      "  batch 850 loss: 0.5670643538236618\n",
      "  batch 900 loss: 0.5885007309913636\n",
      "LOSS train 0.58850 valid 0.78956, valid PER 23.54%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.5402853524684906\n",
      "  batch 100 loss: 0.5415910589694977\n",
      "  batch 150 loss: 0.5186469984054566\n",
      "  batch 200 loss: 0.5356676852703095\n",
      "  batch 250 loss: 0.5471285456418991\n",
      "  batch 300 loss: 0.5555883824825287\n",
      "  batch 350 loss: 0.5248244279623031\n",
      "  batch 400 loss: 0.5782504302263259\n",
      "  batch 450 loss: 0.566404527425766\n",
      "  batch 500 loss: 0.5398777163028717\n",
      "  batch 550 loss: 0.5563576823472977\n",
      "  batch 600 loss: 0.5660347336530686\n",
      "  batch 650 loss: 0.5523338222503662\n",
      "  batch 700 loss: 0.5483106744289398\n",
      "  batch 750 loss: 0.5374343627691269\n",
      "  batch 800 loss: 0.5363946688175202\n",
      "  batch 850 loss: 0.5736896997690201\n",
      "  batch 900 loss: 0.5540842974185943\n",
      "LOSS train 0.55408 valid 0.78589, valid PER 23.37%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.513633142709732\n",
      "  batch 100 loss: 0.5234647154808044\n",
      "  batch 150 loss: 0.5393072462081909\n",
      "  batch 200 loss: 0.5359580296278\n",
      "  batch 250 loss: 0.5288624936342239\n",
      "  batch 300 loss: 0.5093873256444931\n",
      "  batch 350 loss: 0.5403880041837692\n",
      "  batch 400 loss: 0.504651102423668\n",
      "  batch 450 loss: 0.5444841116666794\n",
      "  batch 500 loss: 0.5259559363126755\n",
      "  batch 550 loss: 0.5342011553049087\n",
      "  batch 600 loss: 0.5060126513242722\n",
      "  batch 650 loss: 0.5308808720111847\n",
      "  batch 700 loss: 0.5800167334079742\n",
      "  batch 750 loss: 0.538392167687416\n",
      "  batch 800 loss: 0.5189481818675995\n",
      "  batch 850 loss: 0.5347665685415268\n",
      "  batch 900 loss: 0.5622228944301605\n",
      "LOSS train 0.56222 valid 0.78205, valid PER 23.03%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.4652156686782837\n",
      "  batch 100 loss: 0.491008375287056\n",
      "  batch 150 loss: 0.4873428976535797\n",
      "  batch 200 loss: 0.495999299287796\n",
      "  batch 250 loss: 0.5050013256072998\n",
      "  batch 300 loss: 0.5113535690307617\n",
      "  batch 350 loss: 0.5017346125841141\n",
      "  batch 400 loss: 0.5153679221868515\n",
      "  batch 450 loss: 0.5396342378854752\n",
      "  batch 500 loss: 0.533930459022522\n",
      "  batch 550 loss: 0.5044954705238343\n",
      "  batch 600 loss: 0.5049418187141419\n",
      "  batch 650 loss: 0.5583255183696747\n",
      "  batch 700 loss: 0.5071674102544784\n",
      "  batch 750 loss: 0.4948098438978195\n",
      "  batch 800 loss: 0.5352150392532349\n",
      "  batch 850 loss: 0.5237813252210617\n",
      "  batch 900 loss: 0.5451931589841843\n",
      "LOSS train 0.54519 valid 0.80750, valid PER 23.24%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.45673073768615724\n",
      "  batch 100 loss: 0.4579881024360657\n",
      "  batch 150 loss: 0.4552032318711281\n",
      "  batch 200 loss: 0.4941829413175583\n",
      "  batch 250 loss: 0.4783812665939331\n",
      "  batch 300 loss: 0.49921774327754975\n",
      "  batch 350 loss: 0.4798204761743545\n",
      "  batch 400 loss: 0.4812858295440674\n",
      "  batch 450 loss: 0.5001832675933838\n",
      "  batch 500 loss: 0.4629482328891754\n",
      "  batch 550 loss: 0.5325517308712006\n",
      "  batch 600 loss: 0.4883408200740814\n",
      "  batch 650 loss: 0.4949285161495209\n",
      "  batch 700 loss: 0.4970168721675873\n",
      "  batch 750 loss: 0.4837064331769943\n",
      "  batch 800 loss: 0.5267505794763565\n",
      "  batch 850 loss: 0.5179201543331147\n",
      "  batch 900 loss: 0.5194052863121033\n",
      "LOSS train 0.51941 valid 0.80669, valid PER 23.36%\n",
      "Training finished in 4.0 minutes.\n",
      "Model saved to checkpoints/20231205_143801/model_18\n",
      "Loading model from checkpoints/20231205_143801/model_18\n",
      "For dropout rate 0.2 the best model has SUB: 15.17%, DEL: 7.99%, INS: 1.82%, COR: 76.84%, PER: 24.98%\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.058522911071777\n",
      "  batch 100 loss: 3.3833644199371338\n",
      "  batch 150 loss: 3.294809412956238\n",
      "  batch 200 loss: 3.180932149887085\n",
      "  batch 250 loss: 3.068543481826782\n",
      "  batch 300 loss: 2.8461153602600096\n",
      "  batch 350 loss: 2.694447407722473\n",
      "  batch 400 loss: 2.579361443519592\n",
      "  batch 450 loss: 2.497289843559265\n",
      "  batch 500 loss: 2.3656967067718506\n",
      "  batch 550 loss: 2.2930536460876465\n",
      "  batch 600 loss: 2.2303746223449705\n",
      "  batch 650 loss: 2.1342503786087037\n",
      "  batch 700 loss: 2.1179461073875427\n",
      "  batch 750 loss: 2.028905816078186\n",
      "  batch 800 loss: 1.996456892490387\n",
      "  batch 850 loss: 1.9374596571922302\n",
      "  batch 900 loss: 1.9003070712089538\n",
      "LOSS train 1.90031 valid 1.78875, valid PER 71.32%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.8160664296150208\n",
      "  batch 100 loss: 1.7506570315361023\n",
      "  batch 150 loss: 1.7029665446281432\n",
      "  batch 200 loss: 1.7008133602142335\n",
      "  batch 250 loss: 1.69946364402771\n",
      "  batch 300 loss: 1.6593864202499389\n",
      "  batch 350 loss: 1.5641432666778565\n",
      "  batch 400 loss: 1.5782406115531922\n",
      "  batch 450 loss: 1.526703486442566\n",
      "  batch 500 loss: 1.5329072165489197\n",
      "  batch 550 loss: 1.5475419068336487\n",
      "  batch 600 loss: 1.473842272758484\n",
      "  batch 650 loss: 1.5084890723228455\n",
      "  batch 700 loss: 1.4607635974884032\n",
      "  batch 750 loss: 1.445204232931137\n",
      "  batch 800 loss: 1.3739501023292542\n",
      "  batch 850 loss: 1.3707162594795228\n",
      "  batch 900 loss: 1.3965498566627503\n",
      "LOSS train 1.39655 valid 1.30055, valid PER 42.71%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.3348722755908966\n",
      "  batch 100 loss: 1.3072279787063599\n",
      "  batch 150 loss: 1.305419042110443\n",
      "  batch 200 loss: 1.28062753200531\n",
      "  batch 250 loss: 1.260759414434433\n",
      "  batch 300 loss: 1.2552678394317627\n",
      "  batch 350 loss: 1.299002355337143\n",
      "  batch 400 loss: 1.2669237017631532\n",
      "  batch 450 loss: 1.2390095043182372\n",
      "  batch 500 loss: 1.2255705082416535\n",
      "  batch 550 loss: 1.2289343547821046\n",
      "  batch 600 loss: 1.207891275882721\n",
      "  batch 650 loss: 1.1593919360637666\n",
      "  batch 700 loss: 1.194311101436615\n",
      "  batch 750 loss: 1.2334346389770507\n",
      "  batch 800 loss: 1.1757002770900726\n",
      "  batch 850 loss: 1.2052717995643616\n",
      "  batch 900 loss: 1.1351183986663818\n",
      "LOSS train 1.13512 valid 1.11900, valid PER 35.13%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.128908566236496\n",
      "  batch 100 loss: 1.144137635231018\n",
      "  batch 150 loss: 1.1197560369968413\n",
      "  batch 200 loss: 1.1500164437294007\n",
      "  batch 250 loss: 1.1416371536254883\n",
      "  batch 300 loss: 1.1471317565441133\n",
      "  batch 350 loss: 1.0710001981258392\n",
      "  batch 400 loss: 1.100542711019516\n",
      "  batch 450 loss: 1.101666520833969\n",
      "  batch 500 loss: 1.0747356843948364\n",
      "  batch 550 loss: 1.0940541052818298\n",
      "  batch 600 loss: 1.1141106486320496\n",
      "  batch 650 loss: 1.099812113046646\n",
      "  batch 700 loss: 1.0679160845279694\n",
      "  batch 750 loss: 1.0538206005096435\n",
      "  batch 800 loss: 1.0154808425903321\n",
      "  batch 850 loss: 1.0492233622074127\n",
      "  batch 900 loss: 1.0846633923053741\n",
      "LOSS train 1.08466 valid 1.03580, valid PER 32.93%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.0083302891254424\n",
      "  batch 100 loss: 1.0073446786403657\n",
      "  batch 150 loss: 1.071237151622772\n",
      "  batch 200 loss: 0.9984134101867675\n",
      "  batch 250 loss: 1.01260284781456\n",
      "  batch 300 loss: 1.0310139977931976\n",
      "  batch 350 loss: 1.002696352005005\n",
      "  batch 400 loss: 1.0272444951534272\n",
      "  batch 450 loss: 0.9936330759525299\n",
      "  batch 500 loss: 1.0310381376743316\n",
      "  batch 550 loss: 0.9744361698627472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 600 loss: 1.0412308597564697\n",
      "  batch 650 loss: 1.0034085965156556\n",
      "  batch 700 loss: 1.0266603875160216\n",
      "  batch 750 loss: 0.9685362911224366\n",
      "  batch 800 loss: 1.006379669904709\n",
      "  batch 850 loss: 0.9903412997722626\n",
      "  batch 900 loss: 0.9812425303459168\n",
      "LOSS train 0.98124 valid 0.96009, valid PER 30.27%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.991793270111084\n",
      "  batch 100 loss: 0.9478887474536896\n",
      "  batch 150 loss: 0.9363275623321533\n",
      "  batch 200 loss: 0.9389587044715881\n",
      "  batch 250 loss: 0.9768990278244019\n",
      "  batch 300 loss: 0.955677160024643\n",
      "  batch 350 loss: 0.9432829308509827\n",
      "  batch 400 loss: 0.9343936204910278\n",
      "  batch 450 loss: 0.9662741053104401\n",
      "  batch 500 loss: 0.9358430671691894\n",
      "  batch 550 loss: 0.9594438862800598\n",
      "  batch 600 loss: 0.9294424343109131\n",
      "  batch 650 loss: 0.9259474420547485\n",
      "  batch 700 loss: 0.9411460590362549\n",
      "  batch 750 loss: 0.9257644736766815\n",
      "  batch 800 loss: 0.9320670747756958\n",
      "  batch 850 loss: 0.9153945326805115\n",
      "  batch 900 loss: 0.9410320901870728\n",
      "LOSS train 0.94103 valid 0.92450, valid PER 29.03%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.9362989342212678\n",
      "  batch 100 loss: 0.9252750360965729\n",
      "  batch 150 loss: 0.877881178855896\n",
      "  batch 200 loss: 0.8944698929786682\n",
      "  batch 250 loss: 0.8797212153673172\n",
      "  batch 300 loss: 0.8744949316978454\n",
      "  batch 350 loss: 0.9007451343536377\n",
      "  batch 400 loss: 0.893544249534607\n",
      "  batch 450 loss: 0.8970347619056702\n",
      "  batch 500 loss: 0.8981860721111298\n",
      "  batch 550 loss: 0.8647955381870269\n",
      "  batch 600 loss: 0.8857386946678162\n",
      "  batch 650 loss: 0.8701196777820587\n",
      "  batch 700 loss: 0.8955688977241516\n",
      "  batch 750 loss: 0.8764066934585572\n",
      "  batch 800 loss: 0.8666112816333771\n",
      "  batch 850 loss: 0.8790198421478271\n",
      "  batch 900 loss: 0.9269610488414765\n",
      "LOSS train 0.92696 valid 0.89640, valid PER 28.24%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8639072179794312\n",
      "  batch 100 loss: 0.8365390586853028\n",
      "  batch 150 loss: 0.8443206489086151\n",
      "  batch 200 loss: 0.8204667031764984\n",
      "  batch 250 loss: 0.8569099014997482\n",
      "  batch 300 loss: 0.8086697793006897\n",
      "  batch 350 loss: 0.8859838545322418\n",
      "  batch 400 loss: 0.8253711760044098\n",
      "  batch 450 loss: 0.8614222168922424\n",
      "  batch 500 loss: 0.8734420323371888\n",
      "  batch 550 loss: 0.8236942672729493\n",
      "  batch 600 loss: 0.8637437379360199\n",
      "  batch 650 loss: 0.8647813987731934\n",
      "  batch 700 loss: 0.8287151968479156\n",
      "  batch 750 loss: 0.8231001913547515\n",
      "  batch 800 loss: 0.8418290573358536\n",
      "  batch 850 loss: 0.8189420342445374\n",
      "  batch 900 loss: 0.8646997463703155\n",
      "LOSS train 0.86470 valid 0.87101, valid PER 27.28%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7940984725952148\n",
      "  batch 100 loss: 0.8048162424564361\n",
      "  batch 150 loss: 0.8064848923683167\n",
      "  batch 200 loss: 0.7755566781759262\n",
      "  batch 250 loss: 0.8309530878067016\n",
      "  batch 300 loss: 0.8121860897541047\n",
      "  batch 350 loss: 0.8574115896224975\n",
      "  batch 400 loss: 0.7964212942123413\n",
      "  batch 450 loss: 0.8161195361614227\n",
      "  batch 500 loss: 0.7755309987068176\n",
      "  batch 550 loss: 0.7966213655471802\n",
      "  batch 600 loss: 0.8218322944641113\n",
      "  batch 650 loss: 0.795003559589386\n",
      "  batch 700 loss: 0.7842763137817382\n",
      "  batch 750 loss: 0.7947612804174423\n",
      "  batch 800 loss: 0.8160517525672912\n",
      "  batch 850 loss: 0.8275038385391236\n",
      "  batch 900 loss: 0.7801948928833008\n",
      "LOSS train 0.78019 valid 0.83439, valid PER 26.43%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7269771957397461\n",
      "  batch 100 loss: 0.7636767458915711\n",
      "  batch 150 loss: 0.7585489392280579\n",
      "  batch 200 loss: 0.7845590317249298\n",
      "  batch 250 loss: 0.7862046122550964\n",
      "  batch 300 loss: 0.7397096979618073\n",
      "  batch 350 loss: 0.813371080160141\n",
      "  batch 400 loss: 0.7694602864980697\n",
      "  batch 450 loss: 0.7440050685405731\n",
      "  batch 500 loss: 0.7943862843513488\n",
      "  batch 550 loss: 0.7961018848419189\n",
      "  batch 600 loss: 0.7779653358459473\n",
      "  batch 650 loss: 0.7748809659481048\n",
      "  batch 700 loss: 0.7935495305061341\n",
      "  batch 750 loss: 0.7689608299732208\n",
      "  batch 800 loss: 0.7897496122121811\n",
      "  batch 850 loss: 0.7740456867218017\n",
      "  batch 900 loss: 0.7911058628559112\n",
      "LOSS train 0.79111 valid 0.84652, valid PER 26.69%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.7171333634853363\n",
      "  batch 100 loss: 0.7265887570381164\n",
      "  batch 150 loss: 0.713619372844696\n",
      "  batch 200 loss: 0.7718822181224823\n",
      "  batch 250 loss: 0.7574106448888779\n",
      "  batch 300 loss: 0.7294985818862915\n",
      "  batch 350 loss: 0.7323479479551316\n",
      "  batch 400 loss: 0.7844618582725524\n",
      "  batch 450 loss: 0.7601213413476944\n",
      "  batch 500 loss: 0.7277899509668351\n",
      "  batch 550 loss: 0.754115959405899\n",
      "  batch 600 loss: 0.7165788578987121\n",
      "  batch 650 loss: 0.8296258974075318\n",
      "  batch 700 loss: 0.7354431015253067\n",
      "  batch 750 loss: 0.7259582996368408\n",
      "  batch 800 loss: 0.7895101374387741\n",
      "  batch 850 loss: 0.7940220332145691\n",
      "  batch 900 loss: 0.7843300187587738\n",
      "LOSS train 0.78433 valid 0.82017, valid PER 25.02%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.7425531232357026\n",
      "  batch 100 loss: 0.7166725367307663\n",
      "  batch 150 loss: 0.6802949100732804\n",
      "  batch 200 loss: 0.7110057699680329\n",
      "  batch 250 loss: 0.7272000980377197\n",
      "  batch 300 loss: 0.7093937849998474\n",
      "  batch 350 loss: 0.7065261077880859\n",
      "  batch 400 loss: 0.7287155556678772\n",
      "  batch 450 loss: 0.7392203426361084\n",
      "  batch 500 loss: 0.7367387622594833\n",
      "  batch 550 loss: 0.6838575053215027\n",
      "  batch 600 loss: 0.7031113058328629\n",
      "  batch 650 loss: 0.7502331531047821\n",
      "  batch 700 loss: 0.7269266474246979\n",
      "  batch 750 loss: 0.7206605702638627\n",
      "  batch 800 loss: 0.6861773014068604\n",
      "  batch 850 loss: 0.743866166472435\n",
      "  batch 900 loss: 0.7323261880874634\n",
      "LOSS train 0.73233 valid 0.80045, valid PER 25.24%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7005098628997802\n",
      "  batch 100 loss: 0.6975779265165329\n",
      "  batch 150 loss: 0.6816817706823349\n",
      "  batch 200 loss: 0.6917718535661698\n",
      "  batch 250 loss: 0.6785027915239334\n",
      "  batch 300 loss: 0.6977568608522415\n",
      "  batch 350 loss: 0.6827973294258117\n",
      "  batch 400 loss: 0.7161683976650238\n",
      "  batch 450 loss: 0.7186190342903137\n",
      "  batch 500 loss: 0.6756787878274918\n",
      "  batch 550 loss: 0.722051020860672\n",
      "  batch 600 loss: 0.7146906560659408\n",
      "  batch 650 loss: 0.7420684230327607\n",
      "  batch 700 loss: 0.742547110915184\n",
      "  batch 750 loss: 0.6846978521347046\n",
      "  batch 800 loss: 0.6873160398006439\n",
      "  batch 850 loss: 0.7179740470647812\n",
      "  batch 900 loss: 0.6971331143379211\n",
      "LOSS train 0.69713 valid 0.80831, valid PER 25.06%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.6604909181594849\n",
      "  batch 100 loss: 0.6693654036521912\n",
      "  batch 150 loss: 0.6672548532485962\n",
      "  batch 200 loss: 0.6483800494670868\n",
      "  batch 250 loss: 0.6520977705717087\n",
      "  batch 300 loss: 0.6774711179733276\n",
      "  batch 350 loss: 0.665616443157196\n",
      "  batch 400 loss: 0.6490798455476761\n",
      "  batch 450 loss: 0.667052875161171\n",
      "  batch 500 loss: 0.6743297475576401\n",
      "  batch 550 loss: 0.7038874584436416\n",
      "  batch 600 loss: 0.6524537324905395\n",
      "  batch 650 loss: 0.6923395246267319\n",
      "  batch 700 loss: 0.7216780269145966\n",
      "  batch 750 loss: 0.6708334904909133\n",
      "  batch 800 loss: 0.643890580534935\n",
      "  batch 850 loss: 0.7006505072116852\n",
      "  batch 900 loss: 0.7017021155357361\n",
      "LOSS train 0.70170 valid 0.81024, valid PER 25.36%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6465762436389924\n",
      "  batch 100 loss: 0.6540616112947464\n",
      "  batch 150 loss: 0.6384971284866333\n",
      "  batch 200 loss: 0.6588222080469132\n",
      "  batch 250 loss: 0.65417052090168\n",
      "  batch 300 loss: 0.6380314415693283\n",
      "  batch 350 loss: 0.6298001676797866\n",
      "  batch 400 loss: 0.6313499671220779\n",
      "  batch 450 loss: 0.6465609735250473\n",
      "  batch 500 loss: 0.6199885314702988\n",
      "  batch 550 loss: 0.6464339298009872\n",
      "  batch 600 loss: 0.6686300611495972\n",
      "  batch 650 loss: 0.6507527136802673\n",
      "  batch 700 loss: 0.674298991560936\n",
      "  batch 750 loss: 0.6815397936105728\n",
      "  batch 800 loss: 0.6508897793292999\n",
      "  batch 850 loss: 0.6396638184785843\n",
      "  batch 900 loss: 0.6476473790407181\n",
      "LOSS train 0.64765 valid 0.78580, valid PER 24.29%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.6329702395200729\n",
      "  batch 100 loss: 0.6136350411176682\n",
      "  batch 150 loss: 0.6197235351800918\n",
      "  batch 200 loss: 0.6076632171869278\n",
      "  batch 250 loss: 0.6426066303253174\n",
      "  batch 300 loss: 0.622633209824562\n",
      "  batch 350 loss: 0.6401486051082611\n",
      "  batch 400 loss: 0.6428346091508865\n",
      "  batch 450 loss: 0.6491799765825271\n",
      "  batch 500 loss: 0.61251848757267\n",
      "  batch 550 loss: 0.6356224381923675\n",
      "  batch 600 loss: 0.6258497941493988\n",
      "  batch 650 loss: 0.6452660602331162\n",
      "  batch 700 loss: 0.6211342626810074\n",
      "  batch 750 loss: 0.6156965869665146\n",
      "  batch 800 loss: 0.6488967090845108\n",
      "  batch 850 loss: 0.6287896013259888\n",
      "  batch 900 loss: 0.6330706387758255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.63307 valid 0.79375, valid PER 24.03%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.595822229385376\n",
      "  batch 100 loss: 0.6046186757087707\n",
      "  batch 150 loss: 0.5873568183183671\n",
      "  batch 200 loss: 0.6070578736066818\n",
      "  batch 250 loss: 0.6088601517677307\n",
      "  batch 300 loss: 0.6076909255981445\n",
      "  batch 350 loss: 0.5879766058921814\n",
      "  batch 400 loss: 0.6228116065263748\n",
      "  batch 450 loss: 0.6273621267080307\n",
      "  batch 500 loss: 0.5882393771409988\n",
      "  batch 550 loss: 0.5966208469867706\n",
      "  batch 600 loss: 0.6305315935611725\n",
      "  batch 650 loss: 0.6219890838861466\n",
      "  batch 700 loss: 0.6006630831956863\n",
      "  batch 750 loss: 0.5853275191783905\n",
      "  batch 800 loss: 0.6006005495786667\n",
      "  batch 850 loss: 0.619127676486969\n",
      "  batch 900 loss: 0.6113909250497818\n",
      "LOSS train 0.61139 valid 0.78189, valid PER 23.52%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.5692068809270858\n",
      "  batch 100 loss: 0.5692355996370315\n",
      "  batch 150 loss: 0.6171820616722107\n",
      "  batch 200 loss: 0.5824350708723068\n",
      "  batch 250 loss: 0.5909203869104386\n",
      "  batch 300 loss: 0.5729901188611984\n",
      "  batch 350 loss: 0.5814148384332657\n",
      "  batch 400 loss: 0.5662361049652099\n",
      "  batch 450 loss: 0.6041121226549149\n",
      "  batch 500 loss: 0.6057087421417237\n",
      "  batch 550 loss: 0.610714385509491\n",
      "  batch 600 loss: 0.5505610495805741\n",
      "  batch 650 loss: 0.5791652101278305\n",
      "  batch 700 loss: 0.6124143916368484\n",
      "  batch 750 loss: 0.5872567784786225\n",
      "  batch 800 loss: 0.5799756538867951\n",
      "  batch 850 loss: 0.5827811723947525\n",
      "  batch 900 loss: 0.6101764649152756\n",
      "LOSS train 0.61018 valid 0.78527, valid PER 23.76%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.524202938079834\n",
      "  batch 100 loss: 0.5381286591291428\n",
      "  batch 150 loss: 0.5422474545240402\n",
      "  batch 200 loss: 0.5673471939563751\n",
      "  batch 250 loss: 0.5634407305717468\n",
      "  batch 300 loss: 0.568288117647171\n",
      "  batch 350 loss: 0.5593472754955292\n",
      "  batch 400 loss: 0.572917895913124\n",
      "  batch 450 loss: 0.567317168712616\n",
      "  batch 500 loss: 0.5829455798864365\n",
      "  batch 550 loss: 0.5458368158340454\n",
      "  batch 600 loss: 0.5597488021850586\n",
      "  batch 650 loss: 0.6089764070510865\n",
      "  batch 700 loss: 0.5544870066642761\n",
      "  batch 750 loss: 0.5641270422935486\n",
      "  batch 800 loss: 0.577838938832283\n",
      "  batch 850 loss: 0.5748786860704422\n",
      "  batch 900 loss: 0.5798647183179856\n",
      "LOSS train 0.57986 valid 0.81966, valid PER 24.36%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.5338494712114334\n",
      "  batch 100 loss: 0.52050237596035\n",
      "  batch 150 loss: 0.5377326971292495\n",
      "  batch 200 loss: 0.5706767988204956\n",
      "  batch 250 loss: 0.5414676743745804\n",
      "  batch 300 loss: 0.5626262593269348\n",
      "  batch 350 loss: 0.5081250563263893\n",
      "  batch 400 loss: 0.558774516582489\n",
      "  batch 450 loss: 0.5553196775913238\n",
      "  batch 500 loss: 0.5118254160881043\n",
      "  batch 550 loss: 0.5783852159976959\n",
      "  batch 600 loss: 0.5372500002384186\n",
      "  batch 650 loss: 0.5518184477090835\n",
      "  batch 700 loss: 0.5620604014396667\n",
      "  batch 750 loss: 0.5320732060074806\n",
      "  batch 800 loss: 0.568268369436264\n",
      "  batch 850 loss: 0.572752280831337\n",
      "  batch 900 loss: 0.573684754371643\n",
      "LOSS train 0.57368 valid 0.77436, valid PER 23.45%\n",
      "Training finished in 4.0 minutes.\n",
      "Model saved to checkpoints/20231205_144257/model_20\n",
      "Loading model from checkpoints/20231205_144257/model_20\n",
      "For dropout rate 0.3 the best model has SUB: 14.79%, DEL: 7.97%, INS: 2.24%, COR: 77.24%, PER: 25.00%\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.056333303451538\n",
      "  batch 100 loss: 3.380490641593933\n",
      "  batch 150 loss: 3.285047483444214\n",
      "  batch 200 loss: 3.184720993041992\n",
      "  batch 250 loss: 3.0725114917755127\n",
      "  batch 300 loss: 2.8574013137817382\n",
      "  batch 350 loss: 2.700239176750183\n",
      "  batch 400 loss: 2.5892829656600953\n",
      "  batch 450 loss: 2.5042537546157835\n",
      "  batch 500 loss: 2.3749501848220826\n",
      "  batch 550 loss: 2.308729820251465\n",
      "  batch 600 loss: 2.2435555505752562\n",
      "  batch 650 loss: 2.157050726413727\n",
      "  batch 700 loss: 2.136485297679901\n",
      "  batch 750 loss: 2.0476533865928648\n",
      "  batch 800 loss: 2.0218465876579286\n",
      "  batch 850 loss: 1.9589692211151124\n",
      "  batch 900 loss: 1.927943823337555\n",
      "LOSS train 1.92794 valid 1.81156, valid PER 71.83%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.8487943816184997\n",
      "  batch 100 loss: 1.7785822677612304\n",
      "  batch 150 loss: 1.7311654925346374\n",
      "  batch 200 loss: 1.7357781744003296\n",
      "  batch 250 loss: 1.735370728969574\n",
      "  batch 300 loss: 1.6941013765335082\n",
      "  batch 350 loss: 1.6001852035522461\n",
      "  batch 400 loss: 1.6142023420333862\n",
      "  batch 450 loss: 1.565001916885376\n",
      "  batch 500 loss: 1.5772548961639403\n",
      "  batch 550 loss: 1.5808320355415344\n",
      "  batch 600 loss: 1.512575283050537\n",
      "  batch 650 loss: 1.5562973999977112\n",
      "  batch 700 loss: 1.4974166297912597\n",
      "  batch 750 loss: 1.4827441787719726\n",
      "  batch 800 loss: 1.417377324104309\n",
      "  batch 850 loss: 1.4173686504364014\n",
      "  batch 900 loss: 1.4371149849891662\n",
      "LOSS train 1.43711 valid 1.35293, valid PER 45.21%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.3893357133865356\n",
      "  batch 100 loss: 1.3470800805091858\n",
      "  batch 150 loss: 1.3496880531311035\n",
      "  batch 200 loss: 1.3266300249099732\n",
      "  batch 250 loss: 1.3107891869544983\n",
      "  batch 300 loss: 1.3074712586402892\n",
      "  batch 350 loss: 1.3233398330211639\n",
      "  batch 400 loss: 1.3194809353351593\n",
      "  batch 450 loss: 1.2863233923912047\n",
      "  batch 500 loss: 1.2699855053424836\n",
      "  batch 550 loss: 1.26770076751709\n",
      "  batch 600 loss: 1.2397598016262055\n",
      "  batch 650 loss: 1.2032387733459473\n",
      "  batch 700 loss: 1.2321941983699798\n",
      "  batch 750 loss: 1.2789089787006378\n",
      "  batch 800 loss: 1.2374067211151123\n",
      "  batch 850 loss: 1.2488288259506226\n",
      "  batch 900 loss: 1.1631367468833924\n",
      "LOSS train 1.16314 valid 1.12345, valid PER 35.37%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.1644949066638945\n",
      "  batch 100 loss: 1.1894391238689423\n",
      "  batch 150 loss: 1.1518287038803101\n",
      "  batch 200 loss: 1.1693479788303376\n",
      "  batch 250 loss: 1.170446207523346\n",
      "  batch 300 loss: 1.1709565949440002\n",
      "  batch 350 loss: 1.097268146276474\n",
      "  batch 400 loss: 1.1485094451904296\n",
      "  batch 450 loss: 1.1393725752830506\n",
      "  batch 500 loss: 1.1175245499610902\n",
      "  batch 550 loss: 1.1484537863731383\n",
      "  batch 600 loss: 1.151376065015793\n",
      "  batch 650 loss: 1.1347434282302857\n",
      "  batch 700 loss: 1.0950909280776977\n",
      "  batch 750 loss: 1.093527135848999\n",
      "  batch 800 loss: 1.0595624399185182\n",
      "  batch 850 loss: 1.1029666447639466\n",
      "  batch 900 loss: 1.1355431222915648\n",
      "LOSS train 1.13554 valid 1.03672, valid PER 33.42%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.0586557877063751\n",
      "  batch 100 loss: 1.04635893702507\n",
      "  batch 150 loss: 1.1060870909690856\n",
      "  batch 200 loss: 1.028441379070282\n",
      "  batch 250 loss: 1.0477968561649322\n",
      "  batch 300 loss: 1.0601300048828124\n",
      "  batch 350 loss: 1.0366734731197358\n",
      "  batch 400 loss: 1.0696593403816224\n",
      "  batch 450 loss: 1.0258750462532042\n",
      "  batch 500 loss: 1.0584606266021728\n",
      "  batch 550 loss: 0.9888122582435608\n",
      "  batch 600 loss: 1.085264686346054\n",
      "  batch 650 loss: 1.0371559178829193\n",
      "  batch 700 loss: 1.0656551766395568\n",
      "  batch 750 loss: 1.0050170695781708\n",
      "  batch 800 loss: 1.0440042996406556\n",
      "  batch 850 loss: 1.0323660206794738\n",
      "  batch 900 loss: 1.0330180180072785\n",
      "LOSS train 1.03302 valid 0.97680, valid PER 30.92%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.0440715730190278\n",
      "  batch 100 loss: 0.9980382061004639\n",
      "  batch 150 loss: 0.977371152639389\n",
      "  batch 200 loss: 0.9697057783603669\n",
      "  batch 250 loss: 1.0199926102161407\n",
      "  batch 300 loss: 0.9848948621749878\n",
      "  batch 350 loss: 0.9949267649650574\n",
      "  batch 400 loss: 0.9825330007076264\n",
      "  batch 450 loss: 0.9978394985198975\n",
      "  batch 500 loss: 1.0047382402420044\n",
      "  batch 550 loss: 1.0145804584026337\n",
      "  batch 600 loss: 0.9813599729537964\n",
      "  batch 650 loss: 0.9816689383983612\n",
      "  batch 700 loss: 0.9801446509361267\n",
      "  batch 750 loss: 0.9584236752986908\n",
      "  batch 800 loss: 0.9744609141349793\n",
      "  batch 850 loss: 0.9393237960338593\n",
      "  batch 900 loss: 0.9781277930736542\n",
      "LOSS train 0.97813 valid 0.95547, valid PER 29.83%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.9689418232440948\n",
      "  batch 100 loss: 0.9803206121921539\n",
      "  batch 150 loss: 0.9380541610717773\n",
      "  batch 200 loss: 0.9497741103172302\n",
      "  batch 250 loss: 0.9280214214324951\n",
      "  batch 300 loss: 0.9213410723209381\n",
      "  batch 350 loss: 0.9333400452136993\n",
      "  batch 400 loss: 0.9281506335735321\n",
      "  batch 450 loss: 0.93432546377182\n",
      "  batch 500 loss: 0.934474276304245\n",
      "  batch 550 loss: 0.9061721503734589\n",
      "  batch 600 loss: 0.9484611165523529\n",
      "  batch 650 loss: 0.9244454419612884\n",
      "  batch 700 loss: 0.9440344977378845\n",
      "  batch 750 loss: 0.9194392991065979\n",
      "  batch 800 loss: 0.8998554217815399\n",
      "  batch 850 loss: 0.9193059515953064\n",
      "  batch 900 loss: 0.9742104351520539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.97421 valid 0.91337, valid PER 28.62%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8876850199699402\n",
      "  batch 100 loss: 0.8773271298408508\n",
      "  batch 150 loss: 0.8931175661087036\n",
      "  batch 200 loss: 0.8836658608913421\n",
      "  batch 250 loss: 0.9137315022945404\n",
      "  batch 300 loss: 0.8328019762039185\n",
      "  batch 350 loss: 0.9143855500221253\n",
      "  batch 400 loss: 0.869559268951416\n",
      "  batch 450 loss: 0.8946341276168823\n",
      "  batch 500 loss: 0.9204366517066955\n",
      "  batch 550 loss: 0.8568529117107392\n",
      "  batch 600 loss: 0.9086426544189453\n",
      "  batch 650 loss: 0.9305352950096131\n",
      "  batch 700 loss: 0.8786245501041412\n",
      "  batch 750 loss: 0.8719937098026276\n",
      "  batch 800 loss: 0.8766406321525574\n",
      "  batch 850 loss: 0.8653351473808288\n",
      "  batch 900 loss: 0.8790978229045868\n",
      "LOSS train 0.87910 valid 0.88626, valid PER 27.91%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.8373034703731537\n",
      "  batch 100 loss: 0.8607512807846069\n",
      "  batch 150 loss: 0.8571032214164734\n",
      "  batch 200 loss: 0.8345617818832397\n",
      "  batch 250 loss: 0.8798775947093964\n",
      "  batch 300 loss: 0.8880364680290223\n",
      "  batch 350 loss: 0.9077685523033142\n",
      "  batch 400 loss: 0.8529981255531311\n",
      "  batch 450 loss: 0.8694545960426331\n",
      "  batch 500 loss: 0.8226133799552917\n",
      "  batch 550 loss: 0.8617087841033936\n",
      "  batch 600 loss: 0.8586626613140106\n",
      "  batch 650 loss: 0.8248409700393676\n",
      "  batch 700 loss: 0.8251838332414627\n",
      "  batch 750 loss: 0.8352773880958557\n",
      "  batch 800 loss: 0.8615920448303223\n",
      "  batch 850 loss: 0.8753534317016601\n",
      "  batch 900 loss: 0.8140169322490692\n",
      "LOSS train 0.81402 valid 0.86446, valid PER 27.12%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7785480034351349\n",
      "  batch 100 loss: 0.7990658581256866\n",
      "  batch 150 loss: 0.8137577593326568\n",
      "  batch 200 loss: 0.8325312781333923\n",
      "  batch 250 loss: 0.8333899486064911\n",
      "  batch 300 loss: 0.8082640743255616\n",
      "  batch 350 loss: 0.830594767332077\n",
      "  batch 400 loss: 0.7891202533245086\n",
      "  batch 450 loss: 0.7820240586996079\n",
      "  batch 500 loss: 0.8294008028507233\n",
      "  batch 550 loss: 0.8560017347335815\n",
      "  batch 600 loss: 0.8060513043403625\n",
      "  batch 650 loss: 0.8035177445411682\n",
      "  batch 700 loss: 0.8288284564018249\n",
      "  batch 750 loss: 0.7960029518604279\n",
      "  batch 800 loss: 0.8191413247585296\n",
      "  batch 850 loss: 0.8114784741401673\n",
      "  batch 900 loss: 0.8321170616149902\n",
      "LOSS train 0.83212 valid 0.83535, valid PER 26.85%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.7600546133518219\n",
      "  batch 100 loss: 0.7501559555530548\n",
      "  batch 150 loss: 0.7573648780584336\n",
      "  batch 200 loss: 0.804284046292305\n",
      "  batch 250 loss: 0.7860257810354233\n",
      "  batch 300 loss: 0.7616304111480713\n",
      "  batch 350 loss: 0.7691407883167267\n",
      "  batch 400 loss: 0.794521734714508\n",
      "  batch 450 loss: 0.8084971475601196\n",
      "  batch 500 loss: 0.751064635515213\n",
      "  batch 550 loss: 0.7885021829605102\n",
      "  batch 600 loss: 0.7491989994049072\n",
      "  batch 650 loss: 0.8337184107303619\n",
      "  batch 700 loss: 0.7646969032287597\n",
      "  batch 750 loss: 0.7476528060436248\n",
      "  batch 800 loss: 0.8045698320865631\n",
      "  batch 850 loss: 0.8215162181854248\n",
      "  batch 900 loss: 0.8332591605186462\n",
      "LOSS train 0.83326 valid 0.82841, valid PER 25.66%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.776842919588089\n",
      "  batch 100 loss: 0.7606619381904602\n",
      "  batch 150 loss: 0.7122117805480958\n",
      "  batch 200 loss: 0.7437692618370056\n",
      "  batch 250 loss: 0.7649717092514038\n",
      "  batch 300 loss: 0.7383974671363831\n",
      "  batch 350 loss: 0.7366205197572708\n",
      "  batch 400 loss: 0.7804067766666413\n",
      "  batch 450 loss: 0.7598206436634064\n",
      "  batch 500 loss: 0.7825261515378952\n",
      "  batch 550 loss: 0.7139607834815979\n",
      "  batch 600 loss: 0.7310141450166703\n",
      "  batch 650 loss: 0.7870277750492096\n",
      "  batch 700 loss: 0.7645454162359238\n",
      "  batch 750 loss: 0.7686632883548736\n",
      "  batch 800 loss: 0.7341690200567246\n",
      "  batch 850 loss: 0.7830731177330017\n",
      "  batch 900 loss: 0.7843133842945099\n",
      "LOSS train 0.78431 valid 0.82162, valid PER 25.95%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.726323276758194\n",
      "  batch 100 loss: 0.7282421398162842\n",
      "  batch 150 loss: 0.7044232279062271\n",
      "  batch 200 loss: 0.7396514558792114\n",
      "  batch 250 loss: 0.7141484916210175\n",
      "  batch 300 loss: 0.7282404404878616\n",
      "  batch 350 loss: 0.7023473477363587\n",
      "  batch 400 loss: 0.7468709421157836\n",
      "  batch 450 loss: 0.7448905372619629\n",
      "  batch 500 loss: 0.7038925629854202\n",
      "  batch 550 loss: 0.7746499574184418\n",
      "  batch 600 loss: 0.719308408498764\n",
      "  batch 650 loss: 0.7465883815288543\n",
      "  batch 700 loss: 0.7434777271747589\n",
      "  batch 750 loss: 0.7101802456378937\n",
      "  batch 800 loss: 0.718871408700943\n",
      "  batch 850 loss: 0.7695426452159881\n",
      "  batch 900 loss: 0.7370826756954193\n",
      "LOSS train 0.73708 valid 0.81485, valid PER 25.14%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.6890599775314331\n",
      "  batch 100 loss: 0.7006870567798614\n",
      "  batch 150 loss: 0.6846209180355072\n",
      "  batch 200 loss: 0.6977899867296219\n",
      "  batch 250 loss: 0.7068757152557373\n",
      "  batch 300 loss: 0.7253325605392456\n",
      "  batch 350 loss: 0.6986408692598343\n",
      "  batch 400 loss: 0.6830607461929321\n",
      "  batch 450 loss: 0.7018490570783615\n",
      "  batch 500 loss: 0.7200772690773011\n",
      "  batch 550 loss: 0.7291463083028793\n",
      "  batch 600 loss: 0.6824953871965408\n",
      "  batch 650 loss: 0.7103907078504562\n",
      "  batch 700 loss: 0.7447027170658111\n",
      "  batch 750 loss: 0.7127306401729584\n",
      "  batch 800 loss: 0.6978703671693802\n",
      "  batch 850 loss: 0.7395076233148575\n",
      "  batch 900 loss: 0.7090083754062653\n",
      "LOSS train 0.70901 valid 0.80918, valid PER 25.06%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6756703025102615\n",
      "  batch 100 loss: 0.6868535250425338\n",
      "  batch 150 loss: 0.6785970807075501\n",
      "  batch 200 loss: 0.6935283046960831\n",
      "  batch 250 loss: 0.687754522562027\n",
      "  batch 300 loss: 0.6680541467666626\n",
      "  batch 350 loss: 0.6732130461931228\n",
      "  batch 400 loss: 0.6757599818706512\n",
      "  batch 450 loss: 0.6663940501213074\n",
      "  batch 500 loss: 0.6639103507995605\n",
      "  batch 550 loss: 0.6784507900476455\n",
      "  batch 600 loss: 0.6906985038518906\n",
      "  batch 650 loss: 0.711113229393959\n",
      "  batch 700 loss: 0.7078046405315399\n",
      "  batch 750 loss: 0.7071078383922577\n",
      "  batch 800 loss: 0.6805015766620636\n",
      "  batch 850 loss: 0.6756784725189209\n",
      "  batch 900 loss: 0.6763327717781067\n",
      "LOSS train 0.67633 valid 0.82675, valid PER 24.97%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.6797736430168152\n",
      "  batch 100 loss: 0.6366824829578399\n",
      "  batch 150 loss: 0.6399130362272263\n",
      "  batch 200 loss: 0.6451911079883575\n",
      "  batch 250 loss: 0.6734640210866928\n",
      "  batch 300 loss: 0.6506201589107513\n",
      "  batch 350 loss: 0.685034236907959\n",
      "  batch 400 loss: 0.6761491811275482\n",
      "  batch 450 loss: 0.6946628552675247\n",
      "  batch 500 loss: 0.637001137137413\n",
      "  batch 550 loss: 0.6624773025512696\n",
      "  batch 600 loss: 0.6507202535867691\n",
      "  batch 650 loss: 0.6813613188266754\n",
      "  batch 700 loss: 0.6271325600147247\n",
      "  batch 750 loss: 0.6565299367904663\n",
      "  batch 800 loss: 0.6650137490034104\n",
      "  batch 850 loss: 0.6601655513048172\n",
      "  batch 900 loss: 0.6597228538990021\n",
      "LOSS train 0.65972 valid 0.79093, valid PER 24.14%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.6442271065711975\n",
      "  batch 100 loss: 0.6553415602445603\n",
      "  batch 150 loss: 0.6057355511188507\n",
      "  batch 200 loss: 0.6325589406490326\n",
      "  batch 250 loss: 0.6478679931163788\n",
      "  batch 300 loss: 0.6512349915504455\n",
      "  batch 350 loss: 0.6187869310379028\n",
      "  batch 400 loss: 0.6660522276163101\n",
      "  batch 450 loss: 0.6480586075782776\n",
      "  batch 500 loss: 0.6316868376731872\n",
      "  batch 550 loss: 0.6392251080274582\n",
      "  batch 600 loss: 0.6793014389276505\n",
      "  batch 650 loss: 0.6418841177225113\n",
      "  batch 700 loss: 0.6359681439399719\n",
      "  batch 750 loss: 0.6278818583488465\n",
      "  batch 800 loss: 0.6355651807785034\n",
      "  batch 850 loss: 0.65972032725811\n",
      "  batch 900 loss: 0.6353808462619781\n",
      "LOSS train 0.63538 valid 0.78719, valid PER 23.84%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.6142666763067246\n",
      "  batch 100 loss: 0.623864706158638\n",
      "  batch 150 loss: 0.6436831438541413\n",
      "  batch 200 loss: 0.6336330461502075\n",
      "  batch 250 loss: 0.6242614370584488\n",
      "  batch 300 loss: 0.600049307346344\n",
      "  batch 350 loss: 0.63212475836277\n",
      "  batch 400 loss: 0.6078946572542191\n",
      "  batch 450 loss: 0.6454311442375184\n",
      "  batch 500 loss: 0.6142211049795151\n",
      "  batch 550 loss: 0.637837535738945\n",
      "  batch 600 loss: 0.5885112750530243\n",
      "  batch 650 loss: 0.6207935255765915\n",
      "  batch 700 loss: 0.6504462283849716\n",
      "  batch 750 loss: 0.6012052482366562\n",
      "  batch 800 loss: 0.6062696689367294\n",
      "  batch 850 loss: 0.6154267525672913\n",
      "  batch 900 loss: 0.644799479842186\n",
      "LOSS train 0.64480 valid 0.79063, valid PER 23.62%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.553228588104248\n",
      "  batch 100 loss: 0.5545136296749115\n",
      "  batch 150 loss: 0.5770193827152252\n",
      "  batch 200 loss: 0.6035678768157959\n",
      "  batch 250 loss: 0.6088197755813599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 0.5964150148630142\n",
      "  batch 350 loss: 0.584349552989006\n",
      "  batch 400 loss: 0.6217438161373139\n",
      "  batch 450 loss: 0.6059185075759888\n",
      "  batch 500 loss: 0.6116536611318588\n",
      "  batch 550 loss: 0.586358442902565\n",
      "  batch 600 loss: 0.6103448206186295\n",
      "  batch 650 loss: 0.6484892344474793\n",
      "  batch 700 loss: 0.6030696624517441\n",
      "  batch 750 loss: 0.5944795697927475\n",
      "  batch 800 loss: 0.6122959280014038\n",
      "  batch 850 loss: 0.6109815490245819\n",
      "  batch 900 loss: 0.6378160607814789\n",
      "LOSS train 0.63782 valid 0.78938, valid PER 23.62%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.5672115510702134\n",
      "  batch 100 loss: 0.5634790045022965\n",
      "  batch 150 loss: 0.5358835357427597\n",
      "  batch 200 loss: 0.6013105577230453\n",
      "  batch 250 loss: 0.5662721729278565\n",
      "  batch 300 loss: 0.5920798474550247\n",
      "  batch 350 loss: 0.5576145344972611\n",
      "  batch 400 loss: 0.5852137714624405\n",
      "  batch 450 loss: 0.5750066870450974\n",
      "  batch 500 loss: 0.564138895869255\n",
      "  batch 550 loss: 0.6369296914339065\n",
      "  batch 600 loss: 0.568873291015625\n",
      "  batch 650 loss: 0.5973851519823075\n",
      "  batch 700 loss: 0.585335403084755\n",
      "  batch 750 loss: 0.5613671100139618\n",
      "  batch 800 loss: 0.5950529646873474\n",
      "  batch 850 loss: 0.6149384874105454\n",
      "  batch 900 loss: 0.6149854964017868\n",
      "LOSS train 0.61499 valid 0.78686, valid PER 23.46%\n",
      "Training finished in 4.0 minutes.\n",
      "Model saved to checkpoints/20231205_144754/model_20\n",
      "Loading model from checkpoints/20231205_144754/model_20\n",
      "For dropout rate 0.4 the best model has SUB: 14.90%, DEL: 8.08%, INS: 2.03%, COR: 77.02%, PER: 25.01%\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.055337572097779\n",
      "  batch 100 loss: 3.378162155151367\n",
      "  batch 150 loss: 3.298877897262573\n",
      "  batch 200 loss: 3.1834474563598634\n",
      "  batch 250 loss: 3.0688652849197386\n",
      "  batch 300 loss: 2.864488091468811\n",
      "  batch 350 loss: 2.711943302154541\n",
      "  batch 400 loss: 2.600454025268555\n",
      "  batch 450 loss: 2.5188046216964723\n",
      "  batch 500 loss: 2.3938988780975343\n",
      "  batch 550 loss: 2.319478712081909\n",
      "  batch 600 loss: 2.2618592381477356\n",
      "  batch 650 loss: 2.1873108744621277\n",
      "  batch 700 loss: 2.155688138008118\n",
      "  batch 750 loss: 2.0789805388450624\n",
      "  batch 800 loss: 2.0558137702941894\n",
      "  batch 850 loss: 2.005100166797638\n",
      "  batch 900 loss: 1.9591009616851807\n",
      "LOSS train 1.95910 valid 1.85551, valid PER 74.87%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.8916921997070313\n",
      "  batch 100 loss: 1.8209550547599793\n",
      "  batch 150 loss: 1.7788228392601013\n",
      "  batch 200 loss: 1.7829961848258973\n",
      "  batch 250 loss: 1.7805948734283448\n",
      "  batch 300 loss: 1.7389109873771667\n",
      "  batch 350 loss: 1.6457527017593383\n",
      "  batch 400 loss: 1.6614384484291076\n",
      "  batch 450 loss: 1.6213958740234375\n",
      "  batch 500 loss: 1.6213184952735902\n",
      "  batch 550 loss: 1.6323779249191284\n",
      "  batch 600 loss: 1.5700978446006775\n",
      "  batch 650 loss: 1.605690779685974\n",
      "  batch 700 loss: 1.546125898361206\n",
      "  batch 750 loss: 1.542579116821289\n",
      "  batch 800 loss: 1.4670544838905335\n",
      "  batch 850 loss: 1.4669231915473937\n",
      "  batch 900 loss: 1.5019869661331178\n",
      "LOSS train 1.50199 valid 1.38947, valid PER 48.04%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.443003113269806\n",
      "  batch 100 loss: 1.4069769859313965\n",
      "  batch 150 loss: 1.4041904592514038\n",
      "  batch 200 loss: 1.3772036480903624\n",
      "  batch 250 loss: 1.3752499556541442\n",
      "  batch 300 loss: 1.3594614243507386\n",
      "  batch 350 loss: 1.398993468284607\n",
      "  batch 400 loss: 1.3737478637695313\n",
      "  batch 450 loss: 1.3286640739440918\n",
      "  batch 500 loss: 1.3182237458229065\n",
      "  batch 550 loss: 1.3070194721221924\n",
      "  batch 600 loss: 1.304166306257248\n",
      "  batch 650 loss: 1.2532394814491272\n",
      "  batch 700 loss: 1.276664102077484\n",
      "  batch 750 loss: 1.3221220302581786\n",
      "  batch 800 loss: 1.2679576015472411\n",
      "  batch 850 loss: 1.2955310130119324\n",
      "  batch 900 loss: 1.2161676633358\n",
      "LOSS train 1.21617 valid 1.17067, valid PER 37.30%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.2043390154838562\n",
      "  batch 100 loss: 1.2085798597335815\n",
      "  batch 150 loss: 1.200882341861725\n",
      "  batch 200 loss: 1.224206817150116\n",
      "  batch 250 loss: 1.2138044464588165\n",
      "  batch 300 loss: 1.2232089257240295\n",
      "  batch 350 loss: 1.1480699026584624\n",
      "  batch 400 loss: 1.1859176659584045\n",
      "  batch 450 loss: 1.1709332394599914\n",
      "  batch 500 loss: 1.1450817823410033\n",
      "  batch 550 loss: 1.1728913831710814\n",
      "  batch 600 loss: 1.1951439559459687\n",
      "  batch 650 loss: 1.1635427582263946\n",
      "  batch 700 loss: 1.127829394340515\n",
      "  batch 750 loss: 1.1241765534877777\n",
      "  batch 800 loss: 1.0991792535781861\n",
      "  batch 850 loss: 1.1398838770389557\n",
      "  batch 900 loss: 1.1715529787540435\n",
      "LOSS train 1.17155 valid 1.04420, valid PER 33.33%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.082997236251831\n",
      "  batch 100 loss: 1.0880637288093566\n",
      "  batch 150 loss: 1.141509655714035\n",
      "  batch 200 loss: 1.0749331283569337\n",
      "  batch 250 loss: 1.0763909149169921\n",
      "  batch 300 loss: 1.1062938320636748\n",
      "  batch 350 loss: 1.0681731379032136\n",
      "  batch 400 loss: 1.098630200624466\n",
      "  batch 450 loss: 1.071898124217987\n",
      "  batch 500 loss: 1.1027156698703766\n",
      "  batch 550 loss: 1.0291123127937316\n",
      "  batch 600 loss: 1.1082235181331634\n",
      "  batch 650 loss: 1.0607589876651764\n",
      "  batch 700 loss: 1.1092769026756286\n",
      "  batch 750 loss: 1.0291940975189209\n",
      "  batch 800 loss: 1.0753064775466918\n",
      "  batch 850 loss: 1.055992866754532\n",
      "  batch 900 loss: 1.051375414133072\n",
      "LOSS train 1.05138 valid 0.99505, valid PER 31.36%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.0784382104873658\n",
      "  batch 100 loss: 1.0078161585330963\n",
      "  batch 150 loss: 0.9999142754077911\n",
      "  batch 200 loss: 0.9986950194835663\n",
      "  batch 250 loss: 1.0445714211463928\n",
      "  batch 300 loss: 1.0221974527835846\n",
      "  batch 350 loss: 1.0198438370227814\n",
      "  batch 400 loss: 0.9991192185878753\n",
      "  batch 450 loss: 1.0271618926525117\n",
      "  batch 500 loss: 1.005541146993637\n",
      "  batch 550 loss: 1.0404302167892456\n",
      "  batch 600 loss: 0.9817388331890107\n",
      "  batch 650 loss: 1.0088299798965454\n",
      "  batch 700 loss: 1.0165626907348633\n",
      "  batch 750 loss: 0.9901761162281036\n",
      "  batch 800 loss: 1.0059777331352233\n",
      "  batch 850 loss: 0.9758250153064728\n",
      "  batch 900 loss: 1.003223227262497\n",
      "LOSS train 1.00322 valid 0.94794, valid PER 29.90%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.9912170517444611\n",
      "  batch 100 loss: 1.004398981332779\n",
      "  batch 150 loss: 0.9527222084999084\n",
      "  batch 200 loss: 0.9548740589618683\n",
      "  batch 250 loss: 0.9644789683818817\n",
      "  batch 300 loss: 0.9582276630401612\n",
      "  batch 350 loss: 0.9604912853240967\n",
      "  batch 400 loss: 0.9558212971687317\n",
      "  batch 450 loss: 0.9671535050868988\n",
      "  batch 500 loss: 0.9545050013065338\n",
      "  batch 550 loss: 0.9547633361816407\n",
      "  batch 600 loss: 0.9691019201278687\n",
      "  batch 650 loss: 0.9381708610057831\n",
      "  batch 700 loss: 0.9872271704673767\n",
      "  batch 750 loss: 0.9531960642337799\n",
      "  batch 800 loss: 0.9496687626838685\n",
      "  batch 850 loss: 0.9603145098686219\n",
      "  batch 900 loss: 1.008741489648819\n",
      "LOSS train 1.00874 valid 0.93512, valid PER 29.70%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.9569487249851227\n",
      "  batch 100 loss: 0.9315017509460449\n",
      "  batch 150 loss: 0.9177653014659881\n",
      "  batch 200 loss: 0.9069340312480927\n",
      "  batch 250 loss: 0.9244822919368744\n",
      "  batch 300 loss: 0.874079407453537\n",
      "  batch 350 loss: 0.9552271032333374\n",
      "  batch 400 loss: 0.8898131167888641\n",
      "  batch 450 loss: 0.917240127325058\n",
      "  batch 500 loss: 0.9449190032482148\n",
      "  batch 550 loss: 0.8982329273223877\n",
      "  batch 600 loss: 0.9308996689319611\n",
      "  batch 650 loss: 0.937081344127655\n",
      "  batch 700 loss: 0.9073855316638947\n",
      "  batch 750 loss: 0.8985114288330078\n",
      "  batch 800 loss: 0.9182522016763687\n",
      "  batch 850 loss: 0.9227002108097077\n",
      "  batch 900 loss: 0.9376563131809235\n",
      "LOSS train 0.93766 valid 0.89146, valid PER 28.08%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.8624126577377319\n",
      "  batch 100 loss: 0.9132325577735901\n",
      "  batch 150 loss: 0.8963420140743256\n",
      "  batch 200 loss: 0.8529737663269042\n",
      "  batch 250 loss: 0.9205453431606293\n",
      "  batch 300 loss: 0.9009174633026124\n",
      "  batch 350 loss: 0.910745462179184\n",
      "  batch 400 loss: 0.8804339528083801\n",
      "  batch 450 loss: 0.894365108013153\n",
      "  batch 500 loss: 0.8575557541847229\n",
      "  batch 550 loss: 0.8851503539085388\n",
      "  batch 600 loss: 0.8837516772747039\n",
      "  batch 650 loss: 0.8615368926525115\n",
      "  batch 700 loss: 0.8548426401615142\n",
      "  batch 750 loss: 0.8821660339832306\n",
      "  batch 800 loss: 0.8853033244609833\n",
      "  batch 850 loss: 0.9004802799224854\n",
      "  batch 900 loss: 0.843847508430481\n",
      "LOSS train 0.84385 valid 0.85838, valid PER 26.92%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.8075007379055024\n",
      "  batch 100 loss: 0.8225813102722168\n",
      "  batch 150 loss: 0.8535046660900116\n",
      "  batch 200 loss: 0.8641354632377625\n",
      "  batch 250 loss: 0.8557473063468933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 0.8087916779518127\n",
      "  batch 350 loss: 0.8575797843933105\n",
      "  batch 400 loss: 0.798571162223816\n",
      "  batch 450 loss: 0.8150906187295913\n",
      "  batch 500 loss: 0.8718073892593384\n",
      "  batch 550 loss: 0.8829818296432496\n",
      "  batch 600 loss: 0.8470646035671234\n",
      "  batch 650 loss: 0.8501823830604553\n",
      "  batch 700 loss: 0.8563916122913361\n",
      "  batch 750 loss: 0.8399455380439759\n",
      "  batch 800 loss: 0.8496668922901154\n",
      "  batch 850 loss: 0.8581938958168029\n",
      "  batch 900 loss: 0.8597870779037475\n",
      "LOSS train 0.85979 valid 0.86082, valid PER 27.16%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.7826523685455322\n",
      "  batch 100 loss: 0.7741999769210816\n",
      "  batch 150 loss: 0.7790495038032532\n",
      "  batch 200 loss: 0.8406532180309295\n",
      "  batch 250 loss: 0.8234323173761368\n",
      "  batch 300 loss: 0.7882323038578033\n",
      "  batch 350 loss: 0.8014730489253998\n",
      "  batch 400 loss: 0.8232809960842132\n",
      "  batch 450 loss: 0.8469123458862304\n",
      "  batch 500 loss: 0.7954194748401642\n",
      "  batch 550 loss: 0.818327466249466\n",
      "  batch 600 loss: 0.7994848990440369\n",
      "  batch 650 loss: 0.8700528693199158\n",
      "  batch 700 loss: 0.8031098747253418\n",
      "  batch 750 loss: 0.7935030114650726\n",
      "  batch 800 loss: 0.8452773988246918\n",
      "  batch 850 loss: 0.8634454846382141\n",
      "  batch 900 loss: 0.8422464632987976\n",
      "LOSS train 0.84225 valid 0.82760, valid PER 25.60%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.8027090108394623\n",
      "  batch 100 loss: 0.7779804134368896\n",
      "  batch 150 loss: 0.7496144330501556\n",
      "  batch 200 loss: 0.7764971667528152\n",
      "  batch 250 loss: 0.8119739544391632\n",
      "  batch 300 loss: 0.7837579607963562\n",
      "  batch 350 loss: 0.7753783065080643\n",
      "  batch 400 loss: 0.8048052203655243\n",
      "  batch 450 loss: 0.81420534491539\n",
      "  batch 500 loss: 0.7931834316253662\n",
      "  batch 550 loss: 0.7384275650978088\n",
      "  batch 600 loss: 0.7702188682556153\n",
      "  batch 650 loss: 0.8163809716701508\n",
      "  batch 700 loss: 0.792094761133194\n",
      "  batch 750 loss: 0.7729911977052688\n",
      "  batch 800 loss: 0.7515512448549271\n",
      "  batch 850 loss: 0.8189202582836151\n",
      "  batch 900 loss: 0.8147142004966735\n",
      "LOSS train 0.81471 valid 0.81664, valid PER 25.88%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7577862644195557\n",
      "  batch 100 loss: 0.7503742921352387\n",
      "  batch 150 loss: 0.752239807844162\n",
      "  batch 200 loss: 0.7897437697649002\n",
      "  batch 250 loss: 0.7560915702581406\n",
      "  batch 300 loss: 0.7458947801589966\n",
      "  batch 350 loss: 0.7441725957393647\n",
      "  batch 400 loss: 0.7762324810028076\n",
      "  batch 450 loss: 0.7685381889343261\n",
      "  batch 500 loss: 0.7413711351156235\n",
      "  batch 550 loss: 0.7834169113636017\n",
      "  batch 600 loss: 0.7471801143884659\n",
      "  batch 650 loss: 0.7841722279787063\n",
      "  batch 700 loss: 0.7753855222463608\n",
      "  batch 750 loss: 0.730335333943367\n",
      "  batch 800 loss: 0.7477291154861451\n",
      "  batch 850 loss: 0.8117816197872162\n",
      "  batch 900 loss: 0.767435998916626\n",
      "LOSS train 0.76744 valid 0.82992, valid PER 25.84%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.7170098620653153\n",
      "  batch 100 loss: 0.7333445209264755\n",
      "  batch 150 loss: 0.7384417545795441\n",
      "  batch 200 loss: 0.7303910213708877\n",
      "  batch 250 loss: 0.7293533205986023\n",
      "  batch 300 loss: 0.7783722257614136\n",
      "  batch 350 loss: 0.7216232115030289\n",
      "  batch 400 loss: 0.7154906082153321\n",
      "  batch 450 loss: 0.7272149121761322\n",
      "  batch 500 loss: 0.7462020170688629\n",
      "  batch 550 loss: 0.7513749486207962\n",
      "  batch 600 loss: 0.7324279361963272\n",
      "  batch 650 loss: 0.7444915986061096\n",
      "  batch 700 loss: 0.7938162136077881\n",
      "  batch 750 loss: 0.73971244931221\n",
      "  batch 800 loss: 0.7132739579677582\n",
      "  batch 850 loss: 0.7475349116325378\n",
      "  batch 900 loss: 0.7446709108352662\n",
      "LOSS train 0.74467 valid 0.80903, valid PER 25.63%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.7081997156143188\n",
      "  batch 100 loss: 0.7118939077854156\n",
      "  batch 150 loss: 0.6930060946941375\n",
      "  batch 200 loss: 0.7408320915699005\n",
      "  batch 250 loss: 0.7161533749103546\n",
      "  batch 300 loss: 0.7051955741643906\n",
      "  batch 350 loss: 0.7058773022890091\n",
      "  batch 400 loss: 0.6988315260410309\n",
      "  batch 450 loss: 0.7128099507093429\n",
      "  batch 500 loss: 0.68931556224823\n",
      "  batch 550 loss: 0.7242086279392242\n",
      "  batch 600 loss: 0.7663209474086762\n",
      "  batch 650 loss: 0.740881472826004\n",
      "  batch 700 loss: 0.748481639623642\n",
      "  batch 750 loss: 0.7171108561754227\n",
      "  batch 800 loss: 0.7206590557098389\n",
      "  batch 850 loss: 0.7077936118841172\n",
      "  batch 900 loss: 0.7165684050321579\n",
      "LOSS train 0.71657 valid 0.81216, valid PER 24.87%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.7252326011657715\n",
      "  batch 100 loss: 0.6846485602855682\n",
      "  batch 150 loss: 0.6956729936599731\n",
      "  batch 200 loss: 0.7180292677879333\n",
      "  batch 250 loss: 0.7340075224637985\n",
      "  batch 300 loss: 0.7109565848112106\n",
      "  batch 350 loss: 0.7261928343772888\n",
      "  batch 400 loss: 0.7146313095092773\n",
      "  batch 450 loss: 0.7299492800235748\n",
      "  batch 500 loss: 0.6853267937898636\n",
      "  batch 550 loss: 0.7123604571819305\n",
      "  batch 600 loss: 0.6907437002658844\n",
      "  batch 650 loss: 0.7133105105161667\n",
      "  batch 700 loss: 0.6798069059848786\n",
      "  batch 750 loss: 0.6930339479446411\n",
      "  batch 800 loss: 0.7115756916999817\n",
      "  batch 850 loss: 0.6921260881423951\n",
      "  batch 900 loss: 0.693500034213066\n",
      "LOSS train 0.69350 valid 0.80493, valid PER 24.61%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.6691607570648194\n",
      "  batch 100 loss: 0.685900171995163\n",
      "  batch 150 loss: 0.6540521854162216\n",
      "  batch 200 loss: 0.6651002979278564\n",
      "  batch 250 loss: 0.6764394670724869\n",
      "  batch 300 loss: 0.6642631131410599\n",
      "  batch 350 loss: 0.6670382124185562\n",
      "  batch 400 loss: 0.7034111857414246\n",
      "  batch 450 loss: 0.7058015418052673\n",
      "  batch 500 loss: 0.6932531136274338\n",
      "  batch 550 loss: 0.6806223833560944\n",
      "  batch 600 loss: 0.7079898983240127\n",
      "  batch 650 loss: 0.6643412911891937\n",
      "  batch 700 loss: 0.6754077225923538\n",
      "  batch 750 loss: 0.6663826614618301\n",
      "  batch 800 loss: 0.672912773489952\n",
      "  batch 850 loss: 0.6932647955417633\n",
      "  batch 900 loss: 0.6796768587827683\n",
      "LOSS train 0.67968 valid 0.78442, valid PER 24.26%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.6517591470479965\n",
      "  batch 100 loss: 0.6587793457508088\n",
      "  batch 150 loss: 0.6987282514572144\n",
      "  batch 200 loss: 0.6598040115833282\n",
      "  batch 250 loss: 0.6595689743757248\n",
      "  batch 300 loss: 0.6580593705177307\n",
      "  batch 350 loss: 0.6741148918867111\n",
      "  batch 400 loss: 0.6289611357450485\n",
      "  batch 450 loss: 0.673596710562706\n",
      "  batch 500 loss: 0.6805779659748077\n",
      "  batch 550 loss: 0.6796781504154206\n",
      "  batch 600 loss: 0.6264113003015518\n",
      "  batch 650 loss: 0.6410349494218827\n",
      "  batch 700 loss: 0.6880228716135025\n",
      "  batch 750 loss: 0.6402725040912628\n",
      "  batch 800 loss: 0.64178346991539\n",
      "  batch 850 loss: 0.6556400591135025\n",
      "  batch 900 loss: 0.6845281034708023\n",
      "LOSS train 0.68453 valid 0.79760, valid PER 24.55%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5868169319629669\n",
      "  batch 100 loss: 0.6003839713335037\n",
      "  batch 150 loss: 0.6187722218036652\n",
      "  batch 200 loss: 0.6317346000671387\n",
      "  batch 250 loss: 0.6460939383506775\n",
      "  batch 300 loss: 0.6387024307250977\n",
      "  batch 350 loss: 0.631610963344574\n",
      "  batch 400 loss: 0.6423628628253937\n",
      "  batch 450 loss: 0.6571569281816483\n",
      "  batch 500 loss: 0.653691577911377\n",
      "  batch 550 loss: 0.6362973892688751\n",
      "  batch 600 loss: 0.6473593890666962\n",
      "  batch 650 loss: 0.7030790114402771\n",
      "  batch 700 loss: 0.6353083503246307\n",
      "  batch 750 loss: 0.6100605887174606\n",
      "  batch 800 loss: 0.667486178278923\n",
      "  batch 850 loss: 0.6590318715572357\n",
      "  batch 900 loss: 0.6848163843154907\n",
      "LOSS train 0.68482 valid 0.78833, valid PER 24.03%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.5965230226516723\n",
      "  batch 100 loss: 0.6142318367958068\n",
      "  batch 150 loss: 0.6013580065965652\n",
      "  batch 200 loss: 0.6353179031610489\n",
      "  batch 250 loss: 0.6243785250186921\n",
      "  batch 300 loss: 0.623905457854271\n",
      "  batch 350 loss: 0.6106729853153229\n",
      "  batch 400 loss: 0.6450181531906128\n",
      "  batch 450 loss: 0.6512875556945801\n",
      "  batch 500 loss: 0.6124240404367447\n",
      "  batch 550 loss: 0.6757579070329666\n",
      "  batch 600 loss: 0.6102457094192505\n",
      "  batch 650 loss: 0.651177943944931\n",
      "  batch 700 loss: 0.633495329618454\n",
      "  batch 750 loss: 0.6181286859512329\n",
      "  batch 800 loss: 0.6546012353897095\n",
      "  batch 850 loss: 0.6542414736747741\n",
      "  batch 900 loss: 0.6527075225114822\n",
      "LOSS train 0.65271 valid 0.77637, valid PER 23.58%\n",
      "Training finished in 4.0 minutes.\n",
      "Model saved to checkpoints/20231205_145251/model_20\n",
      "Loading model from checkpoints/20231205_145251/model_20\n",
      "For dropout rate 0.5 the best model has SUB: 14.79%, DEL: 8.41%, INS: 1.90%, COR: 76.80%, PER: 25.11%\n",
      "End dropout tuning For 2 Layer LSTM\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout\n",
    "from datetime import datetime\n",
    "from trainer import train\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start dropout tuning For 2 Layer LSTM\")\n",
    "\n",
    "dropout_rates=[0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    model_with_dropout = model_regularisation_dropout.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "    num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "    print('Total number of model parameters is {}'.format(num_params))\n",
    "    start = datetime.now()\n",
    "    model_with_dropout.to(args.device)\n",
    "    model_path = train(model_with_dropout, args)\n",
    "    end = datetime.now()\n",
    "    duration = (end - start).total_seconds()\n",
    "    print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "    print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print('Loading model from {}'.format(model_path))\n",
    "    model_with_dropout.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model_with_dropout.eval()\n",
    "    results = decode(model_with_dropout, args, args.test_json)\n",
    "    print(\"For dropout rate \"+str(dropout_rate)+\" the best model has \"+\"SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%\".format(*results))\n",
    "\n",
    "print(\"End dropout tuning For 2 Layer LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc9495f",
   "metadata": {},
   "source": [
    "#### Expriment for dropout in the middle of LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b07d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dropout tuning For 2 Layer LSTM, with Dropout between layer\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.159642701148987\n",
      "  batch 100 loss: 3.3973202085494996\n",
      "  batch 150 loss: 3.2978872632980347\n",
      "  batch 200 loss: 3.19533148765564\n",
      "  batch 250 loss: 3.100555667877197\n",
      "  batch 300 loss: 2.9161056089401245\n",
      "  batch 350 loss: 2.7046935987472533\n",
      "  batch 400 loss: 2.5531535291671754\n",
      "  batch 450 loss: 2.424420223236084\n",
      "  batch 500 loss: 2.2945020961761475\n",
      "  batch 550 loss: 2.2176200222969054\n",
      "  batch 600 loss: 2.1382415318489074\n",
      "  batch 650 loss: 2.0364141869544983\n",
      "  batch 700 loss: 2.018544771671295\n",
      "  batch 750 loss: 1.9257749223709106\n",
      "  batch 800 loss: 1.8891727948188781\n",
      "  batch 850 loss: 1.820354528427124\n",
      "  batch 900 loss: 1.7865184712409974\n",
      "LOSS train 1.78652 valid 1.70620, valid PER 65.12%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7227200317382811\n",
      "  batch 100 loss: 1.68451096534729\n",
      "  batch 150 loss: 1.5704996657371522\n",
      "  batch 200 loss: 1.581278338432312\n",
      "  batch 250 loss: 1.538943247795105\n",
      "  batch 300 loss: 1.4886923098564149\n",
      "  batch 350 loss: 1.4837325096130372\n",
      "  batch 400 loss: 1.4366186738014222\n",
      "  batch 450 loss: 1.415432598590851\n",
      "  batch 500 loss: 1.3868090200424195\n",
      "  batch 550 loss: 1.3670479345321656\n",
      "  batch 600 loss: 1.337639811038971\n",
      "  batch 650 loss: 1.3030134344100952\n",
      "  batch 700 loss: 1.2995911145210266\n",
      "  batch 750 loss: 1.28088320851326\n",
      "  batch 800 loss: 1.2434818792343139\n",
      "  batch 850 loss: 1.2589217734336853\n",
      "  batch 900 loss: 1.2166869115829468\n",
      "LOSS train 1.21669 valid 1.23240, valid PER 38.07%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.1628331661224365\n",
      "  batch 100 loss: 1.2340380096435546\n",
      "  batch 150 loss: 1.2233936786651611\n",
      "  batch 200 loss: 1.173705242872238\n",
      "  batch 250 loss: 1.1717883324623108\n",
      "  batch 300 loss: 1.1542453932762147\n",
      "  batch 350 loss: 1.1740548121929169\n",
      "  batch 400 loss: 1.1587669229507447\n",
      "  batch 450 loss: 1.1300101351737977\n",
      "  batch 500 loss: 1.0931492948532104\n",
      "  batch 550 loss: 1.126981703042984\n",
      "  batch 600 loss: 1.0564032351970674\n",
      "  batch 650 loss: 1.096038008928299\n",
      "  batch 700 loss: 1.1038900005817414\n",
      "  batch 750 loss: 1.1011235773563386\n",
      "  batch 800 loss: 1.1369021797180177\n",
      "  batch 850 loss: 1.087635782957077\n",
      "  batch 900 loss: 1.0920088934898375\n",
      "LOSS train 1.09201 valid 1.07618, valid PER 32.93%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.086323094367981\n",
      "  batch 100 loss: 1.00277672290802\n",
      "  batch 150 loss: 1.0414128386974335\n",
      "  batch 200 loss: 1.0248633587360383\n",
      "  batch 250 loss: 1.01929243683815\n",
      "  batch 300 loss: 1.0310580790042878\n",
      "  batch 350 loss: 1.017591564655304\n",
      "  batch 400 loss: 0.9811460745334625\n",
      "  batch 450 loss: 0.9738275194168091\n",
      "  batch 500 loss: 1.056989837884903\n",
      "  batch 550 loss: 0.972475061416626\n",
      "  batch 600 loss: 0.9680914747714996\n",
      "  batch 650 loss: 1.0327863144874572\n",
      "  batch 700 loss: 1.0312879991531372\n",
      "  batch 750 loss: 0.984511067867279\n",
      "  batch 800 loss: 0.9673999404907226\n",
      "  batch 850 loss: 0.9770508646965027\n",
      "  batch 900 loss: 0.9754684007167816\n",
      "LOSS train 0.97547 valid 1.01728, valid PER 31.25%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9314620661735534\n",
      "  batch 100 loss: 0.9210633862018586\n",
      "  batch 150 loss: 0.9330835032463074\n",
      "  batch 200 loss: 0.9615112614631652\n",
      "  batch 250 loss: 0.9169747161865235\n",
      "  batch 300 loss: 0.963139898777008\n",
      "  batch 350 loss: 0.9069539535045624\n",
      "  batch 400 loss: 0.9039066684246063\n",
      "  batch 450 loss: 0.9178532409667969\n",
      "  batch 500 loss: 0.8886072266101838\n",
      "  batch 550 loss: 0.9517692291736602\n",
      "  batch 600 loss: 0.9314257502555847\n",
      "  batch 650 loss: 0.9389830684661865\n",
      "  batch 700 loss: 0.9293720960617066\n",
      "  batch 750 loss: 0.9048138391971589\n",
      "  batch 800 loss: 0.9376934587955474\n",
      "  batch 850 loss: 0.9251390492916107\n",
      "  batch 900 loss: 0.9070643401145935\n",
      "LOSS train 0.90706 valid 0.94534, valid PER 29.54%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8792961394786835\n",
      "  batch 100 loss: 0.8654650902748108\n",
      "  batch 150 loss: 0.8733768296241761\n",
      "  batch 200 loss: 0.8533356285095215\n",
      "  batch 250 loss: 0.8562548673152923\n",
      "  batch 300 loss: 0.8722034299373627\n",
      "  batch 350 loss: 0.8928811407089233\n",
      "  batch 400 loss: 0.866778234243393\n",
      "  batch 450 loss: 0.8660292804241181\n",
      "  batch 500 loss: 0.8269354796409607\n",
      "  batch 550 loss: 0.8590929019451141\n",
      "  batch 600 loss: 0.8669062519073486\n",
      "  batch 650 loss: 0.8400437098741531\n",
      "  batch 700 loss: 0.8476465970277787\n",
      "  batch 750 loss: 0.8693781459331512\n",
      "  batch 800 loss: 0.8513750290870666\n",
      "  batch 850 loss: 0.876332916021347\n",
      "  batch 900 loss: 0.8907498919963837\n",
      "LOSS train 0.89075 valid 0.90629, valid PER 28.64%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.7903169667720795\n",
      "  batch 100 loss: 0.8385834896564484\n",
      "  batch 150 loss: 0.7858396542072296\n",
      "  batch 200 loss: 0.794074865579605\n",
      "  batch 250 loss: 0.8421109104156494\n",
      "  batch 300 loss: 0.8196541476249695\n",
      "  batch 350 loss: 0.8457872498035431\n",
      "  batch 400 loss: 0.7912480342388153\n",
      "  batch 450 loss: 0.8105024433135987\n",
      "  batch 500 loss: 0.8105929410457611\n",
      "  batch 550 loss: 0.7988225364685059\n",
      "  batch 600 loss: 0.8017344498634338\n",
      "  batch 650 loss: 0.7921744799613952\n",
      "  batch 700 loss: 0.8243578541278839\n",
      "  batch 750 loss: 0.806529108285904\n",
      "  batch 800 loss: 0.7915401148796082\n",
      "  batch 850 loss: 0.7874968016147613\n",
      "  batch 900 loss: 0.7946377825737\n",
      "LOSS train 0.79464 valid 0.88601, valid PER 27.49%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.7797634840011597\n",
      "  batch 100 loss: 0.7449829626083374\n",
      "  batch 150 loss: 0.7566346961259842\n",
      "  batch 200 loss: 0.7558545172214508\n",
      "  batch 250 loss: 0.7457736814022065\n",
      "  batch 300 loss: 0.727281322479248\n",
      "  batch 350 loss: 0.7709815657138824\n",
      "  batch 400 loss: 0.7567911875247956\n",
      "  batch 450 loss: 0.7909895098209381\n",
      "  batch 500 loss: 0.7761501681804657\n",
      "  batch 550 loss: 0.7928045076131821\n",
      "  batch 600 loss: 0.7411480081081391\n",
      "  batch 650 loss: 0.7545747804641724\n",
      "  batch 700 loss: 0.7843175613880158\n",
      "  batch 750 loss: 0.7759147989749908\n",
      "  batch 800 loss: 0.7876545405387878\n",
      "  batch 850 loss: 0.7516384518146515\n",
      "  batch 900 loss: 0.7583925807476044\n",
      "LOSS train 0.75839 valid 0.86076, valid PER 26.53%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.705350650548935\n",
      "  batch 100 loss: 0.7009275710582733\n",
      "  batch 150 loss: 0.7165517830848693\n",
      "  batch 200 loss: 0.7055507957935333\n",
      "  batch 250 loss: 0.6922159719467164\n",
      "  batch 300 loss: 0.7354720067977906\n",
      "  batch 350 loss: 0.7032533186674118\n",
      "  batch 400 loss: 0.7329284864664077\n",
      "  batch 450 loss: 0.7440150785446167\n",
      "  batch 500 loss: 0.727063050866127\n",
      "  batch 550 loss: 0.7419970893859863\n",
      "  batch 600 loss: 0.7468181467056274\n",
      "  batch 650 loss: 0.7484338462352753\n",
      "  batch 700 loss: 0.7179225361347199\n",
      "  batch 750 loss: 0.735645581483841\n",
      "  batch 800 loss: 0.7647330498695374\n",
      "  batch 850 loss: 0.7520129024982453\n",
      "  batch 900 loss: 0.7063206911087037\n",
      "LOSS train 0.70632 valid 0.84547, valid PER 25.67%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6648318833112716\n",
      "  batch 100 loss: 0.6691709864139557\n",
      "  batch 150 loss: 0.6958239275217056\n",
      "  batch 200 loss: 0.6702766162157059\n",
      "  batch 250 loss: 0.7011620569229126\n",
      "  batch 300 loss: 0.7052694624662399\n",
      "  batch 350 loss: 0.6765524989366531\n",
      "  batch 400 loss: 0.6665213865041733\n",
      "  batch 450 loss: 0.6974486678838729\n",
      "  batch 500 loss: 0.6855248028039932\n",
      "  batch 550 loss: 0.689635226726532\n",
      "  batch 600 loss: 0.6914289677143097\n",
      "  batch 650 loss: 0.7104294919967651\n",
      "  batch 700 loss: 0.693996707201004\n",
      "  batch 750 loss: 0.724170446395874\n",
      "  batch 800 loss: 0.721698015332222\n",
      "  batch 850 loss: 0.6814693903923035\n",
      "  batch 900 loss: 0.703471393585205\n",
      "LOSS train 0.70347 valid 0.86800, valid PER 26.61%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6519059485197067\n",
      "  batch 100 loss: 0.6347610104084015\n",
      "  batch 150 loss: 0.633042351603508\n",
      "  batch 200 loss: 0.6286893039941788\n",
      "  batch 250 loss: 0.6612909799814224\n",
      "  batch 300 loss: 0.6310361009836197\n",
      "  batch 350 loss: 0.6722220599651336\n",
      "  batch 400 loss: 0.6323440617322922\n",
      "  batch 450 loss: 0.6539914000034333\n",
      "  batch 500 loss: 0.645524582862854\n",
      "  batch 550 loss: 0.6713502424955368\n",
      "  batch 600 loss: 0.6582103216648102\n",
      "  batch 650 loss: 0.6653394502401352\n",
      "  batch 700 loss: 0.7032509648799896\n",
      "  batch 750 loss: 0.6645630335807801\n",
      "  batch 800 loss: 0.6898973572254181\n",
      "  batch 850 loss: 0.6718141984939575\n",
      "  batch 900 loss: 0.6799320638179779\n",
      "LOSS train 0.67993 valid 0.81975, valid PER 25.40%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.5929230064153671\n",
      "  batch 100 loss: 0.5845521467924119\n",
      "  batch 150 loss: 0.6109878379106521\n",
      "  batch 200 loss: 0.6138136410713195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 250 loss: 0.6078583329916001\n",
      "  batch 300 loss: 0.6378869652748108\n",
      "  batch 350 loss: 0.6335612207651138\n",
      "  batch 400 loss: 0.6514451956748962\n",
      "  batch 450 loss: 0.6207901287078857\n",
      "  batch 500 loss: 0.6351327425241471\n",
      "  batch 550 loss: 0.628686649799347\n",
      "  batch 600 loss: 0.6425564384460449\n",
      "  batch 650 loss: 0.6398937177658081\n",
      "  batch 700 loss: 0.629478639960289\n",
      "  batch 750 loss: 0.6594138967990876\n",
      "  batch 800 loss: 0.6157864356040954\n",
      "  batch 850 loss: 0.6348924052715301\n",
      "  batch 900 loss: 0.6571180975437164\n",
      "LOSS train 0.65712 valid 0.83148, valid PER 24.79%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.5728895884752273\n",
      "  batch 100 loss: 0.5750460773706436\n",
      "  batch 150 loss: 0.6032135576009751\n",
      "  batch 200 loss: 0.5611831277608872\n",
      "  batch 250 loss: 0.5755988365411758\n",
      "  batch 300 loss: 0.6252626669406891\n",
      "  batch 350 loss: 0.5710573083162308\n",
      "  batch 400 loss: 0.5742033255100251\n",
      "  batch 450 loss: 0.6195205962657928\n",
      "  batch 500 loss: 0.5960983198881149\n",
      "  batch 550 loss: 0.6438603734970093\n",
      "  batch 600 loss: 0.621043193936348\n",
      "  batch 650 loss: 0.6110795229673386\n",
      "  batch 700 loss: 0.636656419634819\n",
      "  batch 750 loss: 0.5854236698150634\n",
      "  batch 800 loss: 0.6075633609294891\n",
      "  batch 850 loss: 0.5866264206171036\n",
      "  batch 900 loss: 0.6107972514629364\n",
      "LOSS train 0.61080 valid 0.82255, valid PER 24.21%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.555897199511528\n",
      "  batch 100 loss: 0.5253185921907425\n",
      "  batch 150 loss: 0.5529234856367111\n",
      "  batch 200 loss: 0.5561989748477936\n",
      "  batch 250 loss: 0.5554713588953019\n",
      "  batch 300 loss: 0.5515460121631622\n",
      "  batch 350 loss: 0.5584899085760117\n",
      "  batch 400 loss: 0.5875093042850494\n",
      "  batch 450 loss: 0.5578258323669434\n",
      "  batch 500 loss: 0.5789626276493073\n",
      "  batch 550 loss: 0.5862556433677674\n",
      "  batch 600 loss: 0.5754842704534531\n",
      "  batch 650 loss: 0.5880074793100357\n",
      "  batch 700 loss: 0.5994041633605957\n",
      "  batch 750 loss: 0.5869555330276489\n",
      "  batch 800 loss: 0.5618209326267243\n",
      "  batch 850 loss: 0.6061854153871536\n",
      "  batch 900 loss: 0.6192923855781555\n",
      "LOSS train 0.61929 valid 0.81265, valid PER 24.02%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.5167451721429824\n",
      "  batch 100 loss: 0.5280715596675872\n",
      "  batch 150 loss: 0.5443960350751876\n",
      "  batch 200 loss: 0.5583633828163147\n",
      "  batch 250 loss: 0.5589292693138123\n",
      "  batch 300 loss: 0.5392155885696411\n",
      "  batch 350 loss: 0.5440649992227554\n",
      "  batch 400 loss: 0.5585325330495834\n",
      "  batch 450 loss: 0.5335922878980637\n",
      "  batch 500 loss: 0.5398189008235932\n",
      "  batch 550 loss: 0.5825498503446579\n",
      "  batch 600 loss: 0.599016608595848\n",
      "  batch 650 loss: 0.5595423740148544\n",
      "  batch 700 loss: 0.5414977431297302\n",
      "  batch 750 loss: 0.5519809758663178\n",
      "  batch 800 loss: 0.5427389413118362\n",
      "  batch 850 loss: 0.549021383523941\n",
      "  batch 900 loss: 0.5395965939760208\n",
      "LOSS train 0.53960 valid 0.81818, valid PER 24.22%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.505952681005001\n",
      "  batch 100 loss: 0.5013288009166718\n",
      "  batch 150 loss: 0.516825025677681\n",
      "  batch 200 loss: 0.5355726486444473\n",
      "  batch 250 loss: 0.5140411728620529\n",
      "  batch 300 loss: 0.5392925912141799\n",
      "  batch 350 loss: 0.5448289155960083\n",
      "  batch 400 loss: 0.5298022150993347\n",
      "  batch 450 loss: 0.5405837070941925\n",
      "  batch 500 loss: 0.522885167002678\n",
      "  batch 550 loss: 0.5160300260782242\n",
      "  batch 600 loss: 0.5546047931909561\n",
      "  batch 650 loss: 0.5485718494653702\n",
      "  batch 700 loss: 0.5012793475389481\n",
      "  batch 750 loss: 0.5395265549421311\n",
      "  batch 800 loss: 0.5381659591197967\n",
      "  batch 850 loss: 0.529825354218483\n",
      "  batch 900 loss: 0.5395543140172958\n",
      "LOSS train 0.53955 valid 0.81300, valid PER 24.27%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.4988683921098709\n",
      "  batch 100 loss: 0.4603997188806534\n",
      "  batch 150 loss: 0.5037048488855362\n",
      "  batch 200 loss: 0.45790363729000094\n",
      "  batch 250 loss: 0.49487006485462187\n",
      "  batch 300 loss: 0.4889654314517975\n",
      "  batch 350 loss: 0.5237573674321174\n",
      "  batch 400 loss: 0.5072718435525894\n",
      "  batch 450 loss: 0.4930189502239227\n",
      "  batch 500 loss: 0.50085619866848\n",
      "  batch 550 loss: 0.500244762301445\n",
      "  batch 600 loss: 0.5345819061994552\n",
      "  batch 650 loss: 0.5142568272352218\n",
      "  batch 700 loss: 0.5348916399478912\n",
      "  batch 750 loss: 0.504735850095749\n",
      "  batch 800 loss: 0.5360396188497544\n",
      "  batch 850 loss: 0.5222528564929962\n",
      "  batch 900 loss: 0.5223600792884827\n",
      "LOSS train 0.52236 valid 0.81010, valid PER 23.38%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.4547224277257919\n",
      "  batch 100 loss: 0.4498075741529465\n",
      "  batch 150 loss: 0.49437770843505857\n",
      "  batch 200 loss: 0.45392490476369857\n",
      "  batch 250 loss: 0.4529951286315918\n",
      "  batch 300 loss: 0.4658211129903793\n",
      "  batch 350 loss: 0.4457897111773491\n",
      "  batch 400 loss: 0.4774764293432236\n",
      "  batch 450 loss: 0.4880864614248276\n",
      "  batch 500 loss: 0.4726989871263504\n",
      "  batch 550 loss: 0.5237980562448502\n",
      "  batch 600 loss: 0.49451504945755004\n",
      "  batch 650 loss: 0.49268243432044984\n",
      "  batch 700 loss: 0.4919773840904236\n",
      "  batch 750 loss: 0.5071728307008744\n",
      "  batch 800 loss: 0.5050354033708573\n",
      "  batch 850 loss: 0.5102975302934647\n",
      "  batch 900 loss: 0.5153186708688736\n",
      "LOSS train 0.51532 valid 0.81119, valid PER 23.38%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.43506417632102967\n",
      "  batch 100 loss: 0.44765338718891146\n",
      "  batch 150 loss: 0.4547669145464897\n",
      "  batch 200 loss: 0.45569572687149046\n",
      "  batch 250 loss: 0.4838739141821861\n",
      "  batch 300 loss: 0.5014682459831238\n",
      "  batch 350 loss: 0.47096208095550535\n",
      "  batch 400 loss: 0.468756365776062\n",
      "  batch 450 loss: 0.43095634460449217\n",
      "  batch 500 loss: 0.45665665686130524\n",
      "  batch 550 loss: 0.5037997072935104\n",
      "  batch 600 loss: 0.4741717103123665\n",
      "  batch 650 loss: 0.49215322017669677\n",
      "  batch 700 loss: 0.491900343298912\n",
      "  batch 750 loss: 0.46592657327651976\n",
      "  batch 800 loss: 0.46218821704387664\n",
      "  batch 850 loss: 0.49135522842407225\n",
      "  batch 900 loss: 0.4735277953743935\n",
      "LOSS train 0.47353 valid 0.83935, valid PER 24.06%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.40050454646348954\n",
      "  batch 100 loss: 0.42942787349224093\n",
      "  batch 150 loss: 0.43143949687480926\n",
      "  batch 200 loss: 0.45085310518741606\n",
      "  batch 250 loss: 0.46690000176429747\n",
      "  batch 300 loss: 0.4608911621570587\n",
      "  batch 350 loss: 0.43175828039646147\n",
      "  batch 400 loss: 0.43292824387550355\n",
      "  batch 450 loss: 0.43850381910800934\n",
      "  batch 500 loss: 0.454584658741951\n",
      "  batch 550 loss: 0.45034005522727966\n",
      "  batch 600 loss: 0.45016734540462494\n",
      "  batch 650 loss: 0.46030485570430757\n",
      "  batch 700 loss: 0.4480728054046631\n",
      "  batch 750 loss: 0.4351368802785873\n",
      "  batch 800 loss: 0.4740451455116272\n",
      "  batch 850 loss: 0.45293577015399933\n",
      "  batch 900 loss: 0.44594178438186644\n",
      "LOSS train 0.44594 valid 0.82787, valid PER 23.85%\n",
      "Training finished in 6.0 minutes.\n",
      "Model saved to checkpoints/20231205_152625/model_17\n",
      "Loading model from checkpoints/20231205_152625/model_17\n",
      "For dropout rate 0.1 the best model has SUB: 15.63%, DEL: 6.90%, INS: 2.98%, COR: 77.48%, PER: 25.50%\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.136368312835693\n",
      "  batch 100 loss: 3.395016942024231\n",
      "  batch 150 loss: 3.3025497674942015\n",
      "  batch 200 loss: 3.192100567817688\n",
      "  batch 250 loss: 3.095251774787903\n",
      "  batch 300 loss: 2.8803709888458253\n",
      "  batch 350 loss: 2.7114908695220947\n",
      "  batch 400 loss: 2.5815242528915405\n",
      "  batch 450 loss: 2.4803379821777343\n",
      "  batch 500 loss: 2.356044263839722\n",
      "  batch 550 loss: 2.265074336528778\n",
      "  batch 600 loss: 2.1949778151512147\n",
      "  batch 650 loss: 2.102375910282135\n",
      "  batch 700 loss: 2.0970000267028808\n",
      "  batch 750 loss: 1.9985535430908203\n",
      "  batch 800 loss: 1.9585983157157898\n",
      "  batch 850 loss: 1.896953113079071\n",
      "  batch 900 loss: 1.8494921493530274\n",
      "LOSS train 1.84949 valid 1.75967, valid PER 67.49%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7713443040847778\n",
      "  batch 100 loss: 1.6957419228553772\n",
      "  batch 150 loss: 1.6444205737113953\n",
      "  batch 200 loss: 1.6602046656608582\n",
      "  batch 250 loss: 1.6435453772544861\n",
      "  batch 300 loss: 1.5796461486816407\n",
      "  batch 350 loss: 1.496019880771637\n",
      "  batch 400 loss: 1.5010987687110902\n",
      "  batch 450 loss: 1.436714563369751\n",
      "  batch 500 loss: 1.4533274626731874\n",
      "  batch 550 loss: 1.440134460926056\n",
      "  batch 600 loss: 1.376433756351471\n",
      "  batch 650 loss: 1.3924870991706848\n",
      "  batch 700 loss: 1.3485349917411804\n",
      "  batch 750 loss: 1.3309008491039276\n",
      "  batch 800 loss: 1.2720902347564698\n",
      "  batch 850 loss: 1.2772367238998412\n",
      "  batch 900 loss: 1.2819615340232848\n",
      "LOSS train 1.28196 valid 1.21685, valid PER 38.02%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.236635192632675\n",
      "  batch 100 loss: 1.2154842817783356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 150 loss: 1.2096086835861206\n",
      "  batch 200 loss: 1.1884620404243469\n",
      "  batch 250 loss: 1.1690257263183594\n",
      "  batch 300 loss: 1.1723815131187438\n",
      "  batch 350 loss: 1.205858461856842\n",
      "  batch 400 loss: 1.1960083281993865\n",
      "  batch 450 loss: 1.1695835435390471\n",
      "  batch 500 loss: 1.1414825558662414\n",
      "  batch 550 loss: 1.1609120881557464\n",
      "  batch 600 loss: 1.134669336080551\n",
      "  batch 650 loss: 1.1070166504383088\n",
      "  batch 700 loss: 1.1371642458438873\n",
      "  batch 750 loss: 1.1779406464099884\n",
      "  batch 800 loss: 1.1055290389060974\n",
      "  batch 850 loss: 1.1335516571998596\n",
      "  batch 900 loss: 1.0688273131847381\n",
      "LOSS train 1.06883 valid 1.11555, valid PER 34.22%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0532942175865174\n",
      "  batch 100 loss: 1.0683598721027374\n",
      "  batch 150 loss: 1.034887113571167\n",
      "  batch 200 loss: 1.0705954933166504\n",
      "  batch 250 loss: 1.074849317073822\n",
      "  batch 300 loss: 1.0754039251804353\n",
      "  batch 350 loss: 1.005651081800461\n",
      "  batch 400 loss: 1.047360190153122\n",
      "  batch 450 loss: 1.0238527595996856\n",
      "  batch 500 loss: 1.016045470237732\n",
      "  batch 550 loss: 1.0318252801895142\n",
      "  batch 600 loss: 1.0496195805072785\n",
      "  batch 650 loss: 1.0275782358646393\n",
      "  batch 700 loss: 1.004061816930771\n",
      "  batch 750 loss: 0.9960217452049256\n",
      "  batch 800 loss: 0.9561720538139343\n",
      "  batch 850 loss: 0.9936577880382538\n",
      "  batch 900 loss: 1.0384253084659576\n",
      "LOSS train 1.03843 valid 0.99468, valid PER 30.92%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9528475189208985\n",
      "  batch 100 loss: 0.9469940006732941\n",
      "  batch 150 loss: 1.0061103737354278\n",
      "  batch 200 loss: 0.9306189930438995\n",
      "  batch 250 loss: 0.9449558341503144\n",
      "  batch 300 loss: 0.9592543995380401\n",
      "  batch 350 loss: 0.9441375470161438\n",
      "  batch 400 loss: 0.9614203226566315\n",
      "  batch 450 loss: 0.968839795589447\n",
      "  batch 500 loss: 0.9809225749969482\n",
      "  batch 550 loss: 0.9035398435592651\n",
      "  batch 600 loss: 0.9767899465560913\n",
      "  batch 650 loss: 0.9356097960472107\n",
      "  batch 700 loss: 0.9800960743427276\n",
      "  batch 750 loss: 0.9023167335987091\n",
      "  batch 800 loss: 0.9415611755847931\n",
      "  batch 850 loss: 0.9304787373542786\n",
      "  batch 900 loss: 0.9376847040653229\n",
      "LOSS train 0.93768 valid 0.94881, valid PER 29.42%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9225569903850556\n",
      "  batch 100 loss: 0.8672222757339477\n",
      "  batch 150 loss: 0.8641535544395447\n",
      "  batch 200 loss: 0.8799764502048493\n",
      "  batch 250 loss: 0.9117692935466767\n",
      "  batch 300 loss: 0.8940183877944946\n",
      "  batch 350 loss: 0.8815120649337769\n",
      "  batch 400 loss: 0.8782196617126465\n",
      "  batch 450 loss: 0.8952736639976502\n",
      "  batch 500 loss: 0.8928117644786835\n",
      "  batch 550 loss: 0.9091588461399078\n",
      "  batch 600 loss: 0.8838815212249755\n",
      "  batch 650 loss: 0.8851446759700775\n",
      "  batch 700 loss: 0.8832362639904022\n",
      "  batch 750 loss: 0.8549825048446655\n",
      "  batch 800 loss: 0.87155357837677\n",
      "  batch 850 loss: 0.8492266714572907\n",
      "  batch 900 loss: 0.8726472926139831\n",
      "LOSS train 0.87265 valid 0.93837, valid PER 29.38%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8511110401153564\n",
      "  batch 100 loss: 0.8592632639408112\n",
      "  batch 150 loss: 0.8163146543502807\n",
      "  batch 200 loss: 0.8312977278232574\n",
      "  batch 250 loss: 0.8240038168430328\n",
      "  batch 300 loss: 0.8052141165733337\n",
      "  batch 350 loss: 0.8276610410213471\n",
      "  batch 400 loss: 0.815442715883255\n",
      "  batch 450 loss: 0.824385085105896\n",
      "  batch 500 loss: 0.8377464234828949\n",
      "  batch 550 loss: 0.8230136358737945\n",
      "  batch 600 loss: 0.8281415176391601\n",
      "  batch 650 loss: 0.8290109431743622\n",
      "  batch 700 loss: 0.8480747222900391\n",
      "  batch 750 loss: 0.8268065404891968\n",
      "  batch 800 loss: 0.8207021903991699\n",
      "  batch 850 loss: 0.8455067586898803\n",
      "  batch 900 loss: 0.8684636247158051\n",
      "LOSS train 0.86846 valid 0.89370, valid PER 28.56%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.796733695268631\n",
      "  batch 100 loss: 0.7820167195796967\n",
      "  batch 150 loss: 0.8423606956005096\n",
      "  batch 200 loss: 0.7686403954029083\n",
      "  batch 250 loss: 0.7968198096752167\n",
      "  batch 300 loss: 0.743010401725769\n",
      "  batch 350 loss: 0.8195965611934661\n",
      "  batch 400 loss: 0.7807048678398132\n",
      "  batch 450 loss: 0.7976597380638123\n",
      "  batch 500 loss: 0.8157167518138886\n",
      "  batch 550 loss: 0.7680853188037873\n",
      "  batch 600 loss: 0.7957149249315262\n",
      "  batch 650 loss: 0.8177614521980285\n",
      "  batch 700 loss: 0.7670596218109131\n",
      "  batch 750 loss: 0.778926362991333\n",
      "  batch 800 loss: 0.7885559523105621\n",
      "  batch 850 loss: 0.7699675965309143\n",
      "  batch 900 loss: 0.7981985902786255\n",
      "LOSS train 0.79820 valid 0.85791, valid PER 26.62%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7216088658571244\n",
      "  batch 100 loss: 0.7503460240364075\n",
      "  batch 150 loss: 0.7584906613826752\n",
      "  batch 200 loss: 0.7238933348655701\n",
      "  batch 250 loss: 0.766691598892212\n",
      "  batch 300 loss: 0.7701744294166565\n",
      "  batch 350 loss: 0.772331360578537\n",
      "  batch 400 loss: 0.7460820233821869\n",
      "  batch 450 loss: 0.7596363806724549\n",
      "  batch 500 loss: 0.7437453418970108\n",
      "  batch 550 loss: 0.7586064660549163\n",
      "  batch 600 loss: 0.7767959111928939\n",
      "  batch 650 loss: 0.7385113894939422\n",
      "  batch 700 loss: 0.7246593976020813\n",
      "  batch 750 loss: 0.7499307000637054\n",
      "  batch 800 loss: 0.7555063891410828\n",
      "  batch 850 loss: 0.7948390376567841\n",
      "  batch 900 loss: 0.7215624392032624\n",
      "LOSS train 0.72156 valid 0.85701, valid PER 26.35%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6932578945159912\n",
      "  batch 100 loss: 0.7049002319574356\n",
      "  batch 150 loss: 0.7074890762567521\n",
      "  batch 200 loss: 0.719959157705307\n",
      "  batch 250 loss: 0.7285369378328324\n",
      "  batch 300 loss: 0.7091253370046615\n",
      "  batch 350 loss: 0.73007703602314\n",
      "  batch 400 loss: 0.678302344083786\n",
      "  batch 450 loss: 0.7074439501762391\n",
      "  batch 500 loss: 0.7383940285444259\n",
      "  batch 550 loss: 0.7446198028326034\n",
      "  batch 600 loss: 0.7193011397123337\n",
      "  batch 650 loss: 0.7133105254173279\n",
      "  batch 700 loss: 0.7343098205327988\n",
      "  batch 750 loss: 0.7109585154056549\n",
      "  batch 800 loss: 0.7137079203128814\n",
      "  batch 850 loss: 0.726628110408783\n",
      "  batch 900 loss: 0.7317483627796173\n",
      "LOSS train 0.73175 valid 0.82667, valid PER 26.02%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6431329762935638\n",
      "  batch 100 loss: 0.6330338650941849\n",
      "  batch 150 loss: 0.6514737206697464\n",
      "  batch 200 loss: 0.6977725493907928\n",
      "  batch 250 loss: 0.6939422976970673\n",
      "  batch 300 loss: 0.6733001327514648\n",
      "  batch 350 loss: 0.6883181411027909\n",
      "  batch 400 loss: 0.7080946838855744\n",
      "  batch 450 loss: 0.7195720076560974\n",
      "  batch 500 loss: 0.6765583938360215\n",
      "  batch 550 loss: 0.68726577937603\n",
      "  batch 600 loss: 0.6807594364881515\n",
      "  batch 650 loss: 0.735170795917511\n",
      "  batch 700 loss: 0.6641202807426453\n",
      "  batch 750 loss: 0.6771718138456344\n",
      "  batch 800 loss: 0.7021338665485382\n",
      "  batch 850 loss: 0.7151991617679596\n",
      "  batch 900 loss: 0.7137373954057693\n",
      "LOSS train 0.71374 valid 0.81681, valid PER 24.98%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.6499478548765183\n",
      "  batch 100 loss: 0.6469504451751709\n",
      "  batch 150 loss: 0.6188248610496521\n",
      "  batch 200 loss: 0.6437318205833436\n",
      "  batch 250 loss: 0.6634072315692902\n",
      "  batch 300 loss: 0.6453940594196319\n",
      "  batch 350 loss: 0.6405581623315811\n",
      "  batch 400 loss: 0.6839213532209396\n",
      "  batch 450 loss: 0.6677424156665802\n",
      "  batch 500 loss: 0.6707061076164246\n",
      "  batch 550 loss: 0.6251757884025574\n",
      "  batch 600 loss: 0.6548579317331314\n",
      "  batch 650 loss: 0.6982968473434448\n",
      "  batch 700 loss: 0.676376114487648\n",
      "  batch 750 loss: 0.649100416302681\n",
      "  batch 800 loss: 0.644173401594162\n",
      "  batch 850 loss: 0.708894784450531\n",
      "  batch 900 loss: 0.6982546639442444\n",
      "LOSS train 0.69825 valid 0.80855, valid PER 25.25%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.6028823763132095\n",
      "  batch 100 loss: 0.6239737689495086\n",
      "  batch 150 loss: 0.5924018979072571\n",
      "  batch 200 loss: 0.6581562912464142\n",
      "  batch 250 loss: 0.6366490465402603\n",
      "  batch 300 loss: 0.6145894134044647\n",
      "  batch 350 loss: 0.6300828492641449\n",
      "  batch 400 loss: 0.6285982370376587\n",
      "  batch 450 loss: 0.6421057283878326\n",
      "  batch 500 loss: 0.6156362092494965\n",
      "  batch 550 loss: 0.6539463424682617\n",
      "  batch 600 loss: 0.6364427989721299\n",
      "  batch 650 loss: 0.6578437739610672\n",
      "  batch 700 loss: 0.6349366450309754\n",
      "  batch 750 loss: 0.6069610929489135\n",
      "  batch 800 loss: 0.6352908748388291\n",
      "  batch 850 loss: 0.6841891276836395\n",
      "  batch 900 loss: 0.6491869419813157\n",
      "LOSS train 0.64919 valid 0.81925, valid PER 25.09%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.576584244966507\n",
      "  batch 100 loss: 0.6076319169998169\n",
      "  batch 150 loss: 0.5917672550678253\n",
      "  batch 200 loss: 0.5998155611753464\n",
      "  batch 250 loss: 0.5929349273443222\n",
      "  batch 300 loss: 0.6240102869272232\n",
      "  batch 350 loss: 0.5862137603759766\n",
      "  batch 400 loss: 0.5874668699502945\n",
      "  batch 450 loss: 0.5915330952405929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 500 loss: 0.620779339671135\n",
      "  batch 550 loss: 0.6399252843856812\n",
      "  batch 600 loss: 0.5977893185615539\n",
      "  batch 650 loss: 0.6163865929841995\n",
      "  batch 700 loss: 0.6399544936418533\n",
      "  batch 750 loss: 0.6093517124652863\n",
      "  batch 800 loss: 0.5792429566383361\n",
      "  batch 850 loss: 0.6445688301324844\n",
      "  batch 900 loss: 0.6407246857881546\n",
      "LOSS train 0.64072 valid 0.79840, valid PER 24.60%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.5700673931837081\n",
      "  batch 100 loss: 0.5674907672405243\n",
      "  batch 150 loss: 0.5778180158138275\n",
      "  batch 200 loss: 0.5873262399435043\n",
      "  batch 250 loss: 0.5937344270944596\n",
      "  batch 300 loss: 0.5569453924894333\n",
      "  batch 350 loss: 0.5643184167146683\n",
      "  batch 400 loss: 0.5726757872104645\n",
      "  batch 450 loss: 0.5894898724555969\n",
      "  batch 500 loss: 0.5615575021505356\n",
      "  batch 550 loss: 0.5855351138114929\n",
      "  batch 600 loss: 0.5973487722873688\n",
      "  batch 650 loss: 0.6143311256170273\n",
      "  batch 700 loss: 0.6140314596891403\n",
      "  batch 750 loss: 0.5958060890436172\n",
      "  batch 800 loss: 0.5788542813062668\n",
      "  batch 850 loss: 0.5670280349254608\n",
      "  batch 900 loss: 0.5904940396547318\n",
      "LOSS train 0.59049 valid 0.80478, valid PER 24.35%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.5682483065128326\n",
      "  batch 100 loss: 0.554131578207016\n",
      "  batch 150 loss: 0.5401846033334732\n",
      "  batch 200 loss: 0.5487458389997483\n",
      "  batch 250 loss: 0.5639901810884476\n",
      "  batch 300 loss: 0.5558231210708618\n",
      "  batch 350 loss: 0.567821369767189\n",
      "  batch 400 loss: 0.5841222858428955\n",
      "  batch 450 loss: 0.5789686745405197\n",
      "  batch 500 loss: 0.554159848690033\n",
      "  batch 550 loss: 0.5779264086484909\n",
      "  batch 600 loss: 0.5611397230625152\n",
      "  batch 650 loss: 0.5844335794448853\n",
      "  batch 700 loss: 0.5673388648033142\n",
      "  batch 750 loss: 0.5954238319396973\n",
      "  batch 800 loss: 0.5766111695766449\n",
      "  batch 850 loss: 0.5673064082860947\n",
      "  batch 900 loss: 0.5589201670885086\n",
      "LOSS train 0.55892 valid 0.79803, valid PER 23.92%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.5380074018239975\n",
      "  batch 100 loss: 0.5386176139116288\n",
      "  batch 150 loss: 0.5162756645679474\n",
      "  batch 200 loss: 0.5258856338262557\n",
      "  batch 250 loss: 0.5593622118234635\n",
      "  batch 300 loss: 0.54644864320755\n",
      "  batch 350 loss: 0.537201835513115\n",
      "  batch 400 loss: 0.5861547100543976\n",
      "  batch 450 loss: 0.5659806430339813\n",
      "  batch 500 loss: 0.5595704567432404\n",
      "  batch 550 loss: 0.5408271759748459\n",
      "  batch 600 loss: 0.573810093998909\n",
      "  batch 650 loss: 0.5523709326982498\n",
      "  batch 700 loss: 0.5399066507816315\n",
      "  batch 750 loss: 0.5303683638572693\n",
      "  batch 800 loss: 0.5303397232294083\n",
      "  batch 850 loss: 0.5618772447109223\n",
      "  batch 900 loss: 0.5401136088371277\n",
      "LOSS train 0.54011 valid 0.78450, valid PER 23.82%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.5066241890192031\n",
      "  batch 100 loss: 0.5110408091545104\n",
      "  batch 150 loss: 0.5154940402507782\n",
      "  batch 200 loss: 0.5191783925890923\n",
      "  batch 250 loss: 0.5310158932209015\n",
      "  batch 300 loss: 0.5126080018281937\n",
      "  batch 350 loss: 0.5194873452186585\n",
      "  batch 400 loss: 0.5060897922515869\n",
      "  batch 450 loss: 0.551501014828682\n",
      "  batch 500 loss: 0.528414876461029\n",
      "  batch 550 loss: 0.5531495386362075\n",
      "  batch 600 loss: 0.5138595670461654\n",
      "  batch 650 loss: 0.527017611861229\n",
      "  batch 700 loss: 0.5548366987705231\n",
      "  batch 750 loss: 0.5160122513771057\n",
      "  batch 800 loss: 0.5090334570407867\n",
      "  batch 850 loss: 0.5307897657155991\n",
      "  batch 900 loss: 0.531583508849144\n",
      "LOSS train 0.53158 valid 0.79310, valid PER 23.45%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.4609171676635742\n",
      "  batch 100 loss: 0.48869008958339694\n",
      "  batch 150 loss: 0.48804019927978515\n",
      "  batch 200 loss: 0.502097532749176\n",
      "  batch 250 loss: 0.4961057060956955\n",
      "  batch 300 loss: 0.4926078361272812\n",
      "  batch 350 loss: 0.5076777803897857\n",
      "  batch 400 loss: 0.4981688266992569\n",
      "  batch 450 loss: 0.5120885196328163\n",
      "  batch 500 loss: 0.5173428863286972\n",
      "  batch 550 loss: 0.49994928181171416\n",
      "  batch 600 loss: 0.4867333823442459\n",
      "  batch 650 loss: 0.5467666918039322\n",
      "  batch 700 loss: 0.49336290478706357\n",
      "  batch 750 loss: 0.48490684330463407\n",
      "  batch 800 loss: 0.5278424859046936\n",
      "  batch 850 loss: 0.5252763354778289\n",
      "  batch 900 loss: 0.5256276267766953\n",
      "LOSS train 0.52563 valid 0.81560, valid PER 23.66%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.471136234998703\n",
      "  batch 100 loss: 0.46142259895801546\n",
      "  batch 150 loss: 0.4643589386343956\n",
      "  batch 200 loss: 0.48818734765052796\n",
      "  batch 250 loss: 0.4813917702436447\n",
      "  batch 300 loss: 0.48915604591369627\n",
      "  batch 350 loss: 0.459069187939167\n",
      "  batch 400 loss: 0.477220401763916\n",
      "  batch 450 loss: 0.4798436242341995\n",
      "  batch 500 loss: 0.4582294127345085\n",
      "  batch 550 loss: 0.5189040893316269\n",
      "  batch 600 loss: 0.46770789206027985\n",
      "  batch 650 loss: 0.495274258852005\n",
      "  batch 700 loss: 0.47692409813404085\n",
      "  batch 750 loss: 0.47482930481433866\n",
      "  batch 800 loss: 0.5168371790647507\n",
      "  batch 850 loss: 0.5069041502475738\n",
      "  batch 900 loss: 0.5155059331655503\n",
      "LOSS train 0.51551 valid 0.79278, valid PER 23.08%\n",
      "Training finished in 5.0 minutes.\n",
      "Model saved to checkpoints/20231205_153236/model_17\n",
      "Loading model from checkpoints/20231205_153236/model_17\n",
      "For dropout rate 0.2 the best model has SUB: 15.61%, DEL: 7.35%, INS: 2.42%, COR: 77.03%, PER: 25.39%\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.064542169570923\n",
      "  batch 100 loss: 3.3857819128036497\n",
      "  batch 150 loss: 3.2928116846084596\n",
      "  batch 200 loss: 3.1784022998809816\n",
      "  batch 250 loss: 3.0729869270324706\n",
      "  batch 300 loss: 2.843936266899109\n",
      "  batch 350 loss: 2.69014461517334\n",
      "  batch 400 loss: 2.576163439750671\n",
      "  batch 450 loss: 2.485974221229553\n",
      "  batch 500 loss: 2.364996299743652\n",
      "  batch 550 loss: 2.2743690395355225\n",
      "  batch 600 loss: 2.2106859064102173\n",
      "  batch 650 loss: 2.1043066787719726\n",
      "  batch 700 loss: 2.0900745463371275\n",
      "  batch 750 loss: 1.999966742992401\n",
      "  batch 800 loss: 1.965264630317688\n",
      "  batch 850 loss: 1.896145532131195\n",
      "  batch 900 loss: 1.855135350227356\n",
      "LOSS train 1.85514 valid 1.76172, valid PER 67.97%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.768321635723114\n",
      "  batch 100 loss: 1.6998590064048766\n",
      "  batch 150 loss: 1.6540663075447082\n",
      "  batch 200 loss: 1.6628334760665893\n",
      "  batch 250 loss: 1.6428365516662597\n",
      "  batch 300 loss: 1.5939915943145753\n",
      "  batch 350 loss: 1.5138662004470824\n",
      "  batch 400 loss: 1.5106802105903625\n",
      "  batch 450 loss: 1.4503341341018676\n",
      "  batch 500 loss: 1.4703480124473571\n",
      "  batch 550 loss: 1.4605866932868958\n",
      "  batch 600 loss: 1.3947275018692016\n",
      "  batch 650 loss: 1.4140748977661133\n",
      "  batch 700 loss: 1.3617386388778687\n",
      "  batch 750 loss: 1.3638807439804077\n",
      "  batch 800 loss: 1.2702812147140503\n",
      "  batch 850 loss: 1.2892856907844543\n",
      "  batch 900 loss: 1.3021825766563415\n",
      "LOSS train 1.30218 valid 1.26811, valid PER 39.76%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.2570435190200806\n",
      "  batch 100 loss: 1.2320818281173707\n",
      "  batch 150 loss: 1.2191857063770295\n",
      "  batch 200 loss: 1.2009354329109192\n",
      "  batch 250 loss: 1.19325159907341\n",
      "  batch 300 loss: 1.1832032990455628\n",
      "  batch 350 loss: 1.213159328699112\n",
      "  batch 400 loss: 1.1959753382205962\n",
      "  batch 450 loss: 1.175986523628235\n",
      "  batch 500 loss: 1.143440737724304\n",
      "  batch 550 loss: 1.1727401256561278\n",
      "  batch 600 loss: 1.1446452021598816\n",
      "  batch 650 loss: 1.1112906777858733\n",
      "  batch 700 loss: 1.1422822415828704\n",
      "  batch 750 loss: 1.1793371272087096\n",
      "  batch 800 loss: 1.1135244107246398\n",
      "  batch 850 loss: 1.1259638738632203\n",
      "  batch 900 loss: 1.0650688934326171\n",
      "LOSS train 1.06507 valid 1.10701, valid PER 33.76%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.063600524663925\n",
      "  batch 100 loss: 1.0682569718360901\n",
      "  batch 150 loss: 1.0528873932361602\n",
      "  batch 200 loss: 1.0645338714122772\n",
      "  batch 250 loss: 1.083281284570694\n",
      "  batch 300 loss: 1.0941589295864105\n",
      "  batch 350 loss: 1.0138914370536805\n",
      "  batch 400 loss: 1.0520404553413392\n",
      "  batch 450 loss: 1.0309788858890534\n",
      "  batch 500 loss: 1.0174103796482086\n",
      "  batch 550 loss: 1.0410726726055146\n",
      "  batch 600 loss: 1.0557131242752076\n",
      "  batch 650 loss: 1.038011292219162\n",
      "  batch 700 loss: 1.0107955276966094\n",
      "  batch 750 loss: 0.9996928715705872\n",
      "  batch 800 loss: 0.9729777240753174\n",
      "  batch 850 loss: 1.00449103474617\n",
      "  batch 900 loss: 1.0620799958705902\n",
      "LOSS train 1.06208 valid 0.99961, valid PER 31.26%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9806639194488526\n",
      "  batch 100 loss: 0.9779368376731873\n",
      "  batch 150 loss: 1.023040133714676\n",
      "  batch 200 loss: 0.9285695588588715\n",
      "  batch 250 loss: 0.9596734166145324\n",
      "  batch 300 loss: 0.9632330250740051\n",
      "  batch 350 loss: 0.9491688621044159\n",
      "  batch 400 loss: 0.9768538653850556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 450 loss: 0.941043244600296\n",
      "  batch 500 loss: 0.9723732626438141\n",
      "  batch 550 loss: 0.9100196421146393\n",
      "  batch 600 loss: 1.00697514295578\n",
      "  batch 650 loss: 0.9659905230998993\n",
      "  batch 700 loss: 0.9973909258842468\n",
      "  batch 750 loss: 0.9281650209426879\n",
      "  batch 800 loss: 0.9481864285469055\n",
      "  batch 850 loss: 0.9540855801105499\n",
      "  batch 900 loss: 0.9482898414134979\n",
      "LOSS train 0.94829 valid 0.95004, valid PER 29.79%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9615928161144257\n",
      "  batch 100 loss: 0.9025125396251679\n",
      "  batch 150 loss: 0.8930125784873962\n",
      "  batch 200 loss: 0.8987377691268921\n",
      "  batch 250 loss: 0.9303556144237518\n",
      "  batch 300 loss: 0.9119657778739929\n",
      "  batch 350 loss: 0.9221961581707001\n",
      "  batch 400 loss: 0.882160769701004\n",
      "  batch 450 loss: 0.8995257270336151\n",
      "  batch 500 loss: 0.9061815440654755\n",
      "  batch 550 loss: 0.9325819683074951\n",
      "  batch 600 loss: 0.9087784516811371\n",
      "  batch 650 loss: 0.9018893229961396\n",
      "  batch 700 loss: 0.897092980146408\n",
      "  batch 750 loss: 0.8979576313495636\n",
      "  batch 800 loss: 0.8960505402088166\n",
      "  batch 850 loss: 0.8780468547344208\n",
      "  batch 900 loss: 0.8948510932922363\n",
      "LOSS train 0.89485 valid 0.91727, valid PER 28.38%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8771358489990234\n",
      "  batch 100 loss: 0.8767656135559082\n",
      "  batch 150 loss: 0.835202478170395\n",
      "  batch 200 loss: 0.8576269644498825\n",
      "  batch 250 loss: 0.8572351515293122\n",
      "  batch 300 loss: 0.8468226146697998\n",
      "  batch 350 loss: 0.8487884342670441\n",
      "  batch 400 loss: 0.8459389579296112\n",
      "  batch 450 loss: 0.8620090067386628\n",
      "  batch 500 loss: 0.8340939068794251\n",
      "  batch 550 loss: 0.8310593771934509\n",
      "  batch 600 loss: 0.8424509370326996\n",
      "  batch 650 loss: 0.8355939340591431\n",
      "  batch 700 loss: 0.8835251438617706\n",
      "  batch 750 loss: 0.8502032923698425\n",
      "  batch 800 loss: 0.8330853772163391\n",
      "  batch 850 loss: 0.8556478214263916\n",
      "  batch 900 loss: 0.8841342616081238\n",
      "LOSS train 0.88413 valid 0.88883, valid PER 28.24%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8085604512691498\n",
      "  batch 100 loss: 0.8111781752109528\n",
      "  batch 150 loss: 0.8011467564105987\n",
      "  batch 200 loss: 0.7910391902923584\n",
      "  batch 250 loss: 0.8059131091833115\n",
      "  batch 300 loss: 0.7655501186847686\n",
      "  batch 350 loss: 0.8329754185676574\n",
      "  batch 400 loss: 0.7948372405767441\n",
      "  batch 450 loss: 0.8092848765850067\n",
      "  batch 500 loss: 0.8540127992630004\n",
      "  batch 550 loss: 0.775192037820816\n",
      "  batch 600 loss: 0.8255557775497436\n",
      "  batch 650 loss: 0.8276954913139343\n",
      "  batch 700 loss: 0.7962944209575653\n",
      "  batch 750 loss: 0.7885010528564453\n",
      "  batch 800 loss: 0.8100095963478089\n",
      "  batch 850 loss: 0.7992581999301911\n",
      "  batch 900 loss: 0.8167431616783142\n",
      "LOSS train 0.81674 valid 0.85020, valid PER 26.12%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7439876830577851\n",
      "  batch 100 loss: 0.778462792634964\n",
      "  batch 150 loss: 0.7694683730602264\n",
      "  batch 200 loss: 0.75406887114048\n",
      "  batch 250 loss: 0.7723948335647584\n",
      "  batch 300 loss: 0.7849969494342804\n",
      "  batch 350 loss: 0.8017172539234161\n",
      "  batch 400 loss: 0.7695690631866455\n",
      "  batch 450 loss: 0.778754825592041\n",
      "  batch 500 loss: 0.7506326848268509\n",
      "  batch 550 loss: 0.7886097323894501\n",
      "  batch 600 loss: 0.8111516535282135\n",
      "  batch 650 loss: 0.7747734534740448\n",
      "  batch 700 loss: 0.7578112852573394\n",
      "  batch 750 loss: 0.7738098740577698\n",
      "  batch 800 loss: 0.7919304180145263\n",
      "  batch 850 loss: 0.8041270875930786\n",
      "  batch 900 loss: 0.7574159693717957\n",
      "LOSS train 0.75742 valid 0.83430, valid PER 25.57%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7163019806146622\n",
      "  batch 100 loss: 0.743692216873169\n",
      "  batch 150 loss: 0.7416154301166534\n",
      "  batch 200 loss: 0.7451450079679489\n",
      "  batch 250 loss: 0.7638933938741684\n",
      "  batch 300 loss: 0.7328278172016144\n",
      "  batch 350 loss: 0.7610908162593841\n",
      "  batch 400 loss: 0.7207032728195191\n",
      "  batch 450 loss: 0.7208722692728042\n",
      "  batch 500 loss: 0.768017675280571\n",
      "  batch 550 loss: 0.7762033855915069\n",
      "  batch 600 loss: 0.7436899763345718\n",
      "  batch 650 loss: 0.724255365729332\n",
      "  batch 700 loss: 0.7510811203718185\n",
      "  batch 750 loss: 0.7337437033653259\n",
      "  batch 800 loss: 0.7456778746843338\n",
      "  batch 850 loss: 0.7409687453508377\n",
      "  batch 900 loss: 0.7584845173358917\n",
      "LOSS train 0.75848 valid 0.83311, valid PER 25.92%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6788132894039154\n",
      "  batch 100 loss: 0.6693015396595001\n",
      "  batch 150 loss: 0.6772064119577408\n",
      "  batch 200 loss: 0.7276917362213134\n",
      "  batch 250 loss: 0.7307023483514786\n",
      "  batch 300 loss: 0.6740634155273437\n",
      "  batch 350 loss: 0.7205287873744964\n",
      "  batch 400 loss: 0.7257385110855102\n",
      "  batch 450 loss: 0.7170607447624207\n",
      "  batch 500 loss: 0.6989639019966125\n",
      "  batch 550 loss: 0.7114870315790176\n",
      "  batch 600 loss: 0.684833607673645\n",
      "  batch 650 loss: 0.7607128477096557\n",
      "  batch 700 loss: 0.6905275076627732\n",
      "  batch 750 loss: 0.7025707256793976\n",
      "  batch 800 loss: 0.7263360297679902\n",
      "  batch 850 loss: 0.7334891295433045\n",
      "  batch 900 loss: 0.755736608505249\n",
      "LOSS train 0.75574 valid 0.81432, valid PER 25.14%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.6860833233594894\n",
      "  batch 100 loss: 0.69333023250103\n",
      "  batch 150 loss: 0.6550968182086945\n",
      "  batch 200 loss: 0.683234229683876\n",
      "  batch 250 loss: 0.6943401539325714\n",
      "  batch 300 loss: 0.6698335087299347\n",
      "  batch 350 loss: 0.6718557804822922\n",
      "  batch 400 loss: 0.7006771957874298\n",
      "  batch 450 loss: 0.6840516781806946\n",
      "  batch 500 loss: 0.6906180393695831\n",
      "  batch 550 loss: 0.6615074819326401\n",
      "  batch 600 loss: 0.6711655128002166\n",
      "  batch 650 loss: 0.7230478256940842\n",
      "  batch 700 loss: 0.6882803338766098\n",
      "  batch 750 loss: 0.6752755975723267\n",
      "  batch 800 loss: 0.668583368062973\n",
      "  batch 850 loss: 0.7182556629180908\n",
      "  batch 900 loss: 0.7192432016134263\n",
      "LOSS train 0.71924 valid 0.79442, valid PER 24.87%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.6457681518793106\n",
      "  batch 100 loss: 0.6607839876413345\n",
      "  batch 150 loss: 0.6349844074249268\n",
      "  batch 200 loss: 0.6736845475435257\n",
      "  batch 250 loss: 0.652658405303955\n",
      "  batch 300 loss: 0.6533701980113983\n",
      "  batch 350 loss: 0.6569811892509461\n",
      "  batch 400 loss: 0.6674753159284592\n",
      "  batch 450 loss: 0.6765673798322678\n",
      "  batch 500 loss: 0.6357721996307373\n",
      "  batch 550 loss: 0.6927565217018128\n",
      "  batch 600 loss: 0.6605964106321335\n",
      "  batch 650 loss: 0.6784125190973281\n",
      "  batch 700 loss: 0.6920418411493301\n",
      "  batch 750 loss: 0.6419026106595993\n",
      "  batch 800 loss: 0.667433871626854\n",
      "  batch 850 loss: 0.6971726894378663\n",
      "  batch 900 loss: 0.6722012686729432\n",
      "LOSS train 0.67220 valid 0.80530, valid PER 25.14%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.6175907623767852\n",
      "  batch 100 loss: 0.6330909734964371\n",
      "  batch 150 loss: 0.628076609969139\n",
      "  batch 200 loss: 0.6201577663421631\n",
      "  batch 250 loss: 0.6317711716890335\n",
      "  batch 300 loss: 0.635522603392601\n",
      "  batch 350 loss: 0.6097048771381378\n",
      "  batch 400 loss: 0.6165193742513657\n",
      "  batch 450 loss: 0.634700328707695\n",
      "  batch 500 loss: 0.6417749285697937\n",
      "  batch 550 loss: 0.6574367612600327\n",
      "  batch 600 loss: 0.6293958097696304\n",
      "  batch 650 loss: 0.64985216319561\n",
      "  batch 700 loss: 0.6668536788225174\n",
      "  batch 750 loss: 0.6235819369554519\n",
      "  batch 800 loss: 0.6087645095586777\n",
      "  batch 850 loss: 0.6552698785066604\n",
      "  batch 900 loss: 0.6524562615156174\n",
      "LOSS train 0.65246 valid 0.79719, valid PER 24.80%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6033848106861115\n",
      "  batch 100 loss: 0.6031390649080276\n",
      "  batch 150 loss: 0.5933335936069488\n",
      "  batch 200 loss: 0.6282882374525071\n",
      "  batch 250 loss: 0.6196791309118271\n",
      "  batch 300 loss: 0.6187722504138946\n",
      "  batch 350 loss: 0.6218261325359344\n",
      "  batch 400 loss: 0.6100008004903793\n",
      "  batch 450 loss: 0.6232628738880157\n",
      "  batch 500 loss: 0.5972583496570587\n",
      "  batch 550 loss: 0.6182506710290909\n",
      "  batch 600 loss: 0.6526933240890503\n",
      "  batch 650 loss: 0.6342489373683929\n",
      "  batch 700 loss: 0.640394926071167\n",
      "  batch 750 loss: 0.6596713793277741\n",
      "  batch 800 loss: 0.6169237619638444\n",
      "  batch 850 loss: 0.5967313688993454\n",
      "  batch 900 loss: 0.6214353567361832\n",
      "LOSS train 0.62144 valid 0.80142, valid PER 24.29%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.6022708195447922\n",
      "  batch 100 loss: 0.5712085711956024\n",
      "  batch 150 loss: 0.5680196166038514\n",
      "  batch 200 loss: 0.5767704594135284\n",
      "  batch 250 loss: 0.6069044721126556\n",
      "  batch 300 loss: 0.5910704475641251\n",
      "  batch 350 loss: 0.6108099097013473\n",
      "  batch 400 loss: 0.6129948711395263\n",
      "  batch 450 loss: 0.6297205448150635\n",
      "  batch 500 loss: 0.582719549536705\n",
      "  batch 550 loss: 0.6059100049734115\n",
      "  batch 600 loss: 0.5836885768175125\n",
      "  batch 650 loss: 0.6227057999372483\n",
      "  batch 700 loss: 0.6034915691614151\n",
      "  batch 750 loss: 0.6051856821775436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 800 loss: 0.6136057090759277\n",
      "  batch 850 loss: 0.594724811911583\n",
      "  batch 900 loss: 0.5982856005430222\n",
      "LOSS train 0.59829 valid 0.78509, valid PER 23.37%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.5641423815488815\n",
      "  batch 100 loss: 0.5757684183120727\n",
      "  batch 150 loss: 0.55726655960083\n",
      "  batch 200 loss: 0.5570566582679749\n",
      "  batch 250 loss: 0.5824283641576767\n",
      "  batch 300 loss: 0.5799760454893113\n",
      "  batch 350 loss: 0.5588961988687515\n",
      "  batch 400 loss: 0.6131984454393387\n",
      "  batch 450 loss: 0.5822772723436356\n",
      "  batch 500 loss: 0.5751492547988891\n",
      "  batch 550 loss: 0.5777616864442825\n",
      "  batch 600 loss: 0.5983699554204941\n",
      "  batch 650 loss: 0.5830183255672455\n",
      "  batch 700 loss: 0.5897963726520539\n",
      "  batch 750 loss: 0.5660801291465759\n",
      "  batch 800 loss: 0.5594613248109818\n",
      "  batch 850 loss: 0.6090428054332733\n",
      "  batch 900 loss: 0.5775185847282409\n",
      "LOSS train 0.57752 valid 0.78501, valid PER 23.56%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.5534746891260147\n",
      "  batch 100 loss: 0.5442646652460098\n",
      "  batch 150 loss: 0.571922258734703\n",
      "  batch 200 loss: 0.5642857372760772\n",
      "  batch 250 loss: 0.5665063256025314\n",
      "  batch 300 loss: 0.541594340801239\n",
      "  batch 350 loss: 0.5647687411308289\n",
      "  batch 400 loss: 0.5474821639060974\n",
      "  batch 450 loss: 0.5758694636821747\n",
      "  batch 500 loss: 0.5744941383600235\n",
      "  batch 550 loss: 0.5849584054946899\n",
      "  batch 600 loss: 0.54419517993927\n",
      "  batch 650 loss: 0.5582407021522522\n",
      "  batch 700 loss: 0.6013135093450547\n",
      "  batch 750 loss: 0.5531145870685578\n",
      "  batch 800 loss: 0.5424497640132904\n",
      "  batch 850 loss: 0.5562189906835556\n",
      "  batch 900 loss: 0.5947699129581452\n",
      "LOSS train 0.59477 valid 0.79258, valid PER 23.98%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5123026943206788\n",
      "  batch 100 loss: 0.5247092258930206\n",
      "  batch 150 loss: 0.543488050699234\n",
      "  batch 200 loss: 0.543714656829834\n",
      "  batch 250 loss: 0.5392040145397187\n",
      "  batch 300 loss: 0.5426791688799858\n",
      "  batch 350 loss: 0.550067435503006\n",
      "  batch 400 loss: 0.5557048654556275\n",
      "  batch 450 loss: 0.5649113738536835\n",
      "  batch 500 loss: 0.567422187924385\n",
      "  batch 550 loss: 0.5258400040864945\n",
      "  batch 600 loss: 0.5343790638446808\n",
      "  batch 650 loss: 0.5896848607063293\n",
      "  batch 700 loss: 0.5283407527208328\n",
      "  batch 750 loss: 0.5336380833387375\n",
      "  batch 800 loss: 0.5697926765680313\n",
      "  batch 850 loss: 0.5578811377286911\n",
      "  batch 900 loss: 0.5643165081739425\n",
      "LOSS train 0.56432 valid 0.78119, valid PER 23.65%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.4981375271081924\n",
      "  batch 100 loss: 0.4941982913017273\n",
      "  batch 150 loss: 0.5077334326505661\n",
      "  batch 200 loss: 0.5370147573947907\n",
      "  batch 250 loss: 0.517667053937912\n",
      "  batch 300 loss: 0.541905266046524\n",
      "  batch 350 loss: 0.49761121660470964\n",
      "  batch 400 loss: 0.5177930533885956\n",
      "  batch 450 loss: 0.5284029799699783\n",
      "  batch 500 loss: 0.491532576084137\n",
      "  batch 550 loss: 0.5685511684417724\n",
      "  batch 600 loss: 0.5241603577136993\n",
      "  batch 650 loss: 0.5424761432409286\n",
      "  batch 700 loss: 0.5390926885604859\n",
      "  batch 750 loss: 0.5088746505975723\n",
      "  batch 800 loss: 0.5430368208885192\n",
      "  batch 850 loss: 0.5669466334581376\n",
      "  batch 900 loss: 0.545268195271492\n",
      "LOSS train 0.54527 valid 0.77780, valid PER 22.89%\n",
      "Training finished in 5.0 minutes.\n",
      "Model saved to checkpoints/20231205_153741/model_20\n",
      "Loading model from checkpoints/20231205_153741/model_20\n",
      "For dropout rate 0.3 the best model has SUB: 14.81%, DEL: 7.44%, INS: 2.98%, COR: 77.76%, PER: 25.22%\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.065970520973206\n",
      "  batch 100 loss: 3.38427339553833\n",
      "  batch 150 loss: 3.293650913238525\n",
      "  batch 200 loss: 3.180246639251709\n",
      "  batch 250 loss: 3.0743468761444093\n",
      "  batch 300 loss: 2.8459902667999266\n",
      "  batch 350 loss: 2.696459560394287\n",
      "  batch 400 loss: 2.5790224075317383\n",
      "  batch 450 loss: 2.4885457658767702\n",
      "  batch 500 loss: 2.3710439205169678\n",
      "  batch 550 loss: 2.282239766120911\n",
      "  batch 600 loss: 2.2106885623931887\n",
      "  batch 650 loss: 2.113369491100311\n",
      "  batch 700 loss: 2.094190719127655\n",
      "  batch 750 loss: 2.0074678301811217\n",
      "  batch 800 loss: 1.9679069232940674\n",
      "  batch 850 loss: 1.9129993724822998\n",
      "  batch 900 loss: 1.871046929359436\n",
      "LOSS train 1.87105 valid 1.77684, valid PER 68.62%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7844133973121643\n",
      "  batch 100 loss: 1.7064624500274659\n",
      "  batch 150 loss: 1.6708806037902832\n",
      "  batch 200 loss: 1.664121150970459\n",
      "  batch 250 loss: 1.6633401250839233\n",
      "  batch 300 loss: 1.6096744155883789\n",
      "  batch 350 loss: 1.523233494758606\n",
      "  batch 400 loss: 1.5347899389266968\n",
      "  batch 450 loss: 1.4675718641281128\n",
      "  batch 500 loss: 1.483903980255127\n",
      "  batch 550 loss: 1.4807051920890808\n",
      "  batch 600 loss: 1.4141278648376465\n",
      "  batch 650 loss: 1.4318450951576234\n",
      "  batch 700 loss: 1.3823156976699829\n",
      "  batch 750 loss: 1.3749654507637024\n",
      "  batch 800 loss: 1.2933611845970154\n",
      "  batch 850 loss: 1.311850962638855\n",
      "  batch 900 loss: 1.316279363632202\n",
      "LOSS train 1.31628 valid 1.25903, valid PER 39.71%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.2663920950889587\n",
      "  batch 100 loss: 1.2380797600746154\n",
      "  batch 150 loss: 1.2340570950508118\n",
      "  batch 200 loss: 1.2078732025623322\n",
      "  batch 250 loss: 1.2115008223056793\n",
      "  batch 300 loss: 1.2003567433357238\n",
      "  batch 350 loss: 1.2273998415470124\n",
      "  batch 400 loss: 1.2070765173435212\n",
      "  batch 450 loss: 1.183535200357437\n",
      "  batch 500 loss: 1.1759211707115174\n",
      "  batch 550 loss: 1.1815239202976227\n",
      "  batch 600 loss: 1.1432759284973144\n",
      "  batch 650 loss: 1.114413959980011\n",
      "  batch 700 loss: 1.1626129674911498\n",
      "  batch 750 loss: 1.2060322058200836\n",
      "  batch 800 loss: 1.136898819208145\n",
      "  batch 850 loss: 1.1619845056533813\n",
      "  batch 900 loss: 1.0890012979507446\n",
      "LOSS train 1.08900 valid 1.09551, valid PER 33.20%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0765294742584228\n",
      "  batch 100 loss: 1.0965463066101073\n",
      "  batch 150 loss: 1.0654569280147552\n",
      "  batch 200 loss: 1.0887134206295013\n",
      "  batch 250 loss: 1.0875602865219116\n",
      "  batch 300 loss: 1.1068847036361695\n",
      "  batch 350 loss: 1.0139339077472687\n",
      "  batch 400 loss: 1.0686544966697693\n",
      "  batch 450 loss: 1.0494624972343445\n",
      "  batch 500 loss: 1.035157768726349\n",
      "  batch 550 loss: 1.059887968301773\n",
      "  batch 600 loss: 1.0812555932998658\n",
      "  batch 650 loss: 1.0588630974292754\n",
      "  batch 700 loss: 1.025388287305832\n",
      "  batch 750 loss: 1.029572412967682\n",
      "  batch 800 loss: 0.989776680469513\n",
      "  batch 850 loss: 1.024720458984375\n",
      "  batch 900 loss: 1.064455397129059\n",
      "LOSS train 1.06446 valid 1.00021, valid PER 31.34%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9952601563930511\n",
      "  batch 100 loss: 0.9752600455284118\n",
      "  batch 150 loss: 1.0276777815818787\n",
      "  batch 200 loss: 0.9572712647914886\n",
      "  batch 250 loss: 0.9725342583656311\n",
      "  batch 300 loss: 0.9820231294631958\n",
      "  batch 350 loss: 0.9818887197971344\n",
      "  batch 400 loss: 1.0083058393001556\n",
      "  batch 450 loss: 0.9673033261299133\n",
      "  batch 500 loss: 1.0052287673950195\n",
      "  batch 550 loss: 0.9250341868400573\n",
      "  batch 600 loss: 0.9960328471660614\n",
      "  batch 650 loss: 0.9651187312602997\n",
      "  batch 700 loss: 1.0150817573070525\n",
      "  batch 750 loss: 0.9388602781295776\n",
      "  batch 800 loss: 0.9652389681339264\n",
      "  batch 850 loss: 0.9672258222103118\n",
      "  batch 900 loss: 0.9558497953414917\n",
      "LOSS train 0.95585 valid 0.96735, valid PER 29.99%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9672599673271179\n",
      "  batch 100 loss: 0.9082073628902435\n",
      "  batch 150 loss: 0.9017095971107483\n",
      "  batch 200 loss: 0.9107726204395294\n",
      "  batch 250 loss: 0.9569592034816742\n",
      "  batch 300 loss: 0.9265389728546143\n",
      "  batch 350 loss: 0.9231425106525422\n",
      "  batch 400 loss: 0.9115412306785583\n",
      "  batch 450 loss: 0.9248730325698853\n",
      "  batch 500 loss: 0.9271096444129944\n",
      "  batch 550 loss: 0.9237747085094452\n",
      "  batch 600 loss: 0.9062915956974029\n",
      "  batch 650 loss: 0.911095564365387\n",
      "  batch 700 loss: 0.9117650508880615\n",
      "  batch 750 loss: 0.90062420129776\n",
      "  batch 800 loss: 0.9164548516273499\n",
      "  batch 850 loss: 0.8898197031021118\n",
      "  batch 900 loss: 0.8981847584247589\n",
      "LOSS train 0.89818 valid 0.94158, valid PER 28.39%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8969845819473267\n",
      "  batch 100 loss: 0.8918079507350921\n",
      "  batch 150 loss: 0.853981784582138\n",
      "  batch 200 loss: 0.8564410901069641\n",
      "  batch 250 loss: 0.8573158192634582\n",
      "  batch 300 loss: 0.858417171239853\n",
      "  batch 350 loss: 0.8664914214611054\n",
      "  batch 400 loss: 0.8671807789802551\n",
      "  batch 450 loss: 0.8616480982303619\n",
      "  batch 500 loss: 0.8620249938964843\n",
      "  batch 550 loss: 0.8388473558425903\n",
      "  batch 600 loss: 0.8648755824565888\n",
      "  batch 650 loss: 0.8514971208572387\n",
      "  batch 700 loss: 0.8839413058757782\n",
      "  batch 750 loss: 0.8627173924446105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 800 loss: 0.8555577909946441\n",
      "  batch 850 loss: 0.859435658454895\n",
      "  batch 900 loss: 0.9081391489505768\n",
      "LOSS train 0.90814 valid 0.88513, valid PER 28.38%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8405251121520996\n",
      "  batch 100 loss: 0.8214319336414337\n",
      "  batch 150 loss: 0.8381071650981903\n",
      "  batch 200 loss: 0.8113514482975006\n",
      "  batch 250 loss: 0.8337798285484314\n",
      "  batch 300 loss: 0.7827784180641174\n",
      "  batch 350 loss: 0.8463729596138001\n",
      "  batch 400 loss: 0.8112364780902862\n",
      "  batch 450 loss: 0.8192383325099946\n",
      "  batch 500 loss: 0.8515306162834168\n",
      "  batch 550 loss: 0.7975841104984284\n",
      "  batch 600 loss: 0.8451152229309082\n",
      "  batch 650 loss: 0.8435632967948914\n",
      "  batch 700 loss: 0.8020948612689972\n",
      "  batch 750 loss: 0.8217782306671143\n",
      "  batch 800 loss: 0.8267990458011627\n",
      "  batch 850 loss: 0.8088662123680115\n",
      "  batch 900 loss: 0.8475421631336212\n",
      "LOSS train 0.84754 valid 0.84467, valid PER 25.67%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7642648577690124\n",
      "  batch 100 loss: 0.7957785105705262\n",
      "  batch 150 loss: 0.7906216609477997\n",
      "  batch 200 loss: 0.7726015043258667\n",
      "  batch 250 loss: 0.8053491127490997\n",
      "  batch 300 loss: 0.7882620704174041\n",
      "  batch 350 loss: 0.8219576323032379\n",
      "  batch 400 loss: 0.8003772008419037\n",
      "  batch 450 loss: 0.8146780478954315\n",
      "  batch 500 loss: 0.7694697272777558\n",
      "  batch 550 loss: 0.8057511961460113\n",
      "  batch 600 loss: 0.8027044701576233\n",
      "  batch 650 loss: 0.7920675992965698\n",
      "  batch 700 loss: 0.7751913344860077\n",
      "  batch 750 loss: 0.7662288427352906\n",
      "  batch 800 loss: 0.8121352481842041\n",
      "  batch 850 loss: 0.8142846584320068\n",
      "  batch 900 loss: 0.7615319967269898\n",
      "LOSS train 0.76153 valid 0.84865, valid PER 25.86%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7265384727716446\n",
      "  batch 100 loss: 0.7436116057634353\n",
      "  batch 150 loss: 0.7626929026842117\n",
      "  batch 200 loss: 0.7780825769901276\n",
      "  batch 250 loss: 0.7797402143478394\n",
      "  batch 300 loss: 0.7393577539920807\n",
      "  batch 350 loss: 0.7730946743488312\n",
      "  batch 400 loss: 0.726675089597702\n",
      "  batch 450 loss: 0.7210243606567383\n",
      "  batch 500 loss: 0.780443606376648\n",
      "  batch 550 loss: 0.7862380909919738\n",
      "  batch 600 loss: 0.7687966787815094\n",
      "  batch 650 loss: 0.7419867593050004\n",
      "  batch 700 loss: 0.7600247275829315\n",
      "  batch 750 loss: 0.7492736303806304\n",
      "  batch 800 loss: 0.7755299580097198\n",
      "  batch 850 loss: 0.7618420147895812\n",
      "  batch 900 loss: 0.7798758780956269\n",
      "LOSS train 0.77988 valid 0.83512, valid PER 26.20%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.7038751643896103\n",
      "  batch 100 loss: 0.6858798563480377\n",
      "  batch 150 loss: 0.6971107411384583\n",
      "  batch 200 loss: 0.7636396074295044\n",
      "  batch 250 loss: 0.7287522518634796\n",
      "  batch 300 loss: 0.6961728310585023\n",
      "  batch 350 loss: 0.7145994067192077\n",
      "  batch 400 loss: 0.7427286446094513\n",
      "  batch 450 loss: 0.7551517474651337\n",
      "  batch 500 loss: 0.7045257306098938\n",
      "  batch 550 loss: 0.723074072599411\n",
      "  batch 600 loss: 0.7146858263015747\n",
      "  batch 650 loss: 0.7658783221244811\n",
      "  batch 700 loss: 0.7135720455646515\n",
      "  batch 750 loss: 0.7111875611543655\n",
      "  batch 800 loss: 0.7649806237220764\n",
      "  batch 850 loss: 0.7660378193855286\n",
      "  batch 900 loss: 0.76545050740242\n",
      "LOSS train 0.76545 valid 0.82847, valid PER 25.32%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.7082106482982635\n",
      "  batch 100 loss: 0.7007425808906556\n",
      "  batch 150 loss: 0.6652371948957443\n",
      "  batch 200 loss: 0.7018313258886337\n",
      "  batch 250 loss: 0.7182233798503875\n",
      "  batch 300 loss: 0.6932905626296997\n",
      "  batch 350 loss: 0.6886240386962891\n",
      "  batch 400 loss: 0.7132221019268036\n",
      "  batch 450 loss: 0.6986271631717682\n",
      "  batch 500 loss: 0.7271122336387634\n",
      "  batch 550 loss: 0.6590058094263077\n",
      "  batch 600 loss: 0.6766674518585205\n",
      "  batch 650 loss: 0.735706307888031\n",
      "  batch 700 loss: 0.7115703529119491\n",
      "  batch 750 loss: 0.7015912580490112\n",
      "  batch 800 loss: 0.6834310972690583\n",
      "  batch 850 loss: 0.7443586212396621\n",
      "  batch 900 loss: 0.7355549615621567\n",
      "LOSS train 0.73555 valid 0.79672, valid PER 24.84%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.656978942155838\n",
      "  batch 100 loss: 0.6748041141033173\n",
      "  batch 150 loss: 0.6510704642534256\n",
      "  batch 200 loss: 0.706560006737709\n",
      "  batch 250 loss: 0.6740182262659072\n",
      "  batch 300 loss: 0.6736656218767166\n",
      "  batch 350 loss: 0.6638000172376632\n",
      "  batch 400 loss: 0.6878427535295486\n",
      "  batch 450 loss: 0.6910585719347\n",
      "  batch 500 loss: 0.6576688313484191\n",
      "  batch 550 loss: 0.7127744841575623\n",
      "  batch 600 loss: 0.6591601502895356\n",
      "  batch 650 loss: 0.7082815384864807\n",
      "  batch 700 loss: 0.6819401526451111\n",
      "  batch 750 loss: 0.6451013845205307\n",
      "  batch 800 loss: 0.6678886443376542\n",
      "  batch 850 loss: 0.7153139078617096\n",
      "  batch 900 loss: 0.6952260339260101\n",
      "LOSS train 0.69523 valid 0.80046, valid PER 24.40%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.6510258865356445\n",
      "  batch 100 loss: 0.6522599440813065\n",
      "  batch 150 loss: 0.6481119602918625\n",
      "  batch 200 loss: 0.6525405073165893\n",
      "  batch 250 loss: 0.6501506918668747\n",
      "  batch 300 loss: 0.6836058551073074\n",
      "  batch 350 loss: 0.6345418363809585\n",
      "  batch 400 loss: 0.6370923757553101\n",
      "  batch 450 loss: 0.6635074192285537\n",
      "  batch 500 loss: 0.6633235448598862\n",
      "  batch 550 loss: 0.6742277210950851\n",
      "  batch 600 loss: 0.6528954267501831\n",
      "  batch 650 loss: 0.6704193997383118\n",
      "  batch 700 loss: 0.7006020319461822\n",
      "  batch 750 loss: 0.6456015622615814\n",
      "  batch 800 loss: 0.6281636893749237\n",
      "  batch 850 loss: 0.6840133911371231\n",
      "  batch 900 loss: 0.6767125356197358\n",
      "LOSS train 0.67671 valid 0.79301, valid PER 24.59%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6367205685377121\n",
      "  batch 100 loss: 0.629146842956543\n",
      "  batch 150 loss: 0.6207719498872757\n",
      "  batch 200 loss: 0.6344934248924256\n",
      "  batch 250 loss: 0.6390218967199326\n",
      "  batch 300 loss: 0.6357111436128616\n",
      "  batch 350 loss: 0.612821894288063\n",
      "  batch 400 loss: 0.6315784591436386\n",
      "  batch 450 loss: 0.6256507641077041\n",
      "  batch 500 loss: 0.6151741927862168\n",
      "  batch 550 loss: 0.6250041270256043\n",
      "  batch 600 loss: 0.6730825221538543\n",
      "  batch 650 loss: 0.6498086792230606\n",
      "  batch 700 loss: 0.6599274545907974\n",
      "  batch 750 loss: 0.6550234240293503\n",
      "  batch 800 loss: 0.6407814878225326\n",
      "  batch 850 loss: 0.6201830989122391\n",
      "  batch 900 loss: 0.6398252409696579\n",
      "LOSS train 0.63983 valid 0.79675, valid PER 24.35%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.6242930644750595\n",
      "  batch 100 loss: 0.6057689297199249\n",
      "  batch 150 loss: 0.5927363926172257\n",
      "  batch 200 loss: 0.5975976049900055\n",
      "  batch 250 loss: 0.6190427678823471\n",
      "  batch 300 loss: 0.617021176815033\n",
      "  batch 350 loss: 0.6413343673944474\n",
      "  batch 400 loss: 0.6185867762565613\n",
      "  batch 450 loss: 0.6367894959449768\n",
      "  batch 500 loss: 0.6071790635585785\n",
      "  batch 550 loss: 0.6318295681476593\n",
      "  batch 600 loss: 0.611836982369423\n",
      "  batch 650 loss: 0.6331565475463867\n",
      "  batch 700 loss: 0.5948108947277069\n",
      "  batch 750 loss: 0.6144289398193359\n",
      "  batch 800 loss: 0.62381403028965\n",
      "  batch 850 loss: 0.6171913754940033\n",
      "  batch 900 loss: 0.6250464028120041\n",
      "LOSS train 0.62505 valid 0.77362, valid PER 23.56%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.5886787617206574\n",
      "  batch 100 loss: 0.5986810523271561\n",
      "  batch 150 loss: 0.5654431372880936\n",
      "  batch 200 loss: 0.5822432851791381\n",
      "  batch 250 loss: 0.6065687698125839\n",
      "  batch 300 loss: 0.6021976810693741\n",
      "  batch 350 loss: 0.5739230996370316\n",
      "  batch 400 loss: 0.6207853072881698\n",
      "  batch 450 loss: 0.6017406606674194\n",
      "  batch 500 loss: 0.6014959651231766\n",
      "  batch 550 loss: 0.5959659028053284\n",
      "  batch 600 loss: 0.6340971159934997\n",
      "  batch 650 loss: 0.5952248495817184\n",
      "  batch 700 loss: 0.6005070358514786\n",
      "  batch 750 loss: 0.588636707663536\n",
      "  batch 800 loss: 0.588085680603981\n",
      "  batch 850 loss: 0.6084805834293365\n",
      "  batch 900 loss: 0.5845665895938873\n",
      "LOSS train 0.58457 valid 0.77514, valid PER 23.57%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.5628993874788284\n",
      "  batch 100 loss: 0.5608324337005616\n",
      "  batch 150 loss: 0.5964805114269257\n",
      "  batch 200 loss: 0.578430387377739\n",
      "  batch 250 loss: 0.603964359164238\n",
      "  batch 300 loss: 0.5736829578876496\n",
      "  batch 350 loss: 0.5943996155261994\n",
      "  batch 400 loss: 0.5599917924404144\n",
      "  batch 450 loss: 0.5906263500452041\n",
      "  batch 500 loss: 0.5924624180793763\n",
      "  batch 550 loss: 0.5953894025087356\n",
      "  batch 600 loss: 0.5465028184652329\n",
      "  batch 650 loss: 0.566838436126709\n",
      "  batch 700 loss: 0.6208578419685363\n",
      "  batch 750 loss: 0.567596801519394\n",
      "  batch 800 loss: 0.5625968641042709\n",
      "  batch 850 loss: 0.5741275519132614\n",
      "  batch 900 loss: 0.592201583981514\n",
      "LOSS train 0.59220 valid 0.76745, valid PER 23.40%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5251520717144013\n",
      "  batch 100 loss: 0.5391706401109695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 150 loss: 0.5480594873428345\n",
      "  batch 200 loss: 0.5484050571918487\n",
      "  batch 250 loss: 0.5587198388576508\n",
      "  batch 300 loss: 0.5612789684534073\n",
      "  batch 350 loss: 0.5593012899160386\n",
      "  batch 400 loss: 0.5686094760894775\n",
      "  batch 450 loss: 0.5771916115283966\n",
      "  batch 500 loss: 0.57904261469841\n",
      "  batch 550 loss: 0.5479791694879532\n",
      "  batch 600 loss: 0.5465494191646576\n",
      "  batch 650 loss: 0.6090065342187881\n",
      "  batch 700 loss: 0.538641282916069\n",
      "  batch 750 loss: 0.5391879332065582\n",
      "  batch 800 loss: 0.5789473533630372\n",
      "  batch 850 loss: 0.5608839809894561\n",
      "  batch 900 loss: 0.5693364453315735\n",
      "LOSS train 0.56934 valid 0.77426, valid PER 23.14%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.5242852258682251\n",
      "  batch 100 loss: 0.5167475575208664\n",
      "  batch 150 loss: 0.5206020820140839\n",
      "  batch 200 loss: 0.5425648641586304\n",
      "  batch 250 loss: 0.5406535828113556\n",
      "  batch 300 loss: 0.5395273864269257\n",
      "  batch 350 loss: 0.5276573824882508\n",
      "  batch 400 loss: 0.5531994462013244\n",
      "  batch 450 loss: 0.5473116832971573\n",
      "  batch 500 loss: 0.5328069072961807\n",
      "  batch 550 loss: 0.5843740993738175\n",
      "  batch 600 loss: 0.5389937818050384\n",
      "  batch 650 loss: 0.556335551738739\n",
      "  batch 700 loss: 0.5707626575231552\n",
      "  batch 750 loss: 0.5394290536642075\n",
      "  batch 800 loss: 0.577112820148468\n",
      "  batch 850 loss: 0.5754272729158402\n",
      "  batch 900 loss: 0.5651242864131928\n",
      "LOSS train 0.56512 valid 0.79874, valid PER 23.50%\n",
      "Training finished in 5.0 minutes.\n",
      "Model saved to checkpoints/20231205_154247/model_18\n",
      "Loading model from checkpoints/20231205_154247/model_18\n",
      "For dropout rate 0.4 the best model has SUB: 15.83%, DEL: 7.74%, INS: 2.03%, COR: 76.42%, PER: 25.61%\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.069010100364685\n",
      "  batch 100 loss: 3.383691554069519\n",
      "  batch 150 loss: 3.291188917160034\n",
      "  batch 200 loss: 3.180890598297119\n",
      "  batch 250 loss: 3.078599066734314\n",
      "  batch 300 loss: 2.8658764362335205\n",
      "  batch 350 loss: 2.7054309701919554\n",
      "  batch 400 loss: 2.5840375661849975\n",
      "  batch 450 loss: 2.4976597690582274\n",
      "  batch 500 loss: 2.3754436349868775\n",
      "  batch 550 loss: 2.2884048223495483\n",
      "  batch 600 loss: 2.2207690191268923\n",
      "  batch 650 loss: 2.1235109615325927\n",
      "  batch 700 loss: 2.1055906558036805\n",
      "  batch 750 loss: 2.023874819278717\n",
      "  batch 800 loss: 1.984990656375885\n",
      "  batch 850 loss: 1.9310210299491883\n",
      "  batch 900 loss: 1.8833370471000672\n",
      "LOSS train 1.88334 valid 1.78432, valid PER 68.30%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.8016167664527893\n",
      "  batch 100 loss: 1.7267240357398987\n",
      "  batch 150 loss: 1.6903561329841614\n",
      "  batch 200 loss: 1.6873645091056824\n",
      "  batch 250 loss: 1.679081711769104\n",
      "  batch 300 loss: 1.6243418860435486\n",
      "  batch 350 loss: 1.5391701459884644\n",
      "  batch 400 loss: 1.5530083227157592\n",
      "  batch 450 loss: 1.4883153820037842\n",
      "  batch 500 loss: 1.5094127321243287\n",
      "  batch 550 loss: 1.518559720516205\n",
      "  batch 600 loss: 1.4486658263206482\n",
      "  batch 650 loss: 1.4713386368751527\n",
      "  batch 700 loss: 1.4154849696159362\n",
      "  batch 750 loss: 1.4078081774711608\n",
      "  batch 800 loss: 1.3274227476119995\n",
      "  batch 850 loss: 1.3261609327793122\n",
      "  batch 900 loss: 1.337191243171692\n",
      "LOSS train 1.33719 valid 1.28287, valid PER 39.55%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.285601714849472\n",
      "  batch 100 loss: 1.2738928282260895\n",
      "  batch 150 loss: 1.2552609515190125\n",
      "  batch 200 loss: 1.2426614260673523\n",
      "  batch 250 loss: 1.2420938766002656\n",
      "  batch 300 loss: 1.226308706998825\n",
      "  batch 350 loss: 1.2548077201843262\n",
      "  batch 400 loss: 1.2400705683231354\n",
      "  batch 450 loss: 1.1936429309844971\n",
      "  batch 500 loss: 1.1829301166534423\n",
      "  batch 550 loss: 1.1916942930221557\n",
      "  batch 600 loss: 1.1635730254650116\n",
      "  batch 650 loss: 1.1459665751457215\n",
      "  batch 700 loss: 1.1823728251457215\n",
      "  batch 750 loss: 1.219803627729416\n",
      "  batch 800 loss: 1.1540598094463348\n",
      "  batch 850 loss: 1.1748633289337158\n",
      "  batch 900 loss: 1.1110308361053467\n",
      "LOSS train 1.11103 valid 1.12795, valid PER 33.77%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.1060303139686585\n",
      "  batch 100 loss: 1.1148610198497773\n",
      "  batch 150 loss: 1.0778660368919373\n",
      "  batch 200 loss: 1.108242244720459\n",
      "  batch 250 loss: 1.1203720784187317\n",
      "  batch 300 loss: 1.112857996225357\n",
      "  batch 350 loss: 1.0216091394424438\n",
      "  batch 400 loss: 1.0793157756328582\n",
      "  batch 450 loss: 1.0646047389507294\n",
      "  batch 500 loss: 1.0615726554393767\n",
      "  batch 550 loss: 1.0968684732913971\n",
      "  batch 600 loss: 1.0881290638446808\n",
      "  batch 650 loss: 1.063072726726532\n",
      "  batch 700 loss: 1.0476315009593964\n",
      "  batch 750 loss: 1.026574981212616\n",
      "  batch 800 loss: 1.013547352552414\n",
      "  batch 850 loss: 1.034303026199341\n",
      "  batch 900 loss: 1.0813793385028838\n",
      "LOSS train 1.08138 valid 1.01183, valid PER 31.71%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.0161632657051087\n",
      "  batch 100 loss: 1.0070837330818176\n",
      "  batch 150 loss: 1.042241998910904\n",
      "  batch 200 loss: 0.9846491277217865\n",
      "  batch 250 loss: 0.9755321836471558\n",
      "  batch 300 loss: 0.991602498292923\n",
      "  batch 350 loss: 0.9982853388786316\n",
      "  batch 400 loss: 1.011731812953949\n",
      "  batch 450 loss: 0.9940816926956176\n",
      "  batch 500 loss: 1.0281791472434998\n",
      "  batch 550 loss: 0.9359857189655304\n",
      "  batch 600 loss: 1.0204056024551391\n",
      "  batch 650 loss: 0.976296010017395\n",
      "  batch 700 loss: 1.027280707359314\n",
      "  batch 750 loss: 0.9528469443321228\n",
      "  batch 800 loss: 0.9901327931880951\n",
      "  batch 850 loss: 0.9817873787879944\n",
      "  batch 900 loss: 0.9875595152378083\n",
      "LOSS train 0.98756 valid 0.96558, valid PER 29.64%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9999056780338287\n",
      "  batch 100 loss: 0.9396831977367401\n",
      "  batch 150 loss: 0.9155515885353088\n",
      "  batch 200 loss: 0.9364273238182068\n",
      "  batch 250 loss: 0.9685219073295593\n",
      "  batch 300 loss: 0.9521815848350524\n",
      "  batch 350 loss: 0.9582353234291077\n",
      "  batch 400 loss: 0.9368280875682831\n",
      "  batch 450 loss: 0.9825550210475922\n",
      "  batch 500 loss: 0.935172553062439\n",
      "  batch 550 loss: 0.9616726434230805\n",
      "  batch 600 loss: 0.9461953973770142\n",
      "  batch 650 loss: 0.9534858381748199\n",
      "  batch 700 loss: 0.9368570399284363\n",
      "  batch 750 loss: 0.9274074590206146\n",
      "  batch 800 loss: 0.9298599064350128\n",
      "  batch 850 loss: 0.9152035248279572\n",
      "  batch 900 loss: 0.9102888619899749\n",
      "LOSS train 0.91029 valid 0.94008, valid PER 28.58%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.9063650846481324\n",
      "  batch 100 loss: 0.9210390269756317\n",
      "  batch 150 loss: 0.8921858358383179\n",
      "  batch 200 loss: 0.8883315050601959\n",
      "  batch 250 loss: 0.8868248271942138\n",
      "  batch 300 loss: 0.8771779954433441\n",
      "  batch 350 loss: 0.8801375901699067\n",
      "  batch 400 loss: 0.8963784193992614\n",
      "  batch 450 loss: 0.8802016210556031\n",
      "  batch 500 loss: 0.8728763961791992\n",
      "  batch 550 loss: 0.8795361757278443\n",
      "  batch 600 loss: 0.8884165489673614\n",
      "  batch 650 loss: 0.8771063148975372\n",
      "  batch 700 loss: 0.9036220705509186\n",
      "  batch 750 loss: 0.8726877784729004\n",
      "  batch 800 loss: 0.8677712452411651\n",
      "  batch 850 loss: 0.9160277783870697\n",
      "  batch 900 loss: 0.9538978171348572\n",
      "LOSS train 0.95390 valid 0.89095, valid PER 28.35%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8731661593914032\n",
      "  batch 100 loss: 0.8655095052719116\n",
      "  batch 150 loss: 0.852227258682251\n",
      "  batch 200 loss: 0.8396995162963867\n",
      "  batch 250 loss: 0.849737844467163\n",
      "  batch 300 loss: 0.8028268325328827\n",
      "  batch 350 loss: 0.8623940193653107\n",
      "  batch 400 loss: 0.837832189798355\n",
      "  batch 450 loss: 0.8566612219810485\n",
      "  batch 500 loss: 0.883009889125824\n",
      "  batch 550 loss: 0.8212375044822693\n",
      "  batch 600 loss: 0.8641575872898102\n",
      "  batch 650 loss: 0.876399643421173\n",
      "  batch 700 loss: 0.8383820164203644\n",
      "  batch 750 loss: 0.8272986960411072\n",
      "  batch 800 loss: 0.8528278321027756\n",
      "  batch 850 loss: 0.8480948984622956\n",
      "  batch 900 loss: 0.8807298159599304\n",
      "LOSS train 0.88073 valid 0.87227, valid PER 27.09%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7951692473888398\n",
      "  batch 100 loss: 0.8375094473361969\n",
      "  batch 150 loss: 0.821637533903122\n",
      "  batch 200 loss: 0.7926159620285034\n",
      "  batch 250 loss: 0.8210513818264008\n",
      "  batch 300 loss: 0.809567185640335\n",
      "  batch 350 loss: 0.8381249833106995\n",
      "  batch 400 loss: 0.8060131645202637\n",
      "  batch 450 loss: 0.8129216516017914\n",
      "  batch 500 loss: 0.7933016192913055\n",
      "  batch 550 loss: 0.8342628192901611\n",
      "  batch 600 loss: 0.8300914299488068\n",
      "  batch 650 loss: 0.8185215961933135\n",
      "  batch 700 loss: 0.7920647954940796\n",
      "  batch 750 loss: 0.7976290982961655\n",
      "  batch 800 loss: 0.8358685529232025\n",
      "  batch 850 loss: 0.8498670089244843\n",
      "  batch 900 loss: 0.7863599455356598\n",
      "LOSS train 0.78636 valid 0.84123, valid PER 25.78%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7544958460330963\n",
      "  batch 100 loss: 0.765458055138588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 150 loss: 0.7817466545104981\n",
      "  batch 200 loss: 0.8004298949241638\n",
      "  batch 250 loss: 0.8127939867973327\n",
      "  batch 300 loss: 0.7525503706932067\n",
      "  batch 350 loss: 0.7843699550628662\n",
      "  batch 400 loss: 0.7542008453607559\n",
      "  batch 450 loss: 0.7603062313795089\n",
      "  batch 500 loss: 0.807077763080597\n",
      "  batch 550 loss: 0.8168228375911712\n",
      "  batch 600 loss: 0.792718967795372\n",
      "  batch 650 loss: 0.7877954089641571\n",
      "  batch 700 loss: 0.8058598983287811\n",
      "  batch 750 loss: 0.7677250504493713\n",
      "  batch 800 loss: 0.7968602502346038\n",
      "  batch 850 loss: 0.7865158331394195\n",
      "  batch 900 loss: 0.7970047056674957\n",
      "LOSS train 0.79700 valid 0.83991, valid PER 26.59%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.7283470356464385\n",
      "  batch 100 loss: 0.7113836658000946\n",
      "  batch 150 loss: 0.7411256635189056\n",
      "  batch 200 loss: 0.7751183843612671\n",
      "  batch 250 loss: 0.7557965618371963\n",
      "  batch 300 loss: 0.7558820939064026\n",
      "  batch 350 loss: 0.7549328815937042\n",
      "  batch 400 loss: 0.7963260293006897\n",
      "  batch 450 loss: 0.7746511602401733\n",
      "  batch 500 loss: 0.7487483823299408\n",
      "  batch 550 loss: 0.7509062600135803\n",
      "  batch 600 loss: 0.757656324505806\n",
      "  batch 650 loss: 0.8184457314014435\n",
      "  batch 700 loss: 0.7448781955242157\n",
      "  batch 750 loss: 0.730283807516098\n",
      "  batch 800 loss: 0.7686323857307434\n",
      "  batch 850 loss: 0.7868487215042115\n",
      "  batch 900 loss: 0.7840827131271362\n",
      "LOSS train 0.78408 valid 0.81801, valid PER 25.10%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.747604888677597\n",
      "  batch 100 loss: 0.7182885962724685\n",
      "  batch 150 loss: 0.7100794178247452\n",
      "  batch 200 loss: 0.7171442914009094\n",
      "  batch 250 loss: 0.7398342275619507\n",
      "  batch 300 loss: 0.7230995500087738\n",
      "  batch 350 loss: 0.7103477144241332\n",
      "  batch 400 loss: 0.7593201792240143\n",
      "  batch 450 loss: 0.7493038272857666\n",
      "  batch 500 loss: 0.7485980224609375\n",
      "  batch 550 loss: 0.696275081038475\n",
      "  batch 600 loss: 0.7097994869947434\n",
      "  batch 650 loss: 0.7598880410194397\n",
      "  batch 700 loss: 0.7526264345645904\n",
      "  batch 750 loss: 0.7203453254699707\n",
      "  batch 800 loss: 0.70577849984169\n",
      "  batch 850 loss: 0.7737248659133911\n",
      "  batch 900 loss: 0.7520572000741959\n",
      "LOSS train 0.75206 valid 0.78974, valid PER 24.74%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.6973910844326019\n",
      "  batch 100 loss: 0.70853551030159\n",
      "  batch 150 loss: 0.6956459033489227\n",
      "  batch 200 loss: 0.7083919954299926\n",
      "  batch 250 loss: 0.7077984309196472\n",
      "  batch 300 loss: 0.7015258538722992\n",
      "  batch 350 loss: 0.7114801114797592\n",
      "  batch 400 loss: 0.7339190590381622\n",
      "  batch 450 loss: 0.7331011039018631\n",
      "  batch 500 loss: 0.7030353140830994\n",
      "  batch 550 loss: 0.748297016620636\n",
      "  batch 600 loss: 0.7064174348115921\n",
      "  batch 650 loss: 0.7261770260334015\n",
      "  batch 700 loss: 0.7209819000959397\n",
      "  batch 750 loss: 0.691811249256134\n",
      "  batch 800 loss: 0.7367092370986938\n",
      "  batch 850 loss: 0.7782858753204346\n",
      "  batch 900 loss: 0.7324886071681976\n",
      "LOSS train 0.73249 valid 0.81496, valid PER 25.24%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.6863567966222763\n",
      "  batch 100 loss: 0.6932651722431182\n",
      "  batch 150 loss: 0.6743186593055726\n",
      "  batch 200 loss: 0.6914214086532593\n",
      "  batch 250 loss: 0.7041794139146805\n",
      "  batch 300 loss: 0.7179700452089309\n",
      "  batch 350 loss: 0.6693358379602432\n",
      "  batch 400 loss: 0.6776545614004135\n",
      "  batch 450 loss: 0.6869984382390976\n",
      "  batch 500 loss: 0.6912615263462066\n",
      "  batch 550 loss: 0.7075777864456176\n",
      "  batch 600 loss: 0.6777710103988648\n",
      "  batch 650 loss: 0.7082378327846527\n",
      "  batch 700 loss: 0.7264830708503723\n",
      "  batch 750 loss: 0.6931853359937667\n",
      "  batch 800 loss: 0.6668931394815445\n",
      "  batch 850 loss: 0.7115390276908875\n",
      "  batch 900 loss: 0.7251983493566513\n",
      "LOSS train 0.72520 valid 0.79443, valid PER 24.53%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6673932260274887\n",
      "  batch 100 loss: 0.670303708910942\n",
      "  batch 150 loss: 0.6694684076309204\n",
      "  batch 200 loss: 0.6805972117185592\n",
      "  batch 250 loss: 0.6716676038503647\n",
      "  batch 300 loss: 0.6573877447843551\n",
      "  batch 350 loss: 0.6638360625505447\n",
      "  batch 400 loss: 0.6699009776115418\n",
      "  batch 450 loss: 0.6801471316814423\n",
      "  batch 500 loss: 0.6541925245523452\n",
      "  batch 550 loss: 0.6645727068185806\n",
      "  batch 600 loss: 0.6948397552967072\n",
      "  batch 650 loss: 0.7000614017248153\n",
      "  batch 700 loss: 0.6922275871038437\n",
      "  batch 750 loss: 0.6891112142801284\n",
      "  batch 800 loss: 0.681371266245842\n",
      "  batch 850 loss: 0.6629317551851273\n",
      "  batch 900 loss: 0.6663337278366089\n",
      "LOSS train 0.66633 valid 0.79390, valid PER 24.39%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.651211416721344\n",
      "  batch 100 loss: 0.6330184704065323\n",
      "  batch 150 loss: 0.6458095353841782\n",
      "  batch 200 loss: 0.6690463596582412\n",
      "  batch 250 loss: 0.6789710152149201\n",
      "  batch 300 loss: 0.650290607213974\n",
      "  batch 350 loss: 0.6729209208488465\n",
      "  batch 400 loss: 0.6656590747833252\n",
      "  batch 450 loss: 0.6726001995801926\n",
      "  batch 500 loss: 0.6359988421201705\n",
      "  batch 550 loss: 0.6545729863643647\n",
      "  batch 600 loss: 0.6490439546108245\n",
      "  batch 650 loss: 0.6666433781385421\n",
      "  batch 700 loss: 0.6289742285013199\n",
      "  batch 750 loss: 0.6606352394819259\n",
      "  batch 800 loss: 0.6613806080818176\n",
      "  batch 850 loss: 0.6575963044166565\n",
      "  batch 900 loss: 0.653434436917305\n",
      "LOSS train 0.65343 valid 0.78150, valid PER 23.57%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.6308712136745452\n",
      "  batch 100 loss: 0.6553383088111877\n",
      "  batch 150 loss: 0.6231186348199844\n",
      "  batch 200 loss: 0.6282868242263794\n",
      "  batch 250 loss: 0.6371711730957031\n",
      "  batch 300 loss: 0.6291134202480316\n",
      "  batch 350 loss: 0.6178095412254333\n",
      "  batch 400 loss: 0.6521054273843765\n",
      "  batch 450 loss: 0.648735790848732\n",
      "  batch 500 loss: 0.6293322390317917\n",
      "  batch 550 loss: 0.6351348000764847\n",
      "  batch 600 loss: 0.6633702629804611\n",
      "  batch 650 loss: 0.643171905875206\n",
      "  batch 700 loss: 0.6317527151107788\n",
      "  batch 750 loss: 0.6180152064561844\n",
      "  batch 800 loss: 0.6188130414485932\n",
      "  batch 850 loss: 0.6614527773857116\n",
      "  batch 900 loss: 0.6267511796951294\n",
      "LOSS train 0.62675 valid 0.77971, valid PER 23.22%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.6026183772087097\n",
      "  batch 100 loss: 0.625768923163414\n",
      "  batch 150 loss: 0.638158866763115\n",
      "  batch 200 loss: 0.6177832740545273\n",
      "  batch 250 loss: 0.6331335896253586\n",
      "  batch 300 loss: 0.6082028090953827\n",
      "  batch 350 loss: 0.6332795894145966\n",
      "  batch 400 loss: 0.5841807460784912\n",
      "  batch 450 loss: 0.6356244647502899\n",
      "  batch 500 loss: 0.6206754636764527\n",
      "  batch 550 loss: 0.6442540299892425\n",
      "  batch 600 loss: 0.5826393675804138\n",
      "  batch 650 loss: 0.61420170545578\n",
      "  batch 700 loss: 0.6523061871528626\n",
      "  batch 750 loss: 0.6162085729837418\n",
      "  batch 800 loss: 0.6049174696207047\n",
      "  batch 850 loss: 0.6161290442943573\n",
      "  batch 900 loss: 0.6466184377670288\n",
      "LOSS train 0.64662 valid 0.77010, valid PER 23.56%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5698120868206025\n",
      "  batch 100 loss: 0.5912727189064025\n",
      "  batch 150 loss: 0.5898641139268875\n",
      "  batch 200 loss: 0.5901941448450089\n",
      "  batch 250 loss: 0.6136914664506912\n",
      "  batch 300 loss: 0.5895616823434829\n",
      "  batch 350 loss: 0.6095170736312866\n",
      "  batch 400 loss: 0.6153639209270477\n",
      "  batch 450 loss: 0.6045901960134507\n",
      "  batch 500 loss: 0.6192011195421219\n",
      "  batch 550 loss: 0.5838879168033599\n",
      "  batch 600 loss: 0.6157538133859635\n",
      "  batch 650 loss: 0.6594706076383591\n",
      "  batch 700 loss: 0.6020805281400681\n",
      "  batch 750 loss: 0.5912415915727616\n",
      "  batch 800 loss: 0.6304368889331817\n",
      "  batch 850 loss: 0.6048828583955764\n",
      "  batch 900 loss: 0.6157937175035477\n",
      "LOSS train 0.61579 valid 0.77255, valid PER 23.09%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.5761362218856811\n",
      "  batch 100 loss: 0.5592924892902374\n",
      "  batch 150 loss: 0.5846157890558242\n",
      "  batch 200 loss: 0.6042052686214447\n",
      "  batch 250 loss: 0.5848853576183319\n",
      "  batch 300 loss: 0.6088708120584488\n",
      "  batch 350 loss: 0.5687569934129715\n",
      "  batch 400 loss: 0.581307725906372\n",
      "  batch 450 loss: 0.594053081870079\n",
      "  batch 500 loss: 0.5738407701253891\n",
      "  batch 550 loss: 0.6382910710573196\n",
      "  batch 600 loss: 0.565403230190277\n",
      "  batch 650 loss: 0.5953035193681717\n",
      "  batch 700 loss: 0.5774069535732269\n",
      "  batch 750 loss: 0.5689285254478454\n",
      "  batch 800 loss: 0.6234950852394104\n",
      "  batch 850 loss: 0.6052657854557038\n",
      "  batch 900 loss: 0.6080264729261399\n",
      "LOSS train 0.60803 valid 0.75695, valid PER 22.64%\n",
      "Training finished in 5.0 minutes.\n",
      "Model saved to checkpoints/20231205_154752/model_20\n",
      "Loading model from checkpoints/20231205_154752/model_20\n",
      "For dropout rate 0.5 the best model has SUB: 14.92%, DEL: 6.84%, INS: 2.34%, COR: 78.24%, PER: 24.09%\n",
      "End dropout tuning For 2 Layer LSTM\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout_between_layer\n",
    "from datetime import datetime\n",
    "from trainer import train\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start dropout tuning For 2 Layer LSTM, with Dropout between layer\")\n",
    "\n",
    "dropout_rates=[0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    model_with_dropout = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "    num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "    print('Total number of model parameters is {}'.format(num_params))\n",
    "    start = datetime.now()\n",
    "    model_with_dropout.to(args.device)\n",
    "    model_path = train(model_with_dropout, args)\n",
    "    end = datetime.now()\n",
    "    duration = (end - start).total_seconds()\n",
    "    print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "    print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print('Loading model from {}'.format(model_path))\n",
    "    model_with_dropout.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model_with_dropout.eval()\n",
    "    results = decode(model_with_dropout, args, args.test_json)\n",
    "    print(\"For dropout rate \"+str(dropout_rate)+\" the best model has \"+\"SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%\".format(*results))\n",
    "\n",
    "print(\"End dropout tuning For 2 Layer LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a2ba7c",
   "metadata": {},
   "source": [
    "### Optimiser Study here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992df26e",
   "metadata": {},
   "source": [
    "#### Try Adam Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762e510",
   "metadata": {},
   "source": [
    "###### Adam Optimiser setup (Original setup start with learning rate of 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a141a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'val_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 128,\n",
    "        'concat': 1,\n",
    "        'lr': 0.01,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "args = namedtuple('x', args)(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4cacf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters is 166952\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.356326017379761\n",
      "  batch 100 loss: 3.1330445051193236\n",
      "  batch 150 loss: 2.8419583797454835\n",
      "  batch 200 loss: 2.561711673736572\n",
      "  batch 250 loss: 2.3617538261413573\n",
      "  batch 300 loss: 2.1399513053894044\n",
      "  batch 350 loss: 1.9726394629478454\n",
      "  batch 400 loss: 1.9568773770332337\n",
      "  batch 450 loss: 1.8864873385429382\n",
      "  batch 500 loss: 1.797443811893463\n",
      "  batch 550 loss: 1.7341630864143371\n",
      "  batch 600 loss: 1.7134889769554138\n",
      "  batch 650 loss: 1.6561346220970155\n",
      "  batch 700 loss: 1.7090842294692994\n",
      "  batch 750 loss: 1.6420587301254272\n",
      "  batch 800 loss: 1.648264808654785\n",
      "  batch 850 loss: 1.5968303894996643\n",
      "  batch 900 loss: 1.5695223665237428\n",
      "LOSS train 1.56952 valid 1.57492, valid PER 50.75%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.5474369525909424\n",
      "  batch 100 loss: 1.5270094859600067\n",
      "  batch 150 loss: 1.5477495312690734\n",
      "  batch 200 loss: 1.5761040234565735\n",
      "  batch 250 loss: 1.545525939464569\n",
      "  batch 300 loss: 1.5422319173812866\n",
      "  batch 350 loss: 1.4920556378364562\n",
      "  batch 400 loss: 1.5635398888587952\n",
      "  batch 450 loss: 1.5257338762283326\n",
      "  batch 500 loss: 1.4952103805541992\n",
      "  batch 550 loss: 1.5484512066841125\n",
      "  batch 600 loss: 1.4802296209335326\n",
      "  batch 650 loss: 1.5036049914360046\n",
      "  batch 700 loss: 1.4670267653465272\n",
      "  batch 750 loss: 1.5253474688529969\n",
      "  batch 800 loss: 1.4851404786109925\n",
      "  batch 850 loss: 1.4550790214538574\n",
      "  batch 900 loss: 1.4875486516952514\n",
      "LOSS train 1.48755 valid 1.46269, valid PER 46.57%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4302307629585267\n",
      "  batch 100 loss: 1.4247882628440858\n",
      "  batch 150 loss: 1.428119149208069\n",
      "  batch 200 loss: 1.449331283569336\n",
      "  batch 250 loss: 1.4795065855979919\n",
      "  batch 300 loss: 1.6377298164367675\n",
      "  batch 350 loss: 1.6317272210121154\n",
      "  batch 400 loss: 1.5714254951477051\n",
      "  batch 450 loss: 1.5180228233337403\n",
      "  batch 500 loss: 1.55448011636734\n",
      "  batch 550 loss: 1.4967997074127197\n",
      "  batch 600 loss: 1.4534127736091613\n",
      "  batch 650 loss: 1.46831476688385\n",
      "  batch 700 loss: 1.475775363445282\n",
      "  batch 750 loss: 1.5493613076210022\n",
      "  batch 800 loss: 1.444367437362671\n",
      "  batch 850 loss: 1.4627161526679993\n",
      "  batch 900 loss: 1.401395981311798\n",
      "LOSS train 1.40140 valid 1.47296, valid PER 45.27%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.3886298656463623\n",
      "  batch 100 loss: 1.4138471651077271\n",
      "  batch 150 loss: 1.3773029172420501\n",
      "  batch 200 loss: 1.4087059712409973\n",
      "  batch 250 loss: 1.407333149909973\n",
      "  batch 300 loss: 1.423729622364044\n",
      "  batch 350 loss: 1.381827826499939\n",
      "  batch 400 loss: 1.435449502468109\n",
      "  batch 450 loss: 1.4526770210266113\n",
      "  batch 500 loss: 1.405384464263916\n",
      "  batch 550 loss: 1.4305956697463988\n",
      "  batch 600 loss: 1.4173257732391358\n",
      "  batch 650 loss: 1.4101577186584473\n",
      "  batch 700 loss: 1.3550627303123475\n",
      "  batch 750 loss: 1.3269486784934998\n",
      "  batch 800 loss: 1.3081317162513733\n",
      "  batch 850 loss: 1.3617096257209778\n",
      "  batch 900 loss: 1.4068611335754395\n",
      "LOSS train 1.40686 valid 1.35163, valid PER 41.89%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.2919630050659179\n",
      "  batch 100 loss: 1.2916299831867217\n",
      "  batch 150 loss: 1.3730022096633911\n",
      "  batch 200 loss: 1.2689142560958862\n",
      "  batch 250 loss: 1.3070989418029786\n",
      "  batch 300 loss: 1.295923398733139\n",
      "  batch 350 loss: 1.3291483664512633\n",
      "  batch 400 loss: 1.3264323472976685\n",
      "  batch 450 loss: 1.3095070290565491\n",
      "  batch 500 loss: 1.3315545201301575\n",
      "  batch 550 loss: 1.2653757452964782\n",
      "  batch 600 loss: 1.3201899135112762\n",
      "  batch 650 loss: 1.2875234293937683\n",
      "  batch 700 loss: 1.3231615483760835\n",
      "  batch 750 loss: 1.2720748949050904\n",
      "  batch 800 loss: 1.3013607823848725\n",
      "  batch 850 loss: 1.307726263999939\n",
      "  batch 900 loss: 1.3216836225986481\n",
      "LOSS train 1.32168 valid 1.34444, valid PER 40.78%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.2758348083496094\n",
      "  batch 100 loss: 1.2711329388618469\n",
      "  batch 150 loss: 1.2243079257011413\n",
      "  batch 200 loss: 1.262287654876709\n",
      "  batch 250 loss: 1.2936150979995729\n",
      "  batch 300 loss: 1.2718500792980194\n",
      "  batch 350 loss: 1.25744295835495\n",
      "  batch 400 loss: 1.2616099143028259\n",
      "  batch 450 loss: 1.2629011690616607\n",
      "  batch 500 loss: 1.2440286350250245\n",
      "  batch 550 loss: 1.2745157039165498\n",
      "  batch 600 loss: 1.220014910697937\n",
      "  batch 650 loss: 1.2524285340309143\n",
      "  batch 700 loss: 1.2600775229930878\n",
      "  batch 750 loss: 1.2209335470199585\n",
      "  batch 800 loss: 1.2540477061271667\n",
      "  batch 850 loss: 1.2526817834377288\n",
      "  batch 900 loss: 1.2618205904960633\n",
      "LOSS train 1.26182 valid 1.27289, valid PER 40.59%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.2188418459892274\n",
      "  batch 100 loss: 1.2621419930458069\n",
      "  batch 150 loss: 1.2065017199516297\n",
      "  batch 200 loss: 1.1845108771324158\n",
      "  batch 250 loss: 1.2078925085067749\n",
      "  batch 300 loss: 1.181524099111557\n",
      "  batch 350 loss: 1.2023655927181245\n",
      "  batch 400 loss: 1.215173864364624\n",
      "  batch 450 loss: 1.2260970664024353\n",
      "  batch 500 loss: 1.1980058479309081\n",
      "  batch 550 loss: 1.2312458622455598\n",
      "  batch 600 loss: 1.2728857481479645\n",
      "  batch 650 loss: 1.2235301434993744\n",
      "  batch 700 loss: 1.2203524732589721\n",
      "  batch 750 loss: 1.2197032690048217\n",
      "  batch 800 loss: 1.2125063419342041\n",
      "  batch 850 loss: 1.238434113264084\n",
      "  batch 900 loss: 1.2465424466133117\n",
      "LOSS train 1.24654 valid 1.27641, valid PER 39.43%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.1783300304412843\n",
      "  batch 100 loss: 1.173218799829483\n",
      "  batch 150 loss: 1.1813891315460205\n",
      "  batch 200 loss: 1.147455245256424\n",
      "  batch 250 loss: 1.1843860566616058\n",
      "  batch 300 loss: 1.1121928882598877\n",
      "  batch 350 loss: 1.199073108434677\n",
      "  batch 400 loss: 1.1656158149242402\n",
      "  batch 450 loss: 1.1974010694026946\n",
      "  batch 500 loss: 1.2243623757362365\n",
      "  batch 550 loss: 1.208049898147583\n",
      "  batch 600 loss: 1.2382756471633911\n",
      "  batch 650 loss: 1.2355476999282837\n",
      "  batch 700 loss: 1.1981872618198395\n",
      "  batch 750 loss: 1.1890185177326202\n",
      "  batch 800 loss: 1.204565827846527\n",
      "  batch 850 loss: 1.1859800219535828\n",
      "  batch 900 loss: 1.206971185207367\n",
      "LOSS train 1.20697 valid 1.26677, valid PER 38.46%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 1.114147584438324\n",
      "  batch 100 loss: 1.1626976454257965\n",
      "  batch 150 loss: 1.1537888658046722\n",
      "  batch 200 loss: 1.1092070746421814\n",
      "  batch 250 loss: 1.1604195153713226\n",
      "  batch 300 loss: 1.1583888471126556\n",
      "  batch 350 loss: 1.1798584461212158\n",
      "  batch 400 loss: 1.2009411013126374\n",
      "  batch 450 loss: 1.175886423587799\n",
      "  batch 500 loss: 1.1374593448638917\n",
      "  batch 550 loss: 1.1652270662784576\n",
      "  batch 600 loss: 1.2058725988864898\n",
      "  batch 650 loss: 1.161648360490799\n",
      "  batch 700 loss: 1.1676940214633942\n",
      "  batch 750 loss: 1.1977306282520295\n",
      "  batch 800 loss: 1.218612608909607\n",
      "  batch 850 loss: 1.2142780673503877\n",
      "  batch 900 loss: 1.1681936049461366\n",
      "LOSS train 1.16819 valid 1.26162, valid PER 38.23%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 1.118037796020508\n",
      "  batch 100 loss: 1.1368311822414399\n",
      "  batch 150 loss: 1.1600837445259093\n",
      "  batch 200 loss: 1.1578565657138824\n",
      "  batch 250 loss: 1.1678914678096772\n",
      "  batch 300 loss: 1.1154902648925782\n",
      "  batch 350 loss: 1.1596937036514283\n",
      "  batch 400 loss: 1.1162163054943084\n",
      "  batch 450 loss: 1.1069030022621156\n",
      "  batch 500 loss: 1.1684223186969758\n",
      "  batch 550 loss: 1.1772471392154693\n",
      "  batch 600 loss: 1.1514859521389007\n",
      "  batch 650 loss: 1.1794890248775483\n",
      "  batch 700 loss: 1.1890106225013732\n",
      "  batch 750 loss: 1.1497867953777314\n",
      "  batch 800 loss: 1.1724796211719513\n",
      "  batch 850 loss: 1.1666426467895508\n",
      "  batch 900 loss: 1.1878401803970338\n",
      "LOSS train 1.18784 valid 1.23569, valid PER 37.53%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 1.114526174068451\n",
      "  batch 100 loss: 1.0986888372898103\n",
      "  batch 150 loss: 1.0973649799823761\n",
      "  batch 200 loss: 1.1555234909057617\n",
      "  batch 250 loss: 1.1550422239303588\n",
      "  batch 300 loss: 1.130422751903534\n",
      "  batch 350 loss: 1.136036355495453\n",
      "  batch 400 loss: 1.1532007801532744\n",
      "  batch 450 loss: 1.145215287208557\n",
      "  batch 500 loss: 1.1592906713485718\n",
      "  batch 550 loss: 1.1324838125705718\n",
      "  batch 600 loss: 1.1438939106464385\n",
      "  batch 650 loss: 1.2142422151565553\n",
      "  batch 700 loss: 1.1042902040481568\n",
      "  batch 750 loss: 1.1284662449359895\n",
      "  batch 800 loss: 1.146821300983429\n",
      "  batch 850 loss: 1.176410825252533\n",
      "  batch 900 loss: 1.1692765927314759\n",
      "LOSS train 1.16928 valid 1.22358, valid PER 37.08%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 1.0957744193077088\n",
      "  batch 100 loss: 1.0952240324020386\n",
      "  batch 150 loss: 1.0855587756633758\n",
      "  batch 200 loss: 1.1017900562286378\n",
      "  batch 250 loss: 1.1658900201320648\n",
      "  batch 300 loss: 1.1229320073127746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 350 loss: 1.0934350502490997\n",
      "  batch 400 loss: 1.1315586709976195\n",
      "  batch 450 loss: 1.152372659444809\n",
      "  batch 500 loss: 1.1632296407222749\n",
      "  batch 550 loss: 1.0659365367889404\n",
      "  batch 600 loss: 1.10746701836586\n",
      "  batch 650 loss: 1.1494214141368866\n",
      "  batch 700 loss: 1.1435328757762908\n",
      "  batch 750 loss: 1.1246064937114715\n",
      "  batch 800 loss: 1.0996622264385223\n",
      "  batch 850 loss: 1.1402192294597626\n",
      "  batch 900 loss: 1.153761556148529\n",
      "LOSS train 1.15376 valid 1.22602, valid PER 37.63%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 1.067593069076538\n",
      "  batch 100 loss: 1.1089253973960878\n",
      "  batch 150 loss: 1.107014797925949\n",
      "  batch 200 loss: 1.1286681735515594\n",
      "  batch 250 loss: 1.1153715670108795\n",
      "  batch 300 loss: 1.11741379737854\n",
      "  batch 350 loss: 1.125499370098114\n",
      "  batch 400 loss: 1.136279535293579\n",
      "  batch 450 loss: 1.1461881160736085\n",
      "  batch 500 loss: 1.101652899980545\n",
      "  batch 550 loss: 1.116063462495804\n",
      "  batch 600 loss: 1.0991935217380524\n",
      "  batch 650 loss: 1.1078052914142609\n",
      "  batch 700 loss: 1.1080184400081634\n",
      "  batch 750 loss: 1.1012226021289826\n",
      "  batch 800 loss: 1.1245033895969392\n",
      "  batch 850 loss: 1.1449981796741486\n",
      "  batch 900 loss: 1.1497714960575103\n",
      "LOSS train 1.14977 valid 1.23843, valid PER 37.26%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 1.0417991399765014\n",
      "  batch 100 loss: 1.1189849698543548\n",
      "  batch 150 loss: 1.0812446856498719\n",
      "  batch 200 loss: 1.0869334602355958\n",
      "  batch 250 loss: 1.1066083264350892\n",
      "  batch 300 loss: 1.1294581305980682\n",
      "  batch 350 loss: 1.0945210039615632\n",
      "  batch 400 loss: 1.105142982006073\n",
      "  batch 450 loss: 1.0987180292606353\n",
      "  batch 500 loss: 1.1067558801174164\n",
      "  batch 550 loss: 1.1473868334293365\n",
      "  batch 600 loss: 1.1075685465335845\n",
      "  batch 650 loss: 1.1249217128753661\n",
      "  batch 700 loss: 1.1332530272006989\n",
      "  batch 750 loss: 1.0909758913516998\n",
      "  batch 800 loss: 1.0880493664741515\n",
      "  batch 850 loss: 1.1562369072437286\n",
      "  batch 900 loss: 1.129296737909317\n",
      "LOSS train 1.12930 valid 1.21257, valid PER 37.08%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 1.0791868042945862\n",
      "  batch 100 loss: 1.0660722959041595\n",
      "  batch 150 loss: 1.0638097214698792\n",
      "  batch 200 loss: 1.1123707568645478\n",
      "  batch 250 loss: 1.0927304589748383\n",
      "  batch 300 loss: 1.0766297507286071\n",
      "  batch 350 loss: 1.0854035663604735\n",
      "  batch 400 loss: 1.0756165981292725\n",
      "  batch 450 loss: 1.0835229527950287\n",
      "  batch 500 loss: 1.0610943233966827\n",
      "  batch 550 loss: 1.0941481137275695\n",
      "  batch 600 loss: 1.0903640604019165\n",
      "  batch 650 loss: 1.1037703800201415\n",
      "  batch 700 loss: 1.1008745121955872\n",
      "  batch 750 loss: 1.0949112451076508\n",
      "  batch 800 loss: 1.0841604363918305\n",
      "  batch 850 loss: 1.081031972169876\n",
      "  batch 900 loss: 1.138611252307892\n",
      "LOSS train 1.13861 valid 1.21875, valid PER 37.34%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 1.1073154056072234\n",
      "  batch 100 loss: 1.021431803703308\n",
      "  batch 150 loss: 1.068662724494934\n",
      "  batch 200 loss: 1.0815925073623658\n",
      "  batch 250 loss: 1.0986068844795227\n",
      "  batch 300 loss: 1.088708690404892\n",
      "  batch 350 loss: 1.092880803346634\n",
      "  batch 400 loss: 1.0732123124599457\n",
      "  batch 450 loss: 1.1098440635204314\n",
      "  batch 500 loss: 1.0484976005554199\n",
      "  batch 550 loss: 1.102842252254486\n",
      "  batch 600 loss: 1.0805144786834717\n",
      "  batch 650 loss: 1.093008371591568\n",
      "  batch 700 loss: 1.0837631046772003\n",
      "  batch 750 loss: 1.0877346217632293\n",
      "  batch 800 loss: 1.1238228034973146\n",
      "  batch 850 loss: 1.1129545211791991\n",
      "  batch 900 loss: 1.1164448487758636\n",
      "LOSS train 1.11644 valid 1.25970, valid PER 38.49%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 1.0858013772964477\n",
      "  batch 100 loss: 1.0717774617671967\n",
      "  batch 150 loss: 1.060873919725418\n",
      "  batch 200 loss: 1.0404774630069733\n",
      "  batch 250 loss: 1.091494812965393\n",
      "  batch 300 loss: 1.093409321308136\n",
      "  batch 350 loss: 1.0478318583965303\n",
      "  batch 400 loss: 1.1208594524860382\n",
      "  batch 450 loss: 1.1084820425510407\n",
      "  batch 500 loss: 1.0779514074325562\n",
      "  batch 550 loss: 1.096657783985138\n",
      "  batch 600 loss: 1.1330625796318055\n",
      "  batch 650 loss: 1.0964784824848175\n",
      "  batch 700 loss: 1.049894301891327\n",
      "  batch 750 loss: 1.0478806924819946\n",
      "  batch 800 loss: 1.0729557085037231\n",
      "  batch 850 loss: 1.0905399215221405\n",
      "  batch 900 loss: 1.0962491405010224\n",
      "LOSS train 1.09625 valid 1.22202, valid PER 36.92%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 1.0872005438804626\n",
      "  batch 100 loss: 1.1003011345863343\n",
      "  batch 150 loss: 1.1088987624645232\n",
      "  batch 200 loss: 1.0818470799922943\n",
      "  batch 250 loss: 1.0850981068611145\n",
      "  batch 300 loss: 1.078638721704483\n",
      "  batch 350 loss: 1.1150175416469574\n",
      "  batch 400 loss: 1.0484326410293578\n",
      "  batch 450 loss: 1.1021103727817536\n",
      "  batch 500 loss: 1.058433530330658\n",
      "  batch 550 loss: 1.0917306983470916\n",
      "  batch 600 loss: 1.0502587234973908\n",
      "  batch 650 loss: 1.0589812076091767\n",
      "  batch 700 loss: 1.0974287962913514\n",
      "  batch 750 loss: 1.0924518430233001\n",
      "  batch 800 loss: 1.088109667301178\n",
      "  batch 850 loss: 1.0525961244106292\n",
      "  batch 900 loss: 1.0968963408470154\n",
      "LOSS train 1.09690 valid 1.22568, valid PER 36.73%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 1.0194323348999024\n",
      "  batch 100 loss: 1.0211388385295868\n",
      "  batch 150 loss: 1.0296532475948335\n",
      "  batch 200 loss: 1.0750692939758302\n",
      "  batch 250 loss: 1.1185306763648988\n",
      "  batch 300 loss: 1.0850870716571808\n",
      "  batch 350 loss: 1.0650264263153075\n",
      "  batch 400 loss: 1.0816081929206849\n",
      "  batch 450 loss: 1.0882053005695342\n",
      "  batch 500 loss: 1.068117505311966\n",
      "  batch 550 loss: 1.0550106823444367\n",
      "  batch 600 loss: 1.0683657121658325\n",
      "  batch 650 loss: 1.1520835971832275\n",
      "  batch 700 loss: 1.0490435087680816\n",
      "  batch 750 loss: 1.0332632398605346\n",
      "  batch 800 loss: 1.092230087518692\n",
      "  batch 850 loss: 1.0737052500247954\n",
      "  batch 900 loss: 1.1014503514766694\n",
      "LOSS train 1.10145 valid 1.20367, valid PER 36.53%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 1.0463848412036896\n",
      "  batch 100 loss: 1.0687535214424133\n",
      "  batch 150 loss: 1.0653358829021453\n",
      "  batch 200 loss: 1.0577985036373139\n",
      "  batch 250 loss: 1.0627217304706573\n",
      "  batch 300 loss: 1.090232663154602\n",
      "  batch 350 loss: 1.0462479603290558\n",
      "  batch 400 loss: 1.092117519378662\n",
      "  batch 450 loss: 1.083805410861969\n",
      "  batch 500 loss: 1.0519845521450042\n",
      "  batch 550 loss: 1.1255798518657685\n",
      "  batch 600 loss: 1.0638926994800568\n",
      "  batch 650 loss: 1.1380202412605285\n",
      "  batch 700 loss: 1.1057164478302002\n",
      "  batch 750 loss: 1.039332035779953\n",
      "  batch 800 loss: 1.0914014065265656\n",
      "  batch 850 loss: 1.1086547076702118\n",
      "  batch 900 loss: 1.1082852578163147\n",
      "LOSS train 1.10829 valid 1.21267, valid PER 36.24%\n",
      "Training finished in 3.0 minutes.\n",
      "Model saved to checkpoints/20231205_210803/model_19\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "from datetime import datetime\n",
    "from trainer_Adam import train\n",
    "\n",
    "\n",
    "model = models.BiLSTM(\n",
    "    args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab))\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of model parameters is {}'.format(num_params))\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "model.to(args.device)\n",
    "model_path = train(model, args)\n",
    "end = datetime.now()\n",
    "duration = (end - start).total_seconds()\n",
    "print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "print('Model saved to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9371675",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'val_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 128,\n",
    "        'concat': 1,\n",
    "        'lr': 0.001,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "args = namedtuple('x', args)(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66de54f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters is 166952\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 8.703279008865357\n",
      "  batch 100 loss: 3.3648267698287966\n",
      "  batch 150 loss: 3.2602467775344848\n",
      "  batch 200 loss: 3.208204164505005\n",
      "  batch 250 loss: 3.131103720664978\n",
      "  batch 300 loss: 3.054433708190918\n",
      "  batch 350 loss: 2.9456876945495605\n",
      "  batch 400 loss: 2.868926486968994\n",
      "  batch 450 loss: 2.8202915954589844\n",
      "  batch 500 loss: 2.7227843618392944\n",
      "  batch 550 loss: 2.666769208908081\n",
      "  batch 600 loss: 2.5605522918701173\n",
      "  batch 650 loss: 2.4976044416427614\n",
      "  batch 700 loss: 2.399198498725891\n",
      "  batch 750 loss: 2.3446082973480227\n",
      "  batch 800 loss: 2.283764147758484\n",
      "  batch 850 loss: 2.24986615896225\n",
      "  batch 900 loss: 2.164026358127594\n",
      "LOSS train 2.16403 valid 2.14735, valid PER 77.52%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 2.1206107711791993\n",
      "  batch 100 loss: 2.041600911617279\n",
      "  batch 150 loss: 1.9594013357162476\n",
      "  batch 200 loss: 2.004628264904022\n",
      "  batch 250 loss: 1.9373662757873535\n",
      "  batch 300 loss: 1.878817048072815\n",
      "  batch 350 loss: 1.8030624032020568\n",
      "  batch 400 loss: 1.8035776972770692\n",
      "  batch 450 loss: 1.7567819881439208\n",
      "  batch 500 loss: 1.7553316235542298\n",
      "  batch 550 loss: 1.7605349159240722\n",
      "  batch 600 loss: 1.7050267696380614\n",
      "  batch 650 loss: 1.73072815656662\n",
      "  batch 700 loss: 1.675780816078186\n",
      "  batch 750 loss: 1.6573375725746156\n",
      "  batch 800 loss: 1.5931312799453736\n",
      "  batch 850 loss: 1.604389476776123\n",
      "  batch 900 loss: 1.6383951950073241\n",
      "LOSS train 1.63840 valid 1.61347, valid PER 61.43%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.5978309798240662\n",
      "  batch 100 loss: 1.5507041168212892\n",
      "  batch 150 loss: 1.546634588241577\n",
      "  batch 200 loss: 1.5366473364830018\n",
      "  batch 250 loss: 1.51157710313797\n",
      "  batch 300 loss: 1.49229718208313\n",
      "  batch 350 loss: 1.5247433257102967\n",
      "  batch 400 loss: 1.4978370332717896\n",
      "  batch 450 loss: 1.4897469782829285\n",
      "  batch 500 loss: 1.4655992102622986\n",
      "  batch 550 loss: 1.4714495515823365\n",
      "  batch 600 loss: 1.4327962279319764\n",
      "  batch 650 loss: 1.4120820617675782\n",
      "  batch 700 loss: 1.4293555593490601\n",
      "  batch 750 loss: 1.4911818480491639\n",
      "  batch 800 loss: 1.4089117407798768\n",
      "  batch 850 loss: 1.4391319847106934\n",
      "  batch 900 loss: 1.3752091217041016\n",
      "LOSS train 1.37521 valid 1.42998, valid PER 51.23%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.3638615918159485\n",
      "  batch 100 loss: 1.3896212315559386\n",
      "  batch 150 loss: 1.3335543990135192\n",
      "  batch 200 loss: 1.3767804026603698\n",
      "  batch 250 loss: 1.3762696743011475\n",
      "  batch 300 loss: 1.375822160243988\n",
      "  batch 350 loss: 1.2994161796569825\n",
      "  batch 400 loss: 1.3363113355636598\n",
      "  batch 450 loss: 1.3101724076271057\n",
      "  batch 500 loss: 1.2945100307464599\n",
      "  batch 550 loss: 1.3244503188133239\n",
      "  batch 600 loss: 1.3329011416435241\n",
      "  batch 650 loss: 1.301614258289337\n",
      "  batch 700 loss: 1.290935113430023\n",
      "  batch 750 loss: 1.2562797737121583\n",
      "  batch 800 loss: 1.2230342161655425\n",
      "  batch 850 loss: 1.2678821563720704\n",
      "  batch 900 loss: 1.281199951171875\n",
      "LOSS train 1.28120 valid 1.27929, valid PER 40.93%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.2197519886493682\n",
      "  batch 100 loss: 1.2070985233783722\n",
      "  batch 150 loss: 1.2486372637748717\n",
      "  batch 200 loss: 1.1668117797374726\n",
      "  batch 250 loss: 1.188209937810898\n",
      "  batch 300 loss: 1.221814147233963\n",
      "  batch 350 loss: 1.2460582852363586\n",
      "  batch 400 loss: 1.220126576423645\n",
      "  batch 450 loss: 1.2161192095279694\n",
      "  batch 500 loss: 1.2128844559192657\n",
      "  batch 550 loss: 1.1603156268596648\n",
      "  batch 600 loss: 1.2053048634529113\n",
      "  batch 650 loss: 1.1772573065757752\n",
      "  batch 700 loss: 1.2034792292118073\n",
      "  batch 750 loss: 1.1456182062625886\n",
      "  batch 800 loss: 1.1481575059890747\n",
      "  batch 850 loss: 1.194233731031418\n",
      "  batch 900 loss: 1.1701165449619293\n",
      "LOSS train 1.17012 valid 1.21371, valid PER 38.43%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.1553121864795686\n",
      "  batch 100 loss: 1.1163894867897033\n",
      "  batch 150 loss: 1.0956973493099214\n",
      "  batch 200 loss: 1.1253649151325227\n",
      "  batch 250 loss: 1.1516876137256622\n",
      "  batch 300 loss: 1.1114724528789521\n",
      "  batch 350 loss: 1.1235251379013063\n",
      "  batch 400 loss: 1.1026823830604553\n",
      "  batch 450 loss: 1.1402538907527924\n",
      "  batch 500 loss: 1.1146731328964234\n",
      "  batch 550 loss: 1.1366269147396089\n",
      "  batch 600 loss: 1.0999636518955231\n",
      "  batch 650 loss: 1.1100859725475312\n",
      "  batch 700 loss: 1.0993683111667634\n",
      "  batch 750 loss: 1.0954397809505463\n",
      "  batch 800 loss: 1.0763450753688812\n",
      "  batch 850 loss: 1.0747523951530455\n",
      "  batch 900 loss: 1.106244020462036\n",
      "LOSS train 1.10624 valid 1.15058, valid PER 37.50%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.067668492794037\n",
      "  batch 100 loss: 1.0810318088531494\n",
      "  batch 150 loss: 1.0642846775054933\n",
      "  batch 200 loss: 1.0581896197795868\n",
      "  batch 250 loss: 1.0547215163707733\n",
      "  batch 300 loss: 1.0500721383094787\n",
      "  batch 350 loss: 1.0470583415031434\n",
      "  batch 400 loss: 1.0487202310562134\n",
      "  batch 450 loss: 1.0545532989501953\n",
      "  batch 500 loss: 1.0580059707164764\n",
      "  batch 550 loss: 1.0336685299873352\n",
      "  batch 600 loss: 1.0451252925395966\n",
      "  batch 650 loss: 1.0213326013088226\n",
      "  batch 700 loss: 1.0625391924381256\n",
      "  batch 750 loss: 1.029310929775238\n",
      "  batch 800 loss: 1.025207711458206\n",
      "  batch 850 loss: 1.0447287905216216\n",
      "  batch 900 loss: 1.0706198501586914\n",
      "LOSS train 1.07062 valid 1.10301, valid PER 35.50%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.022224577665329\n",
      "  batch 100 loss: 1.0127610456943512\n",
      "  batch 150 loss: 0.9868279874324799\n",
      "  batch 200 loss: 0.9744755065441132\n",
      "  batch 250 loss: 1.0050185477733613\n",
      "  batch 300 loss: 0.931840797662735\n",
      "  batch 350 loss: 1.021369080543518\n",
      "  batch 400 loss: 0.9674929583072662\n",
      "  batch 450 loss: 1.0247757649421692\n",
      "  batch 500 loss: 1.0414649105072022\n",
      "  batch 550 loss: 0.9799219489097595\n",
      "  batch 600 loss: 1.0189776968955995\n",
      "  batch 650 loss: 1.0631942212581635\n",
      "  batch 700 loss: 0.9831082403659821\n",
      "  batch 750 loss: 1.0053040635585786\n",
      "  batch 800 loss: 1.0036701250076294\n",
      "  batch 850 loss: 0.9779033327102661\n",
      "  batch 900 loss: 0.9835424816608429\n",
      "LOSS train 0.98354 valid 1.06475, valid PER 33.61%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.9216276907920837\n",
      "  batch 100 loss: 0.9670578849315643\n",
      "  batch 150 loss: 0.9785934484004974\n",
      "  batch 200 loss: 0.9316741311550141\n",
      "  batch 250 loss: 0.9736937510967255\n",
      "  batch 300 loss: 0.9776780533790589\n",
      "  batch 350 loss: 0.9802694594860077\n",
      "  batch 400 loss: 0.9568647694587707\n",
      "  batch 450 loss: 0.9587163782119751\n",
      "  batch 500 loss: 0.9433926558494568\n",
      "  batch 550 loss: 0.9915747165679931\n",
      "  batch 600 loss: 0.9793040800094605\n",
      "  batch 650 loss: 0.9567863941192627\n",
      "  batch 700 loss: 0.943225691318512\n",
      "  batch 750 loss: 0.9470043039321899\n",
      "  batch 800 loss: 0.9672517013549805\n",
      "  batch 850 loss: 0.9925208711624145\n",
      "  batch 900 loss: 0.9602033019065856\n",
      "LOSS train 0.96020 valid 1.04052, valid PER 33.12%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.8996936047077179\n",
      "  batch 100 loss: 0.9273363173007965\n",
      "  batch 150 loss: 0.94484161734581\n",
      "  batch 200 loss: 0.949828724861145\n",
      "  batch 250 loss: 0.9478096652030945\n",
      "  batch 300 loss: 0.9126179909706116\n",
      "  batch 350 loss: 0.9532393682003021\n",
      "  batch 400 loss: 0.8780588483810425\n",
      "  batch 450 loss: 0.90435302734375\n",
      "  batch 500 loss: 0.9338494169712067\n",
      "  batch 550 loss: 0.946411669254303\n",
      "  batch 600 loss: 0.9248276126384735\n",
      "  batch 650 loss: 0.9139561450481415\n",
      "  batch 700 loss: 0.939614896774292\n",
      "  batch 750 loss: 0.9206897759437561\n",
      "  batch 800 loss: 0.9321948790550232\n",
      "  batch 850 loss: 0.9375165450572968\n",
      "  batch 900 loss: 0.9342212009429932\n",
      "LOSS train 0.93422 valid 1.01801, valid PER 32.81%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.8758719396591187\n",
      "  batch 100 loss: 0.8534934282302856\n",
      "  batch 150 loss: 0.8812339031696319\n",
      "  batch 200 loss: 0.9198918747901916\n",
      "  batch 250 loss: 0.9045221161842346\n",
      "  batch 300 loss: 0.8678642213344574\n",
      "  batch 350 loss: 0.8895058476924896\n",
      "  batch 400 loss: 0.92299684882164\n",
      "  batch 450 loss: 0.9073521649837494\n",
      "  batch 500 loss: 0.8866267263889313\n",
      "  batch 550 loss: 0.8985035741329193\n",
      "  batch 600 loss: 0.8692197692394257\n",
      "  batch 650 loss: 0.9558497059345246\n",
      "  batch 700 loss: 0.861363195180893\n",
      "  batch 750 loss: 0.8928570783138275\n",
      "  batch 800 loss: 0.9202289581298828\n",
      "  batch 850 loss: 0.9279775822162628\n",
      "  batch 900 loss: 0.9151125955581665\n",
      "LOSS train 0.91511 valid 1.00224, valid PER 31.31%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.8612049424648285\n",
      "  batch 100 loss: 0.8442684531211853\n",
      "  batch 150 loss: 0.8472312259674072\n",
      "  batch 200 loss: 0.8639320027828217\n",
      "  batch 250 loss: 0.8923620760440827\n",
      "  batch 300 loss: 0.8695007538795472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 350 loss: 0.865443787574768\n",
      "  batch 400 loss: 0.8717297947406769\n",
      "  batch 450 loss: 0.8707527434825897\n",
      "  batch 500 loss: 0.8925782990455627\n",
      "  batch 550 loss: 0.8321598029136658\n",
      "  batch 600 loss: 0.8653612720966339\n",
      "  batch 650 loss: 0.8958389794826508\n",
      "  batch 700 loss: 0.8833250510692596\n",
      "  batch 750 loss: 0.8462914633750915\n",
      "  batch 800 loss: 0.8674984323978424\n",
      "  batch 850 loss: 0.9173091053962708\n",
      "  batch 900 loss: 0.8950191617012024\n",
      "LOSS train 0.89502 valid 1.00286, valid PER 31.61%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.820296665430069\n",
      "  batch 100 loss: 0.8368103361129761\n",
      "  batch 150 loss: 0.815930073261261\n",
      "  batch 200 loss: 0.8390223336219788\n",
      "  batch 250 loss: 0.8462725257873536\n",
      "  batch 300 loss: 0.8443914639949799\n",
      "  batch 350 loss: 0.852672780752182\n",
      "  batch 400 loss: 0.8487203133106231\n",
      "  batch 450 loss: 0.854149786233902\n",
      "  batch 500 loss: 0.815921481847763\n",
      "  batch 550 loss: 0.8599018323421478\n",
      "  batch 600 loss: 0.8529881203174591\n",
      "  batch 650 loss: 0.8528653573989868\n",
      "  batch 700 loss: 0.8570992529392243\n",
      "  batch 750 loss: 0.8157042670249939\n",
      "  batch 800 loss: 0.8283688199520111\n",
      "  batch 850 loss: 0.8785298609733582\n",
      "  batch 900 loss: 0.8699463760852814\n",
      "LOSS train 0.86995 valid 0.99467, valid PER 31.32%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.8143694591522217\n",
      "  batch 100 loss: 0.812204600572586\n",
      "  batch 150 loss: 0.805617311000824\n",
      "  batch 200 loss: 0.8153668057918548\n",
      "  batch 250 loss: 0.8252656495571137\n",
      "  batch 300 loss: 0.8528752624988556\n",
      "  batch 350 loss: 0.7958685839176178\n",
      "  batch 400 loss: 0.8087394869327545\n",
      "  batch 450 loss: 0.8161596262454986\n",
      "  batch 500 loss: 0.8487365615367889\n",
      "  batch 550 loss: 0.8396381509304046\n",
      "  batch 600 loss: 0.81240254342556\n",
      "  batch 650 loss: 0.8506437611579895\n",
      "  batch 700 loss: 0.8561898577213287\n",
      "  batch 750 loss: 0.7987785243988037\n",
      "  batch 800 loss: 0.7985803377628327\n",
      "  batch 850 loss: 0.8527327382564545\n",
      "  batch 900 loss: 0.8341310048103332\n",
      "LOSS train 0.83413 valid 0.99349, valid PER 31.87%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.782970768213272\n",
      "  batch 100 loss: 0.7902315390110016\n",
      "  batch 150 loss: 0.7957312440872193\n",
      "  batch 200 loss: 0.8364199519157409\n",
      "  batch 250 loss: 0.8323039317131042\n",
      "  batch 300 loss: 0.776976330280304\n",
      "  batch 350 loss: 0.7892785787582397\n",
      "  batch 400 loss: 0.7826679337024689\n",
      "  batch 450 loss: 0.8039384359121322\n",
      "  batch 500 loss: 0.7711142385005951\n",
      "  batch 550 loss: 0.8033195161819457\n",
      "  batch 600 loss: 0.8310533607006073\n",
      "  batch 650 loss: 0.833645190000534\n",
      "  batch 700 loss: 0.8205020964145661\n",
      "  batch 750 loss: 0.8303151071071625\n",
      "  batch 800 loss: 0.7820969486236572\n",
      "  batch 850 loss: 0.7896823966503144\n",
      "  batch 900 loss: 0.8028258728981018\n",
      "LOSS train 0.80283 valid 0.98161, valid PER 30.31%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.7959676456451416\n",
      "  batch 100 loss: 0.7482759988307953\n",
      "  batch 150 loss: 0.7605588561296464\n",
      "  batch 200 loss: 0.7584979057312011\n",
      "  batch 250 loss: 0.7906865656375885\n",
      "  batch 300 loss: 0.7745809972286224\n",
      "  batch 350 loss: 0.7988183182477951\n",
      "  batch 400 loss: 0.7762652695178985\n",
      "  batch 450 loss: 0.8096575057506561\n",
      "  batch 500 loss: 0.7463368117809296\n",
      "  batch 550 loss: 0.771879849433899\n",
      "  batch 600 loss: 0.7964666724205017\n",
      "  batch 650 loss: 0.792486094236374\n",
      "  batch 700 loss: 0.7601587796211242\n",
      "  batch 750 loss: 0.792804456949234\n",
      "  batch 800 loss: 0.7913720190525055\n",
      "  batch 850 loss: 0.768978054523468\n",
      "  batch 900 loss: 0.7827837705612183\n",
      "LOSS train 0.78278 valid 0.96001, valid PER 30.33%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.7459677815437317\n",
      "  batch 100 loss: 0.7534232056140899\n",
      "  batch 150 loss: 0.749447700381279\n",
      "  batch 200 loss: 0.761134957075119\n",
      "  batch 250 loss: 0.7812171947956085\n",
      "  batch 300 loss: 0.7682707726955413\n",
      "  batch 350 loss: 0.735867201089859\n",
      "  batch 400 loss: 0.7895507264137268\n",
      "  batch 450 loss: 0.7865925204753875\n",
      "  batch 500 loss: 0.7569657629728317\n",
      "  batch 550 loss: 0.7835774570703506\n",
      "  batch 600 loss: 0.8065763580799102\n",
      "  batch 650 loss: 0.7740485310554505\n",
      "  batch 700 loss: 0.7557676470279694\n",
      "  batch 750 loss: 0.7271986562013626\n",
      "  batch 800 loss: 0.7320752191543579\n",
      "  batch 850 loss: 0.7606852108240127\n",
      "  batch 900 loss: 0.7409175342321396\n",
      "LOSS train 0.74092 valid 0.98839, valid PER 30.28%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.7210725033283234\n",
      "  batch 100 loss: 0.7375417160987854\n",
      "  batch 150 loss: 0.7319240152835846\n",
      "  batch 200 loss: 0.7345648169517517\n",
      "  batch 250 loss: 0.742766591310501\n",
      "  batch 300 loss: 0.724274971485138\n",
      "  batch 350 loss: 0.73447498857975\n",
      "  batch 400 loss: 0.7185646384954453\n",
      "  batch 450 loss: 0.7672350811958313\n",
      "  batch 500 loss: 0.7523976242542267\n",
      "  batch 550 loss: 0.7521085321903229\n",
      "  batch 600 loss: 0.7426823770999909\n",
      "  batch 650 loss: 0.7215488409996033\n",
      "  batch 700 loss: 0.7777834260463714\n",
      "  batch 750 loss: 0.7516041642427445\n",
      "  batch 800 loss: 0.7558783400058746\n",
      "  batch 850 loss: 0.7473649060726166\n",
      "  batch 900 loss: 0.7644315195083619\n",
      "LOSS train 0.76443 valid 0.96596, valid PER 29.56%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.6927857816219329\n",
      "  batch 100 loss: 0.681695186495781\n",
      "  batch 150 loss: 0.704315676689148\n",
      "  batch 200 loss: 0.7019077718257904\n",
      "  batch 250 loss: 0.7271747708320617\n",
      "  batch 300 loss: 0.7640662157535553\n",
      "  batch 350 loss: 0.728625259399414\n",
      "  batch 400 loss: 0.7270409667491913\n",
      "  batch 450 loss: 0.7181641376018524\n",
      "  batch 500 loss: 0.7286924666166306\n",
      "  batch 550 loss: 0.7295387864112854\n",
      "  batch 600 loss: 0.7269369691610337\n",
      "  batch 650 loss: 0.7738099229335785\n",
      "  batch 700 loss: 0.6994650095701218\n",
      "  batch 750 loss: 0.6983900874853134\n",
      "  batch 800 loss: 0.7249657505750656\n",
      "  batch 850 loss: 0.7485309660434722\n",
      "  batch 900 loss: 0.7448435962200165\n",
      "LOSS train 0.74484 valid 0.97235, valid PER 29.74%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.6720851159095764\n",
      "  batch 100 loss: 0.6821427273750306\n",
      "  batch 150 loss: 0.684298055768013\n",
      "  batch 200 loss: 0.6917743360996247\n",
      "  batch 250 loss: 0.6807078230381012\n",
      "  batch 300 loss: 0.7181441950798034\n",
      "  batch 350 loss: 0.6815580415725708\n",
      "  batch 400 loss: 0.6951725500822067\n",
      "  batch 450 loss: 0.7049776411056519\n",
      "  batch 500 loss: 0.6777162235975266\n",
      "  batch 550 loss: 0.753449696302414\n",
      "  batch 600 loss: 0.7023301434516906\n",
      "  batch 650 loss: 0.7259462130069733\n",
      "  batch 700 loss: 0.7337565863132477\n",
      "  batch 750 loss: 0.7073220896720886\n",
      "  batch 800 loss: 0.7390882229804993\n",
      "  batch 850 loss: 0.7145226275920868\n",
      "  batch 900 loss: 0.7301882636547089\n",
      "LOSS train 0.73019 valid 0.97461, valid PER 29.74%\n",
      "Training finished in 3.0 minutes.\n",
      "Model saved to checkpoints/20231205_211327/model_16\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "from datetime import datetime\n",
    "from trainer_Adam import train\n",
    "\n",
    "\n",
    "model = models.BiLSTM(\n",
    "    args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab))\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of model parameters is {}'.format(num_params))\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "model.to(args.device)\n",
    "model_path = train(model, args)\n",
    "end = datetime.now()\n",
    "duration = (end - start).total_seconds()\n",
    "print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "print('Model saved to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d083010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'val_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 128,\n",
    "        'concat': 1,\n",
    "        'lr': 0.1,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "args = namedtuple('x', args)(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fccfd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters is 166952\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.506333332061768\n",
      "  batch 100 loss: 2.8623961925506594\n",
      "  batch 150 loss: 2.678375926017761\n",
      "  batch 200 loss: 2.5562650871276857\n",
      "  batch 250 loss: 2.44685085773468\n",
      "  batch 300 loss: 2.404448504447937\n",
      "  batch 350 loss: 2.245123052597046\n",
      "  batch 400 loss: 2.3010426998138427\n",
      "  batch 450 loss: 2.220965223312378\n",
      "  batch 500 loss: 2.164877219200134\n",
      "  batch 550 loss: 2.168926432132721\n",
      "  batch 600 loss: 2.174325647354126\n",
      "  batch 650 loss: 2.1885672116279604\n",
      "  batch 700 loss: 2.2873722553253173\n",
      "  batch 750 loss: 2.1368629145622253\n",
      "  batch 800 loss: 2.1436072731018068\n",
      "  batch 850 loss: 2.136396086215973\n",
      "  batch 900 loss: 2.0891373324394227\n",
      "LOSS train 2.08914 valid 2.05262, valid PER 64.96%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 2.064674139022827\n",
      "  batch 100 loss: 2.0597986578941345\n",
      "  batch 150 loss: 2.021226227283478\n",
      "  batch 200 loss: 2.1158048629760744\n",
      "  batch 250 loss: 2.1334808588027956\n",
      "  batch 300 loss: 2.100814940929413\n",
      "  batch 350 loss: 1.9982937526702882\n",
      "  batch 400 loss: 2.0700559782981873\n",
      "  batch 450 loss: 2.0100667834281922\n",
      "  batch 500 loss: 2.1111078882217407\n",
      "  batch 550 loss: 2.0910375905036926\n",
      "  batch 600 loss: 2.026878056526184\n",
      "  batch 650 loss: 2.078592095375061\n",
      "  batch 700 loss: 2.080567147731781\n",
      "  batch 750 loss: 2.117524881362915\n",
      "  batch 800 loss: 2.1226915884017945\n",
      "  batch 850 loss: 2.1757728171348574\n",
      "  batch 900 loss: 2.158236258029938\n",
      "LOSS train 2.15824 valid 2.08321, valid PER 61.65%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 2.1511451196670532\n",
      "  batch 100 loss: 2.410970478057861\n",
      "  batch 150 loss: 2.6518904399871825\n",
      "  batch 200 loss: 2.630094289779663\n",
      "  batch 250 loss: 2.6700247716903687\n",
      "  batch 300 loss: 2.5588001203536987\n",
      "  batch 350 loss: 2.4684105587005614\n",
      "  batch 400 loss: 2.4254747343063356\n",
      "  batch 450 loss: 2.5316667556762695\n",
      "  batch 500 loss: 2.4698240184783935\n",
      "  batch 550 loss: 2.36024032831192\n",
      "  batch 600 loss: 2.4230151915550233\n",
      "  batch 650 loss: 2.299037184715271\n",
      "  batch 700 loss: 2.3804370665550234\n",
      "  batch 750 loss: 2.3833404684066775\n",
      "  batch 800 loss: 2.278770396709442\n",
      "  batch 850 loss: 2.343817558288574\n",
      "  batch 900 loss: 2.325981369018555\n",
      "LOSS train 2.32598 valid 2.27893, valid PER 66.18%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 2.2883235478401183\n",
      "  batch 100 loss: 2.3548480749130247\n",
      "  batch 150 loss: 2.3306107187271117\n",
      "  batch 200 loss: 2.383357939720154\n",
      "  batch 250 loss: 2.515197796821594\n",
      "  batch 300 loss: 2.5635276508331297\n",
      "  batch 350 loss: 2.375148153305054\n",
      "  batch 400 loss: 2.3959599113464356\n",
      "  batch 450 loss: 2.3559446787834166\n",
      "  batch 500 loss: 2.298703033924103\n",
      "  batch 550 loss: 2.32571830034256\n",
      "  batch 600 loss: 2.441952095031738\n",
      "  batch 650 loss: 2.415855259895325\n",
      "  batch 700 loss: 2.3578179383277895\n",
      "  batch 750 loss: 2.2749726724624635\n",
      "  batch 800 loss: 2.2685898327827454\n",
      "  batch 850 loss: 2.2859180879592897\n",
      "  batch 900 loss: 2.288405780792236\n",
      "LOSS train 2.28841 valid 2.25107, valid PER 64.88%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 2.2738266372680664\n",
      "  batch 100 loss: 2.264123196601868\n",
      "  batch 150 loss: 2.3428395271301268\n",
      "  batch 200 loss: 2.278021585941315\n",
      "  batch 250 loss: 2.2651556658744814\n",
      "  batch 300 loss: 2.30798597574234\n",
      "  batch 350 loss: 2.2758402824401855\n",
      "  batch 400 loss: 2.2489654326438906\n",
      "  batch 450 loss: 2.3109191036224366\n",
      "  batch 500 loss: 2.3727065038681032\n",
      "  batch 550 loss: 2.33355970621109\n",
      "  batch 600 loss: 2.4185581541061403\n",
      "  batch 650 loss: 2.536501922607422\n",
      "  batch 700 loss: 2.502291269302368\n",
      "  batch 750 loss: 2.4342411613464354\n",
      "  batch 800 loss: 2.4247203397750856\n",
      "  batch 850 loss: 2.4699870681762697\n",
      "  batch 900 loss: 2.453842868804932\n",
      "LOSS train 2.45384 valid 2.42110, valid PER 65.00%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 2.36858615398407\n",
      "  batch 100 loss: 2.3297384142875672\n",
      "  batch 150 loss: 2.386566333770752\n",
      "  batch 200 loss: 2.4180245923995973\n",
      "  batch 250 loss: 2.4069449281692505\n",
      "  batch 300 loss: 2.340447738170624\n",
      "  batch 350 loss: 2.497800703048706\n",
      "  batch 400 loss: 2.427882685661316\n",
      "  batch 450 loss: 2.387989137172699\n",
      "  batch 500 loss: 2.280763876438141\n",
      "  batch 550 loss: 2.3667937898635865\n",
      "  batch 600 loss: 2.351092176437378\n",
      "  batch 650 loss: 2.3919326496124267\n",
      "  batch 700 loss: 2.3917359590530394\n",
      "  batch 750 loss: 2.385536012649536\n",
      "  batch 800 loss: 2.415203354358673\n",
      "  batch 850 loss: 2.397063217163086\n",
      "  batch 900 loss: 2.401260643005371\n",
      "LOSS train 2.40126 valid 2.28850, valid PER 65.57%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 2.3749340534210206\n",
      "  batch 100 loss: 2.4302252030372617\n",
      "  batch 150 loss: 2.3252412605285646\n",
      "  batch 200 loss: 2.3219069623947144\n",
      "  batch 250 loss: 2.360571904182434\n",
      "  batch 300 loss: 2.3596104741096497\n",
      "  batch 350 loss: 2.3426916933059694\n",
      "  batch 400 loss: 2.362153413295746\n",
      "  batch 450 loss: 2.3626719379425047\n",
      "  batch 500 loss: 2.3505661296844482\n",
      "  batch 550 loss: 2.310175156593323\n",
      "  batch 600 loss: 2.346096751689911\n",
      "  batch 650 loss: 2.4112150192260744\n",
      "  batch 700 loss: 2.3965885400772096\n",
      "  batch 750 loss: 2.2995205545425415\n",
      "  batch 800 loss: 2.3611880564689636\n",
      "  batch 850 loss: 2.3950170135498046\n",
      "  batch 900 loss: 2.3994196224212647\n",
      "LOSS train 2.39942 valid 2.44063, valid PER 69.96%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 2.3436685037612914\n",
      "  batch 100 loss: 2.36711790561676\n",
      "  batch 150 loss: 2.3687330794334414\n",
      "  batch 200 loss: 2.3102725458145144\n",
      "  batch 250 loss: 2.3766097021102905\n",
      "  batch 300 loss: 2.3005334186553954\n",
      "  batch 350 loss: 2.3004241704940798\n",
      "  batch 400 loss: 2.324329283237457\n",
      "  batch 450 loss: 2.345825123786926\n",
      "  batch 500 loss: 2.409847960472107\n",
      "  batch 550 loss: 2.3507923030853273\n",
      "  batch 600 loss: 2.3561997032165527\n",
      "  batch 650 loss: 2.3939498138427733\n",
      "  batch 700 loss: 2.3870812892913817\n",
      "  batch 750 loss: 2.4080678462982177\n",
      "  batch 800 loss: 2.4014899587631224\n",
      "  batch 850 loss: 2.4711556911468504\n",
      "  batch 900 loss: 2.372131636142731\n",
      "LOSS train 2.37213 valid 2.35929, valid PER 66.51%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 2.3123866057395936\n",
      "  batch 100 loss: 2.4081289100646974\n",
      "  batch 150 loss: 2.395362710952759\n",
      "  batch 200 loss: 2.31225257396698\n",
      "  batch 250 loss: 2.39121851682663\n",
      "  batch 300 loss: 2.3749076175689696\n",
      "  batch 350 loss: 2.4534614276885987\n",
      "  batch 400 loss: 2.396454710960388\n",
      "  batch 450 loss: 2.4256623220443725\n",
      "  batch 500 loss: 2.3628294038772584\n",
      "  batch 550 loss: 2.4241144967079165\n",
      "  batch 600 loss: 2.4364431381225584\n",
      "  batch 650 loss: 2.4084030628204345\n",
      "  batch 700 loss: 2.4799632930755617\n",
      "  batch 750 loss: 2.476102199554443\n",
      "  batch 800 loss: 2.488078169822693\n",
      "  batch 850 loss: 2.4562232303619385\n",
      "  batch 900 loss: 2.394437413215637\n",
      "LOSS train 2.39444 valid 2.55663, valid PER 64.02%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 2.4980530023574827\n",
      "  batch 100 loss: 2.5233947038650513\n",
      "  batch 150 loss: 2.5118496417999268\n",
      "  batch 200 loss: 2.5921295785903933\n",
      "  batch 250 loss: 2.7354266023635865\n",
      "  batch 300 loss: 2.649064092636108\n",
      "  batch 350 loss: 2.7790661430358887\n",
      "  batch 400 loss: 2.684840202331543\n",
      "  batch 450 loss: 2.507626256942749\n",
      "  batch 500 loss: 2.5028054761886596\n",
      "  batch 550 loss: 2.5670036363601683\n",
      "  batch 600 loss: 2.5743372368812563\n",
      "  batch 650 loss: 2.493228735923767\n",
      "  batch 700 loss: 2.4739606046676634\n",
      "  batch 750 loss: 2.4420450735092163\n",
      "  batch 800 loss: 2.4969781589508058\n",
      "  batch 850 loss: 2.55889377117157\n",
      "  batch 900 loss: 2.5591102027893067\n",
      "LOSS train 2.55911 valid 2.47124, valid PER 68.57%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 2.5431989574432374\n",
      "  batch 100 loss: 2.4757765769958495\n",
      "  batch 150 loss: 2.3927008724212646\n",
      "  batch 200 loss: 2.389179210662842\n",
      "  batch 250 loss: 2.4303147172927857\n",
      "  batch 300 loss: 2.435804371833801\n",
      "  batch 350 loss: 2.391687207221985\n",
      "  batch 400 loss: 2.4525129699707033\n",
      "  batch 450 loss: 2.4804622459411623\n",
      "  batch 500 loss: 2.4254304552078247\n",
      "  batch 550 loss: 2.4093705153465272\n",
      "  batch 600 loss: 2.432702887058258\n",
      "  batch 650 loss: 2.524335026741028\n",
      "  batch 700 loss: 2.973809223175049\n",
      "  batch 750 loss: 3.1575372409820557\n",
      "  batch 800 loss: 3.0857092714309693\n",
      "  batch 850 loss: 3.1966724681854246\n",
      "  batch 900 loss: 3.0840458822250367\n",
      "LOSS train 3.08405 valid 3.07307, valid PER 78.31%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 2.9705054140090943\n",
      "  batch 100 loss: 3.075571298599243\n",
      "  batch 150 loss: 3.1584359216690063\n",
      "  batch 200 loss: 3.2683508682250975\n",
      "  batch 250 loss: 3.2337506866455077\n",
      "  batch 300 loss: 3.2485459852218628\n",
      "  batch 350 loss: 3.0248194313049317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 400 loss: 2.976087384223938\n",
      "  batch 450 loss: 3.0841492938995363\n",
      "  batch 500 loss: 2.972876386642456\n",
      "  batch 550 loss: 2.933476600646973\n",
      "  batch 600 loss: 2.9375278615951537\n",
      "  batch 650 loss: 2.840574736595154\n",
      "  batch 700 loss: 2.7909598541259766\n",
      "  batch 750 loss: 2.7548926210403444\n",
      "  batch 800 loss: 2.794164819717407\n",
      "  batch 850 loss: 2.819136047363281\n",
      "  batch 900 loss: 2.7712770652770997\n",
      "LOSS train 2.77128 valid 2.77622, valid PER 74.84%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 2.777683501243591\n",
      "  batch 100 loss: 2.8953692007064817\n",
      "  batch 150 loss: 2.793831715583801\n",
      "  batch 200 loss: 2.808894739151001\n",
      "  batch 250 loss: 2.817759747505188\n",
      "  batch 300 loss: 2.8018371534347533\n",
      "  batch 350 loss: 2.821599745750427\n",
      "  batch 400 loss: 2.8821763610839843\n",
      "  batch 450 loss: 2.9553138399124146\n",
      "  batch 500 loss: 2.839025573730469\n",
      "  batch 550 loss: 2.818402719497681\n",
      "  batch 600 loss: 2.8555260419845583\n",
      "  batch 650 loss: 2.8596240091323852\n",
      "  batch 700 loss: 2.8557433605194094\n",
      "  batch 750 loss: 2.888971004486084\n",
      "  batch 800 loss: 2.8943338775634766\n",
      "  batch 850 loss: 2.892783751487732\n",
      "  batch 900 loss: 2.9736876630783082\n",
      "LOSS train 2.97369 valid 3.09608, valid PER 77.88%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 2.9087540531158447\n",
      "  batch 100 loss: 3.0225107145309447\n",
      "  batch 150 loss: 3.0394734477996828\n",
      "  batch 200 loss: 3.081318588256836\n",
      "  batch 250 loss: 2.949159903526306\n",
      "  batch 300 loss: 2.990951519012451\n",
      "  batch 350 loss: 3.101608257293701\n",
      "  batch 400 loss: 3.0241505908966064\n",
      "  batch 450 loss: 2.9798899412155153\n",
      "  batch 500 loss: 2.9604774618148806\n",
      "  batch 550 loss: 3.084382815361023\n",
      "  batch 600 loss: 3.2341994047164917\n",
      "  batch 650 loss: 3.3507995796203613\n",
      "  batch 700 loss: 3.1161190557479856\n",
      "  batch 750 loss: 2.9809634065628052\n",
      "  batch 800 loss: 3.0431922006607057\n",
      "  batch 850 loss: 3.114467158317566\n",
      "  batch 900 loss: nan\n",
      "LOSS train nan valid nan, valid PER 100.00%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: nan\n",
      "  batch 100 loss: nan\n",
      "  batch 150 loss: nan\n",
      "  batch 200 loss: nan\n",
      "  batch 250 loss: nan\n",
      "  batch 300 loss: nan\n",
      "  batch 350 loss: nan\n",
      "  batch 400 loss: nan\n",
      "  batch 450 loss: nan\n",
      "  batch 500 loss: nan\n",
      "  batch 550 loss: nan\n",
      "  batch 600 loss: nan\n",
      "  batch 650 loss: nan\n",
      "  batch 700 loss: nan\n",
      "  batch 750 loss: nan\n",
      "  batch 800 loss: nan\n",
      "  batch 850 loss: nan\n",
      "  batch 900 loss: nan\n",
      "LOSS train nan valid nan, valid PER 100.00%\n",
      "EPOCH 16:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 14\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     16\u001b[0m duration \u001b[38;5;241m=\u001b[39m (end \u001b[38;5;241m-\u001b[39m start)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "File \u001b[0;32m/rds/user/wa285/hpc-work/MLMI2/exp/trainer_Adam.py:63\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 63\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m running_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n",
      "File \u001b[0;32m/rds/user/wa285/hpc-work/MLMI2/exp/trainer_Adam.py:41\u001b[0m, in \u001b[0;36mtrain.<locals>.train_one_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m log_softmax(model(inputs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets, in_lens, out_lens)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#add gradient clipping\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:166\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    162\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    165\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 166\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2022/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:68\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import models\n",
    "from datetime import datetime\n",
    "from trainer_Adam import train\n",
    "\n",
    "\n",
    "model = models.BiLSTM(\n",
    "    args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab))\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of model parameters is {}'.format(num_params))\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "model.to(args.device)\n",
    "model_path = train(model, args)\n",
    "end = datetime.now()\n",
    "duration = (end - start).total_seconds()\n",
    "print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "print('Model saved to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d42c3",
   "metadata": {},
   "source": [
    "###### Add gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faac6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'val_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 128,\n",
    "        'concat': 1,\n",
    "        'lr': 0.1,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "args = namedtuple('x', args)(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1cb3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters is 166952\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.2683671283721925\n",
      "  batch 100 loss: 3.01337543964386\n",
      "  batch 150 loss: 2.6641845703125\n",
      "  batch 200 loss: 2.478918595314026\n",
      "  batch 250 loss: 2.410011293888092\n",
      "  batch 300 loss: 2.3164991569519042\n",
      "  batch 350 loss: 2.312526626586914\n",
      "  batch 400 loss: 2.271858158111572\n",
      "  batch 450 loss: 2.2899438977241515\n",
      "  batch 500 loss: 2.263788242340088\n",
      "  batch 550 loss: 2.228834981918335\n",
      "  batch 600 loss: 2.2524332904815676\n",
      "  batch 650 loss: 2.202117931842804\n",
      "  batch 700 loss: 2.2307498025894166\n",
      "  batch 750 loss: 2.1722647523880005\n",
      "  batch 800 loss: 2.2001182770729066\n",
      "  batch 850 loss: 2.2330781030654907\n",
      "  batch 900 loss: 2.207210223674774\n",
      "LOSS train 2.20721 valid 2.22778, valid PER 74.67%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 2.2363215637207032\n",
      "  batch 100 loss: 2.179481258392334\n",
      "  batch 150 loss: 2.157339792251587\n",
      "  batch 200 loss: 2.2023068952560423\n",
      "  batch 250 loss: 2.220174775123596\n",
      "  batch 300 loss: 2.179911856651306\n",
      "  batch 350 loss: 2.0958424973487855\n",
      "  batch 400 loss: 2.2030288863182066\n",
      "  batch 450 loss: 2.1673611903190615\n",
      "  batch 500 loss: 2.1575648260116576\n",
      "  batch 550 loss: 2.1789730381965637\n",
      "  batch 600 loss: 2.14920081615448\n",
      "  batch 650 loss: 2.1719490242004396\n",
      "  batch 700 loss: 2.328309495449066\n",
      "  batch 750 loss: 2.3178052163124083\n",
      "  batch 800 loss: 2.2225197911262513\n",
      "  batch 850 loss: 2.233171479701996\n",
      "  batch 900 loss: 2.280182228088379\n",
      "LOSS train 2.28018 valid 2.21970, valid PER 71.98%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 2.219551305770874\n",
      "  batch 100 loss: 2.1462279319763184\n",
      "  batch 150 loss: 2.141667354106903\n",
      "  batch 200 loss: 2.1969812607765196\n",
      "  batch 250 loss: 2.187828857898712\n",
      "  batch 300 loss: 2.208344292640686\n",
      "  batch 350 loss: 2.4013459157943724\n",
      "  batch 400 loss: 2.256113820075989\n",
      "  batch 450 loss: 2.298295202255249\n",
      "  batch 500 loss: 2.2888972330093384\n",
      "  batch 550 loss: 2.297193219661713\n",
      "  batch 600 loss: 2.2422061204910277\n",
      "  batch 650 loss: 2.2059504890441897\n",
      "  batch 700 loss: 2.271667811870575\n",
      "  batch 750 loss: 2.369466063976288\n",
      "  batch 800 loss: 2.234660723209381\n",
      "  batch 850 loss: 2.260202054977417\n",
      "  batch 900 loss: 2.1953243589401246\n",
      "LOSS train 2.19532 valid 2.26508, valid PER 67.70%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 2.21831193447113\n",
      "  batch 100 loss: 2.229811120033264\n",
      "  batch 150 loss: 2.1747144293785095\n",
      "  batch 200 loss: 2.316880955696106\n",
      "  batch 250 loss: 2.3429257535934447\n",
      "  batch 300 loss: 2.338513240814209\n",
      "  batch 350 loss: 2.241094186306\n",
      "  batch 400 loss: 2.2898309898376463\n",
      "  batch 450 loss: 2.2854507541656495\n",
      "  batch 500 loss: 2.225909361839294\n",
      "  batch 550 loss: 2.282510666847229\n",
      "  batch 600 loss: 2.3716481924057007\n",
      "  batch 650 loss: 2.2968654251098632\n",
      "  batch 700 loss: 2.2666359090805055\n",
      "  batch 750 loss: 2.205106308460236\n",
      "  batch 800 loss: 2.2066497159004212\n",
      "  batch 850 loss: 2.229464900493622\n",
      "  batch 900 loss: 2.251498668193817\n",
      "LOSS train 2.25150 valid 2.25963, valid PER 67.16%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 2.2208920240402223\n",
      "  batch 100 loss: 2.152317979335785\n",
      "  batch 150 loss: 2.2014328956604006\n",
      "  batch 200 loss: 2.1999525356292726\n",
      "  batch 250 loss: 2.2220757389068604\n",
      "  batch 300 loss: 2.1802078104019165\n",
      "  batch 350 loss: 2.191442370414734\n",
      "  batch 400 loss: 2.1577164101600648\n",
      "  batch 450 loss: 2.1599828243255614\n",
      "  batch 500 loss: 2.1890120553970336\n",
      "  batch 550 loss: 2.1346997594833375\n",
      "  batch 600 loss: 2.1802931237220764\n",
      "  batch 650 loss: 2.2070550990104674\n",
      "  batch 700 loss: 2.2890715909004213\n",
      "  batch 750 loss: 2.2013365626335144\n",
      "  batch 800 loss: 2.1771672701835634\n",
      "  batch 850 loss: 2.220092170238495\n",
      "  batch 900 loss: 2.232298743724823\n",
      "LOSS train 2.23230 valid 2.18619, valid PER 66.11%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 2.180140342712402\n",
      "  batch 100 loss: 2.1814454913139345\n",
      "  batch 150 loss: 2.151323676109314\n",
      "  batch 200 loss: 2.136751136779785\n",
      "  batch 250 loss: 2.2634914660453798\n",
      "  batch 300 loss: 2.1778508734703066\n",
      "  batch 350 loss: 2.223791661262512\n",
      "  batch 400 loss: 2.201690001487732\n",
      "  batch 450 loss: 2.2339817094802856\n",
      "  batch 500 loss: 2.1715450644493104\n",
      "  batch 550 loss: 2.1602206897735594\n",
      "  batch 600 loss: 2.1715125274658202\n",
      "  batch 650 loss: 2.2247694683074952\n",
      "  batch 700 loss: 2.1813141298294068\n",
      "  batch 750 loss: 2.1649629569053648\n",
      "  batch 800 loss: 2.1334730267524717\n",
      "  batch 850 loss: 2.1411286926269533\n",
      "  batch 900 loss: 2.213003101348877\n",
      "LOSS train 2.21300 valid 2.19270, valid PER 68.10%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 2.2244587898254395\n",
      "  batch 100 loss: 2.2351739501953123\n",
      "  batch 150 loss: 2.197829797267914\n",
      "  batch 200 loss: 2.24974280834198\n",
      "  batch 250 loss: 2.179404947757721\n",
      "  batch 300 loss: 2.249880814552307\n",
      "  batch 350 loss: 2.2523147201538087\n",
      "  batch 400 loss: 2.237800271511078\n",
      "  batch 450 loss: 2.2440199136734007\n",
      "  batch 500 loss: 2.282602527141571\n",
      "  batch 550 loss: 2.186105763912201\n",
      "  batch 600 loss: 2.265649952888489\n",
      "  batch 650 loss: 2.265620832443237\n",
      "  batch 700 loss: 2.239864568710327\n",
      "  batch 750 loss: 2.247976098060608\n",
      "  batch 800 loss: 2.2671597003936768\n",
      "  batch 850 loss: 2.2398372721672057\n",
      "  batch 900 loss: 2.2403609204292296\n",
      "LOSS train 2.24036 valid 2.22193, valid PER 75.28%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 2.172789788246155\n",
      "  batch 100 loss: 2.233632860183716\n",
      "  batch 150 loss: 2.212549045085907\n",
      "  batch 200 loss: 2.154027383327484\n",
      "  batch 250 loss: 2.2072493171691896\n",
      "  batch 300 loss: 2.2073573660850525\n",
      "  batch 350 loss: 2.2623682022094727\n",
      "  batch 400 loss: 2.2153243231773376\n",
      "  batch 450 loss: 2.2385357809066773\n",
      "  batch 500 loss: 2.280394287109375\n",
      "  batch 550 loss: 2.371913332939148\n",
      "  batch 600 loss: 2.354541778564453\n",
      "  batch 650 loss: 2.3008353686332703\n",
      "  batch 700 loss: 2.305169651508331\n",
      "  batch 750 loss: 2.296122069358826\n",
      "  batch 800 loss: 2.268731997013092\n",
      "  batch 850 loss: 2.266711363792419\n",
      "  batch 900 loss: 2.2608741092681885\n",
      "LOSS train 2.26087 valid 2.22127, valid PER 73.03%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 2.1637175130844115\n",
      "  batch 100 loss: 2.265108106136322\n",
      "  batch 150 loss: 2.204552969932556\n",
      "  batch 200 loss: 2.1415099930763244\n",
      "  batch 250 loss: 2.2347959446907044\n",
      "  batch 300 loss: 2.2191917181015013\n",
      "  batch 350 loss: 2.2306027936935426\n",
      "  batch 400 loss: 2.1782298922538756\n",
      "  batch 450 loss: 2.2088604283332827\n",
      "  batch 500 loss: 2.1964679074287417\n",
      "  batch 550 loss: 2.214595994949341\n",
      "  batch 600 loss: 2.2145178937911987\n",
      "  batch 650 loss: 2.150474910736084\n",
      "  batch 700 loss: 2.210639841556549\n",
      "  batch 750 loss: 2.1921355056762697\n",
      "  batch 800 loss: 2.1370981192588805\n",
      "  batch 850 loss: 2.19802059173584\n",
      "  batch 900 loss: 2.148552808761597\n",
      "LOSS train 2.14855 valid 2.20153, valid PER 68.74%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 2.167481961250305\n",
      "  batch 100 loss: 2.1830497860908507\n",
      "  batch 150 loss: 2.183422875404358\n",
      "  batch 200 loss: 2.2032951712608337\n",
      "  batch 250 loss: 2.182557382583618\n",
      "  batch 300 loss: 2.191736738681793\n",
      "  batch 350 loss: 2.1852711510658263\n",
      "  batch 400 loss: 2.2505600595474244\n",
      "  batch 450 loss: 2.1633325386047364\n",
      "  batch 500 loss: 2.311830997467041\n",
      "  batch 550 loss: 2.3209178113937377\n",
      "  batch 600 loss: 2.426141872406006\n",
      "  batch 650 loss: 2.29918429851532\n",
      "  batch 700 loss: 2.2421109461784363\n",
      "  batch 750 loss: 2.2320463109016417\n",
      "  batch 800 loss: 2.2781941318511962\n",
      "  batch 850 loss: 2.2732157945632934\n",
      "  batch 900 loss: 2.252552535533905\n",
      "LOSS train 2.25255 valid 2.29508, valid PER 71.28%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 2.2914853382110594\n",
      "  batch 100 loss: 2.2521933770179747\n",
      "  batch 150 loss: 2.2375719022750853\n",
      "  batch 200 loss: 2.2945425367355345\n",
      "  batch 250 loss: 2.3695943212509154\n",
      "  batch 300 loss: 2.3127147436141966\n",
      "  batch 350 loss: 2.2930330204963685\n",
      "  batch 400 loss: 2.3085876274108887\n",
      "  batch 450 loss: 2.351794729232788\n",
      "  batch 500 loss: 2.2905630707740783\n",
      "  batch 550 loss: 2.2733523631095887\n",
      "  batch 600 loss: 2.3004941892623902\n",
      "  batch 650 loss: 2.3416929864883422\n",
      "  batch 700 loss: 2.2806293201446532\n",
      "  batch 750 loss: 2.286892499923706\n",
      "  batch 800 loss: 2.289347777366638\n",
      "  batch 850 loss: 2.3980813217163086\n",
      "  batch 900 loss: 2.314438428878784\n",
      "LOSS train 2.31444 valid 2.32981, valid PER 72.93%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 2.3254663610458373\n",
      "  batch 100 loss: 2.275011773109436\n",
      "  batch 150 loss: 2.317573058605194\n",
      "  batch 200 loss: 2.3087373208999633\n",
      "  batch 250 loss: 2.355293617248535\n",
      "  batch 300 loss: 2.271360991001129\n",
      "  batch 350 loss: 2.265363345146179\n",
      "  batch 400 loss: 2.3559633398056032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 450 loss: 2.36400251865387\n",
      "  batch 500 loss: 2.332400851249695\n",
      "  batch 550 loss: 2.363796339035034\n",
      "  batch 600 loss: 2.4117057657241823\n",
      "  batch 650 loss: 2.469039225578308\n",
      "  batch 700 loss: 2.374113063812256\n",
      "  batch 750 loss: 2.300539391040802\n",
      "  batch 800 loss: 2.3059232902526854\n",
      "  batch 850 loss: 2.3085934638977053\n",
      "  batch 900 loss: 2.3022136998176577\n",
      "LOSS train 2.30221 valid 2.28326, valid PER 72.92%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 2.2597289228439332\n",
      "  batch 100 loss: 2.2930776119232177\n",
      "  batch 150 loss: 2.2755022168159487\n",
      "  batch 200 loss: 2.265809569358826\n",
      "  batch 250 loss: 2.296639850139618\n",
      "  batch 300 loss: 2.255822117328644\n",
      "  batch 350 loss: 2.270432252883911\n",
      "  batch 400 loss: 2.278254849910736\n",
      "  batch 450 loss: 2.32291428565979\n",
      "  batch 500 loss: 2.2877138900756835\n",
      "  batch 550 loss: 2.2807856035232543\n",
      "  batch 600 loss: 2.222296025753021\n",
      "  batch 650 loss: 2.2069998383522034\n",
      "  batch 700 loss: 2.2459900689125063\n",
      "  batch 750 loss: 2.2056263732910155\n",
      "  batch 800 loss: 2.2112379336357115\n",
      "  batch 850 loss: 2.263984892368317\n",
      "  batch 900 loss: 2.2545468020439148\n",
      "LOSS train 2.25455 valid 2.31616, valid PER 71.54%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 2.222740035057068\n",
      "  batch 100 loss: 2.256222519874573\n",
      "  batch 150 loss: 2.261663765907288\n",
      "  batch 200 loss: 2.272357065677643\n",
      "  batch 250 loss: 2.261334400177002\n",
      "  batch 300 loss: 2.259426054954529\n",
      "  batch 350 loss: 2.2211792635917664\n",
      "  batch 400 loss: 2.175834596157074\n",
      "  batch 450 loss: 2.1955465197563173\n",
      "  batch 500 loss: 2.2156556844711304\n",
      "  batch 550 loss: 2.2551338982582094\n",
      "  batch 600 loss: 2.2386909794807432\n",
      "  batch 650 loss: 2.2574922370910646\n",
      "  batch 700 loss: 2.272965979576111\n",
      "  batch 750 loss: 2.2082478642463683\n",
      "  batch 800 loss: 2.226291399002075\n",
      "  batch 850 loss: 2.2548873448371887\n",
      "  batch 900 loss: 2.2666482496261597\n",
      "LOSS train 2.26665 valid 2.26204, valid PER 70.12%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 2.321119270324707\n",
      "  batch 100 loss: 2.223521897792816\n",
      "  batch 150 loss: 2.2543621325492857\n",
      "  batch 200 loss: 2.24913254737854\n",
      "  batch 250 loss: 2.1939735507965086\n",
      "  batch 300 loss: 2.2259049582481385\n",
      "  batch 350 loss: 2.160912082195282\n",
      "  batch 400 loss: 2.1879318714141847\n",
      "  batch 450 loss: 2.2592270302772524\n",
      "  batch 500 loss: 2.2154744172096255\n",
      "  batch 550 loss: 2.2935903882980346\n",
      "  batch 600 loss: 2.2476185536384583\n",
      "  batch 650 loss: 2.2386898136138917\n",
      "  batch 700 loss: 2.212799165248871\n",
      "  batch 750 loss: 2.190481243133545\n",
      "  batch 800 loss: 2.22619113445282\n",
      "  batch 850 loss: 2.193354597091675\n",
      "  batch 900 loss: 2.283257267475128\n",
      "LOSS train 2.28326 valid 2.40129, valid PER 71.80%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 2.401400718688965\n",
      "  batch 100 loss: 2.3897063279151918\n",
      "  batch 150 loss: 2.3922137212753296\n",
      "  batch 200 loss: 2.501610312461853\n",
      "  batch 250 loss: 2.4511519622802735\n",
      "  batch 300 loss: 2.3402029204368593\n",
      "  batch 350 loss: 2.3409558439254763\n",
      "  batch 400 loss: 2.2750378346443174\n",
      "  batch 450 loss: 2.3184837198257444\n",
      "  batch 500 loss: 2.268395714759827\n",
      "  batch 550 loss: 2.3135657048225404\n",
      "  batch 600 loss: 2.278449501991272\n",
      "  batch 650 loss: 2.358614864349365\n",
      "  batch 700 loss: 2.3054046750068666\n",
      "  batch 750 loss: 2.284024941921234\n",
      "  batch 800 loss: 2.2946126079559326\n",
      "  batch 850 loss: 2.237702624797821\n",
      "  batch 900 loss: 2.211592116355896\n",
      "LOSS train 2.21159 valid 2.29782, valid PER 74.99%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 2.2248997497558594\n",
      "  batch 100 loss: 2.269275939464569\n",
      "  batch 150 loss: 2.2367295169830324\n",
      "  batch 200 loss: 2.2304348492622377\n",
      "  batch 250 loss: 2.251746487617493\n",
      "  batch 300 loss: 2.2638339376449585\n",
      "  batch 350 loss: 2.202619681358337\n",
      "  batch 400 loss: 2.2546539402008055\n",
      "  batch 450 loss: 2.2461184954643247\n",
      "  batch 500 loss: 2.2313829135894774\n",
      "  batch 550 loss: 2.218542957305908\n",
      "  batch 600 loss: 2.259174654483795\n",
      "  batch 650 loss: 2.234382381439209\n",
      "  batch 700 loss: 2.2010227608680726\n",
      "  batch 750 loss: 2.2139276123046874\n",
      "  batch 800 loss: 2.2628944158554076\n",
      "  batch 850 loss: 2.1762150955200195\n",
      "  batch 900 loss: 2.2520369505882263\n",
      "LOSS train 2.25204 valid 2.19388, valid PER 73.29%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 2.2628889536857604\n",
      "  batch 100 loss: 2.273098828792572\n",
      "  batch 150 loss: 2.2312036252021787\n",
      "  batch 200 loss: 2.194524095058441\n",
      "  batch 250 loss: 2.213474442958832\n",
      "  batch 300 loss: 2.2537039494514466\n",
      "  batch 350 loss: 2.257191524505615\n",
      "  batch 400 loss: 2.2493646335601807\n",
      "  batch 450 loss: 2.2553386211395265\n",
      "  batch 500 loss: 2.234856948852539\n",
      "  batch 550 loss: 2.188238124847412\n",
      "  batch 600 loss: 2.18433883190155\n",
      "  batch 650 loss: 2.176095542907715\n",
      "  batch 700 loss: 2.240594141483307\n",
      "  batch 750 loss: 2.2031563830375673\n",
      "  batch 800 loss: 2.2283254408836366\n",
      "  batch 850 loss: 2.1930883812904356\n",
      "  batch 900 loss: 2.2952228355407716\n",
      "LOSS train 2.29522 valid 2.29282, valid PER 72.11%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 2.202392911911011\n",
      "  batch 100 loss: 2.230585746765137\n",
      "  batch 150 loss: 2.1938357520103455\n",
      "  batch 200 loss: 2.2280665040016174\n",
      "  batch 250 loss: 2.2019646525382996\n",
      "  batch 300 loss: 2.2621763563156128\n",
      "  batch 350 loss: 2.233375558853149\n",
      "  batch 400 loss: 2.244875977039337\n",
      "  batch 450 loss: 2.2171889686584474\n",
      "  batch 500 loss: 2.253918852806091\n",
      "  batch 550 loss: 2.223057522773743\n",
      "  batch 600 loss: 2.2540374851226805\n",
      "  batch 650 loss: 2.2512871193885804\n",
      "  batch 700 loss: 2.3317960238456727\n",
      "  batch 750 loss: 2.2058437132835387\n",
      "  batch 800 loss: 2.239071171283722\n",
      "  batch 850 loss: 2.2426250314712526\n",
      "  batch 900 loss: 2.2017638897895813\n",
      "LOSS train 2.20176 valid 2.21960, valid PER 73.31%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 2.234723439216614\n",
      "  batch 100 loss: 2.228803741931915\n",
      "  batch 150 loss: 2.229002766609192\n",
      "  batch 200 loss: 2.212204155921936\n",
      "  batch 250 loss: 2.2218061900138855\n",
      "  batch 300 loss: 2.295239255428314\n",
      "  batch 350 loss: 2.2070662665367125\n",
      "  batch 400 loss: 2.196022515296936\n",
      "  batch 450 loss: 2.216532776355743\n",
      "  batch 500 loss: 2.228744957447052\n",
      "  batch 550 loss: 2.27520583152771\n",
      "  batch 600 loss: 2.195004303455353\n",
      "  batch 650 loss: 2.253654327392578\n",
      "  batch 700 loss: 2.225809054374695\n",
      "  batch 750 loss: 2.2133634638786317\n",
      "  batch 800 loss: 2.2817033338546753\n",
      "  batch 850 loss: 2.2698525190353394\n",
      "  batch 900 loss: 2.2883960747718812\n",
      "LOSS train 2.28840 valid 2.37396, valid PER 68.68%\n",
      "Training finished in 3.0 minutes.\n",
      "Model saved to checkpoints/20231205_220104/model_5\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "from datetime import datetime\n",
    "from trainer_Adam import train\n",
    "\n",
    "\n",
    "model = models.BiLSTM(\n",
    "    args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab))\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of model parameters is {}'.format(num_params))\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "model.to(args.device)\n",
    "model_path = train(model, args)\n",
    "end = datetime.now()\n",
    "duration = (end - start).total_seconds()\n",
    "print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "print('Model saved to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e74be8d",
   "metadata": {},
   "source": [
    "#### Try SGD and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ebda2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters is 166952\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "model = models.BiLSTM(\n",
    "    args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab))\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of model parameters is {}'.format(num_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a5c1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 50 loss: 5.0952321672439576\n",
      "  batch 100 loss: 3.3160207796096803\n",
      "  batch 150 loss: 3.1283680248260497\n",
      "  batch 200 loss: 2.86003559589386\n",
      "  batch 250 loss: 2.67761887550354\n",
      "  batch 300 loss: 2.490350155830383\n",
      "  batch 350 loss: 2.382415699958801\n",
      "  batch 400 loss: 2.332326679229736\n",
      "  batch 450 loss: 2.256763663291931\n",
      "  batch 500 loss: 2.150912969112396\n",
      "  batch 550 loss: 2.098514678478241\n",
      "  batch 600 loss: 2.03779465675354\n",
      "  batch 650 loss: 1.970644669532776\n",
      "  batch 700 loss: 1.9605630493164063\n",
      "  batch 750 loss: 1.892756962776184\n",
      "  batch 800 loss: 1.8726566624641419\n",
      "  batch 850 loss: 1.8246976494789124\n",
      "  batch 900 loss: 1.8179186940193177\n",
      "LOSS train 1.81792 valid 1.74112, valid PER 67.86%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.754153950214386\n",
      "  batch 100 loss: 1.69423823595047\n",
      "  batch 150 loss: 1.6750897979736328\n",
      "  batch 200 loss: 1.6854981899261474\n",
      "  batch 250 loss: 1.6781074762344361\n",
      "  batch 300 loss: 1.6559325432777405\n",
      "  batch 350 loss: 1.5633166575431823\n",
      "  batch 400 loss: 1.5735180068016053\n",
      "  batch 450 loss: 1.5381962203979491\n",
      "  batch 500 loss: 1.560617938041687\n",
      "  batch 550 loss: 1.5490953373908996\n",
      "  batch 600 loss: 1.4974487257003783\n",
      "  batch 650 loss: 1.552313940525055\n",
      "  batch 700 loss: 1.4962171292304993\n",
      "  batch 750 loss: 1.4935984539985656\n",
      "  batch 800 loss: 1.4333527016639709\n",
      "  batch 850 loss: 1.442918872833252\n",
      "  batch 900 loss: 1.4676607275009155\n",
      "LOSS train 1.46766 valid 1.41217, valid PER 50.77%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4250319480895997\n",
      "  batch 100 loss: 1.3836843585968017\n",
      "  batch 150 loss: 1.3824661898612975\n",
      "  batch 200 loss: 1.3721792459487916\n",
      "  batch 250 loss: 1.3502893686294555\n",
      "  batch 300 loss: 1.3409212839603424\n",
      "  batch 350 loss: 1.3993731808662415\n",
      "  batch 400 loss: 1.3699065589904784\n",
      "  batch 450 loss: 1.3273127901554107\n",
      "  batch 500 loss: 1.3045419001579284\n",
      "  batch 550 loss: 1.316324303150177\n",
      "  batch 600 loss: 1.2929167592525481\n",
      "  batch 650 loss: 1.2817277443408965\n",
      "  batch 700 loss: 1.2797519195079803\n",
      "  batch 750 loss: 1.3419922089576721\n",
      "  batch 800 loss: 1.2535636925697327\n",
      "  batch 850 loss: 1.30194269657135\n",
      "  batch 900 loss: 1.22025146484375\n",
      "LOSS train 1.22025 valid 1.27836, valid PER 40.11%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.211840616464615\n",
      "  batch 100 loss: 1.2155934751033783\n",
      "  batch 150 loss: 1.1997056317329406\n",
      "  batch 200 loss: 1.2037137472629547\n",
      "  batch 250 loss: 1.2306937181949615\n",
      "  batch 300 loss: 1.215690850019455\n",
      "  batch 350 loss: 1.1375350081920623\n",
      "  batch 400 loss: 1.1869281709194184\n",
      "  batch 450 loss: 1.1715705335140227\n",
      "  batch 500 loss: 1.1595908844470977\n",
      "  batch 550 loss: 1.2003939044475556\n",
      "  batch 600 loss: 1.1956381273269654\n",
      "  batch 650 loss: 1.1684688138961792\n",
      "  batch 700 loss: 1.133233425617218\n",
      "  batch 750 loss: 1.1391667056083679\n",
      "  batch 800 loss: 1.1097060549259186\n",
      "  batch 850 loss: 1.133504490852356\n",
      "  batch 900 loss: 1.1748942124843598\n",
      "LOSS train 1.17489 valid 1.14530, valid PER 36.59%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.093550008535385\n",
      "  batch 100 loss: 1.090077313184738\n",
      "  batch 150 loss: 1.1469501674175262\n",
      "  batch 200 loss: 1.072099311351776\n",
      "  batch 250 loss: 1.0632811307907104\n",
      "  batch 300 loss: 1.0818544387817384\n",
      "  batch 350 loss: 1.0847523212432861\n",
      "  batch 400 loss: 1.0985290360450746\n",
      "  batch 450 loss: 1.0703925490379333\n",
      "  batch 500 loss: 1.0865785837173463\n",
      "  batch 550 loss: 1.0396790266036988\n",
      "  batch 600 loss: 1.107631468772888\n",
      "  batch 650 loss: 1.070506466627121\n",
      "  batch 700 loss: 1.1083322763442993\n",
      "  batch 750 loss: 1.0365183472633361\n",
      "  batch 800 loss: 1.090454398393631\n",
      "  batch 850 loss: 1.0641397762298583\n",
      "  batch 900 loss: 1.076686714887619\n",
      "LOSS train 1.07669 valid 1.09921, valid PER 33.92%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.064879525899887\n",
      "  batch 100 loss: 0.9995841360092164\n",
      "  batch 150 loss: 1.003015819787979\n",
      "  batch 200 loss: 0.9979974520206452\n",
      "  batch 250 loss: 1.0510727679729461\n",
      "  batch 300 loss: 1.036609560251236\n",
      "  batch 350 loss: 1.0147744953632354\n",
      "  batch 400 loss: 1.0085549318790437\n",
      "  batch 450 loss: 1.014736875295639\n",
      "  batch 500 loss: 1.0062509214878081\n",
      "  batch 550 loss: 1.0332730853557586\n",
      "  batch 600 loss: 1.0065790116786957\n",
      "  batch 650 loss: 1.0175069427490235\n",
      "  batch 700 loss: 1.0188953566551209\n",
      "  batch 750 loss: 0.9952565836906433\n",
      "  batch 800 loss: 0.9750145375728607\n",
      "  batch 850 loss: 0.9864048826694488\n",
      "  batch 900 loss: 0.9931457710266113\n",
      "LOSS train 0.99315 valid 1.05193, valid PER 33.27%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.977256315946579\n",
      "  batch 100 loss: 0.9876729822158814\n",
      "  batch 150 loss: 0.9550627565383911\n",
      "  batch 200 loss: 0.9469825935363769\n",
      "  batch 250 loss: 0.9592474317550659\n",
      "  batch 300 loss: 0.9533262300491333\n",
      "  batch 350 loss: 0.9740353727340698\n",
      "  batch 400 loss: 0.9525622236728668\n",
      "  batch 450 loss: 0.9528471124172211\n",
      "  batch 500 loss: 0.9421815001964569\n",
      "  batch 550 loss: 0.9528872179985046\n",
      "  batch 600 loss: 0.9810743021965027\n",
      "  batch 650 loss: 0.9400803053379059\n",
      "  batch 700 loss: 0.9848231148719787\n",
      "  batch 750 loss: 0.9525479888916015\n",
      "  batch 800 loss: 0.9377138435840606\n",
      "  batch 850 loss: 0.978015615940094\n",
      "  batch 900 loss: 0.9908531486988068\n",
      "LOSS train 0.99085 valid 1.02167, valid PER 32.21%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.9202892374992371\n",
      "  batch 100 loss: 0.9002745008468628\n",
      "  batch 150 loss: 0.9002601337432862\n",
      "  batch 200 loss: 0.8788454675674439\n",
      "  batch 250 loss: 0.9170841658115387\n",
      "  batch 300 loss: 0.8745781016349793\n",
      "  batch 350 loss: 0.9521904003620147\n",
      "  batch 400 loss: 0.9067364180088043\n",
      "  batch 450 loss: 0.9209674417972564\n",
      "  batch 500 loss: 0.9567841410636901\n",
      "  batch 550 loss: 0.891649683713913\n",
      "  batch 600 loss: 0.9428314566612244\n",
      "  batch 650 loss: 0.944530576467514\n",
      "  batch 700 loss: 0.9000021302700043\n",
      "  batch 750 loss: 0.9113349521160126\n",
      "  batch 800 loss: 0.9237780630588531\n",
      "  batch 850 loss: 0.920720626115799\n",
      "  batch 900 loss: 0.9293360912799835\n",
      "LOSS train 0.92934 valid 1.01193, valid PER 30.72%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.853445098400116\n",
      "  batch 100 loss: 0.87451376080513\n",
      "  batch 150 loss: 0.890960077047348\n",
      "  batch 200 loss: 0.849829021692276\n",
      "  batch 250 loss: 0.8872257626056671\n",
      "  batch 300 loss: 0.9013537406921387\n",
      "  batch 350 loss: 0.9135573041439057\n",
      "  batch 400 loss: 0.8815892338752747\n",
      "  batch 450 loss: 0.8800281476974487\n",
      "  batch 500 loss: 0.8612251543998718\n",
      "  batch 550 loss: 0.8904681336879731\n",
      "  batch 600 loss: 0.8799685645103454\n",
      "  batch 650 loss: 0.8657240319252014\n",
      "  batch 700 loss: 0.8618838334083557\n",
      "  batch 750 loss: 0.8808263552188873\n",
      "  batch 800 loss: 0.903206639289856\n",
      "  batch 850 loss: 0.8955211186408997\n",
      "  batch 900 loss: 0.8556899273395538\n",
      "LOSS train 0.85569 valid 0.99039, valid PER 30.11%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.8044714951515197\n",
      "  batch 100 loss: 0.8342689836025238\n",
      "  batch 150 loss: 0.8635126209259033\n",
      "  batch 200 loss: 0.8619482159614563\n",
      "  batch 250 loss: 0.8639614677429199\n",
      "  batch 300 loss: 0.8306986463069915\n",
      "  batch 350 loss: 0.8383423030376435\n",
      "  batch 400 loss: 0.8021140134334565\n",
      "  batch 450 loss: 0.8290119695663453\n",
      "  batch 500 loss: 0.86108269572258\n",
      "  batch 550 loss: 0.8735495519638061\n",
      "  batch 600 loss: 0.8510240429639816\n",
      "  batch 650 loss: 0.8257914733886719\n",
      "  batch 700 loss: 0.8610347521305084\n",
      "  batch 750 loss: 0.8431802654266357\n",
      "  batch 800 loss: 0.8567908203601837\n",
      "  batch 850 loss: 0.867970963716507\n",
      "  batch 900 loss: 0.865688750743866\n",
      "LOSS train 0.86569 valid 0.96599, valid PER 30.22%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.7824347841739655\n",
      "  batch 100 loss: 0.7716624331474304\n",
      "  batch 150 loss: 0.7885448563098908\n",
      "  batch 200 loss: 0.8246989250183105\n",
      "  batch 250 loss: 0.8179709798097611\n",
      "  batch 300 loss: 0.7813552701473236\n",
      "  batch 350 loss: 0.8050485324859619\n",
      "  batch 400 loss: 0.82758061170578\n",
      "  batch 450 loss: 0.8333303785324097\n",
      "  batch 500 loss: 0.8215790224075318\n",
      "  batch 550 loss: 0.8216319227218628\n",
      "  batch 600 loss: 0.8079890769720077\n",
      "  batch 650 loss: 0.8556946313381195\n",
      "  batch 700 loss: 0.796388429403305\n",
      "  batch 750 loss: 0.8107077753543854\n",
      "  batch 800 loss: 0.8385069227218628\n",
      "  batch 850 loss: 0.8406523013114929\n",
      "  batch 900 loss: 0.8422089409828186\n",
      "LOSS train 0.84221 valid 0.95558, valid PER 29.27%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.7916794157028199\n",
      "  batch 100 loss: 0.7759236973524094\n",
      "  batch 150 loss: 0.7645035707950592\n",
      "  batch 200 loss: 0.7810063242912293\n",
      "  batch 250 loss: 0.7899763333797455\n",
      "  batch 300 loss: 0.7863978409767151\n",
      "  batch 350 loss: 0.7676592087745666\n",
      "  batch 400 loss: 0.8111073982715606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 450 loss: 0.7993086898326873\n",
      "  batch 500 loss: 0.8112813603878021\n",
      "  batch 550 loss: 0.7378953850269317\n",
      "  batch 600 loss: 0.7759832632541657\n",
      "  batch 650 loss: 0.8329361653327942\n",
      "  batch 700 loss: 0.8032406920194626\n",
      "  batch 750 loss: 0.783981266617775\n",
      "  batch 800 loss: 0.7714516514539719\n",
      "  batch 850 loss: 0.8259145557880402\n",
      "  batch 900 loss: 0.8219052803516388\n",
      "LOSS train 0.82191 valid 0.94374, valid PER 29.17%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7252003991603851\n",
      "  batch 100 loss: 0.764584436416626\n",
      "  batch 150 loss: 0.7489726227521897\n",
      "  batch 200 loss: 0.7628170549869537\n",
      "  batch 250 loss: 0.7518410086631775\n",
      "  batch 300 loss: 0.76351567029953\n",
      "  batch 350 loss: 0.761681181192398\n",
      "  batch 400 loss: 0.7724109297990799\n",
      "  batch 450 loss: 0.7606583511829377\n",
      "  batch 500 loss: 0.738277200460434\n",
      "  batch 550 loss: 0.801950433254242\n",
      "  batch 600 loss: 0.7445122092962265\n",
      "  batch 650 loss: 0.780785500407219\n",
      "  batch 700 loss: 0.7853784263134003\n",
      "  batch 750 loss: 0.7379002737998962\n",
      "  batch 800 loss: 0.7676283073425293\n",
      "  batch 850 loss: 0.7896682941913604\n",
      "  batch 900 loss: 0.7843290936946868\n",
      "Epoch 00013: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.78433 valid 0.95808, valid PER 29.07%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.7048774492740632\n",
      "  batch 100 loss: 0.6870517510175705\n",
      "  batch 150 loss: 0.6884626710414886\n",
      "  batch 200 loss: 0.6766123807430268\n",
      "  batch 250 loss: 0.6647799402475357\n",
      "  batch 300 loss: 0.7049305522441864\n",
      "  batch 350 loss: 0.6536860036849975\n",
      "  batch 400 loss: 0.6705972737073899\n",
      "  batch 450 loss: 0.6798541069030761\n",
      "  batch 500 loss: 0.6770954823493958\n",
      "  batch 550 loss: 0.6851723366975784\n",
      "  batch 600 loss: 0.6533946615457534\n",
      "  batch 650 loss: 0.6940727615356446\n",
      "  batch 700 loss: 0.7201903522014618\n",
      "  batch 750 loss: 0.6592975026369094\n",
      "  batch 800 loss: 0.6470784670114518\n",
      "  batch 850 loss: 0.6894684040546417\n",
      "  batch 900 loss: 0.6685743534564972\n",
      "LOSS train 0.66857 valid 0.91593, valid PER 27.36%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6556781017780304\n",
      "  batch 100 loss: 0.643754358291626\n",
      "  batch 150 loss: 0.6494624674320221\n",
      "  batch 200 loss: 0.6586384999752045\n",
      "  batch 250 loss: 0.6570121538639069\n",
      "  batch 300 loss: 0.6504053854942322\n",
      "  batch 350 loss: 0.6572673606872559\n",
      "  batch 400 loss: 0.6537317335605621\n",
      "  batch 450 loss: 0.6296560281515121\n",
      "  batch 500 loss: 0.6201580393314362\n",
      "  batch 550 loss: 0.646887531876564\n",
      "  batch 600 loss: 0.6665392827987671\n",
      "  batch 650 loss: 0.680447296500206\n",
      "  batch 700 loss: 0.6804777842760086\n",
      "  batch 750 loss: 0.6657286292314529\n",
      "  batch 800 loss: 0.6484955084323883\n",
      "  batch 850 loss: 0.6449417299032212\n",
      "  batch 900 loss: 0.667136303782463\n",
      "LOSS train 0.66714 valid 0.90670, valid PER 27.27%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.6452808141708374\n",
      "  batch 100 loss: 0.6065493577718735\n",
      "  batch 150 loss: 0.6207895249128341\n",
      "  batch 200 loss: 0.6349319797754288\n",
      "  batch 250 loss: 0.6425026208162308\n",
      "  batch 300 loss: 0.6314676880836487\n",
      "  batch 350 loss: 0.6387510931491852\n",
      "  batch 400 loss: 0.6514770901203155\n",
      "  batch 450 loss: 0.6620832628011704\n",
      "  batch 500 loss: 0.6255878722667694\n",
      "  batch 550 loss: 0.635613899230957\n",
      "  batch 600 loss: 0.6242431342601776\n",
      "  batch 650 loss: 0.6576803261041642\n",
      "  batch 700 loss: 0.6250316226482391\n",
      "  batch 750 loss: 0.6402464306354523\n",
      "  batch 800 loss: 0.6421866631507873\n",
      "  batch 850 loss: 0.6268392264842987\n",
      "  batch 900 loss: 0.638152317404747\n",
      "LOSS train 0.63815 valid 0.90133, valid PER 26.88%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.6242558872699737\n",
      "  batch 100 loss: 0.6162260055541993\n",
      "  batch 150 loss: 0.5934063297510147\n",
      "  batch 200 loss: 0.6014073324203492\n",
      "  batch 250 loss: 0.6228998571634292\n",
      "  batch 300 loss: 0.6261813104152679\n",
      "  batch 350 loss: 0.5902260780334473\n",
      "  batch 400 loss: 0.647748835682869\n",
      "  batch 450 loss: 0.6383840203285217\n",
      "  batch 500 loss: 0.6169714456796647\n",
      "  batch 550 loss: 0.6189358276128769\n",
      "  batch 600 loss: 0.6363066917657852\n",
      "  batch 650 loss: 0.6226022553443908\n",
      "  batch 700 loss: 0.6294252461194992\n",
      "  batch 750 loss: 0.6185182338953018\n",
      "  batch 800 loss: 0.6158067435026169\n",
      "  batch 850 loss: 0.6419476038217544\n",
      "  batch 900 loss: 0.6145397251844407\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.61454 valid 0.91108, valid PER 26.87%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.5817510557174682\n",
      "  batch 100 loss: 0.596717169880867\n",
      "  batch 150 loss: 0.6022546094655991\n",
      "  batch 200 loss: 0.5808193296194076\n",
      "  batch 250 loss: 0.5738434547185898\n",
      "  batch 300 loss: 0.5587156540155411\n",
      "  batch 350 loss: 0.5638516622781754\n",
      "  batch 400 loss: 0.5510312050580979\n",
      "  batch 450 loss: 0.5844226318597794\n",
      "  batch 500 loss: 0.5795522791147232\n",
      "  batch 550 loss: 0.5836470830440521\n",
      "  batch 600 loss: 0.5550405323505402\n",
      "  batch 650 loss: 0.5469003480672836\n",
      "  batch 700 loss: 0.5900251132249832\n",
      "  batch 750 loss: 0.5598636478185653\n",
      "  batch 800 loss: 0.5648373937606812\n",
      "  batch 850 loss: 0.5486628276109695\n",
      "  batch 900 loss: 0.5971681523323059\n",
      "Epoch 00018: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.59717 valid 0.90969, valid PER 26.90%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5270647078752517\n",
      "  batch 100 loss: 0.5190035885572434\n",
      "  batch 150 loss: 0.5387415683269501\n",
      "  batch 200 loss: 0.5524224030971527\n",
      "  batch 250 loss: 0.5541458249092102\n",
      "  batch 300 loss: 0.5387283211946488\n",
      "  batch 350 loss: 0.5353441303968429\n",
      "  batch 400 loss: 0.5436434698104858\n",
      "  batch 450 loss: 0.5640394842624664\n",
      "  batch 500 loss: 0.5473935335874558\n",
      "  batch 550 loss: 0.5244343721866608\n",
      "  batch 600 loss: 0.5492979335784912\n",
      "  batch 650 loss: 0.5686778455972672\n",
      "  batch 700 loss: 0.5255138874053955\n",
      "  batch 750 loss: 0.523455741405487\n",
      "  batch 800 loss: 0.5383611023426056\n",
      "  batch 850 loss: 0.5406033843755722\n",
      "  batch 900 loss: 0.548653958439827\n",
      "Epoch 00019: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.54865 valid 0.91208, valid PER 26.67%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.5138728255033493\n",
      "  batch 100 loss: 0.5103406399488449\n",
      "  batch 150 loss: 0.5122574687004089\n",
      "  batch 200 loss: 0.5310775399208069\n",
      "  batch 250 loss: 0.5236593049764633\n",
      "  batch 300 loss: 0.5260211223363876\n",
      "  batch 350 loss: 0.491410693526268\n",
      "  batch 400 loss: 0.5286088460683822\n",
      "  batch 450 loss: 0.5288281625509262\n",
      "  batch 500 loss: 0.5097378271818161\n",
      "  batch 550 loss: 0.5579546540975571\n",
      "  batch 600 loss: 0.5003527444601059\n",
      "  batch 650 loss: 0.5373123341798782\n",
      "  batch 700 loss: 0.5313535022735596\n",
      "  batch 750 loss: 0.4915821793675423\n",
      "  batch 800 loss: 0.5355214834213257\n",
      "  batch 850 loss: 0.542218154668808\n",
      "  batch 900 loss: 0.5500740975141525\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.55007 valid 0.91055, valid PER 26.72%\n",
      "Training finished in 4.0 minutes.\n",
      "Model saved to checkpoints/20231205_222139/model_16\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train\n",
    "start = datetime.now()\n",
    "model.to(args.device)\n",
    "model_path = train(model, args)\n",
    "end = datetime.now()\n",
    "duration = (end - start).total_seconds()\n",
    "print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "print('Model saved to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d32ca5",
   "metadata": {},
   "source": [
    "# Study of model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671bd3cd",
   "metadata": {},
   "source": [
    "## Grid Search fine tune 3 models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb9373",
   "metadata": {},
   "source": [
    "## Basically fine-tune dropout rate and optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99c3059",
   "metadata": {},
   "source": [
    "### Due to limitation in time, choose Dropout Rate=[0.1, 0.2, 0.3, 0.4, 0.5], Optimiser =[ Adam with default setting, SGD with lr Scheduler], talk about why SGD with Scheduler instead of SGD const is inside the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f27d364",
   "metadata": {},
   "source": [
    "## 1. Two Layer LSTM (dropout in the layer between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da90f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "Optimiser = [\"Adam\", \"SGD_Scheduler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe94f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dropout tuning For 2 Layer LSTM, with Dropout between layer\n",
      "Currently using Adam optimiser\n",
      "Currently using dropout rate of 0.1\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.89443835735321\n",
      "  batch 100 loss: 3.216618900299072\n",
      "  batch 150 loss: 2.989476499557495\n",
      "  batch 200 loss: 2.6915573501586914\n",
      "  batch 250 loss: 2.415686373710632\n",
      "  batch 300 loss: 2.1722241330146788\n",
      "  batch 350 loss: 1.98657954454422\n",
      "  batch 400 loss: 1.9281827473640443\n",
      "  batch 450 loss: 1.8057073187828063\n",
      "  batch 500 loss: 1.6893247222900392\n",
      "  batch 550 loss: 1.6369571709632873\n",
      "  batch 600 loss: 1.5868984532356263\n",
      "  batch 650 loss: 1.5016728234291077\n",
      "  batch 700 loss: 1.5052967405319213\n",
      "  batch 750 loss: 1.4288172364234923\n",
      "  batch 800 loss: 1.4431952023506165\n",
      "  batch 850 loss: 1.40240469455719\n",
      "  batch 900 loss: 1.357992215156555\n",
      "LOSS train 1.35799 valid 1.33853, valid PER 42.81%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.318509771823883\n",
      "  batch 100 loss: 1.3002801442146301\n",
      "  batch 150 loss: 1.227840770483017\n",
      "  batch 200 loss: 1.2422858035564424\n",
      "  batch 250 loss: 1.235194778442383\n",
      "  batch 300 loss: 1.1983418834209443\n",
      "  batch 350 loss: 1.2426740324497223\n",
      "  batch 400 loss: 1.1913181221485138\n",
      "  batch 450 loss: 1.1708648633956908\n",
      "  batch 500 loss: 1.176636483669281\n",
      "  batch 550 loss: 1.1433164083957672\n",
      "  batch 600 loss: 1.1329286253452302\n",
      "  batch 650 loss: 1.0796356856822968\n",
      "  batch 700 loss: 1.1228234994411468\n",
      "  batch 750 loss: 1.0940543282032014\n",
      "  batch 800 loss: 1.0935112249851227\n",
      "  batch 850 loss: 1.0692397332191468\n",
      "  batch 900 loss: 1.0455066764354706\n",
      "LOSS train 1.04551 valid 1.05066, valid PER 33.34%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 0.9854331636428832\n",
      "  batch 100 loss: 1.039632933139801\n",
      "  batch 150 loss: 1.0361819076538086\n",
      "  batch 200 loss: 0.9924901127815247\n",
      "  batch 250 loss: 0.9842971062660217\n",
      "  batch 300 loss: 0.9946177911758423\n",
      "  batch 350 loss: 1.001366881132126\n",
      "  batch 400 loss: 0.9799506497383118\n",
      "  batch 450 loss: 0.9617582082748413\n",
      "  batch 500 loss: 0.9455606460571289\n",
      "  batch 550 loss: 0.9791136753559112\n",
      "  batch 600 loss: 0.9103221738338471\n",
      "  batch 650 loss: 0.9440167546272278\n",
      "  batch 700 loss: 0.9523707449436187\n",
      "  batch 750 loss: 0.9462054646015168\n",
      "  batch 800 loss: 0.9824052309989929\n",
      "  batch 850 loss: 0.9221285450458526\n",
      "  batch 900 loss: 0.9518662893772125\n",
      "LOSS train 0.95187 valid 0.94821, valid PER 30.03%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.9042745876312256\n",
      "  batch 100 loss: 0.8512124919891357\n",
      "  batch 150 loss: 0.8944244241714477\n",
      "  batch 200 loss: 0.8686172068119049\n",
      "  batch 250 loss: 0.8683968698978424\n",
      "  batch 300 loss: 0.8776634120941162\n",
      "  batch 350 loss: 0.8543827152252197\n",
      "  batch 400 loss: 0.8280925858020782\n",
      "  batch 450 loss: 0.8295434355735779\n",
      "  batch 500 loss: 0.9029628241062164\n",
      "  batch 550 loss: 0.8251702964305878\n",
      "  batch 600 loss: 0.8325978744029999\n",
      "  batch 650 loss: 0.8883307731151581\n",
      "  batch 700 loss: 0.8884319865703583\n",
      "  batch 750 loss: 0.8524957168102264\n",
      "  batch 800 loss: 0.8482468116283417\n",
      "  batch 850 loss: 0.836804016828537\n",
      "  batch 900 loss: 0.8515898668766022\n",
      "LOSS train 0.85159 valid 0.92465, valid PER 28.56%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.7889515590667725\n",
      "  batch 100 loss: 0.7722098982334137\n",
      "  batch 150 loss: 0.7923332726955414\n",
      "  batch 200 loss: 0.819769241809845\n",
      "  batch 250 loss: 0.7786661839485168\n",
      "  batch 300 loss: 0.8116905915737153\n",
      "  batch 350 loss: 0.7493764412403107\n",
      "  batch 400 loss: 0.7680183279514313\n",
      "  batch 450 loss: 0.7740950053930282\n",
      "  batch 500 loss: 0.7593815886974334\n",
      "  batch 550 loss: 0.7999348628520966\n",
      "  batch 600 loss: 0.7793882429599762\n",
      "  batch 650 loss: 0.7953270864486695\n",
      "  batch 700 loss: 0.7771137130260467\n",
      "  batch 750 loss: 0.7829754114151001\n",
      "  batch 800 loss: 0.8166885471343994\n",
      "  batch 850 loss: 0.8184218239784241\n",
      "  batch 900 loss: 0.7656483602523804\n",
      "LOSS train 0.76565 valid 0.87237, valid PER 27.19%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.7330974489450455\n",
      "  batch 100 loss: 0.717736792564392\n",
      "  batch 150 loss: 0.7229168558120728\n",
      "  batch 200 loss: 0.713571793437004\n",
      "  batch 250 loss: 0.7037719112634658\n",
      "  batch 300 loss: 0.7476140642166138\n",
      "  batch 350 loss: 0.7552349901199341\n",
      "  batch 400 loss: 0.7157419288158416\n",
      "  batch 450 loss: 0.7370010823011398\n",
      "  batch 500 loss: 0.6986988282203674\n",
      "  batch 550 loss: 0.7214767849445343\n",
      "  batch 600 loss: 0.7242503863573074\n",
      "  batch 650 loss: 0.6971317273378372\n",
      "  batch 700 loss: 0.6994375991821289\n",
      "  batch 750 loss: 0.7304641723632812\n",
      "  batch 800 loss: 0.7217547011375427\n",
      "  batch 850 loss: 0.7477887868881226\n",
      "  batch 900 loss: 0.7490553438663483\n",
      "LOSS train 0.74906 valid 0.83604, valid PER 26.10%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.6537396848201752\n",
      "  batch 100 loss: 0.6875640714168548\n",
      "  batch 150 loss: 0.6313280266523361\n",
      "  batch 200 loss: 0.660843551158905\n",
      "  batch 250 loss: 0.7155326282978058\n",
      "  batch 300 loss: 0.6670159685611725\n",
      "  batch 350 loss: 0.6956157582998276\n",
      "  batch 400 loss: 0.6526520067453384\n",
      "  batch 450 loss: 0.6810248947143555\n",
      "  batch 500 loss: 0.6726985919475555\n",
      "  batch 550 loss: 0.6759589147567749\n",
      "  batch 600 loss: 0.67675306558609\n",
      "  batch 650 loss: 0.6595810478925705\n",
      "  batch 700 loss: 0.6996486240625381\n",
      "  batch 750 loss: 0.6783182549476624\n",
      "  batch 800 loss: 0.6753859001398087\n",
      "  batch 850 loss: 0.6732304710149765\n",
      "  batch 900 loss: 0.6687804359197617\n",
      "LOSS train 0.66878 valid 0.82532, valid PER 25.90%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.6255853170156479\n",
      "  batch 100 loss: 0.5962114781141281\n",
      "  batch 150 loss: 0.6217355364561081\n",
      "  batch 200 loss: 0.6283410114049911\n",
      "  batch 250 loss: 0.6024436521530151\n",
      "  batch 300 loss: 0.6056591421365738\n",
      "  batch 350 loss: 0.6363885796070099\n",
      "  batch 400 loss: 0.6117736369371414\n",
      "  batch 450 loss: 0.6562212973833084\n",
      "  batch 500 loss: 0.622144296169281\n",
      "  batch 550 loss: 0.6309790259599686\n",
      "  batch 600 loss: 0.6049857980012894\n",
      "  batch 650 loss: 0.6408968710899353\n",
      "  batch 700 loss: 0.6592527192831039\n",
      "  batch 750 loss: 0.642458553314209\n",
      "  batch 800 loss: 0.6603347098827362\n",
      "  batch 850 loss: 0.6169866687059402\n",
      "  batch 900 loss: 0.6304097551107407\n",
      "LOSS train 0.63041 valid 0.81891, valid PER 25.42%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.5617569971084595\n",
      "  batch 100 loss: 0.569885082244873\n",
      "  batch 150 loss: 0.568795964717865\n",
      "  batch 200 loss: 0.572033874988556\n",
      "  batch 250 loss: 0.5551106429100037\n",
      "  batch 300 loss: 0.5919879704713822\n",
      "  batch 350 loss: 0.5611757779121399\n",
      "  batch 400 loss: 0.597463653087616\n",
      "  batch 450 loss: 0.5966557109355927\n",
      "  batch 500 loss: 0.5774780291318894\n",
      "  batch 550 loss: 0.5876035702228546\n",
      "  batch 600 loss: 0.6170714312791824\n",
      "  batch 650 loss: 0.619426755309105\n",
      "  batch 700 loss: 0.5867733162641525\n",
      "  batch 750 loss: 0.6024243706464767\n",
      "  batch 800 loss: 0.6068926882743836\n",
      "  batch 850 loss: 0.6155378329753876\n",
      "  batch 900 loss: 0.5799888199567795\n",
      "LOSS train 0.57999 valid 0.80121, valid PER 24.52%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.5237151753902435\n",
      "  batch 100 loss: 0.5385405206680298\n",
      "  batch 150 loss: 0.531075639128685\n",
      "  batch 200 loss: 0.5117139121890069\n",
      "  batch 250 loss: 0.5339713108539581\n",
      "  batch 300 loss: 0.5345734757184982\n",
      "  batch 350 loss: 0.5522763240337372\n",
      "  batch 400 loss: 0.5280235570669174\n",
      "  batch 450 loss: 0.5457753390073776\n",
      "  batch 500 loss: 0.5463526731729508\n",
      "  batch 550 loss: 0.5567386388778687\n",
      "  batch 600 loss: 0.552889060974121\n",
      "  batch 650 loss: 0.5709608870744706\n",
      "  batch 700 loss: 0.5543379342556\n",
      "  batch 750 loss: 0.5768005472421646\n",
      "  batch 800 loss: 0.5832807284593582\n",
      "  batch 850 loss: 0.5634432089328766\n",
      "  batch 900 loss: 0.5723767429590225\n",
      "LOSS train 0.57238 valid 0.80343, valid PER 24.45%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.49592955231666563\n",
      "  batch 100 loss: 0.4874447947740555\n",
      "  batch 150 loss: 0.492243595123291\n",
      "  batch 200 loss: 0.4725355166196823\n",
      "  batch 250 loss: 0.5021254202723503\n",
      "  batch 300 loss: 0.477805615067482\n",
      "  batch 350 loss: 0.5232140672206879\n",
      "  batch 400 loss: 0.4967033642530441\n",
      "  batch 450 loss: 0.49951704978942874\n",
      "  batch 500 loss: 0.5155950129032135\n",
      "  batch 550 loss: 0.5256943649053574\n",
      "  batch 600 loss: 0.5176733583211899\n",
      "  batch 650 loss: 0.5253436928987503\n",
      "  batch 700 loss: 0.5746551460027695\n",
      "  batch 750 loss: 0.5314664244651794\n",
      "  batch 800 loss: 0.5466225421428681\n",
      "  batch 850 loss: 0.5333798182010651\n",
      "  batch 900 loss: 0.5341876810789108\n",
      "LOSS train 0.53419 valid 0.79806, valid PER 23.86%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.43703163385391236\n",
      "  batch 100 loss: 0.43930575728416443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 150 loss: 0.4593409463763237\n",
      "  batch 200 loss: 0.4617572242021561\n",
      "  batch 250 loss: 0.4813243979215622\n",
      "  batch 300 loss: 0.4950635641813278\n",
      "  batch 350 loss: 0.4529670214653015\n",
      "  batch 400 loss: 0.5138579666614532\n",
      "  batch 450 loss: 0.4745971179008484\n",
      "  batch 500 loss: 0.4934589648246765\n",
      "  batch 550 loss: 0.4799868881702423\n",
      "  batch 600 loss: 0.49178849935531616\n",
      "  batch 650 loss: 0.48386700749397277\n",
      "  batch 700 loss: 0.49086237609386446\n",
      "  batch 750 loss: 0.5024517822265625\n",
      "  batch 800 loss: 0.480209481716156\n",
      "  batch 850 loss: 0.5043855285644532\n",
      "  batch 900 loss: 0.5117737752199173\n",
      "LOSS train 0.51177 valid 0.81207, valid PER 24.14%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.42272994577884676\n",
      "  batch 100 loss: 0.4382612407207489\n",
      "  batch 150 loss: 0.42835679709911345\n",
      "  batch 200 loss: 0.4145966079831123\n",
      "  batch 250 loss: 0.43551869571208957\n",
      "  batch 300 loss: 0.47151772916316986\n",
      "  batch 350 loss: 0.43700083911418913\n",
      "  batch 400 loss: 0.4424992525577545\n",
      "  batch 450 loss: 0.46599395453929904\n",
      "  batch 500 loss: 0.46460039138793946\n",
      "  batch 550 loss: 0.4801634055376053\n",
      "  batch 600 loss: 0.4590828838944435\n",
      "  batch 650 loss: 0.45869116723537445\n",
      "  batch 700 loss: 0.475477614402771\n",
      "  batch 750 loss: 0.45649372041225433\n",
      "  batch 800 loss: 0.4749399018287659\n",
      "  batch 850 loss: 0.4491416332125664\n",
      "  batch 900 loss: 0.46919843554496765\n",
      "LOSS train 0.46920 valid 0.81292, valid PER 23.60%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.3967436394095421\n",
      "  batch 100 loss: 0.3876172998547554\n",
      "  batch 150 loss: 0.40218391478061677\n",
      "  batch 200 loss: 0.406277434527874\n",
      "  batch 250 loss: 0.4058386731147766\n",
      "  batch 300 loss: 0.42926229536533356\n",
      "  batch 350 loss: 0.4174264967441559\n",
      "  batch 400 loss: 0.44716296255588533\n",
      "  batch 450 loss: 0.4126912462711334\n",
      "  batch 500 loss: 0.43132764875888824\n",
      "  batch 550 loss: 0.4265965995192528\n",
      "  batch 600 loss: 0.44119857788085937\n",
      "  batch 650 loss: 0.45227197766304017\n",
      "  batch 700 loss: 0.4414820843935013\n",
      "  batch 750 loss: 0.42605348616838457\n",
      "  batch 800 loss: 0.4433257669210434\n",
      "  batch 850 loss: 0.44386604309082034\n",
      "  batch 900 loss: 0.4413271051645279\n",
      "LOSS train 0.44133 valid 0.79568, valid PER 23.31%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.34405980169773104\n",
      "  batch 100 loss: 0.3666591450572014\n",
      "  batch 150 loss: 0.38272971361875535\n",
      "  batch 200 loss: 0.3919216164946556\n",
      "  batch 250 loss: 0.39925143361091614\n",
      "  batch 300 loss: 0.3927383568882942\n",
      "  batch 350 loss: 0.39946469783782956\n",
      "  batch 400 loss: 0.38717784225940705\n",
      "  batch 450 loss: 0.38935031205415727\n",
      "  batch 500 loss: 0.3680602550506592\n",
      "  batch 550 loss: 0.4353217494487762\n",
      "  batch 600 loss: 0.4306970277428627\n",
      "  batch 650 loss: 0.3974647238850594\n",
      "  batch 700 loss: 0.38439595460891723\n",
      "  batch 750 loss: 0.42962037086486815\n",
      "  batch 800 loss: 0.39412992030382155\n",
      "  batch 850 loss: 0.3988279131054878\n",
      "  batch 900 loss: 0.3919680014252663\n",
      "LOSS train 0.39197 valid 0.84350, valid PER 24.00%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.342877014875412\n",
      "  batch 100 loss: 0.3408813005685806\n",
      "  batch 150 loss: 0.35247398138046265\n",
      "  batch 200 loss: 0.37077512860298156\n",
      "  batch 250 loss: 0.3637257272005081\n",
      "  batch 300 loss: 0.36141826927661896\n",
      "  batch 350 loss: 0.35710996299982073\n",
      "  batch 400 loss: 0.3757460540533066\n",
      "  batch 450 loss: 0.39686831563711167\n",
      "  batch 500 loss: 0.374911427795887\n",
      "  batch 550 loss: 0.3565005460381508\n",
      "  batch 600 loss: 0.41055629581213\n",
      "  batch 650 loss: 0.413598438501358\n",
      "  batch 700 loss: 0.36671277523040774\n",
      "  batch 750 loss: 0.40107022881507876\n",
      "  batch 800 loss: 0.36685403376817705\n",
      "  batch 850 loss: 0.3848379516601563\n",
      "  batch 900 loss: 0.38970191836357115\n",
      "LOSS train 0.38970 valid 0.85217, valid PER 23.53%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.33017654687166215\n",
      "  batch 100 loss: 0.30237053215503695\n",
      "  batch 150 loss: 0.3482943367958069\n",
      "  batch 200 loss: 0.31232994139194487\n",
      "  batch 250 loss: 0.35779250085353853\n",
      "  batch 300 loss: 0.339188919365406\n",
      "  batch 350 loss: 0.3543283650279045\n",
      "  batch 400 loss: 0.35207634389400483\n",
      "  batch 450 loss: 0.3312436479330063\n",
      "  batch 500 loss: 0.3585741502046585\n",
      "  batch 550 loss: 0.3397121647000313\n",
      "  batch 600 loss: 0.3753233540058136\n",
      "  batch 650 loss: 0.3442710071802139\n",
      "  batch 700 loss: 0.368068445622921\n",
      "  batch 750 loss: 0.3515316289663315\n",
      "  batch 800 loss: 0.3811368492245674\n",
      "  batch 850 loss: 0.36721685737371446\n",
      "  batch 900 loss: 0.3493622890114784\n",
      "LOSS train 0.34936 valid 0.83786, valid PER 23.27%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.29610065758228304\n",
      "  batch 100 loss: 0.30136492490768435\n",
      "  batch 150 loss: 0.3278625521063805\n",
      "  batch 200 loss: 0.30641835123300554\n",
      "  batch 250 loss: 0.3222445982694626\n",
      "  batch 300 loss: 0.32418355643749236\n",
      "  batch 350 loss: 0.30259011894464494\n",
      "  batch 400 loss: 0.3092637473344803\n",
      "  batch 450 loss: 0.32858228325843813\n",
      "  batch 500 loss: 0.3340838846564293\n",
      "  batch 550 loss: 0.33501711308956145\n",
      "  batch 600 loss: 0.34040928781032564\n",
      "  batch 650 loss: 0.34159378737211227\n",
      "  batch 700 loss: 0.32216149777173997\n",
      "  batch 750 loss: 0.35776089936494826\n",
      "  batch 800 loss: 0.35136646032333374\n",
      "  batch 850 loss: 0.35965339094400406\n",
      "  batch 900 loss: 0.3284136497974396\n",
      "LOSS train 0.32841 valid 0.86249, valid PER 23.11%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.27255165874958037\n",
      "  batch 100 loss: 0.27899019613862036\n",
      "  batch 150 loss: 0.27287097841501234\n",
      "  batch 200 loss: 0.2846011850237846\n",
      "  batch 250 loss: 0.30738068908452987\n",
      "  batch 300 loss: 0.3150299264490604\n",
      "  batch 350 loss: 0.29525879621505735\n",
      "  batch 400 loss: 0.30036422699689863\n",
      "  batch 450 loss: 0.2890648677945137\n",
      "  batch 500 loss: 0.29520083546638487\n",
      "  batch 550 loss: 0.31914086997509\n",
      "  batch 600 loss: 0.3215863984823227\n",
      "  batch 650 loss: 0.32681477680802345\n",
      "  batch 700 loss: 0.3191195833683014\n",
      "  batch 750 loss: 0.31426736772060393\n",
      "  batch 800 loss: 0.320829710662365\n",
      "  batch 850 loss: 0.32780384421348574\n",
      "  batch 900 loss: 0.3245445850491524\n",
      "LOSS train 0.32454 valid 0.88632, valid PER 23.62%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.25435169756412507\n",
      "  batch 100 loss: 0.2717333193123341\n",
      "  batch 150 loss: 0.25840853452682494\n",
      "  batch 200 loss: 0.27240160793066026\n",
      "  batch 250 loss: 0.2871854785084724\n",
      "  batch 300 loss: 0.2802902066707611\n",
      "  batch 350 loss: 0.265801467448473\n",
      "  batch 400 loss: 0.2863759207725525\n",
      "  batch 450 loss: 0.2896353757381439\n",
      "  batch 500 loss: 0.3032777315378189\n",
      "  batch 550 loss: 0.29383715450763703\n",
      "  batch 600 loss: 0.29168488949537275\n",
      "  batch 650 loss: 0.30044461160898206\n",
      "  batch 700 loss: 0.28473556399345396\n",
      "  batch 750 loss: 0.2860876658558846\n",
      "  batch 800 loss: 0.3024683994054794\n",
      "  batch 850 loss: 0.3272566306591034\n",
      "  batch 900 loss: 0.3172667470574379\n",
      "LOSS train 0.31727 valid 0.89089, valid PER 23.68%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_122117/model_14\n",
      "Currently using dropout rate of 0.2\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.856044826507568\n",
      "  batch 100 loss: 3.2580306911468506\n",
      "  batch 150 loss: 3.029360570907593\n",
      "  batch 200 loss: 2.792290892601013\n",
      "  batch 250 loss: 2.5704592752456663\n",
      "  batch 300 loss: 2.3529617023468017\n",
      "  batch 350 loss: 2.1685332655906677\n",
      "  batch 400 loss: 2.0872668051719665\n",
      "  batch 450 loss: 1.9643627643585204\n",
      "  batch 500 loss: 1.8326141548156738\n",
      "  batch 550 loss: 1.7474559259414673\n",
      "  batch 600 loss: 1.6777791452407838\n",
      "  batch 650 loss: 1.6143864941596986\n",
      "  batch 700 loss: 1.6051622748374939\n",
      "  batch 750 loss: 1.5302967810630799\n",
      "  batch 800 loss: 1.5229619932174683\n",
      "  batch 850 loss: 1.4798423862457275\n",
      "  batch 900 loss: 1.4251393032073976\n",
      "LOSS train 1.42514 valid 1.39724, valid PER 47.29%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.3626048123836518\n",
      "  batch 100 loss: 1.3172213220596314\n",
      "  batch 150 loss: 1.2785518479347229\n",
      "  batch 200 loss: 1.3028732705116273\n",
      "  batch 250 loss: 1.2936402654647827\n",
      "  batch 300 loss: 1.2501494765281678\n",
      "  batch 350 loss: 1.1930780684947968\n",
      "  batch 400 loss: 1.1959616887569426\n",
      "  batch 450 loss: 1.1638492441177368\n",
      "  batch 500 loss: 1.212626917362213\n",
      "  batch 550 loss: 1.1887080335617066\n",
      "  batch 600 loss: 1.1649732744693757\n",
      "  batch 650 loss: 1.1687156200408935\n",
      "  batch 700 loss: 1.1545538592338562\n",
      "  batch 750 loss: 1.1455556881427764\n",
      "  batch 800 loss: 1.0815330350399017\n",
      "  batch 850 loss: 1.1013234639167786\n",
      "  batch 900 loss: 1.1285746908187866\n",
      "LOSS train 1.12857 valid 1.09703, valid PER 34.14%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.077255243062973\n",
      "  batch 100 loss: 1.0498864901065827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 150 loss: 1.0529650139808655\n",
      "  batch 200 loss: 1.037805564403534\n",
      "  batch 250 loss: 1.0213603842258454\n",
      "  batch 300 loss: 1.0156231784820557\n",
      "  batch 350 loss: 1.0588284718990326\n",
      "  batch 400 loss: 1.018219221830368\n",
      "  batch 450 loss: 1.0154740905761719\n",
      "  batch 500 loss: 1.0006765067577361\n",
      "  batch 550 loss: 1.0288334167003632\n",
      "  batch 600 loss: 0.972760648727417\n",
      "  batch 650 loss: 0.9719089198112488\n",
      "  batch 700 loss: 0.9967114627361298\n",
      "  batch 750 loss: 1.0274904346466065\n",
      "  batch 800 loss: 0.9684587502479554\n",
      "  batch 850 loss: 1.0079766821861267\n",
      "  batch 900 loss: 0.9359860217571259\n",
      "LOSS train 0.93599 valid 1.03182, valid PER 31.71%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.9178520679473877\n",
      "  batch 100 loss: 0.9227063620090484\n",
      "  batch 150 loss: 0.8974242174625396\n",
      "  batch 200 loss: 0.9424401867389679\n",
      "  batch 250 loss: 0.9628462743759155\n",
      "  batch 300 loss: 0.9550552952289582\n",
      "  batch 350 loss: 0.8973564386367798\n",
      "  batch 400 loss: 0.9269311964511872\n",
      "  batch 450 loss: 0.9215065371990204\n",
      "  batch 500 loss: 0.9098722517490387\n",
      "  batch 550 loss: 0.9244026660919189\n",
      "  batch 600 loss: 0.949923323392868\n",
      "  batch 650 loss: 0.9231684935092926\n",
      "  batch 700 loss: 0.9031900703907013\n",
      "  batch 750 loss: 0.8886099600791931\n",
      "  batch 800 loss: 0.8726746940612793\n",
      "  batch 850 loss: 0.8937573456764221\n",
      "  batch 900 loss: 0.9437002778053284\n",
      "LOSS train 0.94370 valid 0.92660, valid PER 28.80%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.8316922891139984\n",
      "  batch 100 loss: 0.840858770608902\n",
      "  batch 150 loss: 0.8866077995300293\n",
      "  batch 200 loss: 0.8212303102016449\n",
      "  batch 250 loss: 0.8389721584320068\n",
      "  batch 300 loss: 0.8425108218193054\n",
      "  batch 350 loss: 0.8479912889003753\n",
      "  batch 400 loss: 0.8617918539047241\n",
      "  batch 450 loss: 0.8436993646621704\n",
      "  batch 500 loss: 0.8590646994113922\n",
      "  batch 550 loss: 0.7949278664588928\n",
      "  batch 600 loss: 0.894712688922882\n",
      "  batch 650 loss: 0.8445599329471588\n",
      "  batch 700 loss: 0.8747567129135132\n",
      "  batch 750 loss: 0.8175527703762054\n",
      "  batch 800 loss: 0.8506150138378143\n",
      "  batch 850 loss: 0.8280101287364959\n",
      "  batch 900 loss: 0.8501033598184585\n",
      "LOSS train 0.85010 valid 0.90378, valid PER 28.10%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8237070381641388\n",
      "  batch 100 loss: 0.7622937071323395\n",
      "  batch 150 loss: 0.7718820768594742\n",
      "  batch 200 loss: 0.7873357850313186\n",
      "  batch 250 loss: 0.8102585232257843\n",
      "  batch 300 loss: 0.8004511415958404\n",
      "  batch 350 loss: 0.8001370310783387\n",
      "  batch 400 loss: 0.7897357821464539\n",
      "  batch 450 loss: 0.7991704416275024\n",
      "  batch 500 loss: 0.7836022061109543\n",
      "  batch 550 loss: 0.8037206029891968\n",
      "  batch 600 loss: 0.7841495859622956\n",
      "  batch 650 loss: 0.8033390522003174\n",
      "  batch 700 loss: 0.8092848801612854\n",
      "  batch 750 loss: 0.7953021669387818\n",
      "  batch 800 loss: 0.7891464841365814\n",
      "  batch 850 loss: 0.7646679866313935\n",
      "  batch 900 loss: 0.7942249763011933\n",
      "LOSS train 0.79422 valid 0.86413, valid PER 27.21%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.757451593875885\n",
      "  batch 100 loss: 0.7774607348442077\n",
      "  batch 150 loss: 0.7367049491405487\n",
      "  batch 200 loss: 0.7238672912120819\n",
      "  batch 250 loss: 0.7360608327388763\n",
      "  batch 300 loss: 0.7284632682800293\n",
      "  batch 350 loss: 0.7306016552448272\n",
      "  batch 400 loss: 0.7431362986564636\n",
      "  batch 450 loss: 0.7375906050205231\n",
      "  batch 500 loss: 0.7523894989490509\n",
      "  batch 550 loss: 0.7286677825450897\n",
      "  batch 600 loss: 0.7656017529964447\n",
      "  batch 650 loss: 0.7245785164833068\n",
      "  batch 700 loss: 0.7755512511730194\n",
      "  batch 750 loss: 0.7220709711313248\n",
      "  batch 800 loss: 0.731640281677246\n",
      "  batch 850 loss: 0.7479243791103363\n",
      "  batch 900 loss: 0.7802901101112366\n",
      "LOSS train 0.78029 valid 0.87033, valid PER 26.52%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.6972492134571076\n",
      "  batch 100 loss: 0.678910740017891\n",
      "  batch 150 loss: 0.7050501137971878\n",
      "  batch 200 loss: 0.6837753134965897\n",
      "  batch 250 loss: 0.7106331998109817\n",
      "  batch 300 loss: 0.6644930672645569\n",
      "  batch 350 loss: 0.7417880702018738\n",
      "  batch 400 loss: 0.6853612321615219\n",
      "  batch 450 loss: 0.7026234638690948\n",
      "  batch 500 loss: 0.7353501868247986\n",
      "  batch 550 loss: 0.6862859332561493\n",
      "  batch 600 loss: 0.7176216822862626\n",
      "  batch 650 loss: 0.7188224697113037\n",
      "  batch 700 loss: 0.6919918894767761\n",
      "  batch 750 loss: 0.7103538060188294\n",
      "  batch 800 loss: 0.7062285578250885\n",
      "  batch 850 loss: 0.6835947173833847\n",
      "  batch 900 loss: 0.7170045346021652\n",
      "LOSS train 0.71700 valid 0.80921, valid PER 25.00%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.628439913392067\n",
      "  batch 100 loss: 0.6450774836540222\n",
      "  batch 150 loss: 0.6740684235095977\n",
      "  batch 200 loss: 0.627269116640091\n",
      "  batch 250 loss: 0.6690191584825516\n",
      "  batch 300 loss: 0.6676791143417359\n",
      "  batch 350 loss: 0.6905162286758423\n",
      "  batch 400 loss: 0.6397293615341186\n",
      "  batch 450 loss: 0.6569151079654694\n",
      "  batch 500 loss: 0.6516799926757812\n",
      "  batch 550 loss: 0.6727555412054061\n",
      "  batch 600 loss: 0.684085459113121\n",
      "  batch 650 loss: 0.6519450634717942\n",
      "  batch 700 loss: 0.6450579762458801\n",
      "  batch 750 loss: 0.6514255011081695\n",
      "  batch 800 loss: 0.6726188009977341\n",
      "  batch 850 loss: 0.6964772802591324\n",
      "  batch 900 loss: 0.6435221499204635\n",
      "LOSS train 0.64352 valid 0.81265, valid PER 24.64%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.5844828522205353\n",
      "  batch 100 loss: 0.6035146546363831\n",
      "  batch 150 loss: 0.6231349426507949\n",
      "  batch 200 loss: 0.6465485519170762\n",
      "  batch 250 loss: 0.6523570191860198\n",
      "  batch 300 loss: 0.6206273293495178\n",
      "  batch 350 loss: 0.6172815364599228\n",
      "  batch 400 loss: 0.6264796870946884\n",
      "  batch 450 loss: 0.6180861842632294\n",
      "  batch 500 loss: 0.6562584376335144\n",
      "  batch 550 loss: 0.6639798146486282\n",
      "  batch 600 loss: 0.6132873886823654\n",
      "  batch 650 loss: 0.6060793507099151\n",
      "  batch 700 loss: 0.6422965377569199\n",
      "  batch 750 loss: 0.64001089990139\n",
      "  batch 800 loss: 0.6388811719417572\n",
      "  batch 850 loss: 0.6311109018325806\n",
      "  batch 900 loss: 0.6583310079574585\n",
      "LOSS train 0.65833 valid 0.79784, valid PER 24.75%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.5781813299655915\n",
      "  batch 100 loss: 0.5620095312595368\n",
      "  batch 150 loss: 0.5804060447216034\n",
      "  batch 200 loss: 0.6174367815256119\n",
      "  batch 250 loss: 0.6276386934518814\n",
      "  batch 300 loss: 0.5685210305452347\n",
      "  batch 350 loss: 0.5971231991052628\n",
      "  batch 400 loss: 0.6192126023769379\n",
      "  batch 450 loss: 0.6252666515111923\n",
      "  batch 500 loss: 0.5768913996219635\n",
      "  batch 550 loss: 0.6132671624422074\n",
      "  batch 600 loss: 0.5955754697322846\n",
      "  batch 650 loss: 0.6670336413383484\n",
      "  batch 700 loss: 0.5971558803319931\n",
      "  batch 750 loss: 0.5802729666233063\n",
      "  batch 800 loss: 0.6309731417894363\n",
      "  batch 850 loss: 0.6564997631311417\n",
      "  batch 900 loss: 0.6402174603939056\n",
      "LOSS train 0.64022 valid 0.80777, valid PER 24.18%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.5791003257036209\n",
      "  batch 100 loss: 0.5732654058933258\n",
      "  batch 150 loss: 0.5338804030418396\n",
      "  batch 200 loss: 0.5598884850740433\n",
      "  batch 250 loss: 0.5799148362874985\n",
      "  batch 300 loss: 0.5728807407617569\n",
      "  batch 350 loss: 0.555218505859375\n",
      "  batch 400 loss: 0.574824247956276\n",
      "  batch 450 loss: 0.5784899550676346\n",
      "  batch 500 loss: 0.6045949178934097\n",
      "  batch 550 loss: 0.5474999415874481\n",
      "  batch 600 loss: 0.5899858766794205\n",
      "  batch 650 loss: 0.6034410744905472\n",
      "  batch 700 loss: 0.5884629440307617\n",
      "  batch 750 loss: 0.5723635429143905\n",
      "  batch 800 loss: 0.5697130262851715\n",
      "  batch 850 loss: 0.6163904726505279\n",
      "  batch 900 loss: 0.6194232600927353\n",
      "LOSS train 0.61942 valid 0.79880, valid PER 24.29%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.5352208507061005\n",
      "  batch 100 loss: 0.5381977289915085\n",
      "  batch 150 loss: 0.5298958718776703\n",
      "  batch 200 loss: 0.5532267957925796\n",
      "  batch 250 loss: 0.549690266251564\n",
      "  batch 300 loss: 0.52272715061903\n",
      "  batch 350 loss: 0.5444490927457809\n",
      "  batch 400 loss: 0.5466748851537705\n",
      "  batch 450 loss: 0.5775015616416931\n",
      "  batch 500 loss: 0.5206717652082443\n",
      "  batch 550 loss: 0.5587948542833329\n",
      "  batch 600 loss: 0.5441358029842377\n",
      "  batch 650 loss: 0.5729852658510208\n",
      "  batch 700 loss: 0.5632494384050369\n",
      "  batch 750 loss: 0.5355759835243226\n",
      "  batch 800 loss: 0.5579161149263382\n",
      "  batch 850 loss: 0.5723690497875213\n",
      "  batch 900 loss: 0.5731836783885956\n",
      "LOSS train 0.57318 valid 0.78900, valid PER 23.99%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.4952693313360214\n",
      "  batch 100 loss: 0.5302422976493836\n",
      "  batch 150 loss: 0.49529034256935117\n",
      "  batch 200 loss: 0.5027480781078338\n",
      "  batch 250 loss: 0.5081202232837677\n",
      "  batch 300 loss: 0.5487522131204605\n",
      "  batch 350 loss: 0.4849587219953537\n",
      "  batch 400 loss: 0.5078439784049987\n",
      "  batch 450 loss: 0.5153650289773941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 500 loss: 0.5360938501358032\n",
      "  batch 550 loss: 0.541271276473999\n",
      "  batch 600 loss: 0.5028195881843567\n",
      "  batch 650 loss: 0.5422133433818818\n",
      "  batch 700 loss: 0.5595635145902633\n",
      "  batch 750 loss: 0.5181500577926635\n",
      "  batch 800 loss: 0.5150188344717026\n",
      "  batch 850 loss: 0.5579300951957703\n",
      "  batch 900 loss: 0.5442490893602371\n",
      "LOSS train 0.54425 valid 0.80401, valid PER 23.87%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.48622271001338957\n",
      "  batch 100 loss: 0.48267479360103605\n",
      "  batch 150 loss: 0.4764319306612015\n",
      "  batch 200 loss: 0.5096748358011246\n",
      "  batch 250 loss: 0.5065958005189896\n",
      "  batch 300 loss: 0.47804209947586057\n",
      "  batch 350 loss: 0.5042109757661819\n",
      "  batch 400 loss: 0.4882518947124481\n",
      "  batch 450 loss: 0.49401808142662046\n",
      "  batch 500 loss: 0.4819461172819138\n",
      "  batch 550 loss: 0.5043445414304734\n",
      "  batch 600 loss: 0.5173967981338501\n",
      "  batch 650 loss: 0.527086751461029\n",
      "  batch 700 loss: 0.5278946548700333\n",
      "  batch 750 loss: 0.5244166368246078\n",
      "  batch 800 loss: 0.5007970821857453\n",
      "  batch 850 loss: 0.47963167011737823\n",
      "  batch 900 loss: 0.5126884257793427\n",
      "LOSS train 0.51269 valid 0.79815, valid PER 23.22%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.4768249350786209\n",
      "  batch 100 loss: 0.44071614503860473\n",
      "  batch 150 loss: 0.4607388997077942\n",
      "  batch 200 loss: 0.46950442373752593\n",
      "  batch 250 loss: 0.4716630780696869\n",
      "  batch 300 loss: 0.4804408460855484\n",
      "  batch 350 loss: 0.48384189903736113\n",
      "  batch 400 loss: 0.4844001168012619\n",
      "  batch 450 loss: 0.4908364188671112\n",
      "  batch 500 loss: 0.4714754158258438\n",
      "  batch 550 loss: 0.484227676987648\n",
      "  batch 600 loss: 0.47793260097503665\n",
      "  batch 650 loss: 0.5331797158718109\n",
      "  batch 700 loss: 0.47990390241146086\n",
      "  batch 750 loss: 0.49737047612667085\n",
      "  batch 800 loss: 0.5107824444770813\n",
      "  batch 850 loss: 0.48978526294231417\n",
      "  batch 900 loss: 0.4948251885175705\n",
      "LOSS train 0.49483 valid 0.80762, valid PER 23.02%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.4427113315463066\n",
      "  batch 100 loss: 0.43346128970384595\n",
      "  batch 150 loss: 0.43073910295963286\n",
      "  batch 200 loss: 0.4344624876976013\n",
      "  batch 250 loss: 0.4497189062833786\n",
      "  batch 300 loss: 0.4547097381949425\n",
      "  batch 350 loss: 0.4352722853422165\n",
      "  batch 400 loss: 0.49110197484493257\n",
      "  batch 450 loss: 0.4542528128623962\n",
      "  batch 500 loss: 0.4558377319574356\n",
      "  batch 550 loss: 0.46485631704330443\n",
      "  batch 600 loss: 0.4865960210561752\n",
      "  batch 650 loss: 0.45210892796516416\n",
      "  batch 700 loss: 0.44902480065822603\n",
      "  batch 750 loss: 0.4565139418840408\n",
      "  batch 800 loss: 0.44894197821617127\n",
      "  batch 850 loss: 0.464978261590004\n",
      "  batch 900 loss: 0.4639499050378799\n",
      "LOSS train 0.46395 valid 0.80586, valid PER 23.42%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.4029938417673111\n",
      "  batch 100 loss: 0.41814273059368134\n",
      "  batch 150 loss: 0.4421685475111008\n",
      "  batch 200 loss: 0.4316418969631195\n",
      "  batch 250 loss: 0.43258322060108184\n",
      "  batch 300 loss: 0.41455908298492433\n",
      "  batch 350 loss: 0.43386628955602646\n",
      "  batch 400 loss: 0.425649990439415\n",
      "  batch 450 loss: 0.43885413646697996\n",
      "  batch 500 loss: 0.4298247641324997\n",
      "  batch 550 loss: 0.44826261162757874\n",
      "  batch 600 loss: 0.42484786212444303\n",
      "  batch 650 loss: 0.4403854548931122\n",
      "  batch 700 loss: 0.4476591789722443\n",
      "  batch 750 loss: 0.44224816203117373\n",
      "  batch 800 loss: 0.43791121304035185\n",
      "  batch 850 loss: 0.44540492713451385\n",
      "  batch 900 loss: 0.4633566266298294\n",
      "LOSS train 0.46336 valid 0.80900, valid PER 23.04%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.3813387656211853\n",
      "  batch 100 loss: 0.38515254139900207\n",
      "  batch 150 loss: 0.3954001158475876\n",
      "  batch 200 loss: 0.3995164352655411\n",
      "  batch 250 loss: 0.4068386071920395\n",
      "  batch 300 loss: 0.42149838209152224\n",
      "  batch 350 loss: 0.4096101063489914\n",
      "  batch 400 loss: 0.42860285699367523\n",
      "  batch 450 loss: 0.44801965028047563\n",
      "  batch 500 loss: 0.4255955654382706\n",
      "  batch 550 loss: 0.4117555242776871\n",
      "  batch 600 loss: 0.42532223522663115\n",
      "  batch 650 loss: 0.4889936935901642\n",
      "  batch 700 loss: 0.43210626155138016\n",
      "  batch 750 loss: 0.41428828716278077\n",
      "  batch 800 loss: 0.44763293087482453\n",
      "  batch 850 loss: 0.4364796578884125\n",
      "  batch 900 loss: 0.45422807335853577\n",
      "LOSS train 0.45423 valid 0.82681, valid PER 23.07%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.38038308173418045\n",
      "  batch 100 loss: 0.3717054983973503\n",
      "  batch 150 loss: 0.38039118230342867\n",
      "  batch 200 loss: 0.40255988240242\n",
      "  batch 250 loss: 0.38755637139081955\n",
      "  batch 300 loss: 0.40941488862037656\n",
      "  batch 350 loss: 0.3755969187617302\n",
      "  batch 400 loss: 0.41067579716444014\n",
      "  batch 450 loss: 0.4115494388341904\n",
      "  batch 500 loss: 0.39458887934684755\n",
      "  batch 550 loss: 0.44453693062067035\n",
      "  batch 600 loss: 0.4031822893023491\n",
      "  batch 650 loss: 0.40519854694604873\n",
      "  batch 700 loss: 0.4079787823557854\n",
      "  batch 750 loss: 0.3933983239531517\n",
      "  batch 800 loss: 0.42198723554611206\n",
      "  batch 850 loss: 0.4403705161809921\n",
      "  batch 900 loss: 0.42632871985435483\n",
      "LOSS train 0.42633 valid 0.84433, valid PER 22.90%\n",
      "Training finished in 5.0 minutes.\n",
      "Model saved to checkpoints/20231206_123001/model_13\n",
      "Currently using dropout rate of 0.3\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.970403409004211\n",
      "  batch 100 loss: 3.209065179824829\n",
      "  batch 150 loss: 2.9906333446502686\n",
      "  batch 200 loss: 2.714827151298523\n",
      "  batch 250 loss: 2.4895772123336792\n",
      "  batch 300 loss: 2.2946367645263672\n",
      "  batch 350 loss: 2.1127197623252867\n",
      "  batch 400 loss: 2.0500676584243775\n",
      "  batch 450 loss: 1.934569194316864\n",
      "  batch 500 loss: 1.8131053495407103\n",
      "  batch 550 loss: 1.7395965051651001\n",
      "  batch 600 loss: 1.688809678554535\n",
      "  batch 650 loss: 1.6056189250946045\n",
      "  batch 700 loss: 1.6051398086547852\n",
      "  batch 750 loss: 1.547949948310852\n",
      "  batch 800 loss: 1.5448282670974731\n",
      "  batch 850 loss: 1.4958257269859314\n",
      "  batch 900 loss: 1.4425191617012023\n",
      "LOSS train 1.44252 valid 1.39650, valid PER 48.83%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.392402994632721\n",
      "  batch 100 loss: 1.3385972809791564\n",
      "  batch 150 loss: 1.295458779335022\n",
      "  batch 200 loss: 1.3249429833889008\n",
      "  batch 250 loss: 1.3227275109291077\n",
      "  batch 300 loss: 1.272614424228668\n",
      "  batch 350 loss: 1.219991353750229\n",
      "  batch 400 loss: 1.2154779350757599\n",
      "  batch 450 loss: 1.1703439855575561\n",
      "  batch 500 loss: 1.2056780230998994\n",
      "  batch 550 loss: 1.1950056958198547\n",
      "  batch 600 loss: 1.1720594942569733\n",
      "  batch 650 loss: 1.1680521190166473\n",
      "  batch 700 loss: 1.158858826160431\n",
      "  batch 750 loss: 1.1492889356613158\n",
      "  batch 800 loss: 1.0813049232959748\n",
      "  batch 850 loss: 1.100489423274994\n",
      "  batch 900 loss: 1.1240291106700897\n",
      "LOSS train 1.12403 valid 1.08985, valid PER 33.78%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.0918195617198945\n",
      "  batch 100 loss: 1.0446776139736176\n",
      "  batch 150 loss: 1.0363916921615601\n",
      "  batch 200 loss: 1.0388222444057464\n",
      "  batch 250 loss: 1.0204119074344635\n",
      "  batch 300 loss: 1.014460244178772\n",
      "  batch 350 loss: 1.0507062911987304\n",
      "  batch 400 loss: 1.0326710748672485\n",
      "  batch 450 loss: 1.0153435730934144\n",
      "  batch 500 loss: 0.9961170852184296\n",
      "  batch 550 loss: 1.0133432793617247\n",
      "  batch 600 loss: 0.975084308385849\n",
      "  batch 650 loss: 0.9771827363967895\n",
      "  batch 700 loss: 0.9954129540920258\n",
      "  batch 750 loss: 1.0389998817443848\n",
      "  batch 800 loss: 0.9647717452049256\n",
      "  batch 850 loss: 1.0141138350963592\n",
      "  batch 900 loss: 0.9492569875717163\n",
      "LOSS train 0.94926 valid 0.99862, valid PER 31.09%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.9094500136375427\n",
      "  batch 100 loss: 0.9336508548259735\n",
      "  batch 150 loss: 0.9110699319839477\n",
      "  batch 200 loss: 0.9451496529579163\n",
      "  batch 250 loss: 0.9393819272518158\n",
      "  batch 300 loss: 0.9464117455482483\n",
      "  batch 350 loss: 0.8891999816894531\n",
      "  batch 400 loss: 0.9261272883415222\n",
      "  batch 450 loss: 0.9236555528640747\n",
      "  batch 500 loss: 0.8933731889724732\n",
      "  batch 550 loss: 0.9078305637836457\n",
      "  batch 600 loss: 0.9451328825950622\n",
      "  batch 650 loss: 0.9265786051750183\n",
      "  batch 700 loss: 0.9032999622821808\n",
      "  batch 750 loss: 0.8825918030738831\n",
      "  batch 800 loss: 0.8735022735595703\n",
      "  batch 850 loss: 0.8926653289794921\n",
      "  batch 900 loss: 0.9454545092582702\n",
      "LOSS train 0.94545 valid 0.90722, valid PER 28.42%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.8459221017360687\n",
      "  batch 100 loss: 0.8354970586299896\n",
      "  batch 150 loss: 0.906262446641922\n",
      "  batch 200 loss: 0.8275563681125641\n",
      "  batch 250 loss: 0.8268599247932434\n",
      "  batch 300 loss: 0.8510349917411805\n",
      "  batch 350 loss: 0.8431585592031479\n",
      "  batch 400 loss: 0.8719975280761719\n",
      "  batch 450 loss: 0.8411369693279266\n",
      "  batch 500 loss: 0.8607541918754578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 550 loss: 0.804352980852127\n",
      "  batch 600 loss: 0.8804282653331756\n",
      "  batch 650 loss: 0.8333887898921967\n",
      "  batch 700 loss: 0.8725360369682312\n",
      "  batch 750 loss: 0.8117343008518219\n",
      "  batch 800 loss: 0.8621259164810181\n",
      "  batch 850 loss: 0.8487148261070252\n",
      "  batch 900 loss: 0.8521582746505737\n",
      "LOSS train 0.85216 valid 0.90621, valid PER 28.52%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8334192991256714\n",
      "  batch 100 loss: 0.7778025412559509\n",
      "  batch 150 loss: 0.7596457952260971\n",
      "  batch 200 loss: 0.7855927866697311\n",
      "  batch 250 loss: 0.8376350200176239\n",
      "  batch 300 loss: 0.7991802304983139\n",
      "  batch 350 loss: 0.7919099676609039\n",
      "  batch 400 loss: 0.7861126446723938\n",
      "  batch 450 loss: 0.8060383319854736\n",
      "  batch 500 loss: 0.7887142336368561\n",
      "  batch 550 loss: 0.8177850568294525\n",
      "  batch 600 loss: 0.7933033263683319\n",
      "  batch 650 loss: 0.8057477974891663\n",
      "  batch 700 loss: 0.8087020230293274\n",
      "  batch 750 loss: 0.7772293084859848\n",
      "  batch 800 loss: 0.796122694015503\n",
      "  batch 850 loss: 0.7728651475906372\n",
      "  batch 900 loss: 0.7900110626220703\n",
      "LOSS train 0.79001 valid 0.86857, valid PER 27.30%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.778984375\n",
      "  batch 100 loss: 0.7659427231550217\n",
      "  batch 150 loss: 0.7407237511873245\n",
      "  batch 200 loss: 0.7373606646060944\n",
      "  batch 250 loss: 0.736672368645668\n",
      "  batch 300 loss: 0.7383528429269791\n",
      "  batch 350 loss: 0.7194483464956284\n",
      "  batch 400 loss: 0.7570062285661697\n",
      "  batch 450 loss: 0.7408637011051178\n",
      "  batch 500 loss: 0.7421180057525635\n",
      "  batch 550 loss: 0.7258847457170486\n",
      "  batch 600 loss: 0.760776765346527\n",
      "  batch 650 loss: 0.7321413254737854\n",
      "  batch 700 loss: 0.7682414954900741\n",
      "  batch 750 loss: 0.7447254419326782\n",
      "  batch 800 loss: 0.7486049652099609\n",
      "  batch 850 loss: 0.7664042592048645\n",
      "  batch 900 loss: 0.7746023094654083\n",
      "LOSS train 0.77460 valid 0.86969, valid PER 26.64%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.7141611164808274\n",
      "  batch 100 loss: 0.6963267207145691\n",
      "  batch 150 loss: 0.721884371638298\n",
      "  batch 200 loss: 0.6916801297664642\n",
      "  batch 250 loss: 0.6983027076721191\n",
      "  batch 300 loss: 0.6688344800472259\n",
      "  batch 350 loss: 0.7370978486537934\n",
      "  batch 400 loss: 0.6875137102603912\n",
      "  batch 450 loss: 0.7088926512002945\n",
      "  batch 500 loss: 0.7365692281723022\n",
      "  batch 550 loss: 0.6854470348358155\n",
      "  batch 600 loss: 0.7351846253871918\n",
      "  batch 650 loss: 0.7359276711940765\n",
      "  batch 700 loss: 0.7047962999343872\n",
      "  batch 750 loss: 0.7101617687940598\n",
      "  batch 800 loss: 0.7152242755889893\n",
      "  batch 850 loss: 0.696611009836197\n",
      "  batch 900 loss: 0.7215513634681702\n",
      "LOSS train 0.72155 valid 0.82590, valid PER 25.38%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.6389469194412232\n",
      "  batch 100 loss: 0.6684508657455445\n",
      "  batch 150 loss: 0.6718543475866318\n",
      "  batch 200 loss: 0.6418580043315888\n",
      "  batch 250 loss: 0.6759436917304993\n",
      "  batch 300 loss: 0.6937947779893875\n",
      "  batch 350 loss: 0.7026162683963776\n",
      "  batch 400 loss: 0.6835739308595657\n",
      "  batch 450 loss: 0.6937059473991394\n",
      "  batch 500 loss: 0.6666525661945343\n",
      "  batch 550 loss: 0.6879438644647599\n",
      "  batch 600 loss: 0.6989050310850143\n",
      "  batch 650 loss: 0.675690706372261\n",
      "  batch 700 loss: 0.6564911013841629\n",
      "  batch 750 loss: 0.6640665739774704\n",
      "  batch 800 loss: 0.7053111165761947\n",
      "  batch 850 loss: 0.708766816854477\n",
      "  batch 900 loss: 0.6483894282579422\n",
      "LOSS train 0.64839 valid 0.83205, valid PER 25.43%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6128707939386367\n",
      "  batch 100 loss: 0.6174433422088623\n",
      "  batch 150 loss: 0.6383044874668121\n",
      "  batch 200 loss: 0.6837997108697891\n",
      "  batch 250 loss: 0.6563510018587112\n",
      "  batch 300 loss: 0.6148281234502793\n",
      "  batch 350 loss: 0.6489418065547943\n",
      "  batch 400 loss: 0.6166593515872956\n",
      "  batch 450 loss: 0.6256974041461945\n",
      "  batch 500 loss: 0.6764987951517105\n",
      "  batch 550 loss: 0.6698976629972457\n",
      "  batch 600 loss: 0.6323230648040772\n",
      "  batch 650 loss: 0.6393444782495499\n",
      "  batch 700 loss: 0.6718841850757599\n",
      "  batch 750 loss: 0.6484152275323868\n",
      "  batch 800 loss: 0.6672412723302841\n",
      "  batch 850 loss: 0.6399382853507996\n",
      "  batch 900 loss: 0.684124100804329\n",
      "LOSS train 0.68412 valid 0.81769, valid PER 24.88%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.5850686007738113\n",
      "  batch 100 loss: 0.5582972437143325\n",
      "  batch 150 loss: 0.5705637967586518\n",
      "  batch 200 loss: 0.6255580514669419\n",
      "  batch 250 loss: 0.6297049427032471\n",
      "  batch 300 loss: 0.570831441283226\n",
      "  batch 350 loss: 0.6251835030317306\n",
      "  batch 400 loss: 0.6168305903673172\n",
      "  batch 450 loss: 0.635997524857521\n",
      "  batch 500 loss: 0.6037915098667145\n",
      "  batch 550 loss: 0.6281211233139038\n",
      "  batch 600 loss: 0.6022140669822693\n",
      "  batch 650 loss: 0.6686002308130264\n",
      "  batch 700 loss: 0.5992588186264038\n",
      "  batch 750 loss: 0.6150070571899414\n",
      "  batch 800 loss: 0.6405813771486283\n",
      "  batch 850 loss: 0.6583065044879913\n",
      "  batch 900 loss: 0.6376057416200638\n",
      "LOSS train 0.63761 valid 0.81296, valid PER 24.10%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.5906107491254806\n",
      "  batch 100 loss: 0.5827207964658737\n",
      "  batch 150 loss: 0.5285184025764466\n",
      "  batch 200 loss: 0.5863791513442993\n",
      "  batch 250 loss: 0.5806437021493912\n",
      "  batch 300 loss: 0.582247211933136\n",
      "  batch 350 loss: 0.5590846830606461\n",
      "  batch 400 loss: 0.603376612663269\n",
      "  batch 450 loss: 0.6035353082418442\n",
      "  batch 500 loss: 0.5957711654901504\n",
      "  batch 550 loss: 0.5555834996700287\n",
      "  batch 600 loss: 0.6092823022603988\n",
      "  batch 650 loss: 0.6475965124368668\n",
      "  batch 700 loss: 0.6143529403209687\n",
      "  batch 750 loss: 0.5886362773180008\n",
      "  batch 800 loss: 0.590216600894928\n",
      "  batch 850 loss: 0.6408327406644821\n",
      "  batch 900 loss: 0.6382905799150467\n",
      "LOSS train 0.63829 valid 0.79089, valid PER 24.44%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.5587191963195801\n",
      "  batch 100 loss: 0.5571819031238556\n",
      "  batch 150 loss: 0.5536349642276764\n",
      "  batch 200 loss: 0.5605351358652115\n",
      "  batch 250 loss: 0.5569977325201034\n",
      "  batch 300 loss: 0.5695102220773697\n",
      "  batch 350 loss: 0.5552949011325836\n",
      "  batch 400 loss: 0.5639126592874527\n",
      "  batch 450 loss: 0.5820690971612931\n",
      "  batch 500 loss: 0.5446414196491242\n",
      "  batch 550 loss: 0.593092469573021\n",
      "  batch 600 loss: 0.5485618418455124\n",
      "  batch 650 loss: 0.5719374740123748\n",
      "  batch 700 loss: 0.5868661028146743\n",
      "  batch 750 loss: 0.5482537746429443\n",
      "  batch 800 loss: 0.5610277199745178\n",
      "  batch 850 loss: 0.5924909377098083\n",
      "  batch 900 loss: 0.5907970410585404\n",
      "LOSS train 0.59080 valid 0.81130, valid PER 24.59%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.5194769120216369\n",
      "  batch 100 loss: 0.540387688279152\n",
      "  batch 150 loss: 0.5227477943897247\n",
      "  batch 200 loss: 0.5271826422214508\n",
      "  batch 250 loss: 0.5299658215045929\n",
      "  batch 300 loss: 0.5728239101171494\n",
      "  batch 350 loss: 0.5238077032566071\n",
      "  batch 400 loss: 0.5477957582473755\n",
      "  batch 450 loss: 0.5395622688531876\n",
      "  batch 500 loss: 0.5769810789823532\n",
      "  batch 550 loss: 0.5769110298156739\n",
      "  batch 600 loss: 0.5263748329877853\n",
      "  batch 650 loss: 0.554609699845314\n",
      "  batch 700 loss: 0.573937429189682\n",
      "  batch 750 loss: 0.5425079375505447\n",
      "  batch 800 loss: 0.5455158823728561\n",
      "  batch 850 loss: 0.5698287069797516\n",
      "  batch 900 loss: 0.5607354563474655\n",
      "LOSS train 0.56074 valid 0.80736, valid PER 23.72%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.49561755955219267\n",
      "  batch 100 loss: 0.492631071805954\n",
      "  batch 150 loss: 0.4988370668888092\n",
      "  batch 200 loss: 0.5334451001882553\n",
      "  batch 250 loss: 0.5476278549432755\n",
      "  batch 300 loss: 0.5132224482297897\n",
      "  batch 350 loss: 0.549005132317543\n",
      "  batch 400 loss: 0.5299521237611771\n",
      "  batch 450 loss: 0.5214594751596451\n",
      "  batch 500 loss: 0.5154583549499512\n",
      "  batch 550 loss: 0.5277582371234893\n",
      "  batch 600 loss: 0.5344260579347611\n",
      "  batch 650 loss: 0.5409757858514785\n",
      "  batch 700 loss: 0.558863309621811\n",
      "  batch 750 loss: 0.5521138721704483\n",
      "  batch 800 loss: 0.5244144362211227\n",
      "  batch 850 loss: 0.510644970536232\n",
      "  batch 900 loss: 0.5229406264424324\n",
      "LOSS train 0.52294 valid 0.79115, valid PER 23.48%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.5123733729124069\n",
      "  batch 100 loss: 0.4785357654094696\n",
      "  batch 150 loss: 0.4892921167612076\n",
      "  batch 200 loss: 0.483110237121582\n",
      "  batch 250 loss: 0.5219427853822708\n",
      "  batch 300 loss: 0.4918873757123947\n",
      "  batch 350 loss: 0.49897001206874847\n",
      "  batch 400 loss: 0.5052392315864563\n",
      "  batch 450 loss: 0.5020618379116059\n",
      "  batch 500 loss: 0.4812720412015915\n",
      "  batch 550 loss: 0.47903857052326204\n",
      "  batch 600 loss: 0.5079582947492599\n",
      "  batch 650 loss: 0.5245242205262184\n",
      "  batch 700 loss: 0.4946912258863449\n",
      "  batch 750 loss: 0.5341221064329147\n",
      "  batch 800 loss: 0.5245211112499237\n",
      "  batch 850 loss: 0.5075814020633698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 900 loss: 0.5167544496059417\n",
      "LOSS train 0.51675 valid 0.81792, valid PER 23.57%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.48689638435840604\n",
      "  batch 100 loss: 0.4792957079410553\n",
      "  batch 150 loss: 0.4680852577090263\n",
      "  batch 200 loss: 0.46640166819095613\n",
      "  batch 250 loss: 0.47899615406990054\n",
      "  batch 300 loss: 0.4888946706056595\n",
      "  batch 350 loss: 0.4682308745384216\n",
      "  batch 400 loss: 0.5013286709785462\n",
      "  batch 450 loss: 0.49879133999347686\n",
      "  batch 500 loss: 0.4799861377477646\n",
      "  batch 550 loss: 0.4664834040403366\n",
      "  batch 600 loss: 0.5188808411359787\n",
      "  batch 650 loss: 0.4800785529613495\n",
      "  batch 700 loss: 0.4763843303918838\n",
      "  batch 750 loss: 0.4799040347337723\n",
      "  batch 800 loss: 0.46122271597385406\n",
      "  batch 850 loss: 0.5001346248388291\n",
      "  batch 900 loss: 0.5005177932977677\n",
      "LOSS train 0.50052 valid 0.80223, valid PER 23.13%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.44202853202819825\n",
      "  batch 100 loss: 0.4474792331457138\n",
      "  batch 150 loss: 0.47312208473682404\n",
      "  batch 200 loss: 0.45899058401584625\n",
      "  batch 250 loss: 0.4508894270658493\n",
      "  batch 300 loss: 0.4671509924530983\n",
      "  batch 350 loss: 0.4658708566427231\n",
      "  batch 400 loss: 0.4506844043731689\n",
      "  batch 450 loss: 0.47597339391708376\n",
      "  batch 500 loss: 0.4678766840696335\n",
      "  batch 550 loss: 0.5002965706586838\n",
      "  batch 600 loss: 0.44646124184131625\n",
      "  batch 650 loss: 0.4640501070022583\n",
      "  batch 700 loss: 0.49291970729827883\n",
      "  batch 750 loss: 0.48200091361999514\n",
      "  batch 800 loss: 0.4634525057673454\n",
      "  batch 850 loss: 0.46337324142456054\n",
      "  batch 900 loss: 0.5043670779466629\n",
      "LOSS train 0.50437 valid 0.81086, valid PER 22.98%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.4132812702655792\n",
      "  batch 100 loss: 0.4171519097685814\n",
      "  batch 150 loss: 0.4330023002624512\n",
      "  batch 200 loss: 0.4380766123533249\n",
      "  batch 250 loss: 0.43122028291225434\n",
      "  batch 300 loss: 0.43920635521411894\n",
      "  batch 350 loss: 0.4508692133426666\n",
      "  batch 400 loss: 0.44402994513511657\n",
      "  batch 450 loss: 0.4638073796033859\n",
      "  batch 500 loss: 0.47063752233982087\n",
      "  batch 550 loss: 0.4462449303269386\n",
      "  batch 600 loss: 0.44329357385635376\n",
      "  batch 650 loss: 0.4964590245485306\n",
      "  batch 700 loss: 0.44273312509059903\n",
      "  batch 750 loss: 0.44693637192249297\n",
      "  batch 800 loss: 0.482420836687088\n",
      "  batch 850 loss: 0.4844542223215103\n",
      "  batch 900 loss: 0.4854102748632431\n",
      "LOSS train 0.48541 valid 0.80885, valid PER 23.06%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.41361273944377897\n",
      "  batch 100 loss: 0.41042688250541687\n",
      "  batch 150 loss: 0.401823088824749\n",
      "  batch 200 loss: 0.4369765746593475\n",
      "  batch 250 loss: 0.4217261552810669\n",
      "  batch 300 loss: 0.44464067220687864\n",
      "  batch 350 loss: 0.42121496826410293\n",
      "  batch 400 loss: 0.44003713577985765\n",
      "  batch 450 loss: 0.4457877939939499\n",
      "  batch 500 loss: 0.424011687040329\n",
      "  batch 550 loss: 0.45731444597244264\n",
      "  batch 600 loss: 0.4094749194383621\n",
      "  batch 650 loss: 0.4459779378771782\n",
      "  batch 700 loss: 0.44438367664813994\n",
      "  batch 750 loss: 0.429186674952507\n",
      "  batch 800 loss: 0.45625997841358185\n",
      "  batch 850 loss: 0.45972835421562197\n",
      "  batch 900 loss: 0.4533568906784058\n",
      "LOSS train 0.45336 valid 0.82034, valid PER 22.94%\n",
      "Training finished in 5.0 minutes.\n",
      "Model saved to checkpoints/20231206_123511/model_12\n",
      "Currently using dropout rate of 0.4\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.986437935829162\n",
      "  batch 100 loss: 3.213336591720581\n",
      "  batch 150 loss: 2.997625608444214\n",
      "  batch 200 loss: 2.723284058570862\n",
      "  batch 250 loss: 2.499472641944885\n",
      "  batch 300 loss: 2.3022377824783327\n",
      "  batch 350 loss: 2.119111671447754\n",
      "  batch 400 loss: 2.06699081659317\n",
      "  batch 450 loss: 1.94366947889328\n",
      "  batch 500 loss: 1.8357885122299193\n",
      "  batch 550 loss: 1.7490499114990234\n",
      "  batch 600 loss: 1.7007198905944825\n",
      "  batch 650 loss: 1.6227713990211488\n",
      "  batch 700 loss: 1.6206971716880798\n",
      "  batch 750 loss: 1.5483234429359436\n",
      "  batch 800 loss: 1.5522752737998962\n",
      "  batch 850 loss: 1.512875771522522\n",
      "  batch 900 loss: 1.4594941234588623\n",
      "LOSS train 1.45949 valid 1.41271, valid PER 50.15%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.4108491468429565\n",
      "  batch 100 loss: 1.362329182624817\n",
      "  batch 150 loss: 1.3216832995414733\n",
      "  batch 200 loss: 1.3468322110176087\n",
      "  batch 250 loss: 1.330231604576111\n",
      "  batch 300 loss: 1.2928634476661682\n",
      "  batch 350 loss: 1.2307329702377319\n",
      "  batch 400 loss: 1.2311062216758728\n",
      "  batch 450 loss: 1.1961676013469695\n",
      "  batch 500 loss: 1.2353295767307282\n",
      "  batch 550 loss: 1.228190143108368\n",
      "  batch 600 loss: 1.186143420934677\n",
      "  batch 650 loss: 1.1961157596111298\n",
      "  batch 700 loss: 1.160166084766388\n",
      "  batch 750 loss: 1.160050119161606\n",
      "  batch 800 loss: 1.088504400253296\n",
      "  batch 850 loss: 1.1182187592983246\n",
      "  batch 900 loss: 1.138307512998581\n",
      "LOSS train 1.13831 valid 1.11203, valid PER 35.15%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.1095876705646515\n",
      "  batch 100 loss: 1.0725795328617096\n",
      "  batch 150 loss: 1.0714208042621614\n",
      "  batch 200 loss: 1.0596324276924134\n",
      "  batch 250 loss: 1.0577222144603728\n",
      "  batch 300 loss: 1.026969349384308\n",
      "  batch 350 loss: 1.080068701505661\n",
      "  batch 400 loss: 1.054708343744278\n",
      "  batch 450 loss: 1.0490421080589294\n",
      "  batch 500 loss: 1.0173848676681518\n",
      "  batch 550 loss: 1.0260600137710572\n",
      "  batch 600 loss: 0.984252438545227\n",
      "  batch 650 loss: 0.9842627227306366\n",
      "  batch 700 loss: 1.0181441366672517\n",
      "  batch 750 loss: 1.0521324169635773\n",
      "  batch 800 loss: 0.9964693820476532\n",
      "  batch 850 loss: 1.0407102596759796\n",
      "  batch 900 loss: 0.9418561518192291\n",
      "LOSS train 0.94186 valid 1.00600, valid PER 31.44%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.9237068486213684\n",
      "  batch 100 loss: 0.9374526405334472\n",
      "  batch 150 loss: 0.9160595667362214\n",
      "  batch 200 loss: 0.9751354706287384\n",
      "  batch 250 loss: 0.9497033357620239\n",
      "  batch 300 loss: 0.9649565398693085\n",
      "  batch 350 loss: 0.8958619415760041\n",
      "  batch 400 loss: 0.9680259418487549\n",
      "  batch 450 loss: 0.9254857087135315\n",
      "  batch 500 loss: 0.9181802260875702\n",
      "  batch 550 loss: 0.929695383310318\n",
      "  batch 600 loss: 0.9563650560379028\n",
      "  batch 650 loss: 0.9678245687484741\n",
      "  batch 700 loss: 0.9176411712169648\n",
      "  batch 750 loss: 0.9103239130973816\n",
      "  batch 800 loss: 0.8816806828975677\n",
      "  batch 850 loss: 0.9271899724006653\n",
      "  batch 900 loss: 0.9580701458454132\n",
      "LOSS train 0.95807 valid 0.92702, valid PER 29.24%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.8699134254455566\n",
      "  batch 100 loss: 0.8514414954185486\n",
      "  batch 150 loss: 0.9291628730297089\n",
      "  batch 200 loss: 0.8403436183929444\n",
      "  batch 250 loss: 0.8611537706851959\n",
      "  batch 300 loss: 0.865372314453125\n",
      "  batch 350 loss: 0.8723091256618499\n",
      "  batch 400 loss: 0.8971310532093049\n",
      "  batch 450 loss: 0.8527213966846466\n",
      "  batch 500 loss: 0.8904756689071656\n",
      "  batch 550 loss: 0.8218496870994568\n",
      "  batch 600 loss: 0.8855543720722199\n",
      "  batch 650 loss: 0.8624838852882385\n",
      "  batch 700 loss: 0.893779547214508\n",
      "  batch 750 loss: 0.8552696239948273\n",
      "  batch 800 loss: 0.8693451237678528\n",
      "  batch 850 loss: 0.8715289270877838\n",
      "  batch 900 loss: 0.8714948964118957\n",
      "LOSS train 0.87149 valid 0.91756, valid PER 28.36%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8477784943580627\n",
      "  batch 100 loss: 0.806564780473709\n",
      "  batch 150 loss: 0.7772921419143677\n",
      "  batch 200 loss: 0.8230393087863922\n",
      "  batch 250 loss: 0.8491851627826691\n",
      "  batch 300 loss: 0.8309590619802475\n",
      "  batch 350 loss: 0.8267653954029083\n",
      "  batch 400 loss: 0.8155076777935029\n",
      "  batch 450 loss: 0.8181683903932572\n",
      "  batch 500 loss: 0.817138661146164\n",
      "  batch 550 loss: 0.8255129492282868\n",
      "  batch 600 loss: 0.8176298946142196\n",
      "  batch 650 loss: 0.8297349679470062\n",
      "  batch 700 loss: 0.8202097058296204\n",
      "  batch 750 loss: 0.8221968483924865\n",
      "  batch 800 loss: 0.8013542068004608\n",
      "  batch 850 loss: 0.7959299850463867\n",
      "  batch 900 loss: 0.8044638311862946\n",
      "LOSS train 0.80446 valid 0.87396, valid PER 26.86%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.7789965271949768\n",
      "  batch 100 loss: 0.7933866691589355\n",
      "  batch 150 loss: 0.7664469808340073\n",
      "  batch 200 loss: 0.7577417635917664\n",
      "  batch 250 loss: 0.7570385926961899\n",
      "  batch 300 loss: 0.7597524487972259\n",
      "  batch 350 loss: 0.7621446573734283\n",
      "  batch 400 loss: 0.8026571631431579\n",
      "  batch 450 loss: 0.7636329704523086\n",
      "  batch 500 loss: 0.7815696769952774\n",
      "  batch 550 loss: 0.7708369106054306\n",
      "  batch 600 loss: 0.7890029442310333\n",
      "  batch 650 loss: 0.7619980847835541\n",
      "  batch 700 loss: 0.7823597800731659\n",
      "  batch 750 loss: 0.7508620667457581\n",
      "  batch 800 loss: 0.7700620740652084\n",
      "  batch 850 loss: 0.7824063014984131\n",
      "  batch 900 loss: 0.8132498216629028\n",
      "LOSS train 0.81325 valid 0.85523, valid PER 26.43%\n",
      "EPOCH 8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 50 loss: 0.7268823367357254\n",
      "  batch 100 loss: 0.7349890881776809\n",
      "  batch 150 loss: 0.7295937353372574\n",
      "  batch 200 loss: 0.7143506240844727\n",
      "  batch 250 loss: 0.7382188314199447\n",
      "  batch 300 loss: 0.6894061255455017\n",
      "  batch 350 loss: 0.7651323974132538\n",
      "  batch 400 loss: 0.694420627951622\n",
      "  batch 450 loss: 0.7323562061786651\n",
      "  batch 500 loss: 0.7702307188510895\n",
      "  batch 550 loss: 0.7111901944875717\n",
      "  batch 600 loss: 0.7469295918941498\n",
      "  batch 650 loss: 0.7620429718494415\n",
      "  batch 700 loss: 0.721162074804306\n",
      "  batch 750 loss: 0.7163517618179321\n",
      "  batch 800 loss: 0.7450493955612183\n",
      "  batch 850 loss: 0.7161841875314713\n",
      "  batch 900 loss: 0.7603015494346619\n",
      "LOSS train 0.76030 valid 0.86214, valid PER 26.19%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.6771011447906494\n",
      "  batch 100 loss: 0.688838055729866\n",
      "  batch 150 loss: 0.7023711949586868\n",
      "  batch 200 loss: 0.6718297672271728\n",
      "  batch 250 loss: 0.7150232744216919\n",
      "  batch 300 loss: 0.7054979377985\n",
      "  batch 350 loss: 0.7228101468086243\n",
      "  batch 400 loss: 0.6896549212932587\n",
      "  batch 450 loss: 0.6896236890554428\n",
      "  batch 500 loss: 0.6858452862501144\n",
      "  batch 550 loss: 0.7162139236927032\n",
      "  batch 600 loss: 0.7219161546230316\n",
      "  batch 650 loss: 0.6868929827213287\n",
      "  batch 700 loss: 0.6786873358488082\n",
      "  batch 750 loss: 0.68762122631073\n",
      "  batch 800 loss: 0.7184194386005401\n",
      "  batch 850 loss: 0.7179134142398834\n",
      "  batch 900 loss: 0.6843269377946853\n",
      "LOSS train 0.68433 valid 0.81250, valid PER 24.70%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6143351978063584\n",
      "  batch 100 loss: 0.6605817502737046\n",
      "  batch 150 loss: 0.6547414362430573\n",
      "  batch 200 loss: 0.6997117102146149\n",
      "  batch 250 loss: 0.6897202605009078\n",
      "  batch 300 loss: 0.6548460453748703\n",
      "  batch 350 loss: 0.6654080194234848\n",
      "  batch 400 loss: 0.646193642616272\n",
      "  batch 450 loss: 0.632842812538147\n",
      "  batch 500 loss: 0.6772971099615097\n",
      "  batch 550 loss: 0.6825526583194733\n",
      "  batch 600 loss: 0.6601685273647309\n",
      "  batch 650 loss: 0.6549890768527985\n",
      "  batch 700 loss: 0.6881439846754074\n",
      "  batch 750 loss: 0.6639242404699326\n",
      "  batch 800 loss: 0.678916477560997\n",
      "  batch 850 loss: 0.6700913316011429\n",
      "  batch 900 loss: 0.6976100397109986\n",
      "LOSS train 0.69761 valid 0.80015, valid PER 24.26%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6079228764772415\n",
      "  batch 100 loss: 0.5863996356725693\n",
      "  batch 150 loss: 0.6204192894697189\n",
      "  batch 200 loss: 0.6670397728681564\n",
      "  batch 250 loss: 0.6489238661527633\n",
      "  batch 300 loss: 0.6057430398464203\n",
      "  batch 350 loss: 0.6425381994247437\n",
      "  batch 400 loss: 0.6571056127548218\n",
      "  batch 450 loss: 0.6344902753829956\n",
      "  batch 500 loss: 0.6149181842803955\n",
      "  batch 550 loss: 0.6393528658151627\n",
      "  batch 600 loss: 0.6185024547576904\n",
      "  batch 650 loss: 0.6884146404266357\n",
      "  batch 700 loss: 0.6256022363901138\n",
      "  batch 750 loss: 0.6286664885282517\n",
      "  batch 800 loss: 0.6516834026575089\n",
      "  batch 850 loss: 0.6619004535675049\n",
      "  batch 900 loss: 0.6612177789211273\n",
      "LOSS train 0.66122 valid 0.81396, valid PER 24.62%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.6190993827581406\n",
      "  batch 100 loss: 0.5949466055631638\n",
      "  batch 150 loss: 0.5783482587337494\n",
      "  batch 200 loss: 0.5944490861892701\n",
      "  batch 250 loss: 0.6106096249818802\n",
      "  batch 300 loss: 0.5975596576929092\n",
      "  batch 350 loss: 0.5861457735300064\n",
      "  batch 400 loss: 0.6289318972826003\n",
      "  batch 450 loss: 0.6224299854040146\n",
      "  batch 500 loss: 0.6128515458106994\n",
      "  batch 550 loss: 0.5778431874513626\n",
      "  batch 600 loss: 0.620552099943161\n",
      "  batch 650 loss: 0.6362422716617584\n",
      "  batch 700 loss: 0.6356189823150635\n",
      "  batch 750 loss: 0.61263019323349\n",
      "  batch 800 loss: 0.6059424966573715\n",
      "  batch 850 loss: 0.6504510927200318\n",
      "  batch 900 loss: 0.6370252370834351\n",
      "LOSS train 0.63703 valid 0.80707, valid PER 24.14%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.577322182059288\n",
      "  batch 100 loss: 0.56931574344635\n",
      "  batch 150 loss: 0.5522187632322312\n",
      "  batch 200 loss: 0.583791783452034\n",
      "  batch 250 loss: 0.585486570596695\n",
      "  batch 300 loss: 0.5707768309116363\n",
      "  batch 350 loss: 0.5701559764146805\n",
      "  batch 400 loss: 0.5920077627897262\n",
      "  batch 450 loss: 0.6035128718614579\n",
      "  batch 500 loss: 0.5651860499382019\n",
      "  batch 550 loss: 0.605081844329834\n",
      "  batch 600 loss: 0.5879665923118591\n",
      "  batch 650 loss: 0.6103687977790833\n",
      "  batch 700 loss: 0.6134331399202346\n",
      "  batch 750 loss: 0.5640433150529861\n",
      "  batch 800 loss: 0.5873657047748566\n",
      "  batch 850 loss: 0.6270662397146225\n",
      "  batch 900 loss: 0.6216471314430236\n",
      "LOSS train 0.62165 valid 0.80942, valid PER 24.53%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.5560235261917115\n",
      "  batch 100 loss: 0.5661659759283065\n",
      "  batch 150 loss: 0.5436565220355988\n",
      "  batch 200 loss: 0.5551998645067215\n",
      "  batch 250 loss: 0.5637469154596328\n",
      "  batch 300 loss: 0.5890085941553116\n",
      "  batch 350 loss: 0.5584345215559006\n",
      "  batch 400 loss: 0.5564284706115723\n",
      "  batch 450 loss: 0.5625885182619095\n",
      "  batch 500 loss: 0.5932125180959702\n",
      "  batch 550 loss: 0.5794657385349273\n",
      "  batch 600 loss: 0.5512433618307113\n",
      "  batch 650 loss: 0.5773890513181686\n",
      "  batch 700 loss: 0.6042525988817214\n",
      "  batch 750 loss: 0.5533537369966507\n",
      "  batch 800 loss: 0.5453748160600662\n",
      "  batch 850 loss: 0.5991463845968247\n",
      "  batch 900 loss: 0.5848142188787461\n",
      "LOSS train 0.58481 valid 0.79443, valid PER 23.82%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.5329840129613876\n",
      "  batch 100 loss: 0.5233863306045532\n",
      "  batch 150 loss: 0.5304237341880799\n",
      "  batch 200 loss: 0.5590069377422333\n",
      "  batch 250 loss: 0.5580606573820114\n",
      "  batch 300 loss: 0.5285183590650558\n",
      "  batch 350 loss: 0.5446593749523163\n",
      "  batch 400 loss: 0.5366864705085754\n",
      "  batch 450 loss: 0.5573906153440475\n",
      "  batch 500 loss: 0.5346631181240081\n",
      "  batch 550 loss: 0.5427989321947098\n",
      "  batch 600 loss: 0.5604544341564178\n",
      "  batch 650 loss: 0.5773209971189499\n",
      "  batch 700 loss: 0.5874683386087418\n",
      "  batch 750 loss: 0.5877271461486816\n",
      "  batch 800 loss: 0.5353526079654694\n",
      "  batch 850 loss: 0.5441679280996322\n",
      "  batch 900 loss: 0.5562080347537994\n",
      "LOSS train 0.55621 valid 0.78459, valid PER 23.05%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.5192606264352798\n",
      "  batch 100 loss: 0.48817374408245084\n",
      "  batch 150 loss: 0.5126826745271683\n",
      "  batch 200 loss: 0.5162253171205521\n",
      "  batch 250 loss: 0.5410866987705231\n",
      "  batch 300 loss: 0.523646502494812\n",
      "  batch 350 loss: 0.530827362537384\n",
      "  batch 400 loss: 0.5309507429599762\n",
      "  batch 450 loss: 0.552693258523941\n",
      "  batch 500 loss: 0.5322282987833024\n",
      "  batch 550 loss: 0.5295489299297332\n",
      "  batch 600 loss: 0.5348717507719993\n",
      "  batch 650 loss: 0.5559389454126358\n",
      "  batch 700 loss: 0.5259134721755981\n",
      "  batch 750 loss: 0.5468527603149415\n",
      "  batch 800 loss: 0.5506205004453659\n",
      "  batch 850 loss: 0.5329314583539962\n",
      "  batch 900 loss: 0.5486842322349549\n",
      "LOSS train 0.54868 valid 0.78599, valid PER 23.37%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.49386129915714266\n",
      "  batch 100 loss: 0.47816305577754975\n",
      "  batch 150 loss: 0.4955395436286926\n",
      "  batch 200 loss: 0.5012156194448472\n",
      "  batch 250 loss: 0.5176350516080857\n",
      "  batch 300 loss: 0.5019056504964828\n",
      "  batch 350 loss: 0.48862300157546995\n",
      "  batch 400 loss: 0.5307853108644486\n",
      "  batch 450 loss: 0.5181306618452072\n",
      "  batch 500 loss: 0.508893039226532\n",
      "  batch 550 loss: 0.517767368555069\n",
      "  batch 600 loss: 0.5480418217182159\n",
      "  batch 650 loss: 0.511104769706726\n",
      "  batch 700 loss: 0.49295532166957856\n",
      "  batch 750 loss: 0.5121911585330963\n",
      "  batch 800 loss: 0.49605809986591337\n",
      "  batch 850 loss: 0.5397790053486824\n",
      "  batch 900 loss: 0.5111944544315338\n",
      "LOSS train 0.51119 valid 0.79266, valid PER 22.79%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.4710391694307327\n",
      "  batch 100 loss: 0.4688717234134674\n",
      "  batch 150 loss: 0.512270901799202\n",
      "  batch 200 loss: 0.47952806532382963\n",
      "  batch 250 loss: 0.48586974799633026\n",
      "  batch 300 loss: 0.4909327745437622\n",
      "  batch 350 loss: 0.5085868096351623\n",
      "  batch 400 loss: 0.4917411410808563\n",
      "  batch 450 loss: 0.5001005816459656\n",
      "  batch 500 loss: 0.5210105556249619\n",
      "  batch 550 loss: 0.5108797985315323\n",
      "  batch 600 loss: 0.4748149484395981\n",
      "  batch 650 loss: 0.4858508360385895\n",
      "  batch 700 loss: 0.5259624236822128\n",
      "  batch 750 loss: 0.5099212908744812\n",
      "  batch 800 loss: 0.5037868309020996\n",
      "  batch 850 loss: 0.49661263763904573\n",
      "  batch 900 loss: 0.5000843316316604\n",
      "LOSS train 0.50008 valid 0.79699, valid PER 22.87%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.44625780642032625\n",
      "  batch 100 loss: 0.4554406100511551\n",
      "  batch 150 loss: 0.45231727659702303\n",
      "  batch 200 loss: 0.476571643948555\n",
      "  batch 250 loss: 0.47014399588108063\n",
      "  batch 300 loss: 0.46574445366859435\n",
      "  batch 350 loss: 0.4768093800544739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 400 loss: 0.4739803194999695\n",
      "  batch 450 loss: 0.49771519780158996\n",
      "  batch 500 loss: 0.48587507784366607\n",
      "  batch 550 loss: 0.47453959822654723\n",
      "  batch 600 loss: 0.48166562139987945\n",
      "  batch 650 loss: 0.5088045483827591\n",
      "  batch 700 loss: 0.47520000994205475\n",
      "  batch 750 loss: 0.4718728107213974\n",
      "  batch 800 loss: 0.5101520848274231\n",
      "  batch 850 loss: 0.4907538664340973\n",
      "  batch 900 loss: 0.5112993305921555\n",
      "LOSS train 0.51130 valid 0.80489, valid PER 23.29%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.4431626093387604\n",
      "  batch 100 loss: 0.44302534967660906\n",
      "  batch 150 loss: 0.4380750918388367\n",
      "  batch 200 loss: 0.464879949092865\n",
      "  batch 250 loss: 0.4507046562433243\n",
      "  batch 300 loss: 0.4627227354049683\n",
      "  batch 350 loss: 0.4472335213422775\n",
      "  batch 400 loss: 0.4643052500486374\n",
      "  batch 450 loss: 0.46353774845600126\n",
      "  batch 500 loss: 0.45222378611564634\n",
      "  batch 550 loss: 0.49999658346176146\n",
      "  batch 600 loss: 0.4585414236783981\n",
      "  batch 650 loss: 0.4629732546210289\n",
      "  batch 700 loss: 0.47838465094566346\n",
      "  batch 750 loss: 0.4433804178237915\n",
      "  batch 800 loss: 0.49865893721580506\n",
      "  batch 850 loss: 0.4887500900030136\n",
      "  batch 900 loss: 0.49921759456396103\n",
      "LOSS train 0.49922 valid 0.81425, valid PER 22.96%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_124021/model_15\n",
      "Currently using dropout rate of 0.5\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.993065524101257\n",
      "  batch 100 loss: 3.2116189336776735\n",
      "  batch 150 loss: 2.994650740623474\n",
      "  batch 200 loss: 2.7214414358139036\n",
      "  batch 250 loss: 2.4992922925949097\n",
      "  batch 300 loss: 2.3078014993667604\n",
      "  batch 350 loss: 2.1322726345062257\n",
      "  batch 400 loss: 2.072984893321991\n",
      "  batch 450 loss: 1.958749644756317\n",
      "  batch 500 loss: 1.8467368626594542\n",
      "  batch 550 loss: 1.76891206741333\n",
      "  batch 600 loss: 1.7241826963424682\n",
      "  batch 650 loss: 1.6445083022117615\n",
      "  batch 700 loss: 1.6385462975502014\n",
      "  batch 750 loss: 1.5709742832183837\n",
      "  batch 800 loss: 1.5815487599372864\n",
      "  batch 850 loss: 1.5314363741874695\n",
      "  batch 900 loss: 1.4732898688316345\n",
      "LOSS train 1.47329 valid 1.41075, valid PER 49.78%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.4248824572563172\n",
      "  batch 100 loss: 1.3738549661636352\n",
      "  batch 150 loss: 1.3310541462898255\n",
      "  batch 200 loss: 1.3636621499061585\n",
      "  batch 250 loss: 1.349673752784729\n",
      "  batch 300 loss: 1.3075543141365051\n",
      "  batch 350 loss: 1.2383904933929444\n",
      "  batch 400 loss: 1.2523345947265625\n",
      "  batch 450 loss: 1.2100362646579743\n",
      "  batch 500 loss: 1.2406157088279723\n",
      "  batch 550 loss: 1.2350387907028197\n",
      "  batch 600 loss: 1.1921093952655792\n",
      "  batch 650 loss: 1.20435244679451\n",
      "  batch 700 loss: 1.1711119258403777\n",
      "  batch 750 loss: 1.1686983489990235\n",
      "  batch 800 loss: 1.0986994922161102\n",
      "  batch 850 loss: 1.1200906610488892\n",
      "  batch 900 loss: 1.15222341299057\n",
      "LOSS train 1.15222 valid 1.09267, valid PER 33.83%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.1394814360141754\n",
      "  batch 100 loss: 1.088429661989212\n",
      "  batch 150 loss: 1.071958496570587\n",
      "  batch 200 loss: 1.074510338306427\n",
      "  batch 250 loss: 1.0741198909282685\n",
      "  batch 300 loss: 1.0462844967842102\n",
      "  batch 350 loss: 1.0937333142757415\n",
      "  batch 400 loss: 1.0557778704166412\n",
      "  batch 450 loss: 1.0609541749954223\n",
      "  batch 500 loss: 1.0222127842903137\n",
      "  batch 550 loss: 1.0455720913410187\n",
      "  batch 600 loss: 1.010640264749527\n",
      "  batch 650 loss: 1.0029127418994903\n",
      "  batch 700 loss: 1.0369101881980896\n",
      "  batch 750 loss: 1.0587828302383422\n",
      "  batch 800 loss: 1.0073761093616485\n",
      "  batch 850 loss: 1.063889560699463\n",
      "  batch 900 loss: 0.975965918302536\n",
      "LOSS train 0.97597 valid 1.01976, valid PER 31.45%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.950640299320221\n",
      "  batch 100 loss: 0.9897846341133117\n",
      "  batch 150 loss: 0.9431533169746399\n",
      "  batch 200 loss: 0.9845858860015869\n",
      "  batch 250 loss: 0.973139830827713\n",
      "  batch 300 loss: 0.9843520748615265\n",
      "  batch 350 loss: 0.913000943660736\n",
      "  batch 400 loss: 0.9533153748512269\n",
      "  batch 450 loss: 0.9490025496482849\n",
      "  batch 500 loss: 0.9364213669300079\n",
      "  batch 550 loss: 0.9412055885791779\n",
      "  batch 600 loss: 0.9744021213054657\n",
      "  batch 650 loss: 0.9665829944610596\n",
      "  batch 700 loss: 0.9351241397857666\n",
      "  batch 750 loss: 0.91182084441185\n",
      "  batch 800 loss: 0.9008646714687347\n",
      "  batch 850 loss: 0.9315731883049011\n",
      "  batch 900 loss: 0.9739961242675781\n",
      "LOSS train 0.97400 valid 0.91369, valid PER 28.69%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.871984441280365\n",
      "  batch 100 loss: 0.8759662532806396\n",
      "  batch 150 loss: 0.9304800796508789\n",
      "  batch 200 loss: 0.8637147343158722\n",
      "  batch 250 loss: 0.8770770967006684\n",
      "  batch 300 loss: 0.8822914230823516\n",
      "  batch 350 loss: 0.8733987867832184\n",
      "  batch 400 loss: 0.8940694499015808\n",
      "  batch 450 loss: 0.8776404654979706\n",
      "  batch 500 loss: 0.9051228928565979\n",
      "  batch 550 loss: 0.8380835354328156\n",
      "  batch 600 loss: 0.9192552697658539\n",
      "  batch 650 loss: 0.8762261259555817\n",
      "  batch 700 loss: 0.9021206414699554\n",
      "  batch 750 loss: 0.8408320164680481\n",
      "  batch 800 loss: 0.8874790513515473\n",
      "  batch 850 loss: 0.8808229219913483\n",
      "  batch 900 loss: 0.8905520510673522\n",
      "LOSS train 0.89055 valid 0.89726, valid PER 27.72%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8630334722995758\n",
      "  batch 100 loss: 0.815269546508789\n",
      "  batch 150 loss: 0.8021423470973968\n",
      "  batch 200 loss: 0.8143737316131592\n",
      "  batch 250 loss: 0.8584231364727021\n",
      "  batch 300 loss: 0.8269492065906525\n",
      "  batch 350 loss: 0.8394139182567596\n",
      "  batch 400 loss: 0.8076103842258453\n",
      "  batch 450 loss: 0.8227969998121262\n",
      "  batch 500 loss: 0.8273443377017975\n",
      "  batch 550 loss: 0.8457860171794891\n",
      "  batch 600 loss: 0.8248691701889038\n",
      "  batch 650 loss: 0.824041121006012\n",
      "  batch 700 loss: 0.841048573255539\n",
      "  batch 750 loss: 0.8219450956583023\n",
      "  batch 800 loss: 0.8085021126270294\n",
      "  batch 850 loss: 0.8027292335033417\n",
      "  batch 900 loss: 0.8060113596916199\n",
      "LOSS train 0.80601 valid 0.85577, valid PER 27.04%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.7844602763652802\n",
      "  batch 100 loss: 0.8105694246292114\n",
      "  batch 150 loss: 0.7678104174137116\n",
      "  batch 200 loss: 0.7691970956325531\n",
      "  batch 250 loss: 0.7670087623596191\n",
      "  batch 300 loss: 0.7656887221336365\n",
      "  batch 350 loss: 0.7711579060554504\n",
      "  batch 400 loss: 0.7780273973941803\n",
      "  batch 450 loss: 0.7884624624252319\n",
      "  batch 500 loss: 0.781240975856781\n",
      "  batch 550 loss: 0.7753800845146179\n",
      "  batch 600 loss: 0.8006812393665313\n",
      "  batch 650 loss: 0.7757935559749604\n",
      "  batch 700 loss: 0.8043119311332703\n",
      "  batch 750 loss: 0.7888392448425293\n",
      "  batch 800 loss: 0.7678100061416626\n",
      "  batch 850 loss: 0.7928106737136841\n",
      "  batch 900 loss: 0.8062017786502839\n",
      "LOSS train 0.80620 valid 0.87308, valid PER 27.00%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.7624953937530518\n",
      "  batch 100 loss: 0.7345405352115632\n",
      "  batch 150 loss: 0.7398858463764191\n",
      "  batch 200 loss: 0.7342996859550476\n",
      "  batch 250 loss: 0.7542152261734009\n",
      "  batch 300 loss: 0.7106826436519623\n",
      "  batch 350 loss: 0.7783032524585723\n",
      "  batch 400 loss: 0.7198969113826752\n",
      "  batch 450 loss: 0.7485450541973114\n",
      "  batch 500 loss: 0.7896384561061859\n",
      "  batch 550 loss: 0.7207875263690948\n",
      "  batch 600 loss: 0.7573882162570953\n",
      "  batch 650 loss: 0.7742203652858735\n",
      "  batch 700 loss: 0.7411414557695388\n",
      "  batch 750 loss: 0.7543703699111939\n",
      "  batch 800 loss: 0.7682350927591324\n",
      "  batch 850 loss: 0.7431522887945176\n",
      "  batch 900 loss: 0.7597134608030319\n",
      "LOSS train 0.75971 valid 0.83317, valid PER 25.08%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.692151905298233\n",
      "  batch 100 loss: 0.7257418406009674\n",
      "  batch 150 loss: 0.7168978154659271\n",
      "  batch 200 loss: 0.7002224558591843\n",
      "  batch 250 loss: 0.7184625345468522\n",
      "  batch 300 loss: 0.7203139400482178\n",
      "  batch 350 loss: 0.7387245690822601\n",
      "  batch 400 loss: 0.7201814991235733\n",
      "  batch 450 loss: 0.7140742921829224\n",
      "  batch 500 loss: 0.7047102630138398\n",
      "  batch 550 loss: 0.7396948730945587\n",
      "  batch 600 loss: 0.7400947105884552\n",
      "  batch 650 loss: 0.7218440222740173\n",
      "  batch 700 loss: 0.6860416424274445\n",
      "  batch 750 loss: 0.7065733534097671\n",
      "  batch 800 loss: 0.7267624485492706\n",
      "  batch 850 loss: 0.7521118175983429\n",
      "  batch 900 loss: 0.681080840229988\n",
      "LOSS train 0.68108 valid 0.81354, valid PER 24.96%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.643329211473465\n",
      "  batch 100 loss: 0.6665934777259827\n",
      "  batch 150 loss: 0.6944410902261734\n",
      "  batch 200 loss: 0.7048025298118591\n",
      "  batch 250 loss: 0.7042098534107208\n",
      "  batch 300 loss: 0.6609479320049286\n",
      "  batch 350 loss: 0.6759650111198425\n",
      "  batch 400 loss: 0.6461769378185273\n",
      "  batch 450 loss: 0.6720521998405456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 500 loss: 0.7085885316133499\n",
      "  batch 550 loss: 0.7165661180019378\n",
      "  batch 600 loss: 0.6815368151664734\n",
      "  batch 650 loss: 0.6775945323705673\n",
      "  batch 700 loss: 0.7237945479154587\n",
      "  batch 750 loss: 0.6785051238536834\n",
      "  batch 800 loss: 0.6899566745758057\n",
      "  batch 850 loss: 0.7004611551761627\n",
      "  batch 900 loss: 0.7223501765727997\n",
      "LOSS train 0.72235 valid 0.80185, valid PER 24.64%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6374377524852752\n",
      "  batch 100 loss: 0.6104527217149734\n",
      "  batch 150 loss: 0.6431829398870468\n",
      "  batch 200 loss: 0.678671418428421\n",
      "  batch 250 loss: 0.6718672221899032\n",
      "  batch 300 loss: 0.6489477509260178\n",
      "  batch 350 loss: 0.6531985241174698\n",
      "  batch 400 loss: 0.6830776917934418\n",
      "  batch 450 loss: 0.6618759876489639\n",
      "  batch 500 loss: 0.6367941266298294\n",
      "  batch 550 loss: 0.678585866689682\n",
      "  batch 600 loss: 0.6407218086719513\n",
      "  batch 650 loss: 0.717633386850357\n",
      "  batch 700 loss: 0.6466231709718704\n",
      "  batch 750 loss: 0.6514138102531433\n",
      "  batch 800 loss: 0.6718869292736054\n",
      "  batch 850 loss: 0.6795138704776764\n",
      "  batch 900 loss: 0.6811715990304947\n",
      "LOSS train 0.68117 valid 0.80575, valid PER 23.65%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.6380769169330597\n",
      "  batch 100 loss: 0.6215267419815064\n",
      "  batch 150 loss: 0.5974989199638366\n",
      "  batch 200 loss: 0.6267481672763825\n",
      "  batch 250 loss: 0.6318683409690857\n",
      "  batch 300 loss: 0.6248892724514008\n",
      "  batch 350 loss: 0.6160413104295731\n",
      "  batch 400 loss: 0.6462644040584564\n",
      "  batch 450 loss: 0.653297844529152\n",
      "  batch 500 loss: 0.6536227768659592\n",
      "  batch 550 loss: 0.5992730057239533\n",
      "  batch 600 loss: 0.6474814999103546\n",
      "  batch 650 loss: 0.6669498485326767\n",
      "  batch 700 loss: 0.6472417742013932\n",
      "  batch 750 loss: 0.63228580057621\n",
      "  batch 800 loss: 0.6218727558851243\n",
      "  batch 850 loss: 0.6773529249429703\n",
      "  batch 900 loss: 0.6734452456235885\n",
      "LOSS train 0.67345 valid 0.79059, valid PER 24.03%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.6066158354282379\n",
      "  batch 100 loss: 0.5976289200782776\n",
      "  batch 150 loss: 0.5970761740207672\n",
      "  batch 200 loss: 0.6241941654682159\n",
      "  batch 250 loss: 0.6273152196407318\n",
      "  batch 300 loss: 0.593216804265976\n",
      "  batch 350 loss: 0.6036085444688797\n",
      "  batch 400 loss: 0.6187949323654175\n",
      "  batch 450 loss: 0.6282419347763062\n",
      "  batch 500 loss: 0.6031070226430892\n",
      "  batch 550 loss: 0.6363414841890335\n",
      "  batch 600 loss: 0.6043528831005096\n",
      "  batch 650 loss: 0.6401362407207489\n",
      "  batch 700 loss: 0.6515016323328018\n",
      "  batch 750 loss: 0.589027755856514\n",
      "  batch 800 loss: 0.639979996085167\n",
      "  batch 850 loss: 0.6608328694105148\n",
      "  batch 900 loss: 0.6487138134241104\n",
      "LOSS train 0.64871 valid 0.79423, valid PER 24.18%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.586745645403862\n",
      "  batch 100 loss: 0.5963788592815399\n",
      "  batch 150 loss: 0.5762717354297638\n",
      "  batch 200 loss: 0.5869277006387711\n",
      "  batch 250 loss: 0.5977785092592239\n",
      "  batch 300 loss: 0.6198856955766678\n",
      "  batch 350 loss: 0.573241503238678\n",
      "  batch 400 loss: 0.5829402565956116\n",
      "  batch 450 loss: 0.5749461162090301\n",
      "  batch 500 loss: 0.6078732466697693\n",
      "  batch 550 loss: 0.6097171407938003\n",
      "  batch 600 loss: 0.583290411233902\n",
      "  batch 650 loss: 0.6240472239255905\n",
      "  batch 700 loss: 0.637268306016922\n",
      "  batch 750 loss: 0.6047526675462723\n",
      "  batch 800 loss: 0.5828601413965225\n",
      "  batch 850 loss: 0.6451879757642746\n",
      "  batch 900 loss: 0.6195497411489487\n",
      "LOSS train 0.61955 valid 0.80142, valid PER 24.19%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.5498839449882508\n",
      "  batch 100 loss: 0.5633493274450302\n",
      "  batch 150 loss: 0.5618662345409393\n",
      "  batch 200 loss: 0.5844867062568665\n",
      "  batch 250 loss: 0.5840722519159317\n",
      "  batch 300 loss: 0.5637110650539399\n",
      "  batch 350 loss: 0.5733705008029938\n",
      "  batch 400 loss: 0.5771822726726532\n",
      "  batch 450 loss: 0.5738305652141571\n",
      "  batch 500 loss: 0.5502394616603852\n",
      "  batch 550 loss: 0.5805907332897187\n",
      "  batch 600 loss: 0.5991980201005935\n",
      "  batch 650 loss: 0.6130034416913986\n",
      "  batch 700 loss: 0.5999776047468185\n",
      "  batch 750 loss: 0.6098072379827499\n",
      "  batch 800 loss: 0.588609561920166\n",
      "  batch 850 loss: 0.5641482901573182\n",
      "  batch 900 loss: 0.5845507317781449\n",
      "LOSS train 0.58455 valid 0.78176, valid PER 23.02%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.5655951565504074\n",
      "  batch 100 loss: 0.5417164218425751\n",
      "  batch 150 loss: 0.5354123908281326\n",
      "  batch 200 loss: 0.5501993209123611\n",
      "  batch 250 loss: 0.5818221974372864\n",
      "  batch 300 loss: 0.557529593706131\n",
      "  batch 350 loss: 0.5639244163036347\n",
      "  batch 400 loss: 0.557728978395462\n",
      "  batch 450 loss: 0.5724526023864747\n",
      "  batch 500 loss: 0.5416869068145752\n",
      "  batch 550 loss: 0.5644937044382096\n",
      "  batch 600 loss: 0.5643214112520218\n",
      "  batch 650 loss: 0.5803376585245132\n",
      "  batch 700 loss: 0.5515703755617142\n",
      "  batch 750 loss: 0.5683364170789719\n",
      "  batch 800 loss: 0.6090733331441879\n",
      "  batch 850 loss: 0.5711124181747437\n",
      "  batch 900 loss: 0.5833424645662307\n",
      "LOSS train 0.58334 valid 0.77869, valid PER 23.38%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.5381139397621155\n",
      "  batch 100 loss: 0.5538809984922409\n",
      "  batch 150 loss: 0.5255408930778503\n",
      "  batch 200 loss: 0.5293735659122467\n",
      "  batch 250 loss: 0.5502309632301331\n",
      "  batch 300 loss: 0.5594865620136261\n",
      "  batch 350 loss: 0.5195971977710724\n",
      "  batch 400 loss: 0.5639180481433869\n",
      "  batch 450 loss: 0.5722379499673843\n",
      "  batch 500 loss: 0.5477535343170166\n",
      "  batch 550 loss: 0.5439492529630661\n",
      "  batch 600 loss: 0.5802114593982697\n",
      "  batch 650 loss: 0.5606722211837769\n",
      "  batch 700 loss: 0.5461624443531037\n",
      "  batch 750 loss: 0.5216126900911331\n",
      "  batch 800 loss: 0.522353036403656\n",
      "  batch 850 loss: 0.5605884927511215\n",
      "  batch 900 loss: 0.5512394678592681\n",
      "LOSS train 0.55124 valid 0.77527, valid PER 22.58%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.5224776303768158\n",
      "  batch 100 loss: 0.5157791185379028\n",
      "  batch 150 loss: 0.5544413816928864\n",
      "  batch 200 loss: 0.526710901260376\n",
      "  batch 250 loss: 0.5414137315750122\n",
      "  batch 300 loss: 0.5203615939617157\n",
      "  batch 350 loss: 0.532207887172699\n",
      "  batch 400 loss: 0.5096535515785218\n",
      "  batch 450 loss: 0.5455881196260453\n",
      "  batch 500 loss: 0.538323910832405\n",
      "  batch 550 loss: 0.5383676695823669\n",
      "  batch 600 loss: 0.5306513053178787\n",
      "  batch 650 loss: 0.533969139456749\n",
      "  batch 700 loss: 0.5498891967535019\n",
      "  batch 750 loss: 0.5218761736154556\n",
      "  batch 800 loss: 0.5141249150037766\n",
      "  batch 850 loss: 0.5227771481871605\n",
      "  batch 900 loss: 0.549410136938095\n",
      "LOSS train 0.54941 valid 0.78334, valid PER 22.91%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.4704409331083298\n",
      "  batch 100 loss: 0.4770858260989189\n",
      "  batch 150 loss: 0.49748498857021334\n",
      "  batch 200 loss: 0.508032186627388\n",
      "  batch 250 loss: 0.5170860850811004\n",
      "  batch 300 loss: 0.5147377222776413\n",
      "  batch 350 loss: 0.5122146743535996\n",
      "  batch 400 loss: 0.508658886551857\n",
      "  batch 450 loss: 0.5384877169132233\n",
      "  batch 500 loss: 0.5372374922037124\n",
      "  batch 550 loss: 0.5050583106279373\n",
      "  batch 600 loss: 0.5197583943605423\n",
      "  batch 650 loss: 0.558694429397583\n",
      "  batch 700 loss: 0.5099033689498902\n",
      "  batch 750 loss: 0.5064245796203614\n",
      "  batch 800 loss: 0.5479660677909851\n",
      "  batch 850 loss: 0.5433174186944961\n",
      "  batch 900 loss: 0.5345678550004959\n",
      "LOSS train 0.53457 valid 0.77640, valid PER 22.47%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.47930162996053693\n",
      "  batch 100 loss: 0.4806774026155472\n",
      "  batch 150 loss: 0.48629733711481093\n",
      "  batch 200 loss: 0.5062659668922425\n",
      "  batch 250 loss: 0.49428695648908616\n",
      "  batch 300 loss: 0.5220155471563339\n",
      "  batch 350 loss: 0.4769363296031952\n",
      "  batch 400 loss: 0.5169933778047562\n",
      "  batch 450 loss: 0.493014400601387\n",
      "  batch 500 loss: 0.4708211678266525\n",
      "  batch 550 loss: 0.5321685111522675\n",
      "  batch 600 loss: 0.48093060255050657\n",
      "  batch 650 loss: 0.5140467131137848\n",
      "  batch 700 loss: 0.5181239938735962\n",
      "  batch 750 loss: 0.490012369453907\n",
      "  batch 800 loss: 0.5195904397964477\n",
      "  batch 850 loss: 0.5246236354112626\n",
      "  batch 900 loss: 0.5233357602357864\n",
      "LOSS train 0.52334 valid 0.79592, valid PER 22.46%\n",
      "Training finished in 5.0 minutes.\n",
      "Model saved to checkpoints/20231206_124859/model_17\n",
      "Finish Adam optimiser\n",
      "Currently using SGD_Scheduler optimiser\n",
      "Currently using dropout rate of 0.1\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.112963171005249\n",
      "  batch 100 loss: 3.4083345222473143\n",
      "  batch 150 loss: 3.291053977012634\n",
      "  batch 200 loss: 3.1773540687561037\n",
      "  batch 250 loss: 3.0799169063568117\n",
      "  batch 300 loss: 2.8645587301254274\n",
      "  batch 350 loss: 2.6539838886260987\n",
      "  batch 400 loss: 2.53737322807312\n",
      "  batch 450 loss: 2.4185153341293333\n",
      "  batch 500 loss: 2.2856779432296754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 550 loss: 2.213016440868378\n",
      "  batch 600 loss: 2.1415928030014038\n",
      "  batch 650 loss: 2.0392748427391054\n",
      "  batch 700 loss: 2.0253085017204286\n",
      "  batch 750 loss: 1.9373744654655456\n",
      "  batch 800 loss: 1.8973784995079042\n",
      "  batch 850 loss: 1.8400518822669982\n",
      "  batch 900 loss: 1.7983426666259765\n",
      "LOSS train 1.79834 valid 1.72494, valid PER 65.65%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7247286224365235\n",
      "  batch 100 loss: 1.6412481760978699\n",
      "  batch 150 loss: 1.6123999047279358\n",
      "  batch 200 loss: 1.6127618265151977\n",
      "  batch 250 loss: 1.614330551624298\n",
      "  batch 300 loss: 1.5500157141685487\n",
      "  batch 350 loss: 1.4685749363899232\n",
      "  batch 400 loss: 1.476932008266449\n",
      "  batch 450 loss: 1.413519811630249\n",
      "  batch 500 loss: 1.4332879114151\n",
      "  batch 550 loss: 1.4254151749610902\n",
      "  batch 600 loss: 1.3675181484222412\n",
      "  batch 650 loss: 1.3657868349552154\n",
      "  batch 700 loss: 1.326122670173645\n",
      "  batch 750 loss: 1.3120778620243072\n",
      "  batch 800 loss: 1.2654885983467101\n",
      "  batch 850 loss: 1.262564994096756\n",
      "  batch 900 loss: 1.2787261319160461\n",
      "LOSS train 1.27873 valid 1.22491, valid PER 37.95%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.2264407503604888\n",
      "  batch 100 loss: 1.2030636394023895\n",
      "  batch 150 loss: 1.1844218373298645\n",
      "  batch 200 loss: 1.1796359467506408\n",
      "  batch 250 loss: 1.1565118765830993\n",
      "  batch 300 loss: 1.1566711068153381\n",
      "  batch 350 loss: 1.2058250463008882\n",
      "  batch 400 loss: 1.1702760982513427\n",
      "  batch 450 loss: 1.1506809306144714\n",
      "  batch 500 loss: 1.1326844537258147\n",
      "  batch 550 loss: 1.1276102602481841\n",
      "  batch 600 loss: 1.1108097910881043\n",
      "  batch 650 loss: 1.0947613644599914\n",
      "  batch 700 loss: 1.1075002288818359\n",
      "  batch 750 loss: 1.154981518983841\n",
      "  batch 800 loss: 1.0789769208431244\n",
      "  batch 850 loss: 1.1234763062000275\n",
      "  batch 900 loss: 1.0463089871406555\n",
      "LOSS train 1.04631 valid 1.10303, valid PER 33.86%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.03757310628891\n",
      "  batch 100 loss: 1.0513542640209197\n",
      "  batch 150 loss: 1.016555792093277\n",
      "  batch 200 loss: 1.0667480838298797\n",
      "  batch 250 loss: 1.0462456023693085\n",
      "  batch 300 loss: 1.0779253435134888\n",
      "  batch 350 loss: 0.9859658920764923\n",
      "  batch 400 loss: 1.0367380154132844\n",
      "  batch 450 loss: 1.0105156564712525\n",
      "  batch 500 loss: 1.003108252286911\n",
      "  batch 550 loss: 1.028813786506653\n",
      "  batch 600 loss: 1.0468278872966765\n",
      "  batch 650 loss: 1.0124425399303436\n",
      "  batch 700 loss: 0.9791027569770813\n",
      "  batch 750 loss: 0.9726574909687042\n",
      "  batch 800 loss: 0.9586788427829742\n",
      "  batch 850 loss: 0.9816803455352783\n",
      "  batch 900 loss: 1.0288679170608521\n",
      "LOSS train 1.02887 valid 1.00988, valid PER 31.24%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9447550189495086\n",
      "  batch 100 loss: 0.9465208315849304\n",
      "  batch 150 loss: 0.9954584646224975\n",
      "  batch 200 loss: 0.9196282899379731\n",
      "  batch 250 loss: 0.9244753670692444\n",
      "  batch 300 loss: 0.9395737612247467\n",
      "  batch 350 loss: 0.9199302053451538\n",
      "  batch 400 loss: 0.946104736328125\n",
      "  batch 450 loss: 0.9228599631786346\n",
      "  batch 500 loss: 0.9364841306209564\n",
      "  batch 550 loss: 0.8984931695461273\n",
      "  batch 600 loss: 0.9623522710800171\n",
      "  batch 650 loss: 0.924678190946579\n",
      "  batch 700 loss: 0.9690850830078125\n",
      "  batch 750 loss: 0.8873436653614044\n",
      "  batch 800 loss: 0.9330130565166473\n",
      "  batch 850 loss: 0.9253574252128601\n",
      "  batch 900 loss: 0.9234047722816467\n",
      "LOSS train 0.92340 valid 0.94415, valid PER 29.46%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9169012534618378\n",
      "  batch 100 loss: 0.8603197157382965\n",
      "  batch 150 loss: 0.8588772869110107\n",
      "  batch 200 loss: 0.8717489624023438\n",
      "  batch 250 loss: 0.9039380598068237\n",
      "  batch 300 loss: 0.8712182593345642\n",
      "  batch 350 loss: 0.8646170210838318\n",
      "  batch 400 loss: 0.8555988729000091\n",
      "  batch 450 loss: 0.8788158857822418\n",
      "  batch 500 loss: 0.8791525030136108\n",
      "  batch 550 loss: 0.8898824107646942\n",
      "  batch 600 loss: 0.8566421067714691\n",
      "  batch 650 loss: 0.8558325231075287\n",
      "  batch 700 loss: 0.8598316133022308\n",
      "  batch 750 loss: 0.8588774299621582\n",
      "  batch 800 loss: 0.8669066989421844\n",
      "  batch 850 loss: 0.8414327085018158\n",
      "  batch 900 loss: 0.8705298018455505\n",
      "LOSS train 0.87053 valid 0.92362, valid PER 28.25%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.842765462398529\n",
      "  batch 100 loss: 0.847227840423584\n",
      "  batch 150 loss: 0.808963624238968\n",
      "  batch 200 loss: 0.8043656265735626\n",
      "  batch 250 loss: 0.8138194006681442\n",
      "  batch 300 loss: 0.7881076192855835\n",
      "  batch 350 loss: 0.8315057289600373\n",
      "  batch 400 loss: 0.8028260540962219\n",
      "  batch 450 loss: 0.8097229444980621\n",
      "  batch 500 loss: 0.8333243775367737\n",
      "  batch 550 loss: 0.8092209684848786\n",
      "  batch 600 loss: 0.8208496654033661\n",
      "  batch 650 loss: 0.8151069831848144\n",
      "  batch 700 loss: 0.8369742655754089\n",
      "  batch 750 loss: 0.8061756002902984\n",
      "  batch 800 loss: 0.8328282845020294\n",
      "  batch 850 loss: 0.8285942089557647\n",
      "  batch 900 loss: 0.8616440403461456\n",
      "LOSS train 0.86164 valid 0.89772, valid PER 27.75%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.7850082015991211\n",
      "  batch 100 loss: 0.7712984347343445\n",
      "  batch 150 loss: 0.7605145335197449\n",
      "  batch 200 loss: 0.7518895196914673\n",
      "  batch 250 loss: 0.7870422029495239\n",
      "  batch 300 loss: 0.728826829791069\n",
      "  batch 350 loss: 0.8055901741981506\n",
      "  batch 400 loss: 0.7636437821388244\n",
      "  batch 450 loss: 0.7870400285720826\n",
      "  batch 500 loss: 0.7995556581020355\n",
      "  batch 550 loss: 0.7485175395011902\n",
      "  batch 600 loss: 0.7982528626918792\n",
      "  batch 650 loss: 0.8117993426322937\n",
      "  batch 700 loss: 0.7704991340637207\n",
      "  batch 750 loss: 0.7728251016139984\n",
      "  batch 800 loss: 0.7753708708286285\n",
      "  batch 850 loss: 0.7597376298904419\n",
      "  batch 900 loss: 0.7732250165939331\n",
      "LOSS train 0.77323 valid 0.84616, valid PER 26.38%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7039301651716232\n",
      "  batch 100 loss: 0.7323020482063294\n",
      "  batch 150 loss: 0.7150851309299469\n",
      "  batch 200 loss: 0.7148518723249435\n",
      "  batch 250 loss: 0.7394424951076508\n",
      "  batch 300 loss: 0.7322012662887574\n",
      "  batch 350 loss: 0.7558101004362107\n",
      "  batch 400 loss: 0.7339941930770874\n",
      "  batch 450 loss: 0.7375810289382935\n",
      "  batch 500 loss: 0.7047211295366287\n",
      "  batch 550 loss: 0.7259035646915436\n",
      "  batch 600 loss: 0.7495107454061508\n",
      "  batch 650 loss: 0.7210232877731323\n",
      "  batch 700 loss: 0.7069062638282776\n",
      "  batch 750 loss: 0.7211920237541198\n",
      "  batch 800 loss: 0.7427452731132508\n",
      "  batch 850 loss: 0.765919988155365\n",
      "  batch 900 loss: 0.7041432142257691\n",
      "LOSS train 0.70414 valid 0.83908, valid PER 25.84%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6641546213626861\n",
      "  batch 100 loss: 0.677178624868393\n",
      "  batch 150 loss: 0.7020592033863068\n",
      "  batch 200 loss: 0.7022341799736023\n",
      "  batch 250 loss: 0.7167424833774567\n",
      "  batch 300 loss: 0.6787017649412155\n",
      "  batch 350 loss: 0.688930807709694\n",
      "  batch 400 loss: 0.6550657492876053\n",
      "  batch 450 loss: 0.6754682433605194\n",
      "  batch 500 loss: 0.709003279209137\n",
      "  batch 550 loss: 0.7406597316265107\n",
      "  batch 600 loss: 0.704450917840004\n",
      "  batch 650 loss: 0.6783589780330658\n",
      "  batch 700 loss: 0.7134384191036225\n",
      "  batch 750 loss: 0.6905584079027176\n",
      "  batch 800 loss: 0.7069499236345291\n",
      "  batch 850 loss: 0.6996871596574783\n",
      "  batch 900 loss: 0.7141527426242829\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.71415 valid 0.84425, valid PER 26.06%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.618818610906601\n",
      "  batch 100 loss: 0.5791173636913299\n",
      "  batch 150 loss: 0.584014083147049\n",
      "  batch 200 loss: 0.6275143885612487\n",
      "  batch 250 loss: 0.6106013941764832\n",
      "  batch 300 loss: 0.5753326100111008\n",
      "  batch 350 loss: 0.6006561386585235\n",
      "  batch 400 loss: 0.6178994131088257\n",
      "  batch 450 loss: 0.6101194369792938\n",
      "  batch 500 loss: 0.5765951597690582\n",
      "  batch 550 loss: 0.5911015564203262\n",
      "  batch 600 loss: 0.580807768702507\n",
      "  batch 650 loss: 0.6383000606298447\n",
      "  batch 700 loss: 0.5716598737239837\n",
      "  batch 750 loss: 0.5727627843618393\n",
      "  batch 800 loss: 0.6155530279874801\n",
      "  batch 850 loss: 0.6193241930007934\n",
      "  batch 900 loss: 0.6264579057693481\n",
      "LOSS train 0.62646 valid 0.77770, valid PER 23.64%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.568175173997879\n",
      "  batch 100 loss: 0.5640254139900207\n",
      "  batch 150 loss: 0.5305596441030502\n",
      "  batch 200 loss: 0.5446876680850983\n",
      "  batch 250 loss: 0.5751201170682907\n",
      "  batch 300 loss: 0.5547552567720413\n",
      "  batch 350 loss: 0.5487572431564331\n",
      "  batch 400 loss: 0.5891264885663986\n",
      "  batch 450 loss: 0.5763765370845795\n",
      "  batch 500 loss: 0.5888717156648636\n",
      "  batch 550 loss: 0.5305725741386413\n",
      "  batch 600 loss: 0.5609339272975922\n",
      "  batch 650 loss: 0.5834819680452347\n",
      "  batch 700 loss: 0.5776916003227234\n",
      "  batch 750 loss: 0.5598401987552643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 800 loss: 0.5669085109233856\n",
      "  batch 850 loss: 0.61966890335083\n",
      "  batch 900 loss: 0.5924657940864563\n",
      "LOSS train 0.59247 valid 0.77436, valid PER 23.83%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.5348767662048339\n",
      "  batch 100 loss: 0.5325357347726822\n",
      "  batch 150 loss: 0.5225240755081176\n",
      "  batch 200 loss: 0.5657066476345062\n",
      "  batch 250 loss: 0.5436020436882972\n",
      "  batch 300 loss: 0.5245435458421707\n",
      "  batch 350 loss: 0.5288926148414612\n",
      "  batch 400 loss: 0.5531266617774964\n",
      "  batch 450 loss: 0.5600366497039795\n",
      "  batch 500 loss: 0.5261759752035141\n",
      "  batch 550 loss: 0.5637225943803787\n",
      "  batch 600 loss: 0.5350692582130432\n",
      "  batch 650 loss: 0.5683085906505585\n",
      "  batch 700 loss: 0.5616145044565201\n",
      "  batch 750 loss: 0.5202095597982407\n",
      "  batch 800 loss: 0.5455037838220597\n",
      "  batch 850 loss: 0.5681731200218201\n",
      "  batch 900 loss: 0.546767663359642\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.54677 valid 0.77905, valid PER 23.36%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.4943955099582672\n",
      "  batch 100 loss: 0.49436034977436066\n",
      "  batch 150 loss: 0.48348936438560486\n",
      "  batch 200 loss: 0.4742891815304756\n",
      "  batch 250 loss: 0.4760698813199997\n",
      "  batch 300 loss: 0.5081981956958771\n",
      "  batch 350 loss: 0.47771427273750305\n",
      "  batch 400 loss: 0.48618940114974973\n",
      "  batch 450 loss: 0.4901260983943939\n",
      "  batch 500 loss: 0.49540471136569975\n",
      "  batch 550 loss: 0.5140026289224625\n",
      "  batch 600 loss: 0.47626673519611357\n",
      "  batch 650 loss: 0.49142969310283663\n",
      "  batch 700 loss: 0.5173092985153198\n",
      "  batch 750 loss: 0.48856025576591494\n",
      "  batch 800 loss: 0.4574881559610367\n",
      "  batch 850 loss: 0.49402993738651274\n",
      "  batch 900 loss: 0.4957447773218155\n",
      "LOSS train 0.49574 valid 0.75664, valid PER 23.10%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.46193897306919096\n",
      "  batch 100 loss: 0.4662352132797241\n",
      "  batch 150 loss: 0.4628488826751709\n",
      "  batch 200 loss: 0.4801687282323837\n",
      "  batch 250 loss: 0.49522343575954436\n",
      "  batch 300 loss: 0.4618013024330139\n",
      "  batch 350 loss: 0.4700234019756317\n",
      "  batch 400 loss: 0.46878437936306\n",
      "  batch 450 loss: 0.46022359251976014\n",
      "  batch 500 loss: 0.44126139879226683\n",
      "  batch 550 loss: 0.479386100769043\n",
      "  batch 600 loss: 0.48401820003986357\n",
      "  batch 650 loss: 0.4945347785949707\n",
      "  batch 700 loss: 0.48884674429893493\n",
      "  batch 750 loss: 0.4728677904605865\n",
      "  batch 800 loss: 0.46347227215766906\n",
      "  batch 850 loss: 0.4511345952749252\n",
      "  batch 900 loss: 0.47333281457424164\n",
      "Epoch 00015: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.47333 valid 0.76123, valid PER 22.73%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.4461034494638443\n",
      "  batch 100 loss: 0.4366679912805557\n",
      "  batch 150 loss: 0.43284661799669266\n",
      "  batch 200 loss: 0.43052457094192503\n",
      "  batch 250 loss: 0.4586885493993759\n",
      "  batch 300 loss: 0.440804095864296\n",
      "  batch 350 loss: 0.4450966942310333\n",
      "  batch 400 loss: 0.4615403997898102\n",
      "  batch 450 loss: 0.456428484916687\n",
      "  batch 500 loss: 0.41806785702705385\n",
      "  batch 550 loss: 0.43282619297504427\n",
      "  batch 600 loss: 0.4179189765453339\n",
      "  batch 650 loss: 0.44932700872421266\n",
      "  batch 700 loss: 0.42760674595832826\n",
      "  batch 750 loss: 0.43745237797498704\n",
      "  batch 800 loss: 0.44255284607410433\n",
      "  batch 850 loss: 0.42654600977897644\n",
      "  batch 900 loss: 0.44628493905067446\n",
      "Epoch 00016: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.44628 valid 0.76082, valid PER 22.56%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.4251496785879135\n",
      "  batch 100 loss: 0.4278616601228714\n",
      "  batch 150 loss: 0.4085837322473526\n",
      "  batch 200 loss: 0.42096364587545393\n",
      "  batch 250 loss: 0.44099769115447995\n",
      "  batch 300 loss: 0.42785232663154604\n",
      "  batch 350 loss: 0.400898842215538\n",
      "  batch 400 loss: 0.44918804228305814\n",
      "  batch 450 loss: 0.4190045553445816\n",
      "  batch 500 loss: 0.40393122971057893\n",
      "  batch 550 loss: 0.4235407894849777\n",
      "  batch 600 loss: 0.4316210886836052\n",
      "  batch 650 loss: 0.4084984350204468\n",
      "  batch 700 loss: 0.41877211928367614\n",
      "  batch 750 loss: 0.4188513779640198\n",
      "  batch 800 loss: 0.39819164007902147\n",
      "  batch 850 loss: 0.43743823528289794\n",
      "  batch 900 loss: 0.40292675971984865\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.40293 valid 0.75775, valid PER 22.53%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.4222629737854004\n",
      "  batch 100 loss: 0.4207550173997879\n",
      "  batch 150 loss: 0.4345539465546608\n",
      "  batch 200 loss: 0.41663833379745485\n",
      "  batch 250 loss: 0.4189514708518982\n",
      "  batch 300 loss: 0.40576334118843077\n",
      "  batch 350 loss: 0.4040959867835045\n",
      "  batch 400 loss: 0.4000231719017029\n",
      "  batch 450 loss: 0.42065835058689116\n",
      "  batch 500 loss: 0.40667747646570207\n",
      "  batch 550 loss: 0.42523690581321716\n",
      "  batch 600 loss: 0.3870889160037041\n",
      "  batch 650 loss: 0.3995784640312195\n",
      "  batch 700 loss: 0.43540552377700803\n",
      "  batch 750 loss: 0.3949406737089157\n",
      "  batch 800 loss: 0.39901295632123945\n",
      "  batch 850 loss: 0.39941304802894595\n",
      "  batch 900 loss: 0.4272772261500359\n",
      "Epoch 00018: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.42728 valid 0.75728, valid PER 22.45%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.4033101934194565\n",
      "  batch 100 loss: 0.39920406430959704\n",
      "  batch 150 loss: 0.3997117125988007\n",
      "  batch 200 loss: 0.398966009914875\n",
      "  batch 250 loss: 0.40415591657161715\n",
      "  batch 300 loss: 0.41132421761751176\n",
      "  batch 350 loss: 0.3995355784893036\n",
      "  batch 400 loss: 0.4054108816385269\n",
      "  batch 450 loss: 0.4207133853435516\n",
      "  batch 500 loss: 0.41922386199235917\n",
      "  batch 550 loss: 0.4011280885338783\n",
      "  batch 600 loss: 0.3996690231561661\n",
      "  batch 650 loss: 0.43491475880146024\n",
      "  batch 700 loss: 0.39808939963579176\n",
      "  batch 750 loss: 0.3856740736961365\n",
      "  batch 800 loss: 0.4227423518896103\n",
      "  batch 850 loss: 0.4038936278223991\n",
      "  batch 900 loss: 0.40938127398490903\n",
      "Epoch 00019: reducing learning rate of group 0 to 3.9062e-03.\n",
      "LOSS train 0.40938 valid 0.75789, valid PER 22.54%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.4047737038135529\n",
      "  batch 100 loss: 0.398990695476532\n",
      "  batch 150 loss: 0.3975230997800827\n",
      "  batch 200 loss: 0.4140754622220993\n",
      "  batch 250 loss: 0.4038762706518173\n",
      "  batch 300 loss: 0.41343859523534776\n",
      "  batch 350 loss: 0.39208564907312393\n",
      "  batch 400 loss: 0.40457555145025254\n",
      "  batch 450 loss: 0.40370287775993347\n",
      "  batch 500 loss: 0.38212381809949875\n",
      "  batch 550 loss: 0.42881704568862916\n",
      "  batch 600 loss: 0.39278471410274507\n",
      "  batch 650 loss: 0.40277843445539474\n",
      "  batch 700 loss: 0.41447610288858416\n",
      "  batch 750 loss: 0.3787813693284988\n",
      "  batch 800 loss: 0.4146318733692169\n",
      "  batch 850 loss: 0.4134785604476929\n",
      "  batch 900 loss: 0.40782320946455003\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.9531e-03.\n",
      "LOSS train 0.40782 valid 0.75874, valid PER 22.52%\n",
      "Training finished in 5.0 minutes.\n",
      "Model saved to checkpoints/20231206_125409/model_14\n",
      "Currently using dropout rate of 0.2\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.114368801116943\n",
      "  batch 100 loss: 3.407193922996521\n",
      "  batch 150 loss: 3.2893711042404177\n",
      "  batch 200 loss: 3.178226799964905\n",
      "  batch 250 loss: 3.0831728267669676\n",
      "  batch 300 loss: 2.8775419282913206\n",
      "  batch 350 loss: 2.658961591720581\n",
      "  batch 400 loss: 2.544035053253174\n",
      "  batch 450 loss: 2.4254748582839967\n",
      "  batch 500 loss: 2.2944930601119995\n",
      "  batch 550 loss: 2.2199030184745787\n",
      "  batch 600 loss: 2.1458784675598146\n",
      "  batch 650 loss: 2.0469731736183165\n",
      "  batch 700 loss: 2.0366332387924193\n",
      "  batch 750 loss: 1.940328221321106\n",
      "  batch 800 loss: 1.9019018912315369\n",
      "  batch 850 loss: 1.8483005714416505\n",
      "  batch 900 loss: 1.8025538206100464\n",
      "LOSS train 1.80255 valid 1.72889, valid PER 65.57%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7345118021965027\n",
      "  batch 100 loss: 1.6533362007141112\n",
      "  batch 150 loss: 1.6184953570365905\n",
      "  batch 200 loss: 1.6238607954978943\n",
      "  batch 250 loss: 1.6230229187011718\n",
      "  batch 300 loss: 1.5614645791053772\n",
      "  batch 350 loss: 1.4757752656936645\n",
      "  batch 400 loss: 1.4894976472854615\n",
      "  batch 450 loss: 1.4240900874137878\n",
      "  batch 500 loss: 1.447181782722473\n",
      "  batch 550 loss: 1.430717694759369\n",
      "  batch 600 loss: 1.3702651715278626\n",
      "  batch 650 loss: 1.3805566501617432\n",
      "  batch 700 loss: 1.3389361262321473\n",
      "  batch 750 loss: 1.3336324524879455\n",
      "  batch 800 loss: 1.2717549479007721\n",
      "  batch 850 loss: 1.269545601606369\n",
      "  batch 900 loss: 1.2982886219024659\n",
      "LOSS train 1.29829 valid 1.24768, valid PER 38.06%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.2409206700325013\n",
      "  batch 100 loss: 1.2177265000343322\n",
      "  batch 150 loss: 1.2091450095176697\n",
      "  batch 200 loss: 1.188531643152237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 250 loss: 1.166030250787735\n",
      "  batch 300 loss: 1.1658530414104462\n",
      "  batch 350 loss: 1.2210389614105224\n",
      "  batch 400 loss: 1.194639993906021\n",
      "  batch 450 loss: 1.1562398302555084\n",
      "  batch 500 loss: 1.1427105927467347\n",
      "  batch 550 loss: 1.1494333088397979\n",
      "  batch 600 loss: 1.1155912017822265\n",
      "  batch 650 loss: 1.1130953347682953\n",
      "  batch 700 loss: 1.1264884865283966\n",
      "  batch 750 loss: 1.1743736314773559\n",
      "  batch 800 loss: 1.09774751663208\n",
      "  batch 850 loss: 1.1432530403137207\n",
      "  batch 900 loss: 1.0657747721672057\n",
      "LOSS train 1.06577 valid 1.11161, valid PER 33.98%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0477769899368286\n",
      "  batch 100 loss: 1.0583287930488587\n",
      "  batch 150 loss: 1.021544268131256\n",
      "  batch 200 loss: 1.0519409167766571\n",
      "  batch 250 loss: 1.088882246017456\n",
      "  batch 300 loss: 1.0704599249362945\n",
      "  batch 350 loss: 1.002511478662491\n",
      "  batch 400 loss: 1.0293425083160401\n",
      "  batch 450 loss: 1.035027756690979\n",
      "  batch 500 loss: 1.0083077609539033\n",
      "  batch 550 loss: 1.0392204070091247\n",
      "  batch 600 loss: 1.0509793281555175\n",
      "  batch 650 loss: 1.0290033936500549\n",
      "  batch 700 loss: 0.9927749741077423\n",
      "  batch 750 loss: 0.9883153462409973\n",
      "  batch 800 loss: 0.9519853746891022\n",
      "  batch 850 loss: 0.9914767873287201\n",
      "  batch 900 loss: 1.0244442176818849\n",
      "LOSS train 1.02444 valid 1.00695, valid PER 30.98%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9461670887470245\n",
      "  batch 100 loss: 0.9467937970161437\n",
      "  batch 150 loss: 0.9949407601356506\n",
      "  batch 200 loss: 0.9253371965885162\n",
      "  batch 250 loss: 0.9380524623394012\n",
      "  batch 300 loss: 0.9423806190490722\n",
      "  batch 350 loss: 0.9336692321300507\n",
      "  batch 400 loss: 0.9502757740020752\n",
      "  batch 450 loss: 0.9374236166477203\n",
      "  batch 500 loss: 0.9565938210487366\n",
      "  batch 550 loss: 0.903771356344223\n",
      "  batch 600 loss: 0.9765594792366028\n",
      "  batch 650 loss: 0.9231149899959564\n",
      "  batch 700 loss: 0.9807201600074769\n",
      "  batch 750 loss: 0.8992891561985016\n",
      "  batch 800 loss: 0.9330065703392029\n",
      "  batch 850 loss: 0.9193752598762512\n",
      "  batch 900 loss: 0.9266881144046784\n",
      "LOSS train 0.92669 valid 0.94602, valid PER 29.11%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9172891652584076\n",
      "  batch 100 loss: 0.8656475770473481\n",
      "  batch 150 loss: 0.8521742343902587\n",
      "  batch 200 loss: 0.8713816809654236\n",
      "  batch 250 loss: 0.9044598484039307\n",
      "  batch 300 loss: 0.9008083820343018\n",
      "  batch 350 loss: 0.876350131034851\n",
      "  batch 400 loss: 0.8557963395118713\n",
      "  batch 450 loss: 0.8883227229118347\n",
      "  batch 500 loss: 0.8752629029750824\n",
      "  batch 550 loss: 0.8936884069442749\n",
      "  batch 600 loss: 0.8732540714740753\n",
      "  batch 650 loss: 0.8592585849761963\n",
      "  batch 700 loss: 0.8812093997001648\n",
      "  batch 750 loss: 0.8606840097904205\n",
      "  batch 800 loss: 0.8650479626655578\n",
      "  batch 850 loss: 0.8506764876842499\n",
      "  batch 900 loss: 0.8593790280818939\n",
      "LOSS train 0.85938 valid 0.92269, valid PER 28.47%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8562346804141998\n",
      "  batch 100 loss: 0.8450939500331879\n",
      "  batch 150 loss: 0.8094314658641815\n",
      "  batch 200 loss: 0.8090589994192123\n",
      "  batch 250 loss: 0.8364319407939911\n",
      "  batch 300 loss: 0.808028655052185\n",
      "  batch 350 loss: 0.8247856247425079\n",
      "  batch 400 loss: 0.8264305233955384\n",
      "  batch 450 loss: 0.8167216229438782\n",
      "  batch 500 loss: 0.8268887996673584\n",
      "  batch 550 loss: 0.8050901055335998\n",
      "  batch 600 loss: 0.8142792809009552\n",
      "  batch 650 loss: 0.8131941401958466\n",
      "  batch 700 loss: 0.8487314140796661\n",
      "  batch 750 loss: 0.8221557104587555\n",
      "  batch 800 loss: 0.8128819978237152\n",
      "  batch 850 loss: 0.8107068991661072\n",
      "  batch 900 loss: 0.8610640740394593\n",
      "LOSS train 0.86106 valid 0.89304, valid PER 27.82%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.7945541024208069\n",
      "  batch 100 loss: 0.7764836525917054\n",
      "  batch 150 loss: 0.7680091273784637\n",
      "  batch 200 loss: 0.7620199382305145\n",
      "  batch 250 loss: 0.7815839862823486\n",
      "  batch 300 loss: 0.7209700608253479\n",
      "  batch 350 loss: 0.8046210575103759\n",
      "  batch 400 loss: 0.7663989132642746\n",
      "  batch 450 loss: 0.7821377837657928\n",
      "  batch 500 loss: 0.8014906454086304\n",
      "  batch 550 loss: 0.7502145421504974\n",
      "  batch 600 loss: 0.7928178906440735\n",
      "  batch 650 loss: 0.8155851316452026\n",
      "  batch 700 loss: 0.7688401222229004\n",
      "  batch 750 loss: 0.7785241675376892\n",
      "  batch 800 loss: 0.7905169850587845\n",
      "  batch 850 loss: 0.7829499614238739\n",
      "  batch 900 loss: 0.7874045372009277\n",
      "LOSS train 0.78740 valid 0.86286, valid PER 26.84%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7137658870220185\n",
      "  batch 100 loss: 0.7467125195264817\n",
      "  batch 150 loss: 0.726889396905899\n",
      "  batch 200 loss: 0.7176389592885971\n",
      "  batch 250 loss: 0.746678341627121\n",
      "  batch 300 loss: 0.7439016199111939\n",
      "  batch 350 loss: 0.7721936088800431\n",
      "  batch 400 loss: 0.7314091455936432\n",
      "  batch 450 loss: 0.7434258162975311\n",
      "  batch 500 loss: 0.7189946395158767\n",
      "  batch 550 loss: 0.7586741417646408\n",
      "  batch 600 loss: 0.7576450252532959\n",
      "  batch 650 loss: 0.7429010558128357\n",
      "  batch 700 loss: 0.7298534399271012\n",
      "  batch 750 loss: 0.7474879825115204\n",
      "  batch 800 loss: 0.7496719253063202\n",
      "  batch 850 loss: 0.7903049218654633\n",
      "  batch 900 loss: 0.7321838176250458\n",
      "LOSS train 0.73218 valid 0.82879, valid PER 25.54%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6641856145858764\n",
      "  batch 100 loss: 0.6899030816555023\n",
      "  batch 150 loss: 0.7097793740034103\n",
      "  batch 200 loss: 0.7235548180341721\n",
      "  batch 250 loss: 0.7227900576591492\n",
      "  batch 300 loss: 0.6819432318210602\n",
      "  batch 350 loss: 0.7188187557458877\n",
      "  batch 400 loss: 0.678060040473938\n",
      "  batch 450 loss: 0.698740165233612\n",
      "  batch 500 loss: 0.7263741731643677\n",
      "  batch 550 loss: 0.7291408759355545\n",
      "  batch 600 loss: 0.7312786227464676\n",
      "  batch 650 loss: 0.6936020809412002\n",
      "  batch 700 loss: 0.7169821202754975\n",
      "  batch 750 loss: 0.6989385044574737\n",
      "  batch 800 loss: 0.7346290630102158\n",
      "  batch 850 loss: 0.7042984718084335\n",
      "  batch 900 loss: 0.733584177494049\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.73358 valid 0.82953, valid PER 26.10%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6338317620754242\n",
      "  batch 100 loss: 0.5959248352050781\n",
      "  batch 150 loss: 0.5915492397546768\n",
      "  batch 200 loss: 0.6318284249305726\n",
      "  batch 250 loss: 0.6335012465715408\n",
      "  batch 300 loss: 0.6000310856103898\n",
      "  batch 350 loss: 0.6057591181993485\n",
      "  batch 400 loss: 0.6154697167873383\n",
      "  batch 450 loss: 0.6210154700279236\n",
      "  batch 500 loss: 0.5977128583192826\n",
      "  batch 550 loss: 0.5939928781986237\n",
      "  batch 600 loss: 0.5774751019477844\n",
      "  batch 650 loss: 0.6523178517818451\n",
      "  batch 700 loss: 0.5977909028530121\n",
      "  batch 750 loss: 0.6055221182107925\n",
      "  batch 800 loss: 0.6275438064336777\n",
      "  batch 850 loss: 0.6336795794963836\n",
      "  batch 900 loss: 0.6462785965204239\n",
      "LOSS train 0.64628 valid 0.77241, valid PER 23.68%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.5925447601079941\n",
      "  batch 100 loss: 0.5753594332933426\n",
      "  batch 150 loss: 0.5421190178394317\n",
      "  batch 200 loss: 0.5795331513881683\n",
      "  batch 250 loss: 0.5834740203619003\n",
      "  batch 300 loss: 0.5731753581762313\n",
      "  batch 350 loss: 0.5556280100345612\n",
      "  batch 400 loss: 0.6022047412395477\n",
      "  batch 450 loss: 0.5965476316213608\n",
      "  batch 500 loss: 0.6076886117458343\n",
      "  batch 550 loss: 0.5524693179130554\n",
      "  batch 600 loss: 0.5722468882799149\n",
      "  batch 650 loss: 0.6031092321872711\n",
      "  batch 700 loss: 0.6069107669591903\n",
      "  batch 750 loss: 0.581187943816185\n",
      "  batch 800 loss: 0.5755150556564331\n",
      "  batch 850 loss: 0.6215253949165345\n",
      "  batch 900 loss: 0.6029599535465241\n",
      "LOSS train 0.60296 valid 0.76574, valid PER 23.41%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.544807807803154\n",
      "  batch 100 loss: 0.5529653358459473\n",
      "  batch 150 loss: 0.547689465880394\n",
      "  batch 200 loss: 0.5783997839689254\n",
      "  batch 250 loss: 0.5568294733762741\n",
      "  batch 300 loss: 0.5445396894216538\n",
      "  batch 350 loss: 0.5425245380401611\n",
      "  batch 400 loss: 0.5760368186235428\n",
      "  batch 450 loss: 0.570423618555069\n",
      "  batch 500 loss: 0.5373587471246719\n",
      "  batch 550 loss: 0.5829558300971985\n",
      "  batch 600 loss: 0.5427399289608001\n",
      "  batch 650 loss: 0.5965297627449035\n",
      "  batch 700 loss: 0.5834423083066941\n",
      "  batch 750 loss: 0.5384735345840455\n",
      "  batch 800 loss: 0.5559131664037704\n",
      "  batch 850 loss: 0.5765751028060913\n",
      "  batch 900 loss: 0.5804359430074691\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.58044 valid 0.77014, valid PER 23.63%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.5069373780488968\n",
      "  batch 100 loss: 0.5228716957569123\n",
      "  batch 150 loss: 0.5009474456310272\n",
      "  batch 200 loss: 0.4898408192396164\n",
      "  batch 250 loss: 0.504370146393776\n",
      "  batch 300 loss: 0.5302812254428864\n",
      "  batch 350 loss: 0.49474413335323336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 400 loss: 0.5053264877200127\n",
      "  batch 450 loss: 0.509397833943367\n",
      "  batch 500 loss: 0.5055256658792495\n",
      "  batch 550 loss: 0.5307370346784591\n",
      "  batch 600 loss: 0.50257248878479\n",
      "  batch 650 loss: 0.5139719933271408\n",
      "  batch 700 loss: 0.5273624897003174\n",
      "  batch 750 loss: 0.5130464774370194\n",
      "  batch 800 loss: 0.4831137791275978\n",
      "  batch 850 loss: 0.5177164828777313\n",
      "  batch 900 loss: 0.5143761825561524\n",
      "LOSS train 0.51438 valid 0.75496, valid PER 22.76%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.4825908374786377\n",
      "  batch 100 loss: 0.48954946756362916\n",
      "  batch 150 loss: 0.48404073297977446\n",
      "  batch 200 loss: 0.5056552296876907\n",
      "  batch 250 loss: 0.5206176829338074\n",
      "  batch 300 loss: 0.47752333104610445\n",
      "  batch 350 loss: 0.4776881778240204\n",
      "  batch 400 loss: 0.488422127366066\n",
      "  batch 450 loss: 0.48879369914531706\n",
      "  batch 500 loss: 0.46431910157203676\n",
      "  batch 550 loss: 0.48940527498722075\n",
      "  batch 600 loss: 0.4947173303365707\n",
      "  batch 650 loss: 0.5217531430721283\n",
      "  batch 700 loss: 0.5231659853458405\n",
      "  batch 750 loss: 0.5073200130462646\n",
      "  batch 800 loss: 0.4856967359781265\n",
      "  batch 850 loss: 0.48201070487499237\n",
      "  batch 900 loss: 0.5002541330456733\n",
      "Epoch 00015: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.50025 valid 0.75978, valid PER 22.85%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.49002380430698395\n",
      "  batch 100 loss: 0.4527725848555565\n",
      "  batch 150 loss: 0.45516634583473203\n",
      "  batch 200 loss: 0.4609488332271576\n",
      "  batch 250 loss: 0.48373270928859713\n",
      "  batch 300 loss: 0.46157943546772\n",
      "  batch 350 loss: 0.46483905911445617\n",
      "  batch 400 loss: 0.46951051712036135\n",
      "  batch 450 loss: 0.4854583179950714\n",
      "  batch 500 loss: 0.43719167351722715\n",
      "  batch 550 loss: 0.46081652224063874\n",
      "  batch 600 loss: 0.45511858522891996\n",
      "  batch 650 loss: 0.47338557600975034\n",
      "  batch 700 loss: 0.4394929546117783\n",
      "  batch 750 loss: 0.4661249652504921\n",
      "  batch 800 loss: 0.4696999138593674\n",
      "  batch 850 loss: 0.46502707034349444\n",
      "  batch 900 loss: 0.4749154740571976\n",
      "LOSS train 0.47492 valid 0.74925, valid PER 22.47%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.45296812027692795\n",
      "  batch 100 loss: 0.45982041299343107\n",
      "  batch 150 loss: 0.4402568492293358\n",
      "  batch 200 loss: 0.45550991386175155\n",
      "  batch 250 loss: 0.46447237372398376\n",
      "  batch 300 loss: 0.4615663856267929\n",
      "  batch 350 loss: 0.43126172602176666\n",
      "  batch 400 loss: 0.4725369441509247\n",
      "  batch 450 loss: 0.45914095163345336\n",
      "  batch 500 loss: 0.4415798857808113\n",
      "  batch 550 loss: 0.4473294985294342\n",
      "  batch 600 loss: 0.4783699268102646\n",
      "  batch 650 loss: 0.4552535331249237\n",
      "  batch 700 loss: 0.4497141474485397\n",
      "  batch 750 loss: 0.4483204108476639\n",
      "  batch 800 loss: 0.4337041774392128\n",
      "  batch 850 loss: 0.4581795686483383\n",
      "  batch 900 loss: 0.44210037499666216\n",
      "Epoch 00017: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.44210 valid 0.75111, valid PER 22.28%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.45754719495773316\n",
      "  batch 100 loss: 0.4501902312040329\n",
      "  batch 150 loss: 0.4612642574310303\n",
      "  batch 200 loss: 0.44025235712528227\n",
      "  batch 250 loss: 0.4481917262077332\n",
      "  batch 300 loss: 0.4348545563220978\n",
      "  batch 350 loss: 0.426741161942482\n",
      "  batch 400 loss: 0.42511544227600095\n",
      "  batch 450 loss: 0.4519286638498306\n",
      "  batch 500 loss: 0.4398051917552948\n",
      "  batch 550 loss: 0.4585873353481293\n",
      "  batch 600 loss: 0.4133849656581879\n",
      "  batch 650 loss: 0.4238623967766762\n",
      "  batch 700 loss: 0.4563272851705551\n",
      "  batch 750 loss: 0.43284702986478807\n",
      "  batch 800 loss: 0.4101307028532028\n",
      "  batch 850 loss: 0.42157325267791745\n",
      "  batch 900 loss: 0.44992287158966066\n",
      "LOSS train 0.44992 valid 0.74669, valid PER 22.12%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.43446065604686734\n",
      "  batch 100 loss: 0.42455001413822174\n",
      "  batch 150 loss: 0.43179368853569033\n",
      "  batch 200 loss: 0.42266339182853696\n",
      "  batch 250 loss: 0.43934715747833253\n",
      "  batch 300 loss: 0.42849980741739274\n",
      "  batch 350 loss: 0.41897839069366455\n",
      "  batch 400 loss: 0.4251681035757065\n",
      "  batch 450 loss: 0.4494267463684082\n",
      "  batch 500 loss: 0.44838113725185397\n",
      "  batch 550 loss: 0.4169086289405823\n",
      "  batch 600 loss: 0.4296241918206215\n",
      "  batch 650 loss: 0.47414334654808044\n",
      "  batch 700 loss: 0.42657932549715044\n",
      "  batch 750 loss: 0.41254863411188125\n",
      "  batch 800 loss: 0.45059489250183105\n",
      "  batch 850 loss: 0.4262735557556152\n",
      "  batch 900 loss: 0.4349295628070831\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.43493 valid 0.75170, valid PER 22.22%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.42118718147277834\n",
      "  batch 100 loss: 0.423354386985302\n",
      "  batch 150 loss: 0.4143270707130432\n",
      "  batch 200 loss: 0.43793259024620057\n",
      "  batch 250 loss: 0.4169506025314331\n",
      "  batch 300 loss: 0.4380983647704124\n",
      "  batch 350 loss: 0.4052703431248665\n",
      "  batch 400 loss: 0.41480076760053636\n",
      "  batch 450 loss: 0.4252525621652603\n",
      "  batch 500 loss: 0.40826860547065735\n",
      "  batch 550 loss: 0.45681659281253817\n",
      "  batch 600 loss: 0.41182767212390897\n",
      "  batch 650 loss: 0.4309538319706917\n",
      "  batch 700 loss: 0.4250301766395569\n",
      "  batch 750 loss: 0.4070367899537086\n",
      "  batch 800 loss: 0.44324698984622957\n",
      "  batch 850 loss: 0.4319220754504204\n",
      "  batch 900 loss: 0.43979754865169524\n",
      "Epoch 00020: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.43980 valid 0.74946, valid PER 22.10%\n",
      "Training finished in 5.0 minutes.\n",
      "Model saved to checkpoints/20231206_125912/model_18\n",
      "Currently using dropout rate of 0.3\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.115360226631164\n",
      "  batch 100 loss: 3.407008957862854\n",
      "  batch 150 loss: 3.2899050331115722\n",
      "  batch 200 loss: 3.1788689613342287\n",
      "  batch 250 loss: 3.08431884765625\n",
      "  batch 300 loss: 2.879959864616394\n",
      "  batch 350 loss: 2.666756281852722\n",
      "  batch 400 loss: 2.5442931604385377\n",
      "  batch 450 loss: 2.4279656314849856\n",
      "  batch 500 loss: 2.2954207825660706\n",
      "  batch 550 loss: 2.223274817466736\n",
      "  batch 600 loss: 2.1545543336868285\n",
      "  batch 650 loss: 2.0578356432914733\n",
      "  batch 700 loss: 2.040001697540283\n",
      "  batch 750 loss: 1.9484357213974\n",
      "  batch 800 loss: 1.9093601298332215\n",
      "  batch 850 loss: 1.8576310181617737\n",
      "  batch 900 loss: 1.8110995292663574\n",
      "LOSS train 1.81110 valid 1.73572, valid PER 65.86%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.745826654434204\n",
      "  batch 100 loss: 1.662190752029419\n",
      "  batch 150 loss: 1.6310642385482788\n",
      "  batch 200 loss: 1.646371841430664\n",
      "  batch 250 loss: 1.6404328060150146\n",
      "  batch 300 loss: 1.5775847053527832\n",
      "  batch 350 loss: 1.4955704998970032\n",
      "  batch 400 loss: 1.5027093887329102\n",
      "  batch 450 loss: 1.4472656106948854\n",
      "  batch 500 loss: 1.4737919521331788\n",
      "  batch 550 loss: 1.4596569204330445\n",
      "  batch 600 loss: 1.399762554168701\n",
      "  batch 650 loss: 1.4057393813133239\n",
      "  batch 700 loss: 1.3664313554763794\n",
      "  batch 750 loss: 1.3470677947998047\n",
      "  batch 800 loss: 1.2759700155258178\n",
      "  batch 850 loss: 1.2979776906967162\n",
      "  batch 900 loss: 1.3025560331344606\n",
      "LOSS train 1.30256 valid 1.24614, valid PER 38.12%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.25521244764328\n",
      "  batch 100 loss: 1.2290155470371247\n",
      "  batch 150 loss: 1.2284182572364808\n",
      "  batch 200 loss: 1.1988202786445619\n",
      "  batch 250 loss: 1.1868038547039033\n",
      "  batch 300 loss: 1.1843341398239136\n",
      "  batch 350 loss: 1.2412493705749512\n",
      "  batch 400 loss: 1.2050835657119752\n",
      "  batch 450 loss: 1.1727615320682525\n",
      "  batch 500 loss: 1.1608565604686738\n",
      "  batch 550 loss: 1.1697641289234162\n",
      "  batch 600 loss: 1.1449890840053558\n",
      "  batch 650 loss: 1.1056495165824891\n",
      "  batch 700 loss: 1.130273540019989\n",
      "  batch 750 loss: 1.1889177680015564\n",
      "  batch 800 loss: 1.1166185116767884\n",
      "  batch 850 loss: 1.1615122377872467\n",
      "  batch 900 loss: 1.0772036004066468\n",
      "LOSS train 1.07720 valid 1.11837, valid PER 34.06%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0603973698616027\n",
      "  batch 100 loss: 1.0877058327198028\n",
      "  batch 150 loss: 1.03241339802742\n",
      "  batch 200 loss: 1.0857140612602234\n",
      "  batch 250 loss: 1.079965853691101\n",
      "  batch 300 loss: 1.0953709602355957\n",
      "  batch 350 loss: 1.0188121056556703\n",
      "  batch 400 loss: 1.0403981041908263\n",
      "  batch 450 loss: 1.0501231801509858\n",
      "  batch 500 loss: 1.0419816505908965\n",
      "  batch 550 loss: 1.0656048202514647\n",
      "  batch 600 loss: 1.0670933377742768\n",
      "  batch 650 loss: 1.0409070992469787\n",
      "  batch 700 loss: 1.0153813672065735\n",
      "  batch 750 loss: 1.0144222247600556\n",
      "  batch 800 loss: 0.9852258706092835\n",
      "  batch 850 loss: 0.9989583826065064\n",
      "  batch 900 loss: 1.0423744857311248\n",
      "LOSS train 1.04237 valid 1.02536, valid PER 31.86%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9794991481304168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 0.9725405848026276\n",
      "  batch 150 loss: 1.0042237663269042\n",
      "  batch 200 loss: 0.9368509650230408\n",
      "  batch 250 loss: 0.9564841270446778\n",
      "  batch 300 loss: 0.9600777947902679\n",
      "  batch 350 loss: 0.9627470517158508\n",
      "  batch 400 loss: 0.972723833322525\n",
      "  batch 450 loss: 0.9594013309478759\n",
      "  batch 500 loss: 0.9720083141326904\n",
      "  batch 550 loss: 0.9187964522838592\n",
      "  batch 600 loss: 0.9919346499443055\n",
      "  batch 650 loss: 0.9509259164333344\n",
      "  batch 700 loss: 0.999151428937912\n",
      "  batch 750 loss: 0.9200812220573426\n",
      "  batch 800 loss: 0.949820761680603\n",
      "  batch 850 loss: 0.948107818365097\n",
      "  batch 900 loss: 0.9615027630329132\n",
      "LOSS train 0.96150 valid 1.01172, valid PER 30.96%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9429016828536987\n",
      "  batch 100 loss: 0.8799566686153412\n",
      "  batch 150 loss: 0.8712810063362122\n",
      "  batch 200 loss: 0.8873822379112244\n",
      "  batch 250 loss: 0.9287836956977844\n",
      "  batch 300 loss: 0.9295876729488373\n",
      "  batch 350 loss: 0.912156126499176\n",
      "  batch 400 loss: 0.8822897005081177\n",
      "  batch 450 loss: 0.9232707023620605\n",
      "  batch 500 loss: 0.9036396336555481\n",
      "  batch 550 loss: 0.920639636516571\n",
      "  batch 600 loss: 0.877639331817627\n",
      "  batch 650 loss: 0.8741898572444916\n",
      "  batch 700 loss: 0.8949061524868012\n",
      "  batch 750 loss: 0.8805676865577697\n",
      "  batch 800 loss: 0.8788893222808838\n",
      "  batch 850 loss: 0.8672884917259216\n",
      "  batch 900 loss: 0.885002989768982\n",
      "LOSS train 0.88500 valid 0.92678, valid PER 28.28%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8696066188812256\n",
      "  batch 100 loss: 0.8638519680500031\n",
      "  batch 150 loss: 0.8341161358356476\n",
      "  batch 200 loss: 0.8544301116466522\n",
      "  batch 250 loss: 0.840589439868927\n",
      "  batch 300 loss: 0.8126184964179992\n",
      "  batch 350 loss: 0.8318857455253601\n",
      "  batch 400 loss: 0.8306243813037872\n",
      "  batch 450 loss: 0.842000676393509\n",
      "  batch 500 loss: 0.8478370988368988\n",
      "  batch 550 loss: 0.8192908930778503\n",
      "  batch 600 loss: 0.8315490388870239\n",
      "  batch 650 loss: 0.8465910327434539\n",
      "  batch 700 loss: 0.8655522346496582\n",
      "  batch 750 loss: 0.8468002724647522\n",
      "  batch 800 loss: 0.8311871182918549\n",
      "  batch 850 loss: 0.8412307953834534\n",
      "  batch 900 loss: 0.8678435230255127\n",
      "LOSS train 0.86784 valid 0.89066, valid PER 28.11%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8147583317756653\n",
      "  batch 100 loss: 0.7948723185062408\n",
      "  batch 150 loss: 0.8060037982463837\n",
      "  batch 200 loss: 0.7997789943218231\n",
      "  batch 250 loss: 0.8050332969427109\n",
      "  batch 300 loss: 0.7630612277984619\n",
      "  batch 350 loss: 0.8197957134246826\n",
      "  batch 400 loss: 0.7895931673049926\n",
      "  batch 450 loss: 0.8102250623703003\n",
      "  batch 500 loss: 0.8197474360466004\n",
      "  batch 550 loss: 0.7672709643840789\n",
      "  batch 600 loss: 0.815575670003891\n",
      "  batch 650 loss: 0.8322169888019562\n",
      "  batch 700 loss: 0.7853571462631226\n",
      "  batch 750 loss: 0.7918248665332794\n",
      "  batch 800 loss: 0.8063640534877777\n",
      "  batch 850 loss: 0.7866276454925537\n",
      "  batch 900 loss: 0.8190945911407471\n",
      "LOSS train 0.81909 valid 0.87841, valid PER 26.74%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7406707620620727\n",
      "  batch 100 loss: 0.7690260493755341\n",
      "  batch 150 loss: 0.7658263731002808\n",
      "  batch 200 loss: 0.7385754334926605\n",
      "  batch 250 loss: 0.7577891707420349\n",
      "  batch 300 loss: 0.7745603930950165\n",
      "  batch 350 loss: 0.7899177896976471\n",
      "  batch 400 loss: 0.7695724022388458\n",
      "  batch 450 loss: 0.7603300929069519\n",
      "  batch 500 loss: 0.7742116105556488\n",
      "  batch 550 loss: 0.7724550181627273\n",
      "  batch 600 loss: 0.781577000617981\n",
      "  batch 650 loss: 0.744918292760849\n",
      "  batch 700 loss: 0.7278627002239227\n",
      "  batch 750 loss: 0.7551334989070893\n",
      "  batch 800 loss: 0.775245755314827\n",
      "  batch 850 loss: 0.8010258460044861\n",
      "  batch 900 loss: 0.7469558709859848\n",
      "LOSS train 0.74696 valid 0.84463, valid PER 25.88%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7028263038396836\n",
      "  batch 100 loss: 0.7402188223600388\n",
      "  batch 150 loss: 0.7429829889535904\n",
      "  batch 200 loss: 0.7484958583116531\n",
      "  batch 250 loss: 0.7436041462421418\n",
      "  batch 300 loss: 0.7255108034610749\n",
      "  batch 350 loss: 0.7452749329805374\n",
      "  batch 400 loss: 0.7043016749620438\n",
      "  batch 450 loss: 0.7175203400850296\n",
      "  batch 500 loss: 0.7597934454679489\n",
      "  batch 550 loss: 0.7705980825424195\n",
      "  batch 600 loss: 0.7477133506536484\n",
      "  batch 650 loss: 0.7326553809642792\n",
      "  batch 700 loss: 0.7587231206893921\n",
      "  batch 750 loss: 0.7308976483345032\n",
      "  batch 800 loss: 0.7550853878259659\n",
      "  batch 850 loss: 0.7366202425956726\n",
      "  batch 900 loss: 0.7628691208362579\n",
      "LOSS train 0.76287 valid 0.84048, valid PER 26.09%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6962456655502319\n",
      "  batch 100 loss: 0.6584881746768951\n",
      "  batch 150 loss: 0.6736463755369186\n",
      "  batch 200 loss: 0.7275161457061767\n",
      "  batch 250 loss: 0.7153961849212647\n",
      "  batch 300 loss: 0.6843976092338562\n",
      "  batch 350 loss: 0.6940647548437119\n",
      "  batch 400 loss: 0.7072615230083465\n",
      "  batch 450 loss: 0.723657249212265\n",
      "  batch 500 loss: 0.694446365237236\n",
      "  batch 550 loss: 0.7053387892246247\n",
      "  batch 600 loss: 0.6933905148506164\n",
      "  batch 650 loss: 0.74859923183918\n",
      "  batch 700 loss: 0.681341757774353\n",
      "  batch 750 loss: 0.6799033606052398\n",
      "  batch 800 loss: 0.7232932841777802\n",
      "  batch 850 loss: 0.723794013261795\n",
      "  batch 900 loss: 0.7361361718177796\n",
      "LOSS train 0.73614 valid 0.82605, valid PER 25.17%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.6905175286531449\n",
      "  batch 100 loss: 0.6769564116001129\n",
      "  batch 150 loss: 0.6366825467348098\n",
      "  batch 200 loss: 0.6663769328594208\n",
      "  batch 250 loss: 0.6944078117609024\n",
      "  batch 300 loss: 0.68428575694561\n",
      "  batch 350 loss: 0.6751525688171387\n",
      "  batch 400 loss: 0.6979571926593781\n",
      "  batch 450 loss: 0.692783887386322\n",
      "  batch 500 loss: 0.7068508756160736\n",
      "  batch 550 loss: 0.641447194814682\n",
      "  batch 600 loss: 0.6573195540904999\n",
      "  batch 650 loss: 0.6970395863056182\n",
      "  batch 700 loss: 0.6936789405345917\n",
      "  batch 750 loss: 0.6742160606384278\n",
      "  batch 800 loss: 0.6666189038753509\n",
      "  batch 850 loss: 0.7163091057538986\n",
      "  batch 900 loss: 0.7152936708927154\n",
      "LOSS train 0.71529 valid 0.80617, valid PER 24.76%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.6469660156965256\n",
      "  batch 100 loss: 0.6461527538299561\n",
      "  batch 150 loss: 0.6318571788072586\n",
      "  batch 200 loss: 0.6665566140413284\n",
      "  batch 250 loss: 0.6422476184368133\n",
      "  batch 300 loss: 0.6478257685899734\n",
      "  batch 350 loss: 0.6409838944673538\n",
      "  batch 400 loss: 0.6634362262487411\n",
      "  batch 450 loss: 0.6502608442306519\n",
      "  batch 500 loss: 0.6351497626304626\n",
      "  batch 550 loss: 0.662505214214325\n",
      "  batch 600 loss: 0.6312604039907456\n",
      "  batch 650 loss: 0.6840294080972672\n",
      "  batch 700 loss: 0.6677230978012085\n",
      "  batch 750 loss: 0.6214610940217972\n",
      "  batch 800 loss: 0.6367953342199325\n",
      "  batch 850 loss: 0.6740268802642823\n",
      "  batch 900 loss: 0.6679254150390626\n",
      "Epoch 00013: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.66793 valid 0.83089, valid PER 25.18%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.5831158345937729\n",
      "  batch 100 loss: 0.5890888214111328\n",
      "  batch 150 loss: 0.5688288986682892\n",
      "  batch 200 loss: 0.5609973055124283\n",
      "  batch 250 loss: 0.5534759378433227\n",
      "  batch 300 loss: 0.5953763860464096\n",
      "  batch 350 loss: 0.554461510181427\n",
      "  batch 400 loss: 0.5686047852039338\n",
      "  batch 450 loss: 0.5640897744894028\n",
      "  batch 500 loss: 0.5522418606281281\n",
      "  batch 550 loss: 0.5897566437721252\n",
      "  batch 600 loss: 0.5689637929201126\n",
      "  batch 650 loss: 0.5715258449316025\n",
      "  batch 700 loss: 0.592013041973114\n",
      "  batch 750 loss: 0.5611345779895782\n",
      "  batch 800 loss: 0.5399561673402786\n",
      "  batch 850 loss: 0.5877303862571717\n",
      "  batch 900 loss: 0.5678186118602753\n",
      "LOSS train 0.56782 valid 0.77403, valid PER 23.63%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.5310054641962051\n",
      "  batch 100 loss: 0.533192321062088\n",
      "  batch 150 loss: 0.534618461728096\n",
      "  batch 200 loss: 0.5529143160581589\n",
      "  batch 250 loss: 0.5553447121381759\n",
      "  batch 300 loss: 0.5335671705007553\n",
      "  batch 350 loss: 0.5279704701900482\n",
      "  batch 400 loss: 0.5395622330904007\n",
      "  batch 450 loss: 0.5387683618068695\n",
      "  batch 500 loss: 0.5137522530555725\n",
      "  batch 550 loss: 0.545888449549675\n",
      "  batch 600 loss: 0.5630490666627884\n",
      "  batch 650 loss: 0.570665295124054\n",
      "  batch 700 loss: 0.562822807431221\n",
      "  batch 750 loss: 0.5596780759096146\n",
      "  batch 800 loss: 0.5413518732786179\n",
      "  batch 850 loss: 0.5184870910644531\n",
      "  batch 900 loss: 0.5460554581880569\n",
      "LOSS train 0.54606 valid 0.76681, valid PER 23.12%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.5394725924730301\n",
      "  batch 100 loss: 0.5007900363206863\n",
      "  batch 150 loss: 0.5204438000917435\n",
      "  batch 200 loss: 0.5164258831739426\n",
      "  batch 250 loss: 0.5365899294614792\n",
      "  batch 300 loss: 0.5106275302171707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 350 loss: 0.5244802129268646\n",
      "  batch 400 loss: 0.5424978291988373\n",
      "  batch 450 loss: 0.5436205965280533\n",
      "  batch 500 loss: 0.49082095086574556\n",
      "  batch 550 loss: 0.5207916647195816\n",
      "  batch 600 loss: 0.5121731650829315\n",
      "  batch 650 loss: 0.5276100486516953\n",
      "  batch 700 loss: 0.509233728647232\n",
      "  batch 750 loss: 0.5267081660032272\n",
      "  batch 800 loss: 0.5499650007486343\n",
      "  batch 850 loss: 0.5255328035354614\n",
      "  batch 900 loss: 0.5301142400503158\n",
      "LOSS train 0.53011 valid 0.76290, valid PER 22.80%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.4959378817677498\n",
      "  batch 100 loss: 0.5161001408100128\n",
      "  batch 150 loss: 0.4895546129345894\n",
      "  batch 200 loss: 0.5125252854824066\n",
      "  batch 250 loss: 0.5217769157886505\n",
      "  batch 300 loss: 0.5145756435394288\n",
      "  batch 350 loss: 0.47896467089653016\n",
      "  batch 400 loss: 0.5370862585306168\n",
      "  batch 450 loss: 0.5105769348144531\n",
      "  batch 500 loss: 0.5075761717557907\n",
      "  batch 550 loss: 0.5053611314296722\n",
      "  batch 600 loss: 0.5324865245819091\n",
      "  batch 650 loss: 0.496419762969017\n",
      "  batch 700 loss: 0.4947367012500763\n",
      "  batch 750 loss: 0.5025084602832794\n",
      "  batch 800 loss: 0.48935149163007735\n",
      "  batch 850 loss: 0.524143842458725\n",
      "  batch 900 loss: 0.4994309890270233\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.49943 valid 0.76693, valid PER 22.78%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.46877509117126465\n",
      "  batch 100 loss: 0.46976479530334475\n",
      "  batch 150 loss: 0.4892018961906433\n",
      "  batch 200 loss: 0.46540466547012327\n",
      "  batch 250 loss: 0.47375544846057893\n",
      "  batch 300 loss: 0.4590922731161118\n",
      "  batch 350 loss: 0.4711578166484833\n",
      "  batch 400 loss: 0.4459821683168411\n",
      "  batch 450 loss: 0.47526364147663114\n",
      "  batch 500 loss: 0.4593919658660889\n",
      "  batch 550 loss: 0.46848248720169067\n",
      "  batch 600 loss: 0.43898685157299044\n",
      "  batch 650 loss: 0.4513998293876648\n",
      "  batch 700 loss: 0.47707136273384093\n",
      "  batch 750 loss: 0.46999102711677554\n",
      "  batch 800 loss: 0.4442387819290161\n",
      "  batch 850 loss: 0.44134340465068816\n",
      "  batch 900 loss: 0.47198395669460297\n",
      "LOSS train 0.47198 valid 0.75248, valid PER 22.19%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.4253074619174004\n",
      "  batch 100 loss: 0.43110652804374694\n",
      "  batch 150 loss: 0.44326509594917296\n",
      "  batch 200 loss: 0.4425328034162521\n",
      "  batch 250 loss: 0.4394570922851562\n",
      "  batch 300 loss: 0.4489274501800537\n",
      "  batch 350 loss: 0.4356505101919174\n",
      "  batch 400 loss: 0.43319688498973846\n",
      "  batch 450 loss: 0.46822372674942014\n",
      "  batch 500 loss: 0.4638423103094101\n",
      "  batch 550 loss: 0.43795068383216856\n",
      "  batch 600 loss: 0.44367691963911055\n",
      "  batch 650 loss: 0.4915382373332977\n",
      "  batch 700 loss: 0.44140947967767713\n",
      "  batch 750 loss: 0.44070978462696075\n",
      "  batch 800 loss: 0.467504016160965\n",
      "  batch 850 loss: 0.45063615441322324\n",
      "  batch 900 loss: 0.4659946382045746\n",
      "Epoch 00019: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.46599 valid 0.75684, valid PER 22.45%\n",
      "EPOCH 20:\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout_between_layer\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start dropout tuning For 2 Layer LSTM, with Dropout between layer\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For 2 Layer LSTM\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f098c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0.3, 0.4, 0.5]\n",
    "Optimiser = [\"SGD_Scheduler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b06107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dropout tuning For 2 Layer LSTM, with Dropout between layer\n",
      "Currently using SGD_Scheduler optimiser\n",
      "Currently using dropout rate of 0.3\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.016525239944458\n",
      "  batch 100 loss: 3.4115882444381715\n",
      "  batch 150 loss: 3.3051007080078123\n",
      "  batch 200 loss: 3.1956518268585206\n",
      "  batch 250 loss: 3.104453806877136\n",
      "  batch 300 loss: 2.915277571678162\n",
      "  batch 350 loss: 2.727032699584961\n",
      "  batch 400 loss: 2.5863119077682497\n",
      "  batch 450 loss: 2.4851508140563965\n",
      "  batch 500 loss: 2.350493211746216\n",
      "  batch 550 loss: 2.279656867980957\n",
      "  batch 600 loss: 2.207398736476898\n",
      "  batch 650 loss: 2.10341272354126\n",
      "  batch 700 loss: 2.088429207801819\n",
      "  batch 750 loss: 2.0015905618667604\n",
      "  batch 800 loss: 1.9573982453346253\n",
      "  batch 850 loss: 1.889895441532135\n",
      "  batch 900 loss: 1.8509030890464784\n",
      "LOSS train 1.85090 valid 1.74306, valid PER 66.74%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7869226622581482\n",
      "  batch 100 loss: 1.7218112778663635\n",
      "  batch 150 loss: 1.6346500825881958\n",
      "  batch 200 loss: 1.6282911014556884\n",
      "  batch 250 loss: 1.6077772188186645\n",
      "  batch 300 loss: 1.5617617106437682\n",
      "  batch 350 loss: 1.5430641293525695\n",
      "  batch 400 loss: 1.4994515442848206\n",
      "  batch 450 loss: 1.471446831226349\n",
      "  batch 500 loss: 1.4541553044319153\n",
      "  batch 550 loss: 1.4219049286842347\n",
      "  batch 600 loss: 1.3962690138816833\n",
      "  batch 650 loss: 1.3457835292816163\n",
      "  batch 700 loss: 1.3513507103919984\n",
      "  batch 750 loss: 1.336753032207489\n",
      "  batch 800 loss: 1.295522975921631\n",
      "  batch 850 loss: 1.2940327048301696\n",
      "  batch 900 loss: 1.2665927970409394\n",
      "LOSS train 1.26659 valid 1.28223, valid PER 40.10%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.1983000922203064\n",
      "  batch 100 loss: 1.2685017013549804\n",
      "  batch 150 loss: 1.2800735449790954\n",
      "  batch 200 loss: 1.1993522119522095\n",
      "  batch 250 loss: 1.2127007448673248\n",
      "  batch 300 loss: 1.2279461848735809\n",
      "  batch 350 loss: 1.201427252292633\n",
      "  batch 400 loss: 1.1994447994232178\n",
      "  batch 450 loss: 1.1799819779396057\n",
      "  batch 500 loss: 1.141128749847412\n",
      "  batch 550 loss: 1.1654224944114686\n",
      "  batch 600 loss: 1.0966929042339324\n",
      "  batch 650 loss: 1.1493729448318482\n",
      "  batch 700 loss: 1.1219207561016082\n",
      "  batch 750 loss: 1.129823853969574\n",
      "  batch 800 loss: 1.1511969566345215\n",
      "  batch 850 loss: 1.1158342778682708\n",
      "  batch 900 loss: 1.1160697793960572\n",
      "LOSS train 1.11607 valid 1.09723, valid PER 33.45%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.1260465335845948\n",
      "  batch 100 loss: 1.0487407493591308\n",
      "  batch 150 loss: 1.0827718651294709\n",
      "  batch 200 loss: 1.0541900408267975\n",
      "  batch 250 loss: 1.057763820886612\n",
      "  batch 300 loss: 1.05882519364357\n",
      "  batch 350 loss: 1.0523363077640533\n",
      "  batch 400 loss: 1.0155179119110107\n",
      "  batch 450 loss: 1.016023050546646\n",
      "  batch 500 loss: 1.1024480891227721\n",
      "  batch 550 loss: 1.0107585632801055\n",
      "  batch 600 loss: 1.0258321690559387\n",
      "  batch 650 loss: 1.0671211385726929\n",
      "  batch 700 loss: 1.060276175737381\n",
      "  batch 750 loss: 1.0206025266647338\n",
      "  batch 800 loss: 1.000769976377487\n",
      "  batch 850 loss: 0.9927668654918671\n",
      "  batch 900 loss: 1.0065618288516998\n",
      "LOSS train 1.00656 valid 1.01613, valid PER 31.20%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9682122325897217\n",
      "  batch 100 loss: 0.9694098305702209\n",
      "  batch 150 loss: 0.9781832051277161\n",
      "  batch 200 loss: 1.0017590177059175\n",
      "  batch 250 loss: 0.946650185585022\n",
      "  batch 300 loss: 0.9983628880977631\n",
      "  batch 350 loss: 0.9509512746334076\n",
      "  batch 400 loss: 0.9262753403186799\n",
      "  batch 450 loss: 0.9490020573139191\n",
      "  batch 500 loss: 0.9334753370285034\n",
      "  batch 550 loss: 0.9789785289764404\n",
      "  batch 600 loss: 0.9782056283950805\n",
      "  batch 650 loss: 0.9772285866737366\n",
      "  batch 700 loss: 0.9525380611419678\n",
      "  batch 750 loss: 0.9447546863555908\n",
      "  batch 800 loss: 0.9831283962726594\n",
      "  batch 850 loss: 0.981271493434906\n",
      "  batch 900 loss: 0.9272187221050262\n",
      "LOSS train 0.92722 valid 0.95209, valid PER 29.37%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9057872867584229\n",
      "  batch 100 loss: 0.9123789525032043\n",
      "  batch 150 loss: 0.9185138070583343\n",
      "  batch 200 loss: 0.9018628859519958\n",
      "  batch 250 loss: 0.9043276405334473\n",
      "  batch 300 loss: 0.9143434393405915\n",
      "  batch 350 loss: 0.9322440588474273\n",
      "  batch 400 loss: 0.8932402491569519\n",
      "  batch 450 loss: 0.9018256282806396\n",
      "  batch 500 loss: 0.8625734901428223\n",
      "  batch 550 loss: 0.892691193819046\n",
      "  batch 600 loss: 0.9046560740470886\n",
      "  batch 650 loss: 0.8616233265399933\n",
      "  batch 700 loss: 0.8844193077087402\n",
      "  batch 750 loss: 0.9195386862754822\n",
      "  batch 800 loss: 0.884308123588562\n",
      "  batch 850 loss: 0.9003671753406525\n",
      "  batch 900 loss: 0.9251151716709137\n",
      "LOSS train 0.92512 valid 0.92173, valid PER 28.27%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8283004915714264\n",
      "  batch 100 loss: 0.8872386372089386\n",
      "  batch 150 loss: 0.8238604283332824\n",
      "  batch 200 loss: 0.828183650970459\n",
      "  batch 250 loss: 0.9057545053958893\n",
      "  batch 300 loss: 0.8561415684223175\n",
      "  batch 350 loss: 0.8889920938014985\n",
      "  batch 400 loss: 0.833681583404541\n",
      "  batch 450 loss: 0.8361987322568893\n",
      "  batch 500 loss: 0.8357340204715729\n",
      "  batch 550 loss: 0.8440580534934997\n",
      "  batch 600 loss: 0.8504884493350983\n",
      "  batch 650 loss: 0.8312160634994507\n",
      "  batch 700 loss: 0.8693075048923492\n",
      "  batch 750 loss: 0.8507389140129089\n",
      "  batch 800 loss: 0.8508766376972199\n",
      "  batch 850 loss: 0.8412682414054871\n",
      "  batch 900 loss: 0.8326853382587432\n",
      "LOSS train 0.83269 valid 0.90201, valid PER 28.55%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8369904637336731\n",
      "  batch 100 loss: 0.7774552845954895\n",
      "  batch 150 loss: 0.8230378031730652\n",
      "  batch 200 loss: 0.8210571539402008\n",
      "  batch 250 loss: 0.7977952182292938\n",
      "  batch 300 loss: 0.7857803040742875\n",
      "  batch 350 loss: 0.8099311792850494\n",
      "  batch 400 loss: 0.7755574560165406\n",
      "  batch 450 loss: 0.8367760074138642\n",
      "  batch 500 loss: 0.8108737993240357\n",
      "  batch 550 loss: 0.8060740411281586\n",
      "  batch 600 loss: 0.7867516016960144\n",
      "  batch 650 loss: 0.8110483229160309\n",
      "  batch 700 loss: 0.8329339492321014\n",
      "  batch 750 loss: 0.816354101896286\n",
      "  batch 800 loss: 0.8250666582584381\n",
      "  batch 850 loss: 0.7852873015403747\n",
      "  batch 900 loss: 0.8088016772270202\n",
      "LOSS train 0.80880 valid 0.90099, valid PER 27.40%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7575202298164367\n",
      "  batch 100 loss: 0.7487248289585113\n",
      "  batch 150 loss: 0.77037109375\n",
      "  batch 200 loss: 0.7508879083395005\n",
      "  batch 250 loss: 0.7350104820728302\n",
      "  batch 300 loss: 0.777656900882721\n",
      "  batch 350 loss: 0.7456205809116363\n",
      "  batch 400 loss: 0.7881748592853546\n",
      "  batch 450 loss: 0.7963632547855377\n",
      "  batch 500 loss: 0.7744641435146332\n",
      "  batch 550 loss: 0.7790017116069794\n",
      "  batch 600 loss: 0.7969647467136383\n",
      "  batch 650 loss: 0.8072917306423187\n",
      "  batch 700 loss: 0.755106954574585\n",
      "  batch 750 loss: 0.7891293197870255\n",
      "  batch 800 loss: 0.8052295398712158\n",
      "  batch 850 loss: 0.8086413931846619\n",
      "  batch 900 loss: 0.7642922925949097\n",
      "LOSS train 0.76429 valid 0.83153, valid PER 25.66%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7208926504850388\n",
      "  batch 100 loss: 0.7366988778114318\n",
      "  batch 150 loss: 0.7761525487899781\n",
      "  batch 200 loss: 0.7135903036594391\n",
      "  batch 250 loss: 0.7093547463417054\n",
      "  batch 300 loss: 0.736155413389206\n",
      "  batch 350 loss: 0.7085898762941361\n",
      "  batch 400 loss: 0.7122299754619599\n",
      "  batch 450 loss: 0.7362467277050019\n",
      "  batch 500 loss: 0.7236336892843247\n",
      "  batch 550 loss: 0.7362785649299621\n",
      "  batch 600 loss: 0.7339115166664123\n",
      "  batch 650 loss: 0.7597622418403626\n",
      "  batch 700 loss: 0.7430193638801574\n",
      "  batch 750 loss: 0.7702657371759415\n",
      "  batch 800 loss: 0.7487301427125931\n",
      "  batch 850 loss: 0.7324789708852768\n",
      "  batch 900 loss: 0.7354717570543289\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.73547 valid 0.85706, valid PER 26.44%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6872478795051574\n",
      "  batch 100 loss: 0.6340378534793853\n",
      "  batch 150 loss: 0.6455072438716889\n",
      "  batch 200 loss: 0.6099332690238952\n",
      "  batch 250 loss: 0.631889768242836\n",
      "  batch 300 loss: 0.6151715749502182\n",
      "  batch 350 loss: 0.6675507557392121\n",
      "  batch 400 loss: 0.6323642361164094\n",
      "  batch 450 loss: 0.6239887690544128\n",
      "  batch 500 loss: 0.6334083670377731\n",
      "  batch 550 loss: 0.6625195103883743\n",
      "  batch 600 loss: 0.6414283299446106\n",
      "  batch 650 loss: 0.6622262555360794\n",
      "  batch 700 loss: 0.6982348585128784\n",
      "  batch 750 loss: 0.647642410993576\n",
      "  batch 800 loss: 0.6646775591373444\n",
      "  batch 850 loss: 0.6546201807260513\n",
      "  batch 900 loss: 0.6663752484321595\n",
      "LOSS train 0.66638 valid 0.77259, valid PER 23.84%\n",
      "EPOCH 12:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 50 loss: 0.5838898283243179\n",
      "  batch 100 loss: 0.584637714624405\n",
      "  batch 150 loss: 0.6082290500402451\n",
      "  batch 200 loss: 0.6123249953985215\n",
      "  batch 250 loss: 0.6082725042104721\n",
      "  batch 300 loss: 0.6399419790506363\n",
      "  batch 350 loss: 0.6068748897314071\n",
      "  batch 400 loss: 0.6516255646944046\n",
      "  batch 450 loss: 0.6073781037330628\n",
      "  batch 500 loss: 0.6215384787321091\n",
      "  batch 550 loss: 0.6249072128534316\n",
      "  batch 600 loss: 0.625762984752655\n",
      "  batch 650 loss: 0.6148609906435013\n",
      "  batch 700 loss: 0.6244295924901963\n",
      "  batch 750 loss: 0.6189127373695373\n",
      "  batch 800 loss: 0.5870182865858078\n",
      "  batch 850 loss: 0.6137241071462631\n",
      "  batch 900 loss: 0.6532756304740905\n",
      "LOSS train 0.65328 valid 0.77217, valid PER 23.45%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.5688279384374618\n",
      "  batch 100 loss: 0.5893058794736862\n",
      "  batch 150 loss: 0.6108884930610656\n",
      "  batch 200 loss: 0.5475782239437104\n",
      "  batch 250 loss: 0.5809630876779557\n",
      "  batch 300 loss: 0.6187974029779434\n",
      "  batch 350 loss: 0.5731125921010971\n",
      "  batch 400 loss: 0.5911568158864975\n",
      "  batch 450 loss: 0.5889756524562836\n",
      "  batch 500 loss: 0.5936414325237274\n",
      "  batch 550 loss: 0.6466967540979386\n",
      "  batch 600 loss: 0.6112267774343491\n",
      "  batch 650 loss: 0.5981363987922669\n",
      "  batch 700 loss: 0.6113721084594727\n",
      "  batch 750 loss: 0.5838558340072632\n",
      "  batch 800 loss: 0.5965940839052201\n",
      "  batch 850 loss: 0.5901650542020798\n",
      "  batch 900 loss: 0.6170648550987243\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.61706 valid 0.78052, valid PER 23.26%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.5625115913152695\n",
      "  batch 100 loss: 0.537470423579216\n",
      "  batch 150 loss: 0.5482563632726669\n",
      "  batch 200 loss: 0.5496416288614273\n",
      "  batch 250 loss: 0.5502858090400696\n",
      "  batch 300 loss: 0.5330179607868195\n",
      "  batch 350 loss: 0.5513360166549682\n",
      "  batch 400 loss: 0.5592491298913955\n",
      "  batch 450 loss: 0.5238718837499619\n",
      "  batch 500 loss: 0.5419824498891831\n",
      "  batch 550 loss: 0.5497694331407547\n",
      "  batch 600 loss: 0.5320793431997299\n",
      "  batch 650 loss: 0.5484482342004776\n",
      "  batch 700 loss: 0.5616664361953735\n",
      "  batch 750 loss: 0.5400290215015411\n",
      "  batch 800 loss: 0.5386568576097488\n",
      "  batch 850 loss: 0.5604304909706116\n",
      "  batch 900 loss: 0.5686420887708664\n",
      "LOSS train 0.56864 valid 0.75657, valid PER 22.90%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.5155099612474442\n",
      "  batch 100 loss: 0.5253719133138657\n",
      "  batch 150 loss: 0.5413406276702881\n",
      "  batch 200 loss: 0.5489791363477707\n",
      "  batch 250 loss: 0.5570387083292008\n",
      "  batch 300 loss: 0.5324136549234391\n",
      "  batch 350 loss: 0.5154699051380157\n",
      "  batch 400 loss: 0.5222200125455856\n",
      "  batch 450 loss: 0.5319228541851043\n",
      "  batch 500 loss: 0.5221447336673737\n",
      "  batch 550 loss: 0.5645264554023742\n",
      "  batch 600 loss: 0.5579008460044861\n",
      "  batch 650 loss: 0.5368390548229217\n",
      "  batch 700 loss: 0.5120510464906692\n",
      "  batch 750 loss: 0.5387040162086487\n",
      "  batch 800 loss: 0.5112686866521835\n",
      "  batch 850 loss: 0.516370222568512\n",
      "  batch 900 loss: 0.49943850755691527\n",
      "Epoch 00015: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.49944 valid 0.75952, valid PER 22.80%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.509243665933609\n",
      "  batch 100 loss: 0.49021223664283753\n",
      "  batch 150 loss: 0.510183008313179\n",
      "  batch 200 loss: 0.5206700819730758\n",
      "  batch 250 loss: 0.504344692826271\n",
      "  batch 300 loss: 0.5047247821092605\n",
      "  batch 350 loss: 0.5035025507211686\n",
      "  batch 400 loss: 0.5050944703817367\n",
      "  batch 450 loss: 0.5121284049749374\n",
      "  batch 500 loss: 0.49576776087284086\n",
      "  batch 550 loss: 0.4793565475940704\n",
      "  batch 600 loss: 0.5273010611534119\n",
      "  batch 650 loss: 0.5357688188552856\n",
      "  batch 700 loss: 0.4810921293497086\n",
      "  batch 750 loss: 0.5011895221471786\n",
      "  batch 800 loss: 0.4820870226621628\n",
      "  batch 850 loss: 0.48622443556785583\n",
      "  batch 900 loss: 0.5093351429700852\n",
      "LOSS train 0.50934 valid 0.74584, valid PER 22.43%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.492132853269577\n",
      "  batch 100 loss: 0.4665250563621521\n",
      "  batch 150 loss: 0.5267694807052612\n",
      "  batch 200 loss: 0.46847326517105103\n",
      "  batch 250 loss: 0.4976110577583313\n",
      "  batch 300 loss: 0.47988450169563296\n",
      "  batch 350 loss: 0.5136677515506745\n",
      "  batch 400 loss: 0.4947259524464607\n",
      "  batch 450 loss: 0.4860866189002991\n",
      "  batch 500 loss: 0.5000346833467484\n",
      "  batch 550 loss: 0.4877198761701584\n",
      "  batch 600 loss: 0.5242085039615632\n",
      "  batch 650 loss: 0.48706485390663146\n",
      "  batch 700 loss: 0.497249082326889\n",
      "  batch 750 loss: 0.47477311968803404\n",
      "  batch 800 loss: 0.5137984335422516\n",
      "  batch 850 loss: 0.49144320011138914\n",
      "  batch 900 loss: 0.5062625730037689\n",
      "Epoch 00017: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.50626 valid 0.75083, valid PER 22.41%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.4717321860790253\n",
      "  batch 100 loss: 0.46077765941619875\n",
      "  batch 150 loss: 0.5164194124937057\n",
      "  batch 200 loss: 0.4894312477111816\n",
      "  batch 250 loss: 0.46453105568885805\n",
      "  batch 300 loss: 0.4782502830028534\n",
      "  batch 350 loss: 0.45602123498916625\n",
      "  batch 400 loss: 0.460879065990448\n",
      "  batch 450 loss: 0.4785713839530945\n",
      "  batch 500 loss: 0.4749076634645462\n",
      "  batch 550 loss: 0.5048173862695694\n",
      "  batch 600 loss: 0.4878788071870804\n",
      "  batch 650 loss: 0.46669278025627137\n",
      "  batch 700 loss: 0.472981761097908\n",
      "  batch 750 loss: 0.4863691765069962\n",
      "  batch 800 loss: 0.4972043043375015\n",
      "  batch 850 loss: 0.4856553375720978\n",
      "  batch 900 loss: 0.48195933282375336\n",
      "LOSS train 0.48196 valid 0.74559, valid PER 22.37%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.4774563318490982\n",
      "  batch 100 loss: 0.47417867958545684\n",
      "  batch 150 loss: 0.45030419051647186\n",
      "  batch 200 loss: 0.4503532046079636\n",
      "  batch 250 loss: 0.48690692365169524\n",
      "  batch 300 loss: 0.5023906582593918\n",
      "  batch 350 loss: 0.4878897327184677\n",
      "  batch 400 loss: 0.48082147777080536\n",
      "  batch 450 loss: 0.44714189291000367\n",
      "  batch 500 loss: 0.46303388714790344\n",
      "  batch 550 loss: 0.4895274668931961\n",
      "  batch 600 loss: 0.4989541158080101\n",
      "  batch 650 loss: 0.5050894910097122\n",
      "  batch 700 loss: 0.5041187703609467\n",
      "  batch 750 loss: 0.4675411427021027\n",
      "  batch 800 loss: 0.44925146460533144\n",
      "  batch 850 loss: 0.47483068525791167\n",
      "  batch 900 loss: 0.4701456028223038\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.47015 valid 0.74867, valid PER 22.30%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.4555512404441833\n",
      "  batch 100 loss: 0.46660413682460783\n",
      "  batch 150 loss: 0.46195118606090546\n",
      "  batch 200 loss: 0.47370875149965286\n",
      "  batch 250 loss: 0.4867411255836487\n",
      "  batch 300 loss: 0.4814020681381226\n",
      "  batch 350 loss: 0.44110492199659346\n",
      "  batch 400 loss: 0.4725341844558716\n",
      "  batch 450 loss: 0.4550920343399048\n",
      "  batch 500 loss: 0.48255683958530426\n",
      "  batch 550 loss: 0.475037037730217\n",
      "  batch 600 loss: 0.4652234297990799\n",
      "  batch 650 loss: 0.4867147356271744\n",
      "  batch 700 loss: 0.45039765000343324\n",
      "  batch 750 loss: 0.4313811606168747\n",
      "  batch 800 loss: 0.4858593308925629\n",
      "  batch 850 loss: 0.4753309315443039\n",
      "  batch 900 loss: 0.4646865874528885\n",
      "Epoch 00020: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.46469 valid 0.74629, valid PER 22.30%\n",
      "Training finished in 6.0 minutes.\n",
      "Model saved to checkpoints/20231206_142949/model_18\n",
      "Currently using dropout rate of 0.4\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.065617213249206\n",
      "  batch 100 loss: 3.385919852256775\n",
      "  batch 150 loss: 3.2918597984313966\n",
      "  batch 200 loss: 3.1789414739608763\n",
      "  batch 250 loss: 3.0747012805938723\n",
      "  batch 300 loss: 2.851919093132019\n",
      "  batch 350 loss: 2.6967429733276367\n",
      "  batch 400 loss: 2.5797464990615846\n",
      "  batch 450 loss: 2.4884943675994875\n",
      "  batch 500 loss: 2.368799934387207\n",
      "  batch 550 loss: 2.280837688446045\n",
      "  batch 600 loss: 2.2188774299621583\n",
      "  batch 650 loss: 2.1123153018951415\n",
      "  batch 700 loss: 2.0991832637786865\n",
      "  batch 750 loss: 2.011476695537567\n",
      "  batch 800 loss: 1.9760394501686096\n",
      "  batch 850 loss: 1.9181361842155455\n",
      "  batch 900 loss: 1.8746191453933716\n",
      "LOSS train 1.87462 valid 1.77589, valid PER 67.72%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7902441811561585\n",
      "  batch 100 loss: 1.7142185878753662\n",
      "  batch 150 loss: 1.6718172001838685\n",
      "  batch 200 loss: 1.6697715282440186\n",
      "  batch 250 loss: 1.667678987979889\n",
      "  batch 300 loss: 1.6046036005020141\n",
      "  batch 350 loss: 1.5246160554885864\n",
      "  batch 400 loss: 1.5329854321479797\n",
      "  batch 450 loss: 1.466339020729065\n",
      "  batch 500 loss: 1.482492504119873\n",
      "  batch 550 loss: 1.4859664392471315\n",
      "  batch 600 loss: 1.4192965960502624\n",
      "  batch 650 loss: 1.4375976300239564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 700 loss: 1.37415354013443\n",
      "  batch 750 loss: 1.3729534506797791\n",
      "  batch 800 loss: 1.2833132910728455\n",
      "  batch 850 loss: 1.2938239800930023\n",
      "  batch 900 loss: 1.3231087374687194\n",
      "LOSS train 1.32311 valid 1.25626, valid PER 39.45%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.2676919984817505\n",
      "  batch 100 loss: 1.239076018333435\n",
      "  batch 150 loss: 1.2357685256004334\n",
      "  batch 200 loss: 1.210247926712036\n",
      "  batch 250 loss: 1.2088853085041047\n",
      "  batch 300 loss: 1.201525353193283\n",
      "  batch 350 loss: 1.2349643754959105\n",
      "  batch 400 loss: 1.212140496969223\n",
      "  batch 450 loss: 1.1798308026790618\n",
      "  batch 500 loss: 1.1663497471809388\n",
      "  batch 550 loss: 1.167036510705948\n",
      "  batch 600 loss: 1.1415396630764008\n",
      "  batch 650 loss: 1.1236836791038514\n",
      "  batch 700 loss: 1.1518934857845307\n",
      "  batch 750 loss: 1.1890851330757142\n",
      "  batch 800 loss: 1.1374614071846008\n",
      "  batch 850 loss: 1.162969640493393\n",
      "  batch 900 loss: 1.0896569728851317\n",
      "LOSS train 1.08966 valid 1.11306, valid PER 33.95%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.089821195602417\n",
      "  batch 100 loss: 1.106292040348053\n",
      "  batch 150 loss: 1.0706041109561921\n",
      "  batch 200 loss: 1.0879540514945985\n",
      "  batch 250 loss: 1.0994010055065155\n",
      "  batch 300 loss: 1.1019143187999725\n",
      "  batch 350 loss: 1.0334138512611388\n",
      "  batch 400 loss: 1.0556906867027283\n",
      "  batch 450 loss: 1.056354627609253\n",
      "  batch 500 loss: 1.0457357847690583\n",
      "  batch 550 loss: 1.0666544210910798\n",
      "  batch 600 loss: 1.0918956768512726\n",
      "  batch 650 loss: 1.0558595192432403\n",
      "  batch 700 loss: 1.0356216073036193\n",
      "  batch 750 loss: 1.0211977517604829\n",
      "  batch 800 loss: 1.0082894372940063\n",
      "  batch 850 loss: 1.0170604133605956\n",
      "  batch 900 loss: 1.0737455070018769\n",
      "LOSS train 1.07375 valid 1.01149, valid PER 31.22%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9905267655849457\n",
      "  batch 100 loss: 0.9743564283847809\n",
      "  batch 150 loss: 1.025994859933853\n",
      "  batch 200 loss: 0.9560802173614502\n",
      "  batch 250 loss: 0.9715639066696167\n",
      "  batch 300 loss: 0.9812860763072968\n",
      "  batch 350 loss: 0.9862114465236664\n",
      "  batch 400 loss: 0.9957498168945312\n",
      "  batch 450 loss: 0.9505194568634033\n",
      "  batch 500 loss: 0.9932983946800232\n",
      "  batch 550 loss: 0.9236093842983246\n",
      "  batch 600 loss: 0.99871612906456\n",
      "  batch 650 loss: 0.9683787167072296\n",
      "  batch 700 loss: 1.0046864247322083\n",
      "  batch 750 loss: 0.929525316953659\n",
      "  batch 800 loss: 0.9684216094017029\n",
      "  batch 850 loss: 0.9711417508125305\n",
      "  batch 900 loss: 0.9610049366950989\n",
      "LOSS train 0.96100 valid 0.94674, valid PER 29.31%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.977328360080719\n",
      "  batch 100 loss: 0.9218443512916565\n",
      "  batch 150 loss: 0.9014620733261108\n",
      "  batch 200 loss: 0.9047194480895996\n",
      "  batch 250 loss: 0.9531989681720734\n",
      "  batch 300 loss: 0.9200991070270539\n",
      "  batch 350 loss: 0.919611052274704\n",
      "  batch 400 loss: 0.9098825871944427\n",
      "  batch 450 loss: 0.9350309181213379\n",
      "  batch 500 loss: 0.9027778887748719\n",
      "  batch 550 loss: 0.926039377450943\n",
      "  batch 600 loss: 0.9129018259048461\n",
      "  batch 650 loss: 0.9000036644935608\n",
      "  batch 700 loss: 0.9065746545791626\n",
      "  batch 750 loss: 0.9014121699333191\n",
      "  batch 800 loss: 0.9044879162311554\n",
      "  batch 850 loss: 0.890307868719101\n",
      "  batch 900 loss: 0.895260580778122\n",
      "LOSS train 0.89526 valid 0.90980, valid PER 28.56%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.9006242561340332\n",
      "  batch 100 loss: 0.9176701152324677\n",
      "  batch 150 loss: 0.8715702342987061\n",
      "  batch 200 loss: 0.8678573155403138\n",
      "  batch 250 loss: 0.8683728438615799\n",
      "  batch 300 loss: 0.8485781401395798\n",
      "  batch 350 loss: 0.8631395983695984\n",
      "  batch 400 loss: 0.8632895934581757\n",
      "  batch 450 loss: 0.8569082856178284\n",
      "  batch 500 loss: 0.8656041193008422\n",
      "  batch 550 loss: 0.8520344090461731\n",
      "  batch 600 loss: 0.8778102886676789\n",
      "  batch 650 loss: 0.8542998158931732\n",
      "  batch 700 loss: 0.8839006173610687\n",
      "  batch 750 loss: 0.8465849459171295\n",
      "  batch 800 loss: 0.8492884421348572\n",
      "  batch 850 loss: 0.8454860389232636\n",
      "  batch 900 loss: 0.9049023604393005\n",
      "LOSS train 0.90490 valid 0.88142, valid PER 27.80%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8416757571697235\n",
      "  batch 100 loss: 0.8175658535957336\n",
      "  batch 150 loss: 0.8180225169658661\n",
      "  batch 200 loss: 0.8058491981029511\n",
      "  batch 250 loss: 0.8287673139572144\n",
      "  batch 300 loss: 0.7744055032730103\n",
      "  batch 350 loss: 0.8574136579036713\n",
      "  batch 400 loss: 0.8052069926261902\n",
      "  batch 450 loss: 0.8244895839691162\n",
      "  batch 500 loss: 0.8622836291790008\n",
      "  batch 550 loss: 0.8090585148334504\n",
      "  batch 600 loss: 0.8440547740459442\n",
      "  batch 650 loss: 0.8353268575668334\n",
      "  batch 700 loss: 0.8085731601715088\n",
      "  batch 750 loss: 0.8230720663070679\n",
      "  batch 800 loss: 0.8298475819826127\n",
      "  batch 850 loss: 0.808729100227356\n",
      "  batch 900 loss: 0.8436270666122436\n",
      "LOSS train 0.84363 valid 0.87135, valid PER 26.88%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7541899281740189\n",
      "  batch 100 loss: 0.787952127456665\n",
      "  batch 150 loss: 0.7895508134365081\n",
      "  batch 200 loss: 0.760765974521637\n",
      "  batch 250 loss: 0.7902187466621399\n",
      "  batch 300 loss: 0.804599916934967\n",
      "  batch 350 loss: 0.8206059050559997\n",
      "  batch 400 loss: 0.7956953024864197\n",
      "  batch 450 loss: 0.7964327394962311\n",
      "  batch 500 loss: 0.7667222058773041\n",
      "  batch 550 loss: 0.8034766459465027\n",
      "  batch 600 loss: 0.8267576515674591\n",
      "  batch 650 loss: 0.7772123420238495\n",
      "  batch 700 loss: 0.770229497551918\n",
      "  batch 750 loss: 0.7900240588188171\n",
      "  batch 800 loss: 0.7861372232437134\n",
      "  batch 850 loss: 0.827430237531662\n",
      "  batch 900 loss: 0.7574887084960937\n",
      "LOSS train 0.75749 valid 0.84260, valid PER 25.80%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7324962085485458\n",
      "  batch 100 loss: 0.7510707968473435\n",
      "  batch 150 loss: 0.7453591591119766\n",
      "  batch 200 loss: 0.7687480908632278\n",
      "  batch 250 loss: 0.7604915940761566\n",
      "  batch 300 loss: 0.741178183555603\n",
      "  batch 350 loss: 0.77252814412117\n",
      "  batch 400 loss: 0.7289919167757034\n",
      "  batch 450 loss: 0.7353517031669616\n",
      "  batch 500 loss: 0.7860658431053161\n",
      "  batch 550 loss: 0.7761807036399841\n",
      "  batch 600 loss: 0.7507172381877899\n",
      "  batch 650 loss: 0.7395597940683365\n",
      "  batch 700 loss: 0.7757392168045044\n",
      "  batch 750 loss: 0.7423748385906219\n",
      "  batch 800 loss: 0.7771114510297775\n",
      "  batch 850 loss: 0.7652051281929017\n",
      "  batch 900 loss: 0.7647300338745118\n",
      "LOSS train 0.76473 valid 0.82198, valid PER 25.86%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6924876475334167\n",
      "  batch 100 loss: 0.6833561044931412\n",
      "  batch 150 loss: 0.6971670997142791\n",
      "  batch 200 loss: 0.7532218980789185\n",
      "  batch 250 loss: 0.7409611922502518\n",
      "  batch 300 loss: 0.7045598012208939\n",
      "  batch 350 loss: 0.7161282920837402\n",
      "  batch 400 loss: 0.749587413072586\n",
      "  batch 450 loss: 0.7291565120220185\n",
      "  batch 500 loss: 0.704442400932312\n",
      "  batch 550 loss: 0.7452598506212235\n",
      "  batch 600 loss: 0.7067137628793716\n",
      "  batch 650 loss: 0.7951344311237335\n",
      "  batch 700 loss: 0.7080576747655869\n",
      "  batch 750 loss: 0.7220081275701523\n",
      "  batch 800 loss: 0.755357803106308\n",
      "  batch 850 loss: 0.761203886270523\n",
      "  batch 900 loss: 0.7620821642875671\n",
      "LOSS train 0.76208 valid 0.81952, valid PER 25.30%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.7097192132472991\n",
      "  batch 100 loss: 0.7040670907497406\n",
      "  batch 150 loss: 0.6659569245576858\n",
      "  batch 200 loss: 0.6914178121089936\n",
      "  batch 250 loss: 0.7116831886768341\n",
      "  batch 300 loss: 0.705707426071167\n",
      "  batch 350 loss: 0.6817407268285751\n",
      "  batch 400 loss: 0.7310042786598205\n",
      "  batch 450 loss: 0.6955900728702545\n",
      "  batch 500 loss: 0.720256364941597\n",
      "  batch 550 loss: 0.6598959392309189\n",
      "  batch 600 loss: 0.6752474814653396\n",
      "  batch 650 loss: 0.730681881904602\n",
      "  batch 700 loss: 0.708377006649971\n",
      "  batch 750 loss: 0.7119711613655091\n",
      "  batch 800 loss: 0.7080382972955703\n",
      "  batch 850 loss: 0.7600580286979676\n",
      "  batch 900 loss: 0.7339287906885147\n",
      "LOSS train 0.73393 valid 0.79622, valid PER 24.70%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.677931119799614\n",
      "  batch 100 loss: 0.6852024358510971\n",
      "  batch 150 loss: 0.6587566471099854\n",
      "  batch 200 loss: 0.6987663620710373\n",
      "  batch 250 loss: 0.6823377579450607\n",
      "  batch 300 loss: 0.6708577531576156\n",
      "  batch 350 loss: 0.6668723154067994\n",
      "  batch 400 loss: 0.6976619058847428\n",
      "  batch 450 loss: 0.7067131078243256\n",
      "  batch 500 loss: 0.6681204080581665\n",
      "  batch 550 loss: 0.7142787623405457\n",
      "  batch 600 loss: 0.6612885427474976\n",
      "  batch 650 loss: 0.6963484537601471\n",
      "  batch 700 loss: 0.723058107495308\n",
      "  batch 750 loss: 0.6533679527044296\n",
      "  batch 800 loss: 0.6771228611469269\n",
      "  batch 850 loss: 0.7102285724878311\n",
      "  batch 900 loss: 0.6873612678050995\n",
      "LOSS train 0.68736 valid 0.79375, valid PER 24.55%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.645925412774086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 0.6504522484540939\n",
      "  batch 150 loss: 0.6578271722793579\n",
      "  batch 200 loss: 0.654436354637146\n",
      "  batch 250 loss: 0.649811784029007\n",
      "  batch 300 loss: 0.6844654375314713\n",
      "  batch 350 loss: 0.6342931139469147\n",
      "  batch 400 loss: 0.6446116667985916\n",
      "  batch 450 loss: 0.6525247675180436\n",
      "  batch 500 loss: 0.6699568223953247\n",
      "  batch 550 loss: 0.6745292925834656\n",
      "  batch 600 loss: 0.6360117310285568\n",
      "  batch 650 loss: 0.6651954269409179\n",
      "  batch 700 loss: 0.7039356762170792\n",
      "  batch 750 loss: 0.6555687648057937\n",
      "  batch 800 loss: 0.6175907528400422\n",
      "  batch 850 loss: 0.681321924328804\n",
      "  batch 900 loss: 0.674006478190422\n",
      "Epoch 00014: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.67401 valid 0.80690, valid PER 24.74%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6155156350135803\n",
      "  batch 100 loss: 0.5976758313179016\n",
      "  batch 150 loss: 0.5831924325227738\n",
      "  batch 200 loss: 0.5984531444311142\n",
      "  batch 250 loss: 0.5909278273582459\n",
      "  batch 300 loss: 0.5673621094226837\n",
      "  batch 350 loss: 0.5681958401203155\n",
      "  batch 400 loss: 0.5704287809133529\n",
      "  batch 450 loss: 0.5693494015932083\n",
      "  batch 500 loss: 0.5490459394454956\n",
      "  batch 550 loss: 0.5677036017179489\n",
      "  batch 600 loss: 0.5931031805276871\n",
      "  batch 650 loss: 0.5873712980747223\n",
      "  batch 700 loss: 0.59067032456398\n",
      "  batch 750 loss: 0.5883246225118637\n",
      "  batch 800 loss: 0.5653562462329864\n",
      "  batch 850 loss: 0.5481833815574646\n",
      "  batch 900 loss: 0.5633386188745498\n",
      "LOSS train 0.56334 valid 0.75393, valid PER 23.16%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.5612675827741623\n",
      "  batch 100 loss: 0.5301577121019363\n",
      "  batch 150 loss: 0.5375380092859268\n",
      "  batch 200 loss: 0.5406558674573898\n",
      "  batch 250 loss: 0.5679173177480697\n",
      "  batch 300 loss: 0.5617765790224075\n",
      "  batch 350 loss: 0.5647972178459167\n",
      "  batch 400 loss: 0.562260747551918\n",
      "  batch 450 loss: 0.5766268920898437\n",
      "  batch 500 loss: 0.5359238344430923\n",
      "  batch 550 loss: 0.5644543439149856\n",
      "  batch 600 loss: 0.5422975140810012\n",
      "  batch 650 loss: 0.579780535697937\n",
      "  batch 700 loss: 0.5296076196432113\n",
      "  batch 750 loss: 0.5621386742591858\n",
      "  batch 800 loss: 0.5810232341289521\n",
      "  batch 850 loss: 0.5504822295904159\n",
      "  batch 900 loss: 0.5579215794801712\n",
      "LOSS train 0.55792 valid 0.74934, valid PER 22.80%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.5377867352962494\n",
      "  batch 100 loss: 0.5477375906705856\n",
      "  batch 150 loss: 0.5215992522239685\n",
      "  batch 200 loss: 0.5310538679361343\n",
      "  batch 250 loss: 0.5437569797039032\n",
      "  batch 300 loss: 0.5427574580907821\n",
      "  batch 350 loss: 0.5224891036748887\n",
      "  batch 400 loss: 0.5770790910720826\n",
      "  batch 450 loss: 0.5381198716163635\n",
      "  batch 500 loss: 0.5366497844457626\n",
      "  batch 550 loss: 0.5426276004314423\n",
      "  batch 600 loss: 0.5578709024190903\n",
      "  batch 650 loss: 0.5338836222887039\n",
      "  batch 700 loss: 0.5292639791965484\n",
      "  batch 750 loss: 0.5325289845466614\n",
      "  batch 800 loss: 0.5241146832704544\n",
      "  batch 850 loss: 0.5527506160736084\n",
      "  batch 900 loss: 0.5327793836593628\n",
      "LOSS train 0.53278 valid 0.74586, valid PER 22.72%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.5127180755138397\n",
      "  batch 100 loss: 0.5114844083786011\n",
      "  batch 150 loss: 0.5356415879726409\n",
      "  batch 200 loss: 0.5290283739566803\n",
      "  batch 250 loss: 0.5308096903562546\n",
      "  batch 300 loss: 0.5121446114778518\n",
      "  batch 350 loss: 0.5246547996997833\n",
      "  batch 400 loss: 0.4989231288433075\n",
      "  batch 450 loss: 0.5350217163562775\n",
      "  batch 500 loss: 0.5236587691307067\n",
      "  batch 550 loss: 0.532667407989502\n",
      "  batch 600 loss: 0.4927708727121353\n",
      "  batch 650 loss: 0.509988065958023\n",
      "  batch 700 loss: 0.5618234932422638\n",
      "  batch 750 loss: 0.5101844704151154\n",
      "  batch 800 loss: 0.5056924760341645\n",
      "  batch 850 loss: 0.5156666082143784\n",
      "  batch 900 loss: 0.5645834398269653\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.56458 valid 0.75878, valid PER 22.64%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.4825368484854698\n",
      "  batch 100 loss: 0.48119815707206726\n",
      "  batch 150 loss: 0.47800386846065523\n",
      "  batch 200 loss: 0.4787797969579697\n",
      "  batch 250 loss: 0.48854731500148774\n",
      "  batch 300 loss: 0.4818029761314392\n",
      "  batch 350 loss: 0.47762970328330995\n",
      "  batch 400 loss: 0.47974786043167117\n",
      "  batch 450 loss: 0.49679563164711\n",
      "  batch 500 loss: 0.4999920916557312\n",
      "  batch 550 loss: 0.46497552275657655\n",
      "  batch 600 loss: 0.4693042874336243\n",
      "  batch 650 loss: 0.5286042529344559\n",
      "  batch 700 loss: 0.47398179948329927\n",
      "  batch 750 loss: 0.4633752852678299\n",
      "  batch 800 loss: 0.4948850607872009\n",
      "  batch 850 loss: 0.489948462843895\n",
      "  batch 900 loss: 0.4835968720912933\n",
      "LOSS train 0.48360 valid 0.74345, valid PER 21.86%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.46799975156784057\n",
      "  batch 100 loss: 0.4607028746604919\n",
      "  batch 150 loss: 0.45599925488233567\n",
      "  batch 200 loss: 0.4807226115465164\n",
      "  batch 250 loss: 0.47071045875549317\n",
      "  batch 300 loss: 0.4677203291654587\n",
      "  batch 350 loss: 0.44439499914646147\n",
      "  batch 400 loss: 0.46431219577789307\n",
      "  batch 450 loss: 0.4630791300535202\n",
      "  batch 500 loss: 0.448175807595253\n",
      "  batch 550 loss: 0.4993881058692932\n",
      "  batch 600 loss: 0.45247150778770445\n",
      "  batch 650 loss: 0.47223029494285584\n",
      "  batch 700 loss: 0.46988130033016207\n",
      "  batch 750 loss: 0.4366401353478432\n",
      "  batch 800 loss: 0.491028373837471\n",
      "  batch 850 loss: 0.4777000987529755\n",
      "  batch 900 loss: 0.4893543380498886\n",
      "Epoch 00020: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.48935 valid 0.74967, valid PER 22.50%\n",
      "Training finished in 4.0 minutes.\n",
      "Model saved to checkpoints/20231206_143558/model_19\n",
      "Currently using dropout rate of 0.5\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.121877889633179\n",
      "  batch 100 loss: 3.4085080909729\n",
      "  batch 150 loss: 3.2915595388412475\n",
      "  batch 200 loss: 3.1784590816497804\n",
      "  batch 250 loss: 3.0862627410888672\n",
      "  batch 300 loss: 2.909693384170532\n",
      "  batch 350 loss: 2.697865719795227\n",
      "  batch 400 loss: 2.562895083427429\n",
      "  batch 450 loss: 2.444727339744568\n",
      "  batch 500 loss: 2.3101111531257628\n",
      "  batch 550 loss: 2.2367101740837096\n",
      "  batch 600 loss: 2.174402284622192\n",
      "  batch 650 loss: 2.0736289691925047\n",
      "  batch 700 loss: 2.0579360795021056\n",
      "  batch 750 loss: 1.98039452791214\n",
      "  batch 800 loss: 1.9309785771369934\n",
      "  batch 850 loss: 1.8846251487731933\n",
      "  batch 900 loss: 1.8463519191741944\n",
      "LOSS train 1.84635 valid 1.75124, valid PER 65.96%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.773252785205841\n",
      "  batch 100 loss: 1.6917673802375794\n",
      "  batch 150 loss: 1.6541601967811586\n",
      "  batch 200 loss: 1.657397882938385\n",
      "  batch 250 loss: 1.6639683079719543\n",
      "  batch 300 loss: 1.6076287484169007\n",
      "  batch 350 loss: 1.5100378441810607\n",
      "  batch 400 loss: 1.5377717876434327\n",
      "  batch 450 loss: 1.4774670577049256\n",
      "  batch 500 loss: 1.4887414503097534\n",
      "  batch 550 loss: 1.4856471252441406\n",
      "  batch 600 loss: 1.4189862036705017\n",
      "  batch 650 loss: 1.4345362949371339\n",
      "  batch 700 loss: 1.3865548014640807\n",
      "  batch 750 loss: 1.3871851074695587\n",
      "  batch 800 loss: 1.3120241570472717\n",
      "  batch 850 loss: 1.3296776247024535\n",
      "  batch 900 loss: 1.345131196975708\n",
      "LOSS train 1.34513 valid 1.28097, valid PER 39.17%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.294438625574112\n",
      "  batch 100 loss: 1.2654127502441406\n",
      "  batch 150 loss: 1.2547011184692383\n",
      "  batch 200 loss: 1.2304020261764526\n",
      "  batch 250 loss: 1.2143814301490783\n",
      "  batch 300 loss: 1.2142371106147767\n",
      "  batch 350 loss: 1.2665608704090119\n",
      "  batch 400 loss: 1.2334165263175965\n",
      "  batch 450 loss: 1.2124947655200957\n",
      "  batch 500 loss: 1.188554663658142\n",
      "  batch 550 loss: 1.1963092195987701\n",
      "  batch 600 loss: 1.1591387319564819\n",
      "  batch 650 loss: 1.146559864282608\n",
      "  batch 700 loss: 1.1760009562969207\n",
      "  batch 750 loss: 1.2355096769332885\n",
      "  batch 800 loss: 1.145611481666565\n",
      "  batch 850 loss: 1.175225055217743\n",
      "  batch 900 loss: 1.12099595785141\n",
      "LOSS train 1.12100 valid 1.13533, valid PER 34.06%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.1156514441967011\n",
      "  batch 100 loss: 1.1378449428081512\n",
      "  batch 150 loss: 1.0948279452323915\n",
      "  batch 200 loss: 1.1122734010219575\n",
      "  batch 250 loss: 1.13097101688385\n",
      "  batch 300 loss: 1.125341649055481\n",
      "  batch 350 loss: 1.068755226135254\n",
      "  batch 400 loss: 1.102780601978302\n",
      "  batch 450 loss: 1.0955008482933044\n",
      "  batch 500 loss: 1.068461490869522\n",
      "  batch 550 loss: 1.099689371585846\n",
      "  batch 600 loss: 1.11532298207283\n",
      "  batch 650 loss: 1.0757308435440063\n",
      "  batch 700 loss: 1.0529229891300202\n",
      "  batch 750 loss: 1.0497066032886506\n",
      "  batch 800 loss: 1.0317001688480376\n",
      "  batch 850 loss: 1.0441294693946839\n",
      "  batch 900 loss: 1.076711755990982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 1.07671 valid 1.02711, valid PER 31.45%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.0100383234024048\n",
      "  batch 100 loss: 0.9997524058818817\n",
      "  batch 150 loss: 1.055371605157852\n",
      "  batch 200 loss: 0.974449725151062\n",
      "  batch 250 loss: 0.9849259543418885\n",
      "  batch 300 loss: 0.991023987531662\n",
      "  batch 350 loss: 0.9944078087806701\n",
      "  batch 400 loss: 1.0231863915920258\n",
      "  batch 450 loss: 0.9803883695602417\n",
      "  batch 500 loss: 1.0212749493122102\n",
      "  batch 550 loss: 0.9542740225791931\n",
      "  batch 600 loss: 1.0282235288619994\n",
      "  batch 650 loss: 0.9903670644760132\n",
      "  batch 700 loss: 1.028361624479294\n",
      "  batch 750 loss: 0.9583185136318206\n",
      "  batch 800 loss: 1.001654224395752\n",
      "  batch 850 loss: 0.9917469620704651\n",
      "  batch 900 loss: 0.9774023735523224\n",
      "LOSS train 0.97740 valid 0.96864, valid PER 29.81%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9934137964248657\n",
      "  batch 100 loss: 0.924176127910614\n",
      "  batch 150 loss: 0.9232482171058655\n",
      "  batch 200 loss: 0.9314591348171234\n",
      "  batch 250 loss: 0.9838048124313354\n",
      "  batch 300 loss: 0.9578409242630005\n",
      "  batch 350 loss: 0.9496782088279724\n",
      "  batch 400 loss: 0.9233792805671692\n",
      "  batch 450 loss: 0.9499958336353302\n",
      "  batch 500 loss: 0.9487275159358979\n",
      "  batch 550 loss: 0.9727215242385864\n",
      "  batch 600 loss: 0.9357984733581542\n",
      "  batch 650 loss: 0.9388948607444764\n",
      "  batch 700 loss: 0.9460076522827149\n",
      "  batch 750 loss: 0.9126913905143738\n",
      "  batch 800 loss: 0.9133423399925232\n",
      "  batch 850 loss: 0.9122700369358063\n",
      "  batch 900 loss: 0.932676305770874\n",
      "LOSS train 0.93268 valid 0.95203, valid PER 29.01%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.934469963312149\n",
      "  batch 100 loss: 0.9194084775447845\n",
      "  batch 150 loss: 0.8971087849140167\n",
      "  batch 200 loss: 0.9000572693347931\n",
      "  batch 250 loss: 0.899037584066391\n",
      "  batch 300 loss: 0.8995284521579743\n",
      "  batch 350 loss: 0.9167129349708557\n",
      "  batch 400 loss: 0.8897417426109314\n",
      "  batch 450 loss: 0.8825461792945862\n",
      "  batch 500 loss: 0.8960362124443054\n",
      "  batch 550 loss: 0.914288820028305\n",
      "  batch 600 loss: 0.9056161856651306\n",
      "  batch 650 loss: 0.9093248295783997\n",
      "  batch 700 loss: 0.9257672798633575\n",
      "  batch 750 loss: 0.8974145150184631\n",
      "  batch 800 loss: 0.8846148085594178\n",
      "  batch 850 loss: 0.8899323797225952\n",
      "  batch 900 loss: 0.9350658857822418\n",
      "LOSS train 0.93507 valid 0.91096, valid PER 28.57%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8645542943477631\n",
      "  batch 100 loss: 0.8419420051574708\n",
      "  batch 150 loss: 0.8503912305831909\n",
      "  batch 200 loss: 0.8367218554019928\n",
      "  batch 250 loss: 0.8687204885482788\n",
      "  batch 300 loss: 0.8045643150806427\n",
      "  batch 350 loss: 0.8785392808914184\n",
      "  batch 400 loss: 0.8388642334938049\n",
      "  batch 450 loss: 0.8580139636993408\n",
      "  batch 500 loss: 0.8822397875785828\n",
      "  batch 550 loss: 0.8149316382408142\n",
      "  batch 600 loss: 0.8596976840496063\n",
      "  batch 650 loss: 0.8872999823093415\n",
      "  batch 700 loss: 0.8433221328258514\n",
      "  batch 750 loss: 0.8573968505859375\n",
      "  batch 800 loss: 0.8554895776510238\n",
      "  batch 850 loss: 0.8383628439903259\n",
      "  batch 900 loss: 0.8652298641204834\n",
      "LOSS train 0.86523 valid 0.87802, valid PER 27.45%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7924324405193329\n",
      "  batch 100 loss: 0.8152323180437088\n",
      "  batch 150 loss: 0.8263566613197326\n",
      "  batch 200 loss: 0.8017194604873658\n",
      "  batch 250 loss: 0.8362433218955994\n",
      "  batch 300 loss: 0.8327681744098663\n",
      "  batch 350 loss: 0.8473439222574234\n",
      "  batch 400 loss: 0.8341669380664826\n",
      "  batch 450 loss: 0.8429761552810668\n",
      "  batch 500 loss: 0.7836625373363495\n",
      "  batch 550 loss: 0.8314884531497956\n",
      "  batch 600 loss: 0.835097987651825\n",
      "  batch 650 loss: 0.8065180253982543\n",
      "  batch 700 loss: 0.7901277542114258\n",
      "  batch 750 loss: 0.8098693490028381\n",
      "  batch 800 loss: 0.8370718383789062\n",
      "  batch 850 loss: 0.8639600419998169\n",
      "  batch 900 loss: 0.7848635649681092\n",
      "LOSS train 0.78486 valid 0.85999, valid PER 26.68%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7579200994968415\n",
      "  batch 100 loss: 0.7671304357051849\n",
      "  batch 150 loss: 0.7909117370843888\n",
      "  batch 200 loss: 0.8011478769779206\n",
      "  batch 250 loss: 0.7996873950958252\n",
      "  batch 300 loss: 0.76827401638031\n",
      "  batch 350 loss: 0.8039595711231232\n",
      "  batch 400 loss: 0.7529563558101654\n",
      "  batch 450 loss: 0.7782105553150177\n",
      "  batch 500 loss: 0.8159935116767884\n",
      "  batch 550 loss: 0.8203744149208069\n",
      "  batch 600 loss: 0.7897077178955079\n",
      "  batch 650 loss: 0.7758444273471832\n",
      "  batch 700 loss: 0.803067935705185\n",
      "  batch 750 loss: 0.7767261040210723\n",
      "  batch 800 loss: 0.7956783145666122\n",
      "  batch 850 loss: 0.788585958480835\n",
      "  batch 900 loss: 0.8143817710876465\n",
      "LOSS train 0.81438 valid 0.84849, valid PER 26.46%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.7325957804918289\n",
      "  batch 100 loss: 0.7180731785297394\n",
      "  batch 150 loss: 0.7248746502399445\n",
      "  batch 200 loss: 0.7817065310478211\n",
      "  batch 250 loss: 0.7488181221485138\n",
      "  batch 300 loss: 0.7385923117399216\n",
      "  batch 350 loss: 0.7565775752067566\n",
      "  batch 400 loss: 0.7621350789070129\n",
      "  batch 450 loss: 0.7723745763301849\n",
      "  batch 500 loss: 0.7469264221191406\n",
      "  batch 550 loss: 0.7653544890880585\n",
      "  batch 600 loss: 0.7313749366998672\n",
      "  batch 650 loss: 0.8074731636047363\n",
      "  batch 700 loss: 0.7436075222492218\n",
      "  batch 750 loss: 0.7529929685592651\n",
      "  batch 800 loss: 0.7954685664176941\n",
      "  batch 850 loss: 0.8078886932134628\n",
      "  batch 900 loss: 0.8141262805461884\n",
      "LOSS train 0.81413 valid 0.82938, valid PER 25.22%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.7648245322704316\n",
      "  batch 100 loss: 0.7400794023275375\n",
      "  batch 150 loss: 0.7049130713939666\n",
      "  batch 200 loss: 0.7429902482032776\n",
      "  batch 250 loss: 0.7598645681142807\n",
      "  batch 300 loss: 0.7486163437366485\n",
      "  batch 350 loss: 0.7336134380102157\n",
      "  batch 400 loss: 0.7677432572841645\n",
      "  batch 450 loss: 0.7607441782951355\n",
      "  batch 500 loss: 0.7721002119779586\n",
      "  batch 550 loss: 0.6956895542144775\n",
      "  batch 600 loss: 0.7100394821166992\n",
      "  batch 650 loss: 0.7653653889894485\n",
      "  batch 700 loss: 0.7477111369371414\n",
      "  batch 750 loss: 0.7340448534488678\n",
      "  batch 800 loss: 0.7200546592473984\n",
      "  batch 850 loss: 0.7768557465076447\n",
      "  batch 900 loss: 0.7671192145347595\n",
      "LOSS train 0.76712 valid 0.82120, valid PER 25.87%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7140570652484893\n",
      "  batch 100 loss: 0.7007806468009948\n",
      "  batch 150 loss: 0.7018885004520417\n",
      "  batch 200 loss: 0.7294473350048065\n",
      "  batch 250 loss: 0.7067368787527084\n",
      "  batch 300 loss: 0.6956040316820145\n",
      "  batch 350 loss: 0.7030963349342346\n",
      "  batch 400 loss: 0.7199870628118515\n",
      "  batch 450 loss: 0.7275613516569137\n",
      "  batch 500 loss: 0.6905570429563522\n",
      "  batch 550 loss: 0.7397666484117508\n",
      "  batch 600 loss: 0.696699515581131\n",
      "  batch 650 loss: 0.7380042862892151\n",
      "  batch 700 loss: 0.738807909488678\n",
      "  batch 750 loss: 0.688085852265358\n",
      "  batch 800 loss: 0.7204079413414002\n",
      "  batch 850 loss: 0.7493098366260529\n",
      "  batch 900 loss: 0.7236666202545166\n",
      "Epoch 00013: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.72367 valid 0.82317, valid PER 25.15%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.6463357597589493\n",
      "  batch 100 loss: 0.6601121056079865\n",
      "  batch 150 loss: 0.6378251528739929\n",
      "  batch 200 loss: 0.630623173713684\n",
      "  batch 250 loss: 0.6366735762357711\n",
      "  batch 300 loss: 0.6744453781843185\n",
      "  batch 350 loss: 0.6163515818119049\n",
      "  batch 400 loss: 0.6301640808582306\n",
      "  batch 450 loss: 0.6255042982101441\n",
      "  batch 500 loss: 0.6466704779863357\n",
      "  batch 550 loss: 0.6601067060232162\n",
      "  batch 600 loss: 0.6256560689210892\n",
      "  batch 650 loss: 0.6380259376764298\n",
      "  batch 700 loss: 0.671351460814476\n",
      "  batch 750 loss: 0.6312450784444809\n",
      "  batch 800 loss: 0.6031212246418\n",
      "  batch 850 loss: 0.6507213765382767\n",
      "  batch 900 loss: 0.6404577100276947\n",
      "LOSS train 0.64046 valid 0.76489, valid PER 23.68%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6132229459285736\n",
      "  batch 100 loss: 0.6090627861022949\n",
      "  batch 150 loss: 0.6066444623470306\n",
      "  batch 200 loss: 0.6263443583250046\n",
      "  batch 250 loss: 0.6284924471378326\n",
      "  batch 300 loss: 0.6074732232093811\n",
      "  batch 350 loss: 0.6021543282270432\n",
      "  batch 400 loss: 0.6085719048976899\n",
      "  batch 450 loss: 0.6044809848070145\n",
      "  batch 500 loss: 0.5714297789335251\n",
      "  batch 550 loss: 0.613468866944313\n",
      "  batch 600 loss: 0.6269917541742325\n",
      "  batch 650 loss: 0.6377664464712143\n",
      "  batch 700 loss: 0.6317395031452179\n",
      "  batch 750 loss: 0.6277424728870392\n",
      "  batch 800 loss: 0.6183718037605286\n",
      "  batch 850 loss: 0.5940298253297805\n",
      "  batch 900 loss: 0.6140838241577149\n",
      "LOSS train 0.61408 valid 0.75757, valid PER 23.14%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.6067694246768951\n",
      "  batch 100 loss: 0.56577432513237\n",
      "  batch 150 loss: 0.5882781970500947\n",
      "  batch 200 loss: 0.5921720111370087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 250 loss: 0.6019264513254166\n",
      "  batch 300 loss: 0.599294223189354\n",
      "  batch 350 loss: 0.6041325533390045\n",
      "  batch 400 loss: 0.6034011167287826\n",
      "  batch 450 loss: 0.620015754699707\n",
      "  batch 500 loss: 0.583728432059288\n",
      "  batch 550 loss: 0.5881794983148575\n",
      "  batch 600 loss: 0.596561968922615\n",
      "  batch 650 loss: 0.6142473244667053\n",
      "  batch 700 loss: 0.5762570124864578\n",
      "  batch 750 loss: 0.6027993375062942\n",
      "  batch 800 loss: 0.6140435397624969\n",
      "  batch 850 loss: 0.5988633340597153\n",
      "  batch 900 loss: 0.5957667988538742\n",
      "LOSS train 0.59577 valid 0.75736, valid PER 23.11%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.5888668358325958\n",
      "  batch 100 loss: 0.5785990309715271\n",
      "  batch 150 loss: 0.5605135995149613\n",
      "  batch 200 loss: 0.5799290180206299\n",
      "  batch 250 loss: 0.5875668567419052\n",
      "  batch 300 loss: 0.5965553420782089\n",
      "  batch 350 loss: 0.5604393470287323\n",
      "  batch 400 loss: 0.6068138247728347\n",
      "  batch 450 loss: 0.5955395770072937\n",
      "  batch 500 loss: 0.5643366122245789\n",
      "  batch 550 loss: 0.5682651746273041\n",
      "  batch 600 loss: 0.6142150509357452\n",
      "  batch 650 loss: 0.5805521416664123\n",
      "  batch 700 loss: 0.5833957415819168\n",
      "  batch 750 loss: 0.5667505651712418\n",
      "  batch 800 loss: 0.5613820475339889\n",
      "  batch 850 loss: 0.5962504172325134\n",
      "  batch 900 loss: 0.5713453775644303\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.57135 valid 0.75839, valid PER 22.68%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.5673123455047607\n",
      "  batch 100 loss: 0.5543404000997544\n",
      "  batch 150 loss: 0.573154861330986\n",
      "  batch 200 loss: 0.5388047069311142\n",
      "  batch 250 loss: 0.5531227302551269\n",
      "  batch 300 loss: 0.5305743855237961\n",
      "  batch 350 loss: 0.5402960067987442\n",
      "  batch 400 loss: 0.5280473554134368\n",
      "  batch 450 loss: 0.5611200332641602\n",
      "  batch 500 loss: 0.541194360256195\n",
      "  batch 550 loss: 0.5593608224391937\n",
      "  batch 600 loss: 0.5241925364732742\n",
      "  batch 650 loss: 0.5349847984313965\n",
      "  batch 700 loss: 0.570717648267746\n",
      "  batch 750 loss: 0.5300548714399338\n",
      "  batch 800 loss: 0.5279298627376556\n",
      "  batch 850 loss: 0.5187773752212524\n",
      "  batch 900 loss: 0.5547877341508866\n",
      "LOSS train 0.55479 valid 0.74039, valid PER 22.36%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5041466993093491\n",
      "  batch 100 loss: 0.5141362863779068\n",
      "  batch 150 loss: 0.530799092054367\n",
      "  batch 200 loss: 0.5175938367843628\n",
      "  batch 250 loss: 0.524610013961792\n",
      "  batch 300 loss: 0.525614760518074\n",
      "  batch 350 loss: 0.5152530324459076\n",
      "  batch 400 loss: 0.5223580694198608\n",
      "  batch 450 loss: 0.5429157966375351\n",
      "  batch 500 loss: 0.5351871711015701\n",
      "  batch 550 loss: 0.5203631049394608\n",
      "  batch 600 loss: 0.5191759240627288\n",
      "  batch 650 loss: 0.5730120617151261\n",
      "  batch 700 loss: 0.5163849455118179\n",
      "  batch 750 loss: 0.5171470361948013\n",
      "  batch 800 loss: 0.5502103734016418\n",
      "  batch 850 loss: 0.5288469046354294\n",
      "  batch 900 loss: 0.5393803489208221\n",
      "Epoch 00019: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.53938 valid 0.74466, valid PER 22.01%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.5133867239952088\n",
      "  batch 100 loss: 0.518446723818779\n",
      "  batch 150 loss: 0.5034048211574554\n",
      "  batch 200 loss: 0.5272807049751281\n",
      "  batch 250 loss: 0.5180901330709458\n",
      "  batch 300 loss: 0.5183740431070327\n",
      "  batch 350 loss: 0.49708518087863923\n",
      "  batch 400 loss: 0.5008306795358658\n",
      "  batch 450 loss: 0.5157888346910476\n",
      "  batch 500 loss: 0.48081460773944856\n",
      "  batch 550 loss: 0.5518513363599777\n",
      "  batch 600 loss: 0.49628135800361634\n",
      "  batch 650 loss: 0.5078215253353119\n",
      "  batch 700 loss: 0.5002294337749481\n",
      "  batch 750 loss: 0.4817688375711441\n",
      "  batch 800 loss: 0.5196371561288834\n",
      "  batch 850 loss: 0.5174411118030549\n",
      "  batch 900 loss: 0.5174569845199585\n",
      "LOSS train 0.51746 valid 0.73030, valid PER 21.79%\n",
      "Training finished in 4.0 minutes.\n",
      "Model saved to checkpoints/20231206_144057/model_20\n",
      "Finish SGD_Scheduler optimiser\n",
      "End tuning For 2 Layer LSTM\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout_between_layer\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start dropout tuning For 2 Layer LSTM, with Dropout between layer\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For 2 Layer LSTM\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09273b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0]\n",
    "Optimiser = [\"Adam\", \"SGD_Scheduler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56a7862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dropout tuning For 2 Layer LSTM, with Dropout between layer\n",
      "Currently using Adam optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.777221741676331\n",
      "  batch 100 loss: 3.231734290122986\n",
      "  batch 150 loss: 3.0117234420776366\n",
      "  batch 200 loss: 2.739203691482544\n",
      "  batch 250 loss: 2.49772696018219\n",
      "  batch 300 loss: 2.280819926261902\n",
      "  batch 350 loss: 2.0985004711151123\n",
      "  batch 400 loss: 2.033837769031525\n",
      "  batch 450 loss: 1.9083922052383422\n",
      "  batch 500 loss: 1.7859423303604125\n",
      "  batch 550 loss: 1.704871723651886\n",
      "  batch 600 loss: 1.6372292494773866\n",
      "  batch 650 loss: 1.5587277841567992\n",
      "  batch 700 loss: 1.5565199494361877\n",
      "  batch 750 loss: 1.4802778553962708\n",
      "  batch 800 loss: 1.4722290134429932\n",
      "  batch 850 loss: 1.4219999361038207\n",
      "  batch 900 loss: 1.3768940234184266\n",
      "LOSS train 1.37689 valid 1.35433, valid PER 44.69%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.3078549885749817\n",
      "  batch 100 loss: 1.263436793088913\n",
      "  batch 150 loss: 1.224011641740799\n",
      "  batch 200 loss: 1.2617140471935273\n",
      "  batch 250 loss: 1.2419705891609192\n",
      "  batch 300 loss: 1.2320953917503357\n",
      "  batch 350 loss: 1.1625029277801513\n",
      "  batch 400 loss: 1.1651783418655395\n",
      "  batch 450 loss: 1.1440741300582886\n",
      "  batch 500 loss: 1.165005052089691\n",
      "  batch 550 loss: 1.1479245233535766\n",
      "  batch 600 loss: 1.1332319641113282\n",
      "  batch 650 loss: 1.1272679591178894\n",
      "  batch 700 loss: 1.1271494901180268\n",
      "  batch 750 loss: 1.0977566134929657\n",
      "  batch 800 loss: 1.053707593679428\n",
      "  batch 850 loss: 1.0576764738559723\n",
      "  batch 900 loss: 1.091867583990097\n",
      "LOSS train 1.09187 valid 1.08245, valid PER 34.39%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.0434457325935365\n",
      "  batch 100 loss: 1.0114498269557952\n",
      "  batch 150 loss: 0.9924567532539368\n",
      "  batch 200 loss: 1.0056888830661774\n",
      "  batch 250 loss: 0.9878874778747558\n",
      "  batch 300 loss: 0.9683946573734283\n",
      "  batch 350 loss: 1.0069474112987518\n",
      "  batch 400 loss: 0.9785836708545684\n",
      "  batch 450 loss: 0.9704953205585479\n",
      "  batch 500 loss: 0.9534753274917602\n",
      "  batch 550 loss: 0.9864003789424897\n",
      "  batch 600 loss: 0.9506516516208648\n",
      "  batch 650 loss: 0.9402216506004334\n",
      "  batch 700 loss: 0.9544363188743591\n",
      "  batch 750 loss: 0.9901849687099457\n",
      "  batch 800 loss: 0.9326689970493317\n",
      "  batch 850 loss: 0.977069263458252\n",
      "  batch 900 loss: 0.9031815123558045\n",
      "LOSS train 0.90318 valid 0.99369, valid PER 30.97%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.8621847414970398\n",
      "  batch 100 loss: 0.8913668429851532\n",
      "  batch 150 loss: 0.8641028499603272\n",
      "  batch 200 loss: 0.9016971278190613\n",
      "  batch 250 loss: 0.8915858936309814\n",
      "  batch 300 loss: 0.8981894218921661\n",
      "  batch 350 loss: 0.8359752881526947\n",
      "  batch 400 loss: 0.8714807307720185\n",
      "  batch 450 loss: 0.871096099615097\n",
      "  batch 500 loss: 0.8514951455593109\n",
      "  batch 550 loss: 0.8941103005409241\n",
      "  batch 600 loss: 0.9100886380672455\n",
      "  batch 650 loss: 0.8718332624435425\n",
      "  batch 700 loss: 0.8414829254150391\n",
      "  batch 750 loss: 0.8358796834945679\n",
      "  batch 800 loss: 0.8255405294895172\n",
      "  batch 850 loss: 0.8536584663391114\n",
      "  batch 900 loss: 0.8936224794387817\n",
      "LOSS train 0.89362 valid 0.89189, valid PER 27.88%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.7897502088546753\n",
      "  batch 100 loss: 0.79323615193367\n",
      "  batch 150 loss: 0.84839768409729\n",
      "  batch 200 loss: 0.7642649972438812\n",
      "  batch 250 loss: 0.7843737006187439\n",
      "  batch 300 loss: 0.8126174581050872\n",
      "  batch 350 loss: 0.7898348546028138\n",
      "  batch 400 loss: 0.8032909166812897\n",
      "  batch 450 loss: 0.7733542501926423\n",
      "  batch 500 loss: 0.8139713883399964\n",
      "  batch 550 loss: 0.7650149285793304\n",
      "  batch 600 loss: 0.8155591487884521\n",
      "  batch 650 loss: 0.7929893219470978\n",
      "  batch 700 loss: 0.8330616927146912\n",
      "  batch 750 loss: 0.7608533638715744\n",
      "  batch 800 loss: 0.79477863073349\n",
      "  batch 850 loss: 0.7855132961273193\n",
      "  batch 900 loss: 0.7892819905281067\n",
      "LOSS train 0.78928 valid 0.86634, valid PER 27.88%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.7544903469085693\n",
      "  batch 100 loss: 0.7081111776828766\n",
      "  batch 150 loss: 0.7083333247900009\n",
      "  batch 200 loss: 0.7348441684246063\n",
      "  batch 250 loss: 0.7596670424938202\n",
      "  batch 300 loss: 0.7493149679899216\n",
      "  batch 350 loss: 0.7288424050807953\n",
      "  batch 400 loss: 0.7131784528493881\n",
      "  batch 450 loss: 0.7396502566337585\n",
      "  batch 500 loss: 0.734083543419838\n",
      "  batch 550 loss: 0.7566558676958084\n",
      "  batch 600 loss: 0.7328791451454163\n",
      "  batch 650 loss: 0.7310094702243805\n",
      "  batch 700 loss: 0.7312759852409363\n",
      "  batch 750 loss: 0.7132273089885711\n",
      "  batch 800 loss: 0.743121235370636\n",
      "  batch 850 loss: 0.7278932654857635\n",
      "  batch 900 loss: 0.7475872230529785\n",
      "LOSS train 0.74759 valid 0.84565, valid PER 26.68%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.6948746860027313\n",
      "  batch 100 loss: 0.6969532310962677\n",
      "  batch 150 loss: 0.6500277668237686\n",
      "  batch 200 loss: 0.666021136045456\n",
      "  batch 250 loss: 0.6632238936424255\n",
      "  batch 300 loss: 0.6792888289690018\n",
      "  batch 350 loss: 0.6817391836643218\n",
      "  batch 400 loss: 0.6712483131885528\n",
      "  batch 450 loss: 0.690027694106102\n",
      "  batch 500 loss: 0.6724334716796875\n",
      "  batch 550 loss: 0.6683337152004242\n",
      "  batch 600 loss: 0.6809934192895889\n",
      "  batch 650 loss: 0.6741552084684372\n",
      "  batch 700 loss: 0.7035341221094131\n",
      "  batch 750 loss: 0.6784878498315812\n",
      "  batch 800 loss: 0.6751841819286346\n",
      "  batch 850 loss: 0.7001402854919434\n",
      "  batch 900 loss: 0.7232175886631012\n",
      "LOSS train 0.72322 valid 0.84890, valid PER 26.09%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.6249355691671371\n",
      "  batch 100 loss: 0.6101155883073807\n",
      "  batch 150 loss: 0.6196860128641128\n",
      "  batch 200 loss: 0.6142733961343765\n",
      "  batch 250 loss: 0.625069631934166\n",
      "  batch 300 loss: 0.5959357970952988\n",
      "  batch 350 loss: 0.6700256592035294\n",
      "  batch 400 loss: 0.6162294429540635\n",
      "  batch 450 loss: 0.6405521422624588\n",
      "  batch 500 loss: 0.6643125051259995\n",
      "  batch 550 loss: 0.6149425619840622\n",
      "  batch 600 loss: 0.6608513593673706\n",
      "  batch 650 loss: 0.6732055652141571\n",
      "  batch 700 loss: 0.6283771288394928\n",
      "  batch 750 loss: 0.6258828246593475\n",
      "  batch 800 loss: 0.6400079506635666\n",
      "  batch 850 loss: 0.6343318009376526\n",
      "  batch 900 loss: 0.6632077884674072\n",
      "LOSS train 0.66321 valid 0.81383, valid PER 25.24%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.5511665171384812\n",
      "  batch 100 loss: 0.5770964854955674\n",
      "  batch 150 loss: 0.5851956021785736\n",
      "  batch 200 loss: 0.5724300321936607\n",
      "  batch 250 loss: 0.5849451404809952\n",
      "  batch 300 loss: 0.5926423567533493\n",
      "  batch 350 loss: 0.6114203673601151\n",
      "  batch 400 loss: 0.5901115310192108\n",
      "  batch 450 loss: 0.5957538866996765\n",
      "  batch 500 loss: 0.5836374336481094\n",
      "  batch 550 loss: 0.6164156359434128\n",
      "  batch 600 loss: 0.6209519135951996\n",
      "  batch 650 loss: 0.5936652940511703\n",
      "  batch 700 loss: 0.5711950951814652\n",
      "  batch 750 loss: 0.58170126080513\n",
      "  batch 800 loss: 0.607311749458313\n",
      "  batch 850 loss: 0.6275660794973373\n",
      "  batch 900 loss: 0.5860909217596054\n",
      "LOSS train 0.58609 valid 0.80853, valid PER 24.95%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.5236785352230072\n",
      "  batch 100 loss: 0.516688283085823\n",
      "  batch 150 loss: 0.5337658506631852\n",
      "  batch 200 loss: 0.5616578042507172\n",
      "  batch 250 loss: 0.569534193277359\n",
      "  batch 300 loss: 0.5217322611808777\n",
      "  batch 350 loss: 0.5491068691015244\n",
      "  batch 400 loss: 0.541050769686699\n",
      "  batch 450 loss: 0.5539269310235977\n",
      "  batch 500 loss: 0.577380839586258\n",
      "  batch 550 loss: 0.5967177790403366\n",
      "  batch 600 loss: 0.5557187581062317\n",
      "  batch 650 loss: 0.5406580412387848\n",
      "  batch 700 loss: 0.567681007385254\n",
      "  batch 750 loss: 0.5546998500823974\n",
      "  batch 800 loss: 0.5690769356489181\n",
      "  batch 850 loss: 0.5752184319496155\n",
      "  batch 900 loss: 0.5857227265834808\n",
      "LOSS train 0.58572 valid 0.79975, valid PER 24.42%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.47705757319927217\n",
      "  batch 100 loss: 0.45925424933433534\n",
      "  batch 150 loss: 0.4986112844944\n",
      "  batch 200 loss: 0.5284274929761886\n",
      "  batch 250 loss: 0.5170103546977043\n",
      "  batch 300 loss: 0.5006292313337326\n",
      "  batch 350 loss: 0.5218992000818252\n",
      "  batch 400 loss: 0.5296480238437653\n",
      "  batch 450 loss: 0.5284454864263535\n",
      "  batch 500 loss: 0.5037715798616409\n",
      "  batch 550 loss: 0.5210379052162171\n",
      "  batch 600 loss: 0.5165403550863266\n",
      "  batch 650 loss: 0.5597834748029709\n",
      "  batch 700 loss: 0.5030138373374939\n",
      "  batch 750 loss: 0.5169956821203232\n",
      "  batch 800 loss: 0.5370718693733215\n",
      "  batch 850 loss: 0.5869311374425888\n",
      "  batch 900 loss: 0.5690043956041336\n",
      "LOSS train 0.56900 valid 0.79841, valid PER 23.77%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.4771674764156342\n",
      "  batch 100 loss: 0.46577437162399293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 150 loss: 0.4270486396551132\n",
      "  batch 200 loss: 0.48605110466480256\n",
      "  batch 250 loss: 0.4761529916524887\n",
      "  batch 300 loss: 0.4680067265033722\n",
      "  batch 350 loss: 0.4652142894268036\n",
      "  batch 400 loss: 0.49012204229831696\n",
      "  batch 450 loss: 0.4800450485944748\n",
      "  batch 500 loss: 0.49930355429649353\n",
      "  batch 550 loss: 0.4645304173231125\n",
      "  batch 600 loss: 0.493482386469841\n",
      "  batch 650 loss: 0.5114696103334427\n",
      "  batch 700 loss: 0.5026888877153397\n",
      "  batch 750 loss: 0.4701381701231003\n",
      "  batch 800 loss: 0.49801873207092284\n",
      "  batch 850 loss: 0.5409854078292846\n",
      "  batch 900 loss: 0.5434351062774658\n",
      "LOSS train 0.54344 valid 0.79501, valid PER 23.84%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.4189873450994492\n",
      "  batch 100 loss: 0.43480975687503814\n",
      "  batch 150 loss: 0.43463573724031446\n",
      "  batch 200 loss: 0.4485123524069786\n",
      "  batch 250 loss: 0.4482845941185951\n",
      "  batch 300 loss: 0.4452875167131424\n",
      "  batch 350 loss: 0.4303961306810379\n",
      "  batch 400 loss: 0.45087008535861967\n",
      "  batch 450 loss: 0.4630530846118927\n",
      "  batch 500 loss: 0.4255120500922203\n",
      "  batch 550 loss: 0.474663183093071\n",
      "  batch 600 loss: 0.4539575463533401\n",
      "  batch 650 loss: 0.4718186092376709\n",
      "  batch 700 loss: 0.47024846911430357\n",
      "  batch 750 loss: 0.4316799819469452\n",
      "  batch 800 loss: 0.47126795738935473\n",
      "  batch 850 loss: 0.4914552012085915\n",
      "  batch 900 loss: 0.47699112832546237\n",
      "LOSS train 0.47699 valid 0.82556, valid PER 24.26%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.41491322427988053\n",
      "  batch 100 loss: 0.41475876182317734\n",
      "  batch 150 loss: 0.3964862015843391\n",
      "  batch 200 loss: 0.39501324862241743\n",
      "  batch 250 loss: 0.40804660141468047\n",
      "  batch 300 loss: 0.43250556409358976\n",
      "  batch 350 loss: 0.40092159986495973\n",
      "  batch 400 loss: 0.4108999866247177\n",
      "  batch 450 loss: 0.41709151983261106\n",
      "  batch 500 loss: 0.43669924437999724\n",
      "  batch 550 loss: 0.4381140348315239\n",
      "  batch 600 loss: 0.4086067569255829\n",
      "  batch 650 loss: 0.4424293142557144\n",
      "  batch 700 loss: 0.4523044627904892\n",
      "  batch 750 loss: 0.43431331068277357\n",
      "  batch 800 loss: 0.4154104200005531\n",
      "  batch 850 loss: 0.47250990152359007\n",
      "  batch 900 loss: 0.45168933510780335\n",
      "LOSS train 0.45169 valid 0.82058, valid PER 23.82%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.3598006746172905\n",
      "  batch 100 loss: 0.35642760276794433\n",
      "  batch 150 loss: 0.37203349351882936\n",
      "  batch 200 loss: 0.3604936310648918\n",
      "  batch 250 loss: 0.38449974477291105\n",
      "  batch 300 loss: 0.3856602743268013\n",
      "  batch 350 loss: 0.38910116970539094\n",
      "  batch 400 loss: 0.4125682455301285\n",
      "  batch 450 loss: 0.3875482153892517\n",
      "  batch 500 loss: 0.38846388578414914\n",
      "  batch 550 loss: 0.40666911125183103\n",
      "  batch 600 loss: 0.41482695877552034\n",
      "  batch 650 loss: 0.42659433305263517\n",
      "  batch 700 loss: 0.41187471568584444\n",
      "  batch 750 loss: 0.4259839206933975\n",
      "  batch 800 loss: 0.3985144314169884\n",
      "  batch 850 loss: 0.3973644635081291\n",
      "  batch 900 loss: 0.4102513307332993\n",
      "LOSS train 0.41025 valid 0.84543, valid PER 23.91%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.3466917476058006\n",
      "  batch 100 loss: 0.33008735060691835\n",
      "  batch 150 loss: 0.35023270338773727\n",
      "  batch 200 loss: 0.348543738424778\n",
      "  batch 250 loss: 0.37088080883026125\n",
      "  batch 300 loss: 0.36178942322731017\n",
      "  batch 350 loss: 0.37377077609300613\n",
      "  batch 400 loss: 0.3805284684896469\n",
      "  batch 450 loss: 0.37688623458147047\n",
      "  batch 500 loss: 0.3637107738852501\n",
      "  batch 550 loss: 0.35955152213573455\n",
      "  batch 600 loss: 0.36677091956138613\n",
      "  batch 650 loss: 0.3975942495465279\n",
      "  batch 700 loss: 0.3774419614672661\n",
      "  batch 750 loss: 0.3846513745188713\n",
      "  batch 800 loss: 0.39866055101156234\n",
      "  batch 850 loss: 0.3790080228447914\n",
      "  batch 900 loss: 0.3824168050289154\n",
      "LOSS train 0.38242 valid 0.84606, valid PER 23.82%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.3192367336153984\n",
      "  batch 100 loss: 0.31151526361703874\n",
      "  batch 150 loss: 0.31643398880958556\n",
      "  batch 200 loss: 0.3263771790266037\n",
      "  batch 250 loss: 0.3357685884833336\n",
      "  batch 300 loss: 0.3377100121974945\n",
      "  batch 350 loss: 0.32638738453388216\n",
      "  batch 400 loss: 0.3724264287948608\n",
      "  batch 450 loss: 0.3520102533698082\n",
      "  batch 500 loss: 0.3433633887767792\n",
      "  batch 550 loss: 0.3433412742614746\n",
      "  batch 600 loss: 0.36863513439893725\n",
      "  batch 650 loss: 0.3515713855624199\n",
      "  batch 700 loss: 0.3542227357625961\n",
      "  batch 750 loss: 0.34962943971157073\n",
      "  batch 800 loss: 0.34298777163028715\n",
      "  batch 850 loss: 0.35698225200176237\n",
      "  batch 900 loss: 0.33957763761281967\n",
      "LOSS train 0.33958 valid 0.87062, valid PER 23.82%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.29308187603950503\n",
      "  batch 100 loss: 0.2918883752822876\n",
      "  batch 150 loss: 0.31614146173000335\n",
      "  batch 200 loss: 0.31885552495718\n",
      "  batch 250 loss: 0.31126669943332674\n",
      "  batch 300 loss: 0.3020176386833191\n",
      "  batch 350 loss: 0.3183522832393646\n",
      "  batch 400 loss: 0.3078991621732712\n",
      "  batch 450 loss: 0.3222241824865341\n",
      "  batch 500 loss: 0.3374798291921616\n",
      "  batch 550 loss: 0.3310110819339752\n",
      "  batch 600 loss: 0.30420165359973905\n",
      "  batch 650 loss: 0.31905777543783187\n",
      "  batch 700 loss: 0.3534311535954475\n",
      "  batch 750 loss: 0.32244858652353287\n",
      "  batch 800 loss: 0.3103723368048668\n",
      "  batch 850 loss: 0.3269265103340149\n",
      "  batch 900 loss: 0.3625765407085419\n",
      "LOSS train 0.36258 valid 0.88507, valid PER 23.74%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.2713080942630768\n",
      "  batch 100 loss: 0.2724939823150635\n",
      "  batch 150 loss: 0.27153974294662475\n",
      "  batch 200 loss: 0.2863161489367485\n",
      "  batch 250 loss: 0.2889182126522064\n",
      "  batch 300 loss: 0.3001382356882095\n",
      "  batch 350 loss: 0.2896885794401169\n",
      "  batch 400 loss: 0.30650788635015486\n",
      "  batch 450 loss: 0.31595456421375273\n",
      "  batch 500 loss: 0.3217743408679962\n",
      "  batch 550 loss: 0.29361252427101137\n",
      "  batch 600 loss: 0.30098074555397036\n",
      "  batch 650 loss: 0.34336612224578855\n",
      "  batch 700 loss: 0.2979366397857666\n",
      "  batch 750 loss: 0.2963048514723778\n",
      "  batch 800 loss: 0.32637764036655426\n",
      "  batch 850 loss: 0.32837497293949125\n",
      "  batch 900 loss: 0.33777151972055436\n",
      "LOSS train 0.33777 valid 0.90088, valid PER 23.96%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.2567490622401237\n",
      "  batch 100 loss: 0.24932525366544722\n",
      "  batch 150 loss: 0.25047632545232773\n",
      "  batch 200 loss: 0.27324807196855544\n",
      "  batch 250 loss: 0.27570966571569444\n",
      "  batch 300 loss: 0.2736931452155113\n",
      "  batch 350 loss: 0.2686056861281395\n",
      "  batch 400 loss: 0.2936501970887184\n",
      "  batch 450 loss: 0.287351011633873\n",
      "  batch 500 loss: 0.27218993693590166\n",
      "  batch 550 loss: 0.2885671499371529\n",
      "  batch 600 loss: 0.27219067633152005\n",
      "  batch 650 loss: 0.2899527040123939\n",
      "  batch 700 loss: 0.2865083798766136\n",
      "  batch 750 loss: 0.27564563453197477\n",
      "  batch 800 loss: 0.313103266954422\n",
      "  batch 850 loss: 0.2938483774662018\n",
      "  batch 900 loss: 0.3069947412610054\n",
      "LOSS train 0.30699 valid 0.91871, valid PER 23.71%\n",
      "Training finished in 6.0 minutes.\n",
      "Model saved to checkpoints/20231206_153444/model_12\n",
      "Finish Adam optimiser\n",
      "Currently using SGD_Scheduler optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 562216\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.1116829061508176\n",
      "  batch 100 loss: 3.4098826122283934\n",
      "  batch 150 loss: 3.2903177452087404\n",
      "  batch 200 loss: 3.177213830947876\n",
      "  batch 250 loss: 3.0795586109161377\n",
      "  batch 300 loss: 2.8660931968688965\n",
      "  batch 350 loss: 2.650646238327026\n",
      "  batch 400 loss: 2.537282319068909\n",
      "  batch 450 loss: 2.416920428276062\n",
      "  batch 500 loss: 2.285665054321289\n",
      "  batch 550 loss: 2.211021363735199\n",
      "  batch 600 loss: 2.1374313497543334\n",
      "  batch 650 loss: 2.0348703956604\n",
      "  batch 700 loss: 2.021361036300659\n",
      "  batch 750 loss: 1.929525489807129\n",
      "  batch 800 loss: 1.8925332188606263\n",
      "  batch 850 loss: 1.8323968601226808\n",
      "  batch 900 loss: 1.7916918563842774\n",
      "LOSS train 1.79169 valid 1.72262, valid PER 64.90%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7187608766555786\n",
      "  batch 100 loss: 1.634839596748352\n",
      "  batch 150 loss: 1.6056838274002074\n",
      "  batch 200 loss: 1.6050900840759277\n",
      "  batch 250 loss: 1.6074833369255066\n",
      "  batch 300 loss: 1.5459772992134093\n",
      "  batch 350 loss: 1.4592710065841674\n",
      "  batch 400 loss: 1.4672535181045532\n",
      "  batch 450 loss: 1.4046416425704955\n",
      "  batch 500 loss: 1.426391944885254\n",
      "  batch 550 loss: 1.4112519216537476\n",
      "  batch 600 loss: 1.3528590774536133\n",
      "  batch 650 loss: 1.3525688195228576\n",
      "  batch 700 loss: 1.3187982714176179\n",
      "  batch 750 loss: 1.3092645263671876\n",
      "  batch 800 loss: 1.2404972207546234\n",
      "  batch 850 loss: 1.2516192722320556\n",
      "  batch 900 loss: 1.2626579117774963\n",
      "LOSS train 1.26266 valid 1.23465, valid PER 38.10%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.209913489818573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 1.1901494026184083\n",
      "  batch 150 loss: 1.1722270369529724\n",
      "  batch 200 loss: 1.1586070513725282\n",
      "  batch 250 loss: 1.1552634203433991\n",
      "  batch 300 loss: 1.1466247510910035\n",
      "  batch 350 loss: 1.1795566546916962\n",
      "  batch 400 loss: 1.1753727996349335\n",
      "  batch 450 loss: 1.1328780114650727\n",
      "  batch 500 loss: 1.1123747742176056\n",
      "  batch 550 loss: 1.1307678949832916\n",
      "  batch 600 loss: 1.0943857967853545\n",
      "  batch 650 loss: 1.0853698945045471\n",
      "  batch 700 loss: 1.1039607322216034\n",
      "  batch 750 loss: 1.1546841776371002\n",
      "  batch 800 loss: 1.0784635210037232\n",
      "  batch 850 loss: 1.1137256383895875\n",
      "  batch 900 loss: 1.0409421741962432\n",
      "LOSS train 1.04094 valid 1.11770, valid PER 33.76%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0222221565246583\n",
      "  batch 100 loss: 1.0432953679561614\n",
      "  batch 150 loss: 0.9977259993553161\n",
      "  batch 200 loss: 1.0301785922050477\n",
      "  batch 250 loss: 1.0364859843254088\n",
      "  batch 300 loss: 1.041960676908493\n",
      "  batch 350 loss: 0.9810562360286713\n",
      "  batch 400 loss: 1.0125023698806763\n",
      "  batch 450 loss: 0.9975814521312714\n",
      "  batch 500 loss: 0.9803336882591247\n",
      "  batch 550 loss: 1.0368608736991882\n",
      "  batch 600 loss: 1.037430876493454\n",
      "  batch 650 loss: 1.0084241330623627\n",
      "  batch 700 loss: 0.9838546097278595\n",
      "  batch 750 loss: 0.9591675162315368\n",
      "  batch 800 loss: 0.9483652114868164\n",
      "  batch 850 loss: 0.9661840462684631\n",
      "  batch 900 loss: 1.0173007190227508\n",
      "LOSS train 1.01730 valid 0.99611, valid PER 31.52%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9226106059551239\n",
      "  batch 100 loss: 0.9151532793045044\n",
      "  batch 150 loss: 0.9717910265922547\n",
      "  batch 200 loss: 0.8968503570556641\n",
      "  batch 250 loss: 0.9106054043769837\n",
      "  batch 300 loss: 0.919277218580246\n",
      "  batch 350 loss: 0.9188604402542114\n",
      "  batch 400 loss: 0.9267465925216675\n",
      "  batch 450 loss: 0.9176740348339081\n",
      "  batch 500 loss: 0.9290049147605896\n",
      "  batch 550 loss: 0.8783856928348541\n",
      "  batch 600 loss: 0.9565024769306183\n",
      "  batch 650 loss: 0.908532440662384\n",
      "  batch 700 loss: 0.9608673334121705\n",
      "  batch 750 loss: 0.8793993282318116\n",
      "  batch 800 loss: 0.9206342995166779\n",
      "  batch 850 loss: 0.8955698347091675\n",
      "  batch 900 loss: 0.9085341536998749\n",
      "LOSS train 0.90853 valid 0.94711, valid PER 29.14%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8969084167480469\n",
      "  batch 100 loss: 0.8486408519744874\n",
      "  batch 150 loss: 0.848024765253067\n",
      "  batch 200 loss: 0.8483868849277496\n",
      "  batch 250 loss: 0.8931555736064911\n",
      "  batch 300 loss: 0.880929799079895\n",
      "  batch 350 loss: 0.8668911790847779\n",
      "  batch 400 loss: 0.8295032477378845\n",
      "  batch 450 loss: 0.867485271692276\n",
      "  batch 500 loss: 0.8572917354106903\n",
      "  batch 550 loss: 0.8728311479091644\n",
      "  batch 600 loss: 0.8410735750198364\n",
      "  batch 650 loss: 0.8454880034923553\n",
      "  batch 700 loss: 0.871601095199585\n",
      "  batch 750 loss: 0.8431267619132996\n",
      "  batch 800 loss: 0.8426527881622314\n",
      "  batch 850 loss: 0.8271211230754852\n",
      "  batch 900 loss: 0.837473931312561\n",
      "LOSS train 0.83747 valid 0.92326, valid PER 28.51%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8221569108963013\n",
      "  batch 100 loss: 0.8087854945659637\n",
      "  batch 150 loss: 0.7877373880147934\n",
      "  batch 200 loss: 0.8019784104824066\n",
      "  batch 250 loss: 0.7949234366416931\n",
      "  batch 300 loss: 0.7716438400745392\n",
      "  batch 350 loss: 0.7948991191387177\n",
      "  batch 400 loss: 0.78917928814888\n",
      "  batch 450 loss: 0.793948038816452\n",
      "  batch 500 loss: 0.8016972625255585\n",
      "  batch 550 loss: 0.783652116060257\n",
      "  batch 600 loss: 0.8031676709651947\n",
      "  batch 650 loss: 0.7858633649349213\n",
      "  batch 700 loss: 0.8145425224304199\n",
      "  batch 750 loss: 0.7865766108036041\n",
      "  batch 800 loss: 0.7852807974815369\n",
      "  batch 850 loss: 0.8042137360572815\n",
      "  batch 900 loss: 0.8407570159435273\n",
      "LOSS train 0.84076 valid 0.88318, valid PER 27.90%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.7576250326633454\n",
      "  batch 100 loss: 0.7421582591533661\n",
      "  batch 150 loss: 0.7470518183708191\n",
      "  batch 200 loss: 0.740654137134552\n",
      "  batch 250 loss: 0.7714844357967376\n",
      "  batch 300 loss: 0.7016405326128006\n",
      "  batch 350 loss: 0.7713175070285797\n",
      "  batch 400 loss: 0.7432233536243439\n",
      "  batch 450 loss: 0.7721696627140046\n",
      "  batch 500 loss: 0.7751019191741944\n",
      "  batch 550 loss: 0.7157001930475235\n",
      "  batch 600 loss: 0.760904325246811\n",
      "  batch 650 loss: 0.7960466849803924\n",
      "  batch 700 loss: 0.7503813862800598\n",
      "  batch 750 loss: 0.7765169978141785\n",
      "  batch 800 loss: 0.7765525197982788\n",
      "  batch 850 loss: 0.7403183919191361\n",
      "  batch 900 loss: 0.7686933112144471\n",
      "LOSS train 0.76869 valid 0.87678, valid PER 26.99%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.6893238532543182\n",
      "  batch 100 loss: 0.7350553208589554\n",
      "  batch 150 loss: 0.7294788002967835\n",
      "  batch 200 loss: 0.7079568821191787\n",
      "  batch 250 loss: 0.7345658993721008\n",
      "  batch 300 loss: 0.7354937314987182\n",
      "  batch 350 loss: 0.7479870611429215\n",
      "  batch 400 loss: 0.7176116508245468\n",
      "  batch 450 loss: 0.7232911741733551\n",
      "  batch 500 loss: 0.690560349225998\n",
      "  batch 550 loss: 0.7142475044727326\n",
      "  batch 600 loss: 0.7392017430067063\n",
      "  batch 650 loss: 0.7118095982074738\n",
      "  batch 700 loss: 0.6880215805768967\n",
      "  batch 750 loss: 0.7278932535648346\n",
      "  batch 800 loss: 0.7096153306961059\n",
      "  batch 850 loss: 0.7524967026710511\n",
      "  batch 900 loss: 0.6939692109823227\n",
      "LOSS train 0.69397 valid 0.85445, valid PER 26.34%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6480540382862091\n",
      "  batch 100 loss: 0.6528149539232254\n",
      "  batch 150 loss: 0.6840362739562988\n",
      "  batch 200 loss: 0.6896617859601974\n",
      "  batch 250 loss: 0.7065374529361725\n",
      "  batch 300 loss: 0.6640437549352646\n",
      "  batch 350 loss: 0.6866088497638703\n",
      "  batch 400 loss: 0.648799415230751\n",
      "  batch 450 loss: 0.6517677694559098\n",
      "  batch 500 loss: 0.6793654954433441\n",
      "  batch 550 loss: 0.7056165587902069\n",
      "  batch 600 loss: 0.6833477610349655\n",
      "  batch 650 loss: 0.6676582419872283\n",
      "  batch 700 loss: 0.6973805183172226\n",
      "  batch 750 loss: 0.6816161096096038\n",
      "  batch 800 loss: 0.6892193055152893\n",
      "  batch 850 loss: 0.6905558806657791\n",
      "  batch 900 loss: 0.7018935084342957\n",
      "LOSS train 0.70189 valid 0.83222, valid PER 25.73%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.632959508895874\n",
      "  batch 100 loss: 0.6028368288278579\n",
      "  batch 150 loss: 0.6011857450008392\n",
      "  batch 200 loss: 0.6510674285888672\n",
      "  batch 250 loss: 0.6306723415851593\n",
      "  batch 300 loss: 0.618299475312233\n",
      "  batch 350 loss: 0.6435454553365707\n",
      "  batch 400 loss: 0.6435649400949478\n",
      "  batch 450 loss: 0.6707468450069427\n",
      "  batch 500 loss: 0.6248768305778504\n",
      "  batch 550 loss: 0.63857202231884\n",
      "  batch 600 loss: 0.6101969373226166\n",
      "  batch 650 loss: 0.6949723756313324\n",
      "  batch 700 loss: 0.6289634430408477\n",
      "  batch 750 loss: 0.6360731488466262\n",
      "  batch 800 loss: 0.673562685251236\n",
      "  batch 850 loss: 0.6744343233108521\n",
      "  batch 900 loss: 0.6870707505941391\n",
      "LOSS train 0.68707 valid 0.81365, valid PER 24.72%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.6032600700855255\n",
      "  batch 100 loss: 0.6030064648389817\n",
      "  batch 150 loss: 0.5511752927303314\n",
      "  batch 200 loss: 0.5979497003555297\n",
      "  batch 250 loss: 0.602964351773262\n",
      "  batch 300 loss: 0.6039423650503158\n",
      "  batch 350 loss: 0.5934745198488236\n",
      "  batch 400 loss: 0.622535091638565\n",
      "  batch 450 loss: 0.6220312565565109\n",
      "  batch 500 loss: 0.6443018853664398\n",
      "  batch 550 loss: 0.5895591729879379\n",
      "  batch 600 loss: 0.6209086519479752\n",
      "  batch 650 loss: 0.6342496746778488\n",
      "  batch 700 loss: 0.6345688790082932\n",
      "  batch 750 loss: 0.6120032083988189\n",
      "  batch 800 loss: 0.5960044038295745\n",
      "  batch 850 loss: 0.6641541123390198\n",
      "  batch 900 loss: 0.6345742535591126\n",
      "Epoch 00012: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.63457 valid 0.81874, valid PER 24.72%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.534906000494957\n",
      "  batch 100 loss: 0.526574005484581\n",
      "  batch 150 loss: 0.5059515976905823\n",
      "  batch 200 loss: 0.5306123906373977\n",
      "  batch 250 loss: 0.5251464712619781\n",
      "  batch 300 loss: 0.507467497587204\n",
      "  batch 350 loss: 0.5027809113264083\n",
      "  batch 400 loss: 0.521304806470871\n",
      "  batch 450 loss: 0.5107923966646194\n",
      "  batch 500 loss: 0.48483508110046386\n",
      "  batch 550 loss: 0.5259968310594558\n",
      "  batch 600 loss: 0.4943772804737091\n",
      "  batch 650 loss: 0.5413186174631118\n",
      "  batch 700 loss: 0.5284219217300415\n",
      "  batch 750 loss: 0.4824589163064957\n",
      "  batch 800 loss: 0.4946086943149567\n",
      "  batch 850 loss: 0.5255445337295532\n",
      "  batch 900 loss: 0.524214214682579\n",
      "LOSS train 0.52421 valid 0.78170, valid PER 23.73%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.46080062329769134\n",
      "  batch 100 loss: 0.4743225300312042\n",
      "  batch 150 loss: 0.4634135913848877\n",
      "  batch 200 loss: 0.46161922574043274\n",
      "  batch 250 loss: 0.4645068520307541\n",
      "  batch 300 loss: 0.5018372747302056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 350 loss: 0.4603298330307007\n",
      "  batch 400 loss: 0.46741651862859723\n",
      "  batch 450 loss: 0.46988209664821623\n",
      "  batch 500 loss: 0.48167844653129577\n",
      "  batch 550 loss: 0.5064626199007034\n",
      "  batch 600 loss: 0.4711774265766144\n",
      "  batch 650 loss: 0.48411192536354064\n",
      "  batch 700 loss: 0.49940930247306825\n",
      "  batch 750 loss: 0.48982963621616366\n",
      "  batch 800 loss: 0.4542622309923172\n",
      "  batch 850 loss: 0.49721058785915373\n",
      "  batch 900 loss: 0.49705726027488706\n",
      "LOSS train 0.49706 valid 0.77112, valid PER 23.06%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.42934529155492784\n",
      "  batch 100 loss: 0.42993021368980405\n",
      "  batch 150 loss: 0.4284532779455185\n",
      "  batch 200 loss: 0.4588908052444458\n",
      "  batch 250 loss: 0.47258240640163424\n",
      "  batch 300 loss: 0.43589336425065994\n",
      "  batch 350 loss: 0.4470321333408356\n",
      "  batch 400 loss: 0.4491481798887253\n",
      "  batch 450 loss: 0.44672223687171936\n",
      "  batch 500 loss: 0.4237887018918991\n",
      "  batch 550 loss: 0.4472121220827103\n",
      "  batch 600 loss: 0.4635932970046997\n",
      "  batch 650 loss: 0.4799240952730179\n",
      "  batch 700 loss: 0.4792197859287262\n",
      "  batch 750 loss: 0.4674625754356384\n",
      "  batch 800 loss: 0.45994964003562927\n",
      "  batch 850 loss: 0.44167022228240965\n",
      "  batch 900 loss: 0.4610109132528305\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.46101 valid 0.78434, valid PER 23.26%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.4121812576055527\n",
      "  batch 100 loss: 0.3898321229219437\n",
      "  batch 150 loss: 0.3878316175937653\n",
      "  batch 200 loss: 0.3887324470281601\n",
      "  batch 250 loss: 0.40698029547929765\n",
      "  batch 300 loss: 0.3936894649267197\n",
      "  batch 350 loss: 0.3884109750390053\n",
      "  batch 400 loss: 0.40702357798814776\n",
      "  batch 450 loss: 0.4099539613723755\n",
      "  batch 500 loss: 0.36980411857366563\n",
      "  batch 550 loss: 0.3896083876490593\n",
      "  batch 600 loss: 0.3875459769368172\n",
      "  batch 650 loss: 0.4075625878572464\n",
      "  batch 700 loss: 0.3801469886302948\n",
      "  batch 750 loss: 0.39729914933443067\n",
      "  batch 800 loss: 0.39822701990604403\n",
      "  batch 850 loss: 0.38409756749868396\n",
      "  batch 900 loss: 0.4134075355529785\n",
      "Epoch 00016: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.41341 valid 0.78513, valid PER 22.88%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.3651871284842491\n",
      "  batch 100 loss: 0.3620499846339226\n",
      "  batch 150 loss: 0.34992466270923617\n",
      "  batch 200 loss: 0.3593652355670929\n",
      "  batch 250 loss: 0.3757224577665329\n",
      "  batch 300 loss: 0.3712947019934654\n",
      "  batch 350 loss: 0.34045020848512647\n",
      "  batch 400 loss: 0.38223812013864517\n",
      "  batch 450 loss: 0.3673432540893555\n",
      "  batch 500 loss: 0.3444220221042633\n",
      "  batch 550 loss: 0.34982615798711775\n",
      "  batch 600 loss: 0.37614371955394743\n",
      "  batch 650 loss: 0.35101468712091444\n",
      "  batch 700 loss: 0.35938204437494276\n",
      "  batch 750 loss: 0.350790399312973\n",
      "  batch 800 loss: 0.34399732381105425\n",
      "  batch 850 loss: 0.3629620623588562\n",
      "  batch 900 loss: 0.34540245473384856\n",
      "Epoch 00017: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.34540 valid 0.78200, valid PER 22.63%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.34328733444213866\n",
      "  batch 100 loss: 0.34792835235595704\n",
      "  batch 150 loss: 0.36225759536027907\n",
      "  batch 200 loss: 0.33449346482753756\n",
      "  batch 250 loss: 0.36095218271017077\n",
      "  batch 300 loss: 0.33043767482042313\n",
      "  batch 350 loss: 0.32684841752052307\n",
      "  batch 400 loss: 0.3302070853114128\n",
      "  batch 450 loss: 0.3427955788373947\n",
      "  batch 500 loss: 0.33058389991521836\n",
      "  batch 550 loss: 0.3475760006904602\n",
      "  batch 600 loss: 0.32778813391923906\n",
      "  batch 650 loss: 0.3202808657288551\n",
      "  batch 700 loss: 0.35889608502388\n",
      "  batch 750 loss: 0.32255488336086274\n",
      "  batch 800 loss: 0.3247525444626808\n",
      "  batch 850 loss: 0.3255606698989868\n",
      "  batch 900 loss: 0.34558935701847077\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.34559 valid 0.78162, valid PER 22.64%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.3291511353850365\n",
      "  batch 100 loss: 0.32164954334497453\n",
      "  batch 150 loss: 0.3158303815126419\n",
      "  batch 200 loss: 0.32349557012319563\n",
      "  batch 250 loss: 0.32791573405265806\n",
      "  batch 300 loss: 0.329108567237854\n",
      "  batch 350 loss: 0.316506615281105\n",
      "  batch 400 loss: 0.3216836458444595\n",
      "  batch 450 loss: 0.3412716099619865\n",
      "  batch 500 loss: 0.3338213077187538\n",
      "  batch 550 loss: 0.318044910132885\n",
      "  batch 600 loss: 0.3179557102918625\n",
      "  batch 650 loss: 0.3516753235459328\n",
      "  batch 700 loss: 0.32686707377433777\n",
      "  batch 750 loss: 0.31508252531290054\n",
      "  batch 800 loss: 0.33548482596874235\n",
      "  batch 850 loss: 0.32645764768123625\n",
      "  batch 900 loss: 0.3360681864619255\n",
      "Epoch 00019: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.33607 valid 0.78495, valid PER 22.54%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.3244634705781937\n",
      "  batch 100 loss: 0.3207687187194824\n",
      "  batch 150 loss: 0.3121427398920059\n",
      "  batch 200 loss: 0.3320671272277832\n",
      "  batch 250 loss: 0.31655082732439044\n",
      "  batch 300 loss: 0.3247166186571121\n",
      "  batch 350 loss: 0.3035204327106476\n",
      "  batch 400 loss: 0.32613517075777054\n",
      "  batch 450 loss: 0.31632587730884554\n",
      "  batch 500 loss: 0.3035401117801666\n",
      "  batch 550 loss: 0.3436766365170479\n",
      "  batch 600 loss: 0.31629462510347367\n",
      "  batch 650 loss: 0.31697377026081086\n",
      "  batch 700 loss: 0.32421843737363815\n",
      "  batch 750 loss: 0.29372053012251853\n",
      "  batch 800 loss: 0.3397113263607025\n",
      "  batch 850 loss: 0.3319213929772377\n",
      "  batch 900 loss: 0.32601006627082824\n",
      "Epoch 00020: reducing learning rate of group 0 to 3.9062e-03.\n",
      "LOSS train 0.32601 valid 0.78674, valid PER 22.44%\n",
      "Training finished in 4.0 minutes.\n",
      "Model saved to checkpoints/20231206_154053/model_14\n",
      "Finish SGD_Scheduler optimiser\n",
      "End tuning For 2 Layer LSTM\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout_between_layer\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start dropout tuning For 2 Layer LSTM, with Dropout between layer\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For 2 Layer LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a50b6",
   "metadata": {},
   "source": [
    "## 2. Wider LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cccbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "Optimiser = [\"Adam\", \"SGD_Scheduler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6429ea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning For wider 1 Layer LSTM\n",
      "Currently using Adam optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.726706247329712\n",
      "  batch 100 loss: 3.138441967964172\n",
      "  batch 150 loss: 2.8376491594314577\n",
      "  batch 200 loss: 2.5527011251449583\n",
      "  batch 250 loss: 2.2950522255897523\n",
      "  batch 300 loss: 2.0545081233978273\n",
      "  batch 350 loss: 1.898708918094635\n",
      "  batch 400 loss: 1.8701470947265626\n",
      "  batch 450 loss: 1.7898587894439697\n",
      "  batch 500 loss: 1.6957488441467286\n",
      "  batch 550 loss: 1.6496214079856872\n",
      "  batch 600 loss: 1.6056160998344422\n",
      "  batch 650 loss: 1.5235937809944153\n",
      "  batch 700 loss: 1.5432402300834656\n",
      "  batch 750 loss: 1.4986415648460387\n",
      "  batch 800 loss: 1.4788236808776856\n",
      "  batch 850 loss: 1.4585586500167846\n",
      "  batch 900 loss: 1.402013213634491\n",
      "LOSS train 1.40201 valid 1.42909, valid PER 46.45%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.371263129711151\n",
      "  batch 100 loss: 1.3106854343414307\n",
      "  batch 150 loss: 1.2843054676055907\n",
      "  batch 200 loss: 1.3194678282737733\n",
      "  batch 250 loss: 1.3066549730300903\n",
      "  batch 300 loss: 1.276835218667984\n",
      "  batch 350 loss: 1.2256913101673126\n",
      "  batch 400 loss: 1.2309428107738496\n",
      "  batch 450 loss: 1.194482479095459\n",
      "  batch 500 loss: 1.2341558468341827\n",
      "  batch 550 loss: 1.2518030822277069\n",
      "  batch 600 loss: 1.2085811686515808\n",
      "  batch 650 loss: 1.2142945861816405\n",
      "  batch 700 loss: 1.2046186661720275\n",
      "  batch 750 loss: 1.1994888544082642\n",
      "  batch 800 loss: 1.1207469034194946\n",
      "  batch 850 loss: 1.1307724249362945\n",
      "  batch 900 loss: 1.1619155168533326\n",
      "LOSS train 1.16192 valid 1.15078, valid PER 36.81%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.1027244663238525\n",
      "  batch 100 loss: 1.0793870294094086\n",
      "  batch 150 loss: 1.0687958335876464\n",
      "  batch 200 loss: 1.0733801603317261\n",
      "  batch 250 loss: 1.0537852025032044\n",
      "  batch 300 loss: 1.058572733402252\n",
      "  batch 350 loss: 1.103026659488678\n",
      "  batch 400 loss: 1.075758011341095\n",
      "  batch 450 loss: 1.0523866820335388\n",
      "  batch 500 loss: 1.0280133128166198\n",
      "  batch 550 loss: 1.0342616164684295\n",
      "  batch 600 loss: 1.0278714144229888\n",
      "  batch 650 loss: 1.0423573398590087\n",
      "  batch 700 loss: 1.0244723927974702\n",
      "  batch 750 loss: 1.0837918305397034\n",
      "  batch 800 loss: 1.0142441380023957\n",
      "  batch 850 loss: 1.0613830232620238\n",
      "  batch 900 loss: 0.9732971310615539\n",
      "LOSS train 0.97330 valid 1.07044, valid PER 33.31%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.9439988768100739\n",
      "  batch 100 loss: 0.960516037940979\n",
      "  batch 150 loss: 0.9381738197803497\n",
      "  batch 200 loss: 0.9797238242626191\n",
      "  batch 250 loss: 0.9835976719856262\n",
      "  batch 300 loss: 0.9775135099887848\n",
      "  batch 350 loss: 0.9311432802677154\n",
      "  batch 400 loss: 0.9519865715503693\n",
      "  batch 450 loss: 0.9379347312450409\n",
      "  batch 500 loss: 0.9255365359783173\n",
      "  batch 550 loss: 0.9588936340808868\n",
      "  batch 600 loss: 0.9692083013057708\n",
      "  batch 650 loss: 0.9381882452964783\n",
      "  batch 700 loss: 0.91719477891922\n",
      "  batch 750 loss: 0.9038376677036285\n",
      "  batch 800 loss: 0.8796601319313049\n",
      "  batch 850 loss: 0.9167038691043854\n",
      "  batch 900 loss: 0.9592879319190979\n",
      "LOSS train 0.95929 valid 0.96657, valid PER 29.92%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.8554429984092713\n",
      "  batch 100 loss: 0.852830798625946\n",
      "  batch 150 loss: 0.9163454008102417\n",
      "  batch 200 loss: 0.8163705325126648\n",
      "  batch 250 loss: 0.8291546273231506\n",
      "  batch 300 loss: 0.8535029900074005\n",
      "  batch 350 loss: 0.8495649349689484\n",
      "  batch 400 loss: 0.8626989924907684\n",
      "  batch 450 loss: 0.8342582511901856\n",
      "  batch 500 loss: 0.8791605246067047\n",
      "  batch 550 loss: 0.8295828902721405\n",
      "  batch 600 loss: 0.8830932867527008\n",
      "  batch 650 loss: 0.858252328634262\n",
      "  batch 700 loss: 0.8829994118213653\n",
      "  batch 750 loss: 0.8303686213493348\n",
      "  batch 800 loss: 0.8620458924770356\n",
      "  batch 850 loss: 0.857847865819931\n",
      "  batch 900 loss: 0.8551088607311249\n",
      "LOSS train 0.85511 valid 0.92866, valid PER 28.59%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8102034556865693\n",
      "  batch 100 loss: 0.7630654609203339\n",
      "  batch 150 loss: 0.7481227833032608\n",
      "  batch 200 loss: 0.7856965088844299\n",
      "  batch 250 loss: 0.8167295944690705\n",
      "  batch 300 loss: 0.7944251763820648\n",
      "  batch 350 loss: 0.8033657336235046\n",
      "  batch 400 loss: 0.7840632653236389\n",
      "  batch 450 loss: 0.7814157235622406\n",
      "  batch 500 loss: 0.7893281507492066\n",
      "  batch 550 loss: 0.8054888343811035\n",
      "  batch 600 loss: 0.7592731583118438\n",
      "  batch 650 loss: 0.7950967228412629\n",
      "  batch 700 loss: 0.7922350990772248\n",
      "  batch 750 loss: 0.7801477140188218\n",
      "  batch 800 loss: 0.7922218930721283\n",
      "  batch 850 loss: 0.7582769596576691\n",
      "  batch 900 loss: 0.8110857760906219\n",
      "LOSS train 0.81109 valid 0.91225, valid PER 28.43%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.7182188987731933\n",
      "  batch 100 loss: 0.7397950541973114\n",
      "  batch 150 loss: 0.6899797040224075\n",
      "  batch 200 loss: 0.696920713186264\n",
      "  batch 250 loss: 0.7104861402511596\n",
      "  batch 300 loss: 0.7172468769550323\n",
      "  batch 350 loss: 0.7296990120410919\n",
      "  batch 400 loss: 0.7421277844905854\n",
      "  batch 450 loss: 0.7387280589342118\n",
      "  batch 500 loss: 0.7153047931194305\n",
      "  batch 550 loss: 0.7084246504306794\n",
      "  batch 600 loss: 0.7258678722381592\n",
      "  batch 650 loss: 0.6987067925930023\n",
      "  batch 700 loss: 0.7597693103551865\n",
      "  batch 750 loss: 0.7102110832929611\n",
      "  batch 800 loss: 0.7270417112112045\n",
      "  batch 850 loss: 0.7249628072977066\n",
      "  batch 900 loss: 0.7713544809818268\n",
      "LOSS train 0.77135 valid 0.90893, valid PER 27.80%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.6496261942386627\n",
      "  batch 100 loss: 0.6444712603092193\n",
      "  batch 150 loss: 0.6617237490415573\n",
      "  batch 200 loss: 0.6332750904560089\n",
      "  batch 250 loss: 0.6558486610651016\n",
      "  batch 300 loss: 0.6267842727899552\n",
      "  batch 350 loss: 0.6812382882833481\n",
      "  batch 400 loss: 0.6381931662559509\n",
      "  batch 450 loss: 0.6862004947662353\n",
      "  batch 500 loss: 0.7119149971008301\n",
      "  batch 550 loss: 0.6640158623456955\n",
      "  batch 600 loss: 0.6912823331356048\n",
      "  batch 650 loss: 0.6881316733360291\n",
      "  batch 700 loss: 0.6590229773521423\n",
      "  batch 750 loss: 0.6627568906545639\n",
      "  batch 800 loss: 0.6788549953699112\n",
      "  batch 850 loss: 0.6796384119987487\n",
      "  batch 900 loss: 0.7052136844396591\n",
      "LOSS train 0.70521 valid 0.87837, valid PER 27.00%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.5696858954429627\n",
      "  batch 100 loss: 0.5807337754964829\n",
      "  batch 150 loss: 0.6022601711750031\n",
      "  batch 200 loss: 0.5817216438055038\n",
      "  batch 250 loss: 0.6186970204114914\n",
      "  batch 300 loss: 0.6135749047994614\n",
      "  batch 350 loss: 0.6333190447092056\n",
      "  batch 400 loss: 0.6077054029703141\n",
      "  batch 450 loss: 0.6098567515611648\n",
      "  batch 500 loss: 0.6004818409681321\n",
      "  batch 550 loss: 0.6351041281223297\n",
      "  batch 600 loss: 0.6401084220409393\n",
      "  batch 650 loss: 0.6208925199508667\n",
      "  batch 700 loss: 0.6015744757652283\n",
      "  batch 750 loss: 0.6228813713788987\n",
      "  batch 800 loss: 0.648002365231514\n",
      "  batch 850 loss: 0.6478427970409393\n",
      "  batch 900 loss: 0.5919878959655762\n",
      "LOSS train 0.59199 valid 0.88920, valid PER 26.40%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.5249535888433456\n",
      "  batch 100 loss: 0.5160338652133941\n",
      "  batch 150 loss: 0.546359452009201\n",
      "  batch 200 loss: 0.5640883755683899\n",
      "  batch 250 loss: 0.5842921417951584\n",
      "  batch 300 loss: 0.5305974906682969\n",
      "  batch 350 loss: 0.5506243759393692\n",
      "  batch 400 loss: 0.5339394676685333\n",
      "  batch 450 loss: 0.5395123153924942\n",
      "  batch 500 loss: 0.5650691455602646\n",
      "  batch 550 loss: 0.5907448303699493\n",
      "  batch 600 loss: 0.5534189081192017\n",
      "  batch 650 loss: 0.5476958096027374\n",
      "  batch 700 loss: 0.5789098876714707\n",
      "  batch 750 loss: 0.5805615001916885\n",
      "  batch 800 loss: 0.5779699259996414\n",
      "  batch 850 loss: 0.5976056379079818\n",
      "  batch 900 loss: 0.5779496896266937\n",
      "LOSS train 0.57795 valid 0.86603, valid PER 25.86%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.46442583441734314\n",
      "  batch 100 loss: 0.4537083524465561\n",
      "  batch 150 loss: 0.4678297656774521\n",
      "  batch 200 loss: 0.5199259382486343\n",
      "  batch 250 loss: 0.5034769183397293\n",
      "  batch 300 loss: 0.48255199372768404\n",
      "  batch 350 loss: 0.5199880927801133\n",
      "  batch 400 loss: 0.5226772844791412\n",
      "  batch 450 loss: 0.5020702230930328\n",
      "  batch 500 loss: 0.5096827405691147\n",
      "  batch 550 loss: 0.5282014667987823\n",
      "  batch 600 loss: 0.5104519551992417\n",
      "  batch 650 loss: 0.5538505184650421\n",
      "  batch 700 loss: 0.4961248821020126\n",
      "  batch 750 loss: 0.5083359926939011\n",
      "  batch 800 loss: 0.5134959828853607\n",
      "  batch 850 loss: 0.5710029029846191\n",
      "  batch 900 loss: 0.5356584930419922\n",
      "LOSS train 0.53566 valid 0.88234, valid PER 25.34%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.4311693939566612\n",
      "  batch 100 loss: 0.43551454961299896\n",
      "  batch 150 loss: 0.3866489797830582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.43397910475730894\n",
      "  batch 250 loss: 0.45739098608493806\n",
      "  batch 300 loss: 0.44846841394901277\n",
      "  batch 350 loss: 0.4416872185468674\n",
      "  batch 400 loss: 0.46438679337501526\n",
      "  batch 450 loss: 0.46913277626037597\n",
      "  batch 500 loss: 0.4553913301229477\n",
      "  batch 550 loss: 0.4454963910579681\n",
      "  batch 600 loss: 0.47365712881088257\n",
      "  batch 650 loss: 0.5065071672201157\n",
      "  batch 700 loss: 0.48891753554344175\n",
      "  batch 750 loss: 0.4605794471502304\n",
      "  batch 800 loss: 0.46001900494098663\n",
      "  batch 850 loss: 0.5104774069786072\n",
      "  batch 900 loss: 0.5006308209896088\n",
      "LOSS train 0.50063 valid 0.90428, valid PER 26.41%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.3751909387111664\n",
      "  batch 100 loss: 0.393096704185009\n",
      "  batch 150 loss: 0.3798890885710716\n",
      "  batch 200 loss: 0.3980745232105255\n",
      "  batch 250 loss: 0.387310808300972\n",
      "  batch 300 loss: 0.39823910921812056\n",
      "  batch 350 loss: 0.38475379019975664\n",
      "  batch 400 loss: 0.41359849393367765\n",
      "  batch 450 loss: 0.41483157098293305\n",
      "  batch 500 loss: 0.39060602635145186\n",
      "  batch 550 loss: 0.4302382481098175\n",
      "  batch 600 loss: 0.4257443830370903\n",
      "  batch 650 loss: 0.44080410718917845\n",
      "  batch 700 loss: 0.444922856092453\n",
      "  batch 750 loss: 0.39792022794485093\n",
      "  batch 800 loss: 0.41214637339115145\n",
      "  batch 850 loss: 0.44960572838783264\n",
      "  batch 900 loss: 0.45398216903209687\n",
      "LOSS train 0.45398 valid 0.91844, valid PER 25.54%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.3413344657421112\n",
      "  batch 100 loss: 0.3500638315081596\n",
      "  batch 150 loss: 0.35568706452846527\n",
      "  batch 200 loss: 0.323366282582283\n",
      "  batch 250 loss: 0.3431426841020584\n",
      "  batch 300 loss: 0.3764155580103397\n",
      "  batch 350 loss: 0.3421567067503929\n",
      "  batch 400 loss: 0.3559079775214195\n",
      "  batch 450 loss: 0.3875610929727554\n",
      "  batch 500 loss: 0.37137970328330994\n",
      "  batch 550 loss: 0.39356418371200563\n",
      "  batch 600 loss: 0.36317994236946105\n",
      "  batch 650 loss: 0.39106354385614395\n",
      "  batch 700 loss: 0.40366663962602617\n",
      "  batch 750 loss: 0.384827418923378\n",
      "  batch 800 loss: 0.3920682540535927\n",
      "  batch 850 loss: 0.4013360878825188\n",
      "  batch 900 loss: 0.40625980019569397\n",
      "LOSS train 0.40626 valid 0.95451, valid PER 26.09%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.2916167974472046\n",
      "  batch 100 loss: 0.3034552818536758\n",
      "  batch 150 loss: 0.3155098280310631\n",
      "  batch 200 loss: 0.319640271961689\n",
      "  batch 250 loss: 0.3316896003484726\n",
      "  batch 300 loss: 0.32139202326536176\n",
      "  batch 350 loss: 0.34740579545497896\n",
      "  batch 400 loss: 0.33130951374769213\n",
      "  batch 450 loss: 0.32196802854537965\n",
      "  batch 500 loss: 0.33015930086374284\n",
      "  batch 550 loss: 0.3463440945744514\n",
      "  batch 600 loss: 0.35109486401081086\n",
      "  batch 650 loss: 0.3585737270116806\n",
      "  batch 700 loss: 0.36802565813064575\n",
      "  batch 750 loss: 0.37848553746938707\n",
      "  batch 800 loss: 0.3383953109383583\n",
      "  batch 850 loss: 0.3297510063648224\n",
      "  batch 900 loss: 0.35273826390504837\n",
      "LOSS train 0.35274 valid 0.98107, valid PER 25.78%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.2873458594083786\n",
      "  batch 100 loss: 0.2740247914195061\n",
      "  batch 150 loss: 0.2873097676038742\n",
      "  batch 200 loss: 0.27108293026685715\n",
      "  batch 250 loss: 0.2879838779568672\n",
      "  batch 300 loss: 0.2812153434753418\n",
      "  batch 350 loss: 0.3016488112509251\n",
      "  batch 400 loss: 0.3035780048370361\n",
      "  batch 450 loss: 0.296626640856266\n",
      "  batch 500 loss: 0.2976029506325722\n",
      "  batch 550 loss: 0.28870630264282227\n",
      "  batch 600 loss: 0.30013500064611437\n",
      "  batch 650 loss: 0.3291305020451546\n",
      "  batch 700 loss: 0.3161680805683136\n",
      "  batch 750 loss: 0.31148087978363037\n",
      "  batch 800 loss: 0.3232536306977272\n",
      "  batch 850 loss: 0.31862897008657454\n",
      "  batch 900 loss: 0.32918630868196486\n",
      "LOSS train 0.32919 valid 1.01817, valid PER 25.98%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.2375163559615612\n",
      "  batch 100 loss: 0.23239869505167007\n",
      "  batch 150 loss: 0.2617707048356533\n",
      "  batch 200 loss: 0.25124471038579943\n",
      "  batch 250 loss: 0.2672548818588257\n",
      "  batch 300 loss: 0.2704386207461357\n",
      "  batch 350 loss: 0.2600197345018387\n",
      "  batch 400 loss: 0.2915363447368145\n",
      "  batch 450 loss: 0.2811556041240692\n",
      "  batch 500 loss: 0.2724483212828636\n",
      "  batch 550 loss: 0.276839359998703\n",
      "  batch 600 loss: 0.2949241033196449\n",
      "  batch 650 loss: 0.28854031175374983\n",
      "  batch 700 loss: 0.29008549749851226\n",
      "  batch 750 loss: 0.28807034134864806\n",
      "  batch 800 loss: 0.29277099460363387\n",
      "  batch 850 loss: 0.30601645618677137\n",
      "  batch 900 loss: 0.29237408220767974\n",
      "LOSS train 0.29237 valid 1.04033, valid PER 25.95%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.21902075454592704\n",
      "  batch 100 loss: 0.2129373347759247\n",
      "  batch 150 loss: 0.23262621968984604\n",
      "  batch 200 loss: 0.23491829186677932\n",
      "  batch 250 loss: 0.23678220480680465\n",
      "  batch 300 loss: 0.23773421853780746\n",
      "  batch 350 loss: 0.25803046464920043\n",
      "  batch 400 loss: 0.23548227518796921\n",
      "  batch 450 loss: 0.23463003993034362\n",
      "  batch 500 loss: 0.2446175682544708\n",
      "  batch 550 loss: 0.2517752392590046\n",
      "  batch 600 loss: 0.24050180613994598\n",
      "  batch 650 loss: 0.24882745027542114\n",
      "  batch 700 loss: 0.2716772246360779\n",
      "  batch 750 loss: 0.24292800888419153\n",
      "  batch 800 loss: 0.26535418346524237\n",
      "  batch 850 loss: 0.2642196016013622\n",
      "  batch 900 loss: 0.2787469917535782\n",
      "LOSS train 0.27875 valid 1.06467, valid PER 26.08%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.20047225743532182\n",
      "  batch 100 loss: 0.17946783378720282\n",
      "  batch 150 loss: 0.2031548549234867\n",
      "  batch 200 loss: 0.20954190731048583\n",
      "  batch 250 loss: 0.19308802470564843\n",
      "  batch 300 loss: 0.21729643493890763\n",
      "  batch 350 loss: 0.21805924981832503\n",
      "  batch 400 loss: 0.21618664801120757\n",
      "  batch 450 loss: 0.236682146191597\n",
      "  batch 500 loss: 0.2328096231818199\n",
      "  batch 550 loss: 0.2272533057630062\n",
      "  batch 600 loss: 0.2259454160928726\n",
      "  batch 650 loss: 0.26782227218151095\n",
      "  batch 700 loss: 0.23274531707167626\n",
      "  batch 750 loss: 0.22193459525704384\n",
      "  batch 800 loss: 0.25369201570749284\n",
      "  batch 850 loss: 0.24743265450000762\n",
      "  batch 900 loss: 0.25416613012552264\n",
      "LOSS train 0.25417 valid 1.10900, valid PER 25.91%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.18510306313633917\n",
      "  batch 100 loss: 0.18864389017224312\n",
      "  batch 150 loss: 0.18461412325501442\n",
      "  batch 200 loss: 0.18805356532335282\n",
      "  batch 250 loss: 0.1781087762117386\n",
      "  batch 300 loss: 0.20694171756505966\n",
      "  batch 350 loss: 0.17918050944805144\n",
      "  batch 400 loss: 0.19799350395798684\n",
      "  batch 450 loss: 0.20360680311918258\n",
      "  batch 500 loss: 0.19351622059941292\n",
      "  batch 550 loss: 0.21783947587013244\n",
      "  batch 600 loss: 0.20705228820443153\n",
      "  batch 650 loss: 0.2227969554066658\n",
      "  batch 700 loss: 0.2072782975435257\n",
      "  batch 750 loss: 0.21145761772990226\n",
      "  batch 800 loss: 0.22144251063466072\n",
      "  batch 850 loss: 0.21580933436751365\n",
      "  batch 900 loss: 0.2279286676645279\n",
      "LOSS train 0.22793 valid 1.13510, valid PER 26.20%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_154549/model_10\n",
      "Currently using dropout rate of 0.1\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.677292766571045\n",
      "  batch 100 loss: 3.0253481912612914\n",
      "  batch 150 loss: 2.696320819854736\n",
      "  batch 200 loss: 2.422658078670502\n",
      "  batch 250 loss: 2.242210719585419\n",
      "  batch 300 loss: 2.0210547065734863\n",
      "  batch 350 loss: 1.8927707934379578\n",
      "  batch 400 loss: 1.883079364299774\n",
      "  batch 450 loss: 1.7927788805961609\n",
      "  batch 500 loss: 1.7080744361877442\n",
      "  batch 550 loss: 1.6584387636184692\n",
      "  batch 600 loss: 1.5996420621871947\n",
      "  batch 650 loss: 1.5611148762702942\n",
      "  batch 700 loss: 1.5602725410461427\n",
      "  batch 750 loss: 1.5115033268928528\n",
      "  batch 800 loss: 1.5252681708335876\n",
      "  batch 850 loss: 1.4902990937232972\n",
      "  batch 900 loss: 1.4331595921516418\n",
      "LOSS train 1.43316 valid 1.41095, valid PER 45.67%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.371708414554596\n",
      "  batch 100 loss: 1.3398526978492737\n",
      "  batch 150 loss: 1.2950312793254852\n",
      "  batch 200 loss: 1.3689155650138856\n",
      "  batch 250 loss: 1.3322941637039185\n",
      "  batch 300 loss: 1.3129347491264343\n",
      "  batch 350 loss: 1.244950590133667\n",
      "  batch 400 loss: 1.275435621738434\n",
      "  batch 450 loss: 1.2283386158943177\n",
      "  batch 500 loss: 1.2750217461585998\n",
      "  batch 550 loss: 1.2483042252063752\n",
      "  batch 600 loss: 1.2478803074359894\n",
      "  batch 650 loss: 1.2435609686374665\n",
      "  batch 700 loss: 1.2169045460224153\n",
      "  batch 750 loss: 1.2142456901073455\n",
      "  batch 800 loss: 1.1364599204063415\n",
      "  batch 850 loss: 1.1662099826335908\n",
      "  batch 900 loss: 1.1937216472625733\n",
      "LOSS train 1.19372 valid 1.17155, valid PER 37.17%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.13697509765625\n",
      "  batch 100 loss: 1.110138508081436\n",
      "  batch 150 loss: 1.1067726588249207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 1.1009695327281952\n",
      "  batch 250 loss: 1.0978144598007202\n",
      "  batch 300 loss: 1.0664593660831452\n",
      "  batch 350 loss: 1.126047250032425\n",
      "  batch 400 loss: 1.0835916411876678\n",
      "  batch 450 loss: 1.0731342446804046\n",
      "  batch 500 loss: 1.0597980511188507\n",
      "  batch 550 loss: 1.0891008639335633\n",
      "  batch 600 loss: 1.1131614124774933\n",
      "  batch 650 loss: 1.0344914877414704\n",
      "  batch 700 loss: 1.0623319482803344\n",
      "  batch 750 loss: 1.0972160494327545\n",
      "  batch 800 loss: 1.0577577531337738\n",
      "  batch 850 loss: 1.0749890291690827\n",
      "  batch 900 loss: 1.0056093621253968\n",
      "LOSS train 1.00561 valid 1.06972, valid PER 33.14%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.9784512102603913\n",
      "  batch 100 loss: 0.9996351313591003\n",
      "  batch 150 loss: 0.9614565968513489\n",
      "  batch 200 loss: 1.0038666474819182\n",
      "  batch 250 loss: 1.012203893661499\n",
      "  batch 300 loss: 1.0148096644878388\n",
      "  batch 350 loss: 0.9487750542163849\n",
      "  batch 400 loss: 0.9766521227359771\n",
      "  batch 450 loss: 0.960549201965332\n",
      "  batch 500 loss: 0.9568304455280304\n",
      "  batch 550 loss: 0.9958623540401459\n",
      "  batch 600 loss: 0.9958238589763642\n",
      "  batch 650 loss: 0.9709369254112243\n",
      "  batch 700 loss: 0.9532993602752685\n",
      "  batch 750 loss: 0.9255573928356171\n",
      "  batch 800 loss: 0.9007041442394257\n",
      "  batch 850 loss: 0.9425188481807709\n",
      "  batch 900 loss: 0.967363897562027\n",
      "LOSS train 0.96736 valid 0.98815, valid PER 30.18%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.8767261147499085\n",
      "  batch 100 loss: 0.8764622676372528\n",
      "  batch 150 loss: 0.9448256850242615\n",
      "  batch 200 loss: 0.8536838710308075\n",
      "  batch 250 loss: 0.8767726624011993\n",
      "  batch 300 loss: 0.8992082214355469\n",
      "  batch 350 loss: 0.8929450750350952\n",
      "  batch 400 loss: 0.9132500791549683\n",
      "  batch 450 loss: 0.8782840657234192\n",
      "  batch 500 loss: 0.9269807708263397\n",
      "  batch 550 loss: 0.8530035388469696\n",
      "  batch 600 loss: 0.9116131269931793\n",
      "  batch 650 loss: 0.8872530889511109\n",
      "  batch 700 loss: 0.9141863322257996\n",
      "  batch 750 loss: 0.8573132765293121\n",
      "  batch 800 loss: 0.8818198907375335\n",
      "  batch 850 loss: 0.87348672747612\n",
      "  batch 900 loss: 0.9070316886901856\n",
      "LOSS train 0.90703 valid 0.95413, valid PER 30.06%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8372855162620545\n",
      "  batch 100 loss: 0.7989292979240418\n",
      "  batch 150 loss: 0.7941385161876678\n",
      "  batch 200 loss: 0.8070316791534424\n",
      "  batch 250 loss: 0.8396895909309388\n",
      "  batch 300 loss: 0.8134771293401718\n",
      "  batch 350 loss: 0.8227473533153534\n",
      "  batch 400 loss: 0.8135365331172943\n",
      "  batch 450 loss: 0.8139925456047058\n",
      "  batch 500 loss: 0.8199905562400818\n",
      "  batch 550 loss: 0.838157262802124\n",
      "  batch 600 loss: 0.8029343330860138\n",
      "  batch 650 loss: 0.8299856662750245\n",
      "  batch 700 loss: 0.8149662590026856\n",
      "  batch 750 loss: 0.8119496381282807\n",
      "  batch 800 loss: 0.8032019317150116\n",
      "  batch 850 loss: 0.7885567712783813\n",
      "  batch 900 loss: 0.83060506939888\n",
      "LOSS train 0.83061 valid 0.92970, valid PER 28.51%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.7708817064762116\n",
      "  batch 100 loss: 0.7604849159717559\n",
      "  batch 150 loss: 0.720920432806015\n",
      "  batch 200 loss: 0.7463987076282501\n",
      "  batch 250 loss: 0.7378894156217575\n",
      "  batch 300 loss: 0.7381620657444\n",
      "  batch 350 loss: 0.7436799025535583\n",
      "  batch 400 loss: 0.7585627388954163\n",
      "  batch 450 loss: 0.7573104739189148\n",
      "  batch 500 loss: 0.7450984048843384\n",
      "  batch 550 loss: 0.7432233428955078\n",
      "  batch 600 loss: 0.7587068974971771\n",
      "  batch 650 loss: 0.7345897316932678\n",
      "  batch 700 loss: 0.796882518529892\n",
      "  batch 750 loss: 0.7676327013969422\n",
      "  batch 800 loss: 0.7545252132415772\n",
      "  batch 850 loss: 0.7629233491420746\n",
      "  batch 900 loss: 0.7843929946422576\n",
      "LOSS train 0.78439 valid 0.91699, valid PER 28.27%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.680667062997818\n",
      "  batch 100 loss: 0.684210114479065\n",
      "  batch 150 loss: 0.7006908988952637\n",
      "  batch 200 loss: 0.6853397536277771\n",
      "  batch 250 loss: 0.7116660737991333\n",
      "  batch 300 loss: 0.6560006541013718\n",
      "  batch 350 loss: 0.7373030632734299\n",
      "  batch 400 loss: 0.6685884118080139\n",
      "  batch 450 loss: 0.7161515283584595\n",
      "  batch 500 loss: 0.7271123200654983\n",
      "  batch 550 loss: 0.6899395042657852\n",
      "  batch 600 loss: 0.7209761732816696\n",
      "  batch 650 loss: 0.7304452031850814\n",
      "  batch 700 loss: 0.6829057466983796\n",
      "  batch 750 loss: 0.7210973715782165\n",
      "  batch 800 loss: 0.7246504771709442\n",
      "  batch 850 loss: 0.7109041476249695\n",
      "  batch 900 loss: 0.7368953716754914\n",
      "LOSS train 0.73690 valid 0.87969, valid PER 27.38%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.5968099898099899\n",
      "  batch 100 loss: 0.6250531429052353\n",
      "  batch 150 loss: 0.6463788002729416\n",
      "  batch 200 loss: 0.6231003779172898\n",
      "  batch 250 loss: 0.654932935833931\n",
      "  batch 300 loss: 0.6715542608499527\n",
      "  batch 350 loss: 0.6805594837665558\n",
      "  batch 400 loss: 0.6366966867446899\n",
      "  batch 450 loss: 0.6508051747083664\n",
      "  batch 500 loss: 0.6385592085123062\n",
      "  batch 550 loss: 0.652769103050232\n",
      "  batch 600 loss: 0.686843044757843\n",
      "  batch 650 loss: 0.6652562683820724\n",
      "  batch 700 loss: 0.6537121278047562\n",
      "  batch 750 loss: 0.66519227206707\n",
      "  batch 800 loss: 0.6743987393379212\n",
      "  batch 850 loss: 0.691841003894806\n",
      "  batch 900 loss: 0.636160494685173\n",
      "LOSS train 0.63616 valid 0.87553, valid PER 26.67%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.556203031539917\n",
      "  batch 100 loss: 0.5634336876869201\n",
      "  batch 150 loss: 0.5907641327381135\n",
      "  batch 200 loss: 0.6200307959318161\n",
      "  batch 250 loss: 0.6059679764509202\n",
      "  batch 300 loss: 0.5843272590637207\n",
      "  batch 350 loss: 0.6001040595769882\n",
      "  batch 400 loss: 0.5635883975028991\n",
      "  batch 450 loss: 0.5868611770868302\n",
      "  batch 500 loss: 0.6196943515539169\n",
      "  batch 550 loss: 0.6410121494531631\n",
      "  batch 600 loss: 0.6047123396396636\n",
      "  batch 650 loss: 0.5884403830766678\n",
      "  batch 700 loss: 0.615963631272316\n",
      "  batch 750 loss: 0.6099194759130477\n",
      "  batch 800 loss: 0.6215080857276917\n",
      "  batch 850 loss: 0.6392031466960907\n",
      "  batch 900 loss: 0.6392268580198288\n",
      "LOSS train 0.63923 valid 0.86813, valid PER 26.14%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.5240955853462219\n",
      "  batch 100 loss: 0.5064459359645843\n",
      "  batch 150 loss: 0.5106245937943459\n",
      "  batch 200 loss: 0.5571811491250992\n",
      "  batch 250 loss: 0.561576001048088\n",
      "  batch 300 loss: 0.5240092879533768\n",
      "  batch 350 loss: 0.5517341148853302\n",
      "  batch 400 loss: 0.5634124904870987\n",
      "  batch 450 loss: 0.556715481877327\n",
      "  batch 500 loss: 0.5523475766181946\n",
      "  batch 550 loss: 0.554039037823677\n",
      "  batch 600 loss: 0.5552160042524338\n",
      "  batch 650 loss: 0.6018767863512039\n",
      "  batch 700 loss: 0.5536065781116486\n",
      "  batch 750 loss: 0.57589251101017\n",
      "  batch 800 loss: 0.594884580373764\n",
      "  batch 850 loss: 0.6035828536748886\n",
      "  batch 900 loss: 0.5962228345870971\n",
      "LOSS train 0.59622 valid 0.89075, valid PER 25.94%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.49944413363933565\n",
      "  batch 100 loss: 0.4857180792093277\n",
      "  batch 150 loss: 0.44028247833251954\n",
      "  batch 200 loss: 0.4771397095918655\n",
      "  batch 250 loss: 0.496040506362915\n",
      "  batch 300 loss: 0.502931513786316\n",
      "  batch 350 loss: 0.5054644823074341\n",
      "  batch 400 loss: 0.5375900769233704\n",
      "  batch 450 loss: 0.5419165772199631\n",
      "  batch 500 loss: 0.544814429283142\n",
      "  batch 550 loss: 0.5192062562704086\n",
      "  batch 600 loss: 0.5311457401514054\n",
      "  batch 650 loss: 0.5385499399900436\n",
      "  batch 700 loss: 0.5353363579511643\n",
      "  batch 750 loss: 0.5125090217590332\n",
      "  batch 800 loss: 0.5128744554519653\n",
      "  batch 850 loss: 0.5834105163812637\n",
      "  batch 900 loss: 0.5380744463205338\n",
      "LOSS train 0.53807 valid 0.87817, valid PER 25.87%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.438627051115036\n",
      "  batch 100 loss: 0.42681398659944536\n",
      "  batch 150 loss: 0.44481598973274233\n",
      "  batch 200 loss: 0.4723069155216217\n",
      "  batch 250 loss: 0.4470989751815796\n",
      "  batch 300 loss: 0.453661048412323\n",
      "  batch 350 loss: 0.447535086274147\n",
      "  batch 400 loss: 0.468082509636879\n",
      "  batch 450 loss: 0.46415660321712493\n",
      "  batch 500 loss: 0.44947098314762113\n",
      "  batch 550 loss: 0.4908443981409073\n",
      "  batch 600 loss: 0.4913563740253448\n",
      "  batch 650 loss: 0.48190561413764954\n",
      "  batch 700 loss: 0.49694969117641447\n",
      "  batch 750 loss: 0.4567362266778946\n",
      "  batch 800 loss: 0.47419905841350557\n",
      "  batch 850 loss: 0.5121823585033417\n",
      "  batch 900 loss: 0.5070918023586273\n",
      "LOSS train 0.50709 valid 0.88568, valid PER 25.54%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.3985530763864517\n",
      "  batch 100 loss: 0.40740869075059893\n",
      "  batch 150 loss: 0.405221461057663\n",
      "  batch 200 loss: 0.39274022728204727\n",
      "  batch 250 loss: 0.4119740915298462\n",
      "  batch 300 loss: 0.4421357077360153\n",
      "  batch 350 loss: 0.40388963371515274\n",
      "  batch 400 loss: 0.4223633834719658\n",
      "  batch 450 loss: 0.4198414611816406\n",
      "  batch 500 loss: 0.42451312601566316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 550 loss: 0.4401443070173264\n",
      "  batch 600 loss: 0.41891752421855927\n",
      "  batch 650 loss: 0.457554806470871\n",
      "  batch 700 loss: 0.4616650241613388\n",
      "  batch 750 loss: 0.44000388860702516\n",
      "  batch 800 loss: 0.43410336434841157\n",
      "  batch 850 loss: 0.45881778419017794\n",
      "  batch 900 loss: 0.4639658087491989\n",
      "LOSS train 0.46397 valid 0.93985, valid PER 26.06%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.3623331347107887\n",
      "  batch 100 loss: 0.370635105073452\n",
      "  batch 150 loss: 0.37327993154525757\n",
      "  batch 200 loss: 0.3916913205385208\n",
      "  batch 250 loss: 0.4001465755701065\n",
      "  batch 300 loss: 0.36505696713924407\n",
      "  batch 350 loss: 0.4057618850469589\n",
      "  batch 400 loss: 0.3949092161655426\n",
      "  batch 450 loss: 0.3913865712285042\n",
      "  batch 500 loss: 0.3921893438696861\n",
      "  batch 550 loss: 0.3975871765613556\n",
      "  batch 600 loss: 0.40130920588970187\n",
      "  batch 650 loss: 0.4351838484406471\n",
      "  batch 700 loss: 0.4321661052107811\n",
      "  batch 750 loss: 0.4383930063247681\n",
      "  batch 800 loss: 0.4126331177353859\n",
      "  batch 850 loss: 0.39263334482908246\n",
      "  batch 900 loss: 0.42292784214019774\n",
      "LOSS train 0.42293 valid 0.98030, valid PER 25.96%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.36849477648735046\n",
      "  batch 100 loss: 0.3508715781569481\n",
      "  batch 150 loss: 0.34908271551132203\n",
      "  batch 200 loss: 0.3576948615908623\n",
      "  batch 250 loss: 0.379207464158535\n",
      "  batch 300 loss: 0.3591753485798836\n",
      "  batch 350 loss: 0.3705831104516983\n",
      "  batch 400 loss: 0.37371553778648375\n",
      "  batch 450 loss: 0.37762994557619095\n",
      "  batch 500 loss: 0.3531310388445854\n",
      "  batch 550 loss: 0.37456620097160337\n",
      "  batch 600 loss: 0.36670836567878723\n",
      "  batch 650 loss: 0.39939806967973707\n",
      "  batch 700 loss: 0.38112209260463714\n",
      "  batch 750 loss: 0.38421415477991105\n",
      "  batch 800 loss: 0.40717845141887665\n",
      "  batch 850 loss: 0.37748871117830274\n",
      "  batch 900 loss: 0.4051629555225372\n",
      "LOSS train 0.40516 valid 0.96696, valid PER 25.15%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.2872371140122414\n",
      "  batch 100 loss: 0.29298309087753294\n",
      "  batch 150 loss: 0.30144536077976225\n",
      "  batch 200 loss: 0.3255565908551216\n",
      "  batch 250 loss: 0.3247047010064125\n",
      "  batch 300 loss: 0.3303887188434601\n",
      "  batch 350 loss: 0.32155703872442243\n",
      "  batch 400 loss: 0.3457020512223244\n",
      "  batch 450 loss: 0.3270093500614166\n",
      "  batch 500 loss: 0.3339351963996887\n",
      "  batch 550 loss: 0.3282094010710716\n",
      "  batch 600 loss: 0.352754562497139\n",
      "  batch 650 loss: 0.3504973915219307\n",
      "  batch 700 loss: 0.34135503351688384\n",
      "  batch 750 loss: 0.33358946174383164\n",
      "  batch 800 loss: 0.35178386360406877\n",
      "  batch 850 loss: 0.3776016339659691\n",
      "  batch 900 loss: 0.3465974819660187\n",
      "LOSS train 0.34660 valid 1.00329, valid PER 26.02%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.2790084859728813\n",
      "  batch 100 loss: 0.2846022889018059\n",
      "  batch 150 loss: 0.3067880854010582\n",
      "  batch 200 loss: 0.3185015207529068\n",
      "  batch 250 loss: 0.302924228310585\n",
      "  batch 300 loss: 0.31115474849939345\n",
      "  batch 350 loss: 0.30412479758262634\n",
      "  batch 400 loss: 0.30319435805082323\n",
      "  batch 450 loss: 0.3077507999539375\n",
      "  batch 500 loss: 0.3172193092107773\n",
      "  batch 550 loss: 0.3183553636074066\n",
      "  batch 600 loss: 0.30583849757909776\n",
      "  batch 650 loss: 0.29248261988162993\n",
      "  batch 700 loss: 0.3252733331918716\n",
      "  batch 750 loss: 0.3275332373380661\n",
      "  batch 800 loss: 0.327214777469635\n",
      "  batch 850 loss: 0.3397071212530136\n",
      "  batch 900 loss: 0.34481184870004655\n",
      "LOSS train 0.34481 valid 1.01290, valid PER 25.32%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.2554043586552143\n",
      "  batch 100 loss: 0.24465934038162232\n",
      "  batch 150 loss: 0.2531004303693771\n",
      "  batch 200 loss: 0.2695933499932289\n",
      "  batch 250 loss: 0.26926917642354964\n",
      "  batch 300 loss: 0.2857535210251808\n",
      "  batch 350 loss: 0.27501495569944384\n",
      "  batch 400 loss: 0.2800416323542595\n",
      "  batch 450 loss: 0.3000690430402756\n",
      "  batch 500 loss: 0.2899800431728363\n",
      "  batch 550 loss: 0.2844308438897133\n",
      "  batch 600 loss: 0.27226333022117616\n",
      "  batch 650 loss: 0.3077634942531586\n",
      "  batch 700 loss: 0.2916963428258896\n",
      "  batch 750 loss: 0.2885870385169983\n",
      "  batch 800 loss: 0.31987888664007186\n",
      "  batch 850 loss: 0.31480670779943465\n",
      "  batch 900 loss: 0.3135269936919212\n",
      "LOSS train 0.31353 valid 1.02990, valid PER 24.96%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.23361657470464706\n",
      "  batch 100 loss: 0.2320969980955124\n",
      "  batch 150 loss: 0.22600497499108316\n",
      "  batch 200 loss: 0.2613592544198036\n",
      "  batch 250 loss: 0.25188645914196967\n",
      "  batch 300 loss: 0.26756548032164573\n",
      "  batch 350 loss: 0.23468504674732685\n",
      "  batch 400 loss: 0.2613691094517708\n",
      "  batch 450 loss: 0.26039656266570094\n",
      "  batch 500 loss: 0.24972990810871123\n",
      "  batch 550 loss: 0.29127977669239047\n",
      "  batch 600 loss: 0.2544152393937111\n",
      "  batch 650 loss: 0.2852706244587898\n",
      "  batch 700 loss: 0.28220599591732026\n",
      "  batch 750 loss: 0.2616529804468155\n",
      "  batch 800 loss: 0.2876101306080818\n",
      "  batch 850 loss: 0.2749058416485786\n",
      "  batch 900 loss: 0.3002305571734905\n",
      "LOSS train 0.30023 valid 1.06867, valid PER 25.10%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_155416/model_10\n",
      "Currently using dropout rate of 0.2\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.7616702795028685\n",
      "  batch 100 loss: 3.0659934091567993\n",
      "  batch 150 loss: 2.7866341924667357\n",
      "  batch 200 loss: 2.534011445045471\n",
      "  batch 250 loss: 2.3387869071960448\n",
      "  batch 300 loss: 2.0789803290367126\n",
      "  batch 350 loss: 1.9181069135665894\n",
      "  batch 400 loss: 1.9003064560890197\n",
      "  batch 450 loss: 1.8059785509109496\n",
      "  batch 500 loss: 1.7178811430931091\n",
      "  batch 550 loss: 1.6619313883781432\n",
      "  batch 600 loss: 1.6180374836921692\n",
      "  batch 650 loss: 1.5585137104988098\n",
      "  batch 700 loss: 1.5545879197120667\n",
      "  batch 750 loss: 1.4998223209381103\n",
      "  batch 800 loss: 1.4945924162864686\n",
      "  batch 850 loss: 1.445406415462494\n",
      "  batch 900 loss: 1.418414418697357\n",
      "LOSS train 1.41841 valid 1.38128, valid PER 45.88%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.3625785052776336\n",
      "  batch 100 loss: 1.3354505205154419\n",
      "  batch 150 loss: 1.2899180519580842\n",
      "  batch 200 loss: 1.3582868719100951\n",
      "  batch 250 loss: 1.3309106886386872\n",
      "  batch 300 loss: 1.2917059016227723\n",
      "  batch 350 loss: 1.246201285123825\n",
      "  batch 400 loss: 1.2662839424610137\n",
      "  batch 450 loss: 1.2276590800285339\n",
      "  batch 500 loss: 1.2576244246959687\n",
      "  batch 550 loss: 1.2620637762546538\n",
      "  batch 600 loss: 1.199387676715851\n",
      "  batch 650 loss: 1.2472769236564636\n",
      "  batch 700 loss: 1.221045082807541\n",
      "  batch 750 loss: 1.240670793056488\n",
      "  batch 800 loss: 1.1508986020088197\n",
      "  batch 850 loss: 1.1864315557479859\n",
      "  batch 900 loss: 1.2010201191902161\n",
      "LOSS train 1.20102 valid 1.16914, valid PER 37.88%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.1583611524105073\n",
      "  batch 100 loss: 1.113206946849823\n",
      "  batch 150 loss: 1.117079781293869\n",
      "  batch 200 loss: 1.1211376309394836\n",
      "  batch 250 loss: 1.083729157447815\n",
      "  batch 300 loss: 1.109984076023102\n",
      "  batch 350 loss: 1.130624941587448\n",
      "  batch 400 loss: 1.0899810230731963\n",
      "  batch 450 loss: 1.0831039464473724\n",
      "  batch 500 loss: 1.0713557088375092\n",
      "  batch 550 loss: 1.1035760831832886\n",
      "  batch 600 loss: 1.0410939860343933\n",
      "  batch 650 loss: 1.0528780114650726\n",
      "  batch 700 loss: 1.0945057106018066\n",
      "  batch 750 loss: 1.111971026659012\n",
      "  batch 800 loss: 1.0413307797908784\n",
      "  batch 850 loss: 1.0901584243774414\n",
      "  batch 900 loss: 1.0315954113006591\n",
      "LOSS train 1.03160 valid 1.11416, valid PER 34.81%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0060095524787902\n",
      "  batch 100 loss: 1.0054311871528625\n",
      "  batch 150 loss: 0.9805507457256317\n",
      "  batch 200 loss: 1.0150471580028535\n",
      "  batch 250 loss: 1.0284674167633057\n",
      "  batch 300 loss: 1.002838786840439\n",
      "  batch 350 loss: 0.9496855545043945\n",
      "  batch 400 loss: 0.9946167623996734\n",
      "  batch 450 loss: 0.9766181254386902\n",
      "  batch 500 loss: 0.9499833142757416\n",
      "  batch 550 loss: 1.0162460672855378\n",
      "  batch 600 loss: 1.0044545090198518\n",
      "  batch 650 loss: 0.984364902973175\n",
      "  batch 700 loss: 0.9924086487293243\n",
      "  batch 750 loss: 0.9544500946998596\n",
      "  batch 800 loss: 0.934699934720993\n",
      "  batch 850 loss: 0.9896399700641632\n",
      "  batch 900 loss: 0.9943002068996429\n",
      "LOSS train 0.99430 valid 0.99702, valid PER 32.25%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.8908241581916809\n",
      "  batch 100 loss: 0.888501843214035\n",
      "  batch 150 loss: 0.9544343888759613\n",
      "  batch 200 loss: 0.8632392311096191\n",
      "  batch 250 loss: 0.881355369091034\n",
      "  batch 300 loss: 0.8930426907539367\n",
      "  batch 350 loss: 0.8823714768886566\n",
      "  batch 400 loss: 0.8885000479221344\n",
      "  batch 450 loss: 0.8758567118644714\n",
      "  batch 500 loss: 0.917592453956604\n",
      "  batch 550 loss: 0.849247715473175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 600 loss: 0.9399257600307465\n",
      "  batch 650 loss: 0.9166931736469269\n",
      "  batch 700 loss: 0.9380678427219391\n",
      "  batch 750 loss: 0.8745142185688018\n",
      "  batch 800 loss: 0.8929941749572754\n",
      "  batch 850 loss: 0.889417519569397\n",
      "  batch 900 loss: 0.9058685731887818\n",
      "LOSS train 0.90587 valid 0.98061, valid PER 31.34%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8558193945884705\n",
      "  batch 100 loss: 0.808549028635025\n",
      "  batch 150 loss: 0.7946283864974976\n",
      "  batch 200 loss: 0.8170462894439697\n",
      "  batch 250 loss: 0.8496098268032074\n",
      "  batch 300 loss: 0.8247335481643677\n",
      "  batch 350 loss: 0.8310136151313782\n",
      "  batch 400 loss: 0.81220743060112\n",
      "  batch 450 loss: 0.8423798310756684\n",
      "  batch 500 loss: 0.8253158605098725\n",
      "  batch 550 loss: 0.8691041207313538\n",
      "  batch 600 loss: 0.8117052364349365\n",
      "  batch 650 loss: 0.8599151539802551\n",
      "  batch 700 loss: 0.8403218531608582\n",
      "  batch 750 loss: 0.8203808391094207\n",
      "  batch 800 loss: 0.8105385470390319\n",
      "  batch 850 loss: 0.8106956112384797\n",
      "  batch 900 loss: 0.8315553915500641\n",
      "LOSS train 0.83156 valid 0.94027, valid PER 29.65%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.7849358868598938\n",
      "  batch 100 loss: 0.7628773242235184\n",
      "  batch 150 loss: 0.7629348158836364\n",
      "  batch 200 loss: 0.7485218918323517\n",
      "  batch 250 loss: 0.7373828786611557\n",
      "  batch 300 loss: 0.7722855746746063\n",
      "  batch 350 loss: 0.7583804428577423\n",
      "  batch 400 loss: 0.7792713487148285\n",
      "  batch 450 loss: 0.7887424945831298\n",
      "  batch 500 loss: 0.7651957511901856\n",
      "  batch 550 loss: 0.7592340445518494\n",
      "  batch 600 loss: 0.7618767011165619\n",
      "  batch 650 loss: 0.7503186786174774\n",
      "  batch 700 loss: 0.7778315436840058\n",
      "  batch 750 loss: 0.7690110766887664\n",
      "  batch 800 loss: 0.7670947915315628\n",
      "  batch 850 loss: 0.7817920243740082\n",
      "  batch 900 loss: 0.7923947632312774\n",
      "LOSS train 0.79239 valid 0.90869, valid PER 29.24%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.671139907836914\n",
      "  batch 100 loss: 0.6779355806112289\n",
      "  batch 150 loss: 0.6988960611820221\n",
      "  batch 200 loss: 0.6991709834337234\n",
      "  batch 250 loss: 0.7001290565729141\n",
      "  batch 300 loss: 0.6758671855926514\n",
      "  batch 350 loss: 0.7396900582313538\n",
      "  batch 400 loss: 0.6995384532213211\n",
      "  batch 450 loss: 0.7190337628126144\n",
      "  batch 500 loss: 0.7439865058660508\n",
      "  batch 550 loss: 0.697460725903511\n",
      "  batch 600 loss: 0.7598528361320496\n",
      "  batch 650 loss: 0.7404167318344116\n",
      "  batch 700 loss: 0.6948867869377137\n",
      "  batch 750 loss: 0.7090265518426895\n",
      "  batch 800 loss: 0.7336646664142609\n",
      "  batch 850 loss: 0.7197906625270843\n",
      "  batch 900 loss: 0.7122944080829621\n",
      "LOSS train 0.71229 valid 0.90272, valid PER 27.92%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.6139872533082962\n",
      "  batch 100 loss: 0.6405360800027847\n",
      "  batch 150 loss: 0.6794475704431534\n",
      "  batch 200 loss: 0.6255403327941894\n",
      "  batch 250 loss: 0.6609916257858276\n",
      "  batch 300 loss: 0.6688976776599884\n",
      "  batch 350 loss: 0.6775043177604675\n",
      "  batch 400 loss: 0.6437586945295334\n",
      "  batch 450 loss: 0.6533852452039719\n",
      "  batch 500 loss: 0.6658860224485398\n",
      "  batch 550 loss: 0.6866135621070861\n",
      "  batch 600 loss: 0.6741263484954834\n",
      "  batch 650 loss: 0.6848379278182983\n",
      "  batch 700 loss: 0.6449684393405914\n",
      "  batch 750 loss: 0.6576376307010651\n",
      "  batch 800 loss: 0.6794384336471557\n",
      "  batch 850 loss: 0.6869993311166763\n",
      "  batch 900 loss: 0.6737057489156723\n",
      "LOSS train 0.67371 valid 0.88694, valid PER 27.45%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.5708148050308227\n",
      "  batch 100 loss: 0.5931007999181748\n",
      "  batch 150 loss: 0.6179008603096008\n",
      "  batch 200 loss: 0.6182041889429093\n",
      "  batch 250 loss: 0.6263402205705643\n",
      "  batch 300 loss: 0.606660783290863\n",
      "  batch 350 loss: 0.6380367892980575\n",
      "  batch 400 loss: 0.5953799355030059\n",
      "  batch 450 loss: 0.5999078339338303\n",
      "  batch 500 loss: 0.6287264168262482\n",
      "  batch 550 loss: 0.6513573408126831\n",
      "  batch 600 loss: 0.620859967470169\n",
      "  batch 650 loss: 0.621886419057846\n",
      "  batch 700 loss: 0.6354174011945725\n",
      "  batch 750 loss: 0.6310490906238556\n",
      "  batch 800 loss: 0.6364632725715638\n",
      "  batch 850 loss: 0.6486304533481598\n",
      "  batch 900 loss: 0.6569065713882446\n",
      "LOSS train 0.65691 valid 1.00679, valid PER 27.66%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.5684865772724151\n",
      "  batch 100 loss: 0.527938289642334\n",
      "  batch 150 loss: 0.5392658615112305\n",
      "  batch 200 loss: 0.593791759610176\n",
      "  batch 250 loss: 0.5811845731735229\n",
      "  batch 300 loss: 0.5510456240177155\n",
      "  batch 350 loss: 0.5801494139432907\n",
      "  batch 400 loss: 0.5910811764001846\n",
      "  batch 450 loss: 0.5868560707569123\n",
      "  batch 500 loss: 0.5866428518295288\n",
      "  batch 550 loss: 0.5893064051866531\n",
      "  batch 600 loss: 0.5741732162237168\n",
      "  batch 650 loss: 0.6551339226961136\n",
      "  batch 700 loss: 0.5639262861013412\n",
      "  batch 750 loss: 0.5931221920251847\n",
      "  batch 800 loss: 0.6071713161468506\n",
      "  batch 850 loss: 0.6085969722270965\n",
      "  batch 900 loss: 0.6057035499811172\n",
      "LOSS train 0.60570 valid 0.93285, valid PER 27.32%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.5192050635814667\n",
      "  batch 100 loss: 0.5039007830619812\n",
      "  batch 150 loss: 0.49672220706939696\n",
      "  batch 200 loss: 0.5023659491539001\n",
      "  batch 250 loss: 0.5221193063259125\n",
      "  batch 300 loss: 0.5178932547569275\n",
      "  batch 350 loss: 0.496977271437645\n",
      "  batch 400 loss: 0.5493338429927825\n",
      "  batch 450 loss: 0.5483104652166366\n",
      "  batch 500 loss: 0.5519108343124389\n",
      "  batch 550 loss: 0.51641361951828\n",
      "  batch 600 loss: 0.5331793570518494\n",
      "  batch 650 loss: 0.555661393404007\n",
      "  batch 700 loss: 0.5719701260328293\n",
      "  batch 750 loss: 0.5380978119373322\n",
      "  batch 800 loss: 0.5558736699819565\n",
      "  batch 850 loss: 0.587217720746994\n",
      "  batch 900 loss: 0.6054888987541198\n",
      "LOSS train 0.60549 valid 0.91925, valid PER 26.61%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.4790359127521515\n",
      "  batch 100 loss: 0.4801993456482887\n",
      "  batch 150 loss: 0.48347590029239657\n",
      "  batch 200 loss: 0.4771757137775421\n",
      "  batch 250 loss: 0.4982451993227005\n",
      "  batch 300 loss: 0.49900148272514344\n",
      "  batch 350 loss: 0.4977149611711502\n",
      "  batch 400 loss: 0.5052569401264191\n",
      "  batch 450 loss: 0.5053158766031265\n",
      "  batch 500 loss: 0.5165184170007706\n",
      "  batch 550 loss: 0.5136100393533707\n",
      "  batch 600 loss: 0.5022858560085297\n",
      "  batch 650 loss: 0.5151415389776229\n",
      "  batch 700 loss: 0.5344434773921967\n",
      "  batch 750 loss: 0.4857399922609329\n",
      "  batch 800 loss: 0.5096243703365326\n",
      "  batch 850 loss: 0.5453511846065521\n",
      "  batch 900 loss: 0.5183894401788711\n",
      "LOSS train 0.51839 valid 0.93480, valid PER 26.75%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.43829243957996367\n",
      "  batch 100 loss: 0.44522860527038577\n",
      "  batch 150 loss: 0.43216741949319837\n",
      "  batch 200 loss: 0.44043016493320464\n",
      "  batch 250 loss: 0.4642725658416748\n",
      "  batch 300 loss: 0.4955825990438461\n",
      "  batch 350 loss: 0.4254511833190918\n",
      "  batch 400 loss: 0.45922140777111053\n",
      "  batch 450 loss: 0.45542744159698484\n",
      "  batch 500 loss: 0.45866438090801237\n",
      "  batch 550 loss: 0.47028611540794374\n",
      "  batch 600 loss: 0.4556263166666031\n",
      "  batch 650 loss: 0.49794129431247713\n",
      "  batch 700 loss: 0.49920818090438845\n",
      "  batch 750 loss: 0.46825760185718535\n",
      "  batch 800 loss: 0.47178622215986254\n",
      "  batch 850 loss: 0.482750728726387\n",
      "  batch 900 loss: 0.48928984940052034\n",
      "LOSS train 0.48929 valid 0.93876, valid PER 26.80%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.40303644895553586\n",
      "  batch 100 loss: 0.4056831681728363\n",
      "  batch 150 loss: 0.4086452531814575\n",
      "  batch 200 loss: 0.41689964294433596\n",
      "  batch 250 loss: 0.46319699823856353\n",
      "  batch 300 loss: 0.40351521015167235\n",
      "  batch 350 loss: 0.42437214851379396\n",
      "  batch 400 loss: 0.4330237740278244\n",
      "  batch 450 loss: 0.42164168059825896\n",
      "  batch 500 loss: 0.4219962772727013\n",
      "  batch 550 loss: 0.44188795804977415\n",
      "  batch 600 loss: 0.44834972739219664\n",
      "  batch 650 loss: 0.4597252327203751\n",
      "  batch 700 loss: 0.46670009911060334\n",
      "  batch 750 loss: 0.4592969936132431\n",
      "  batch 800 loss: 0.4524879252910614\n",
      "  batch 850 loss: 0.4188166251778603\n",
      "  batch 900 loss: 0.4511528098583221\n",
      "LOSS train 0.45115 valid 0.97806, valid PER 26.68%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.39000629365444184\n",
      "  batch 100 loss: 0.38806803703308107\n",
      "  batch 150 loss: 0.38946724355220796\n",
      "  batch 200 loss: 0.3669762283563614\n",
      "  batch 250 loss: 0.3944183909893036\n",
      "  batch 300 loss: 0.3785757777094841\n",
      "  batch 350 loss: 0.4114170604944229\n",
      "  batch 400 loss: 0.40875783920288083\n",
      "  batch 450 loss: 0.42407562375068664\n",
      "  batch 500 loss: 0.3804046356678009\n",
      "  batch 550 loss: 0.4052983123064041\n",
      "  batch 600 loss: 0.40843592047691346\n",
      "  batch 650 loss: 0.4412796610593796\n",
      "  batch 700 loss: 0.3974874955415726\n",
      "  batch 750 loss: 0.41834027707576754\n",
      "  batch 800 loss: 0.42857435941696165\n",
      "  batch 850 loss: 0.4298136109113693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 900 loss: 0.4502281594276428\n",
      "LOSS train 0.45023 valid 0.96834, valid PER 26.26%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.35019983410835265\n",
      "  batch 100 loss: 0.3477883306145668\n",
      "  batch 150 loss: 0.345917287170887\n",
      "  batch 200 loss: 0.36272934824228287\n",
      "  batch 250 loss: 0.3991453218460083\n",
      "  batch 300 loss: 0.37132582426071165\n",
      "  batch 350 loss: 0.36983677059412\n",
      "  batch 400 loss: 0.4002676439285278\n",
      "  batch 450 loss: 0.3858865350484848\n",
      "  batch 500 loss: 0.3746282762289047\n",
      "  batch 550 loss: 0.36956184208393095\n",
      "  batch 600 loss: 0.4082554429769516\n",
      "  batch 650 loss: 0.39082739055156707\n",
      "  batch 700 loss: 0.37902988731861115\n",
      "  batch 750 loss: 0.3884440550208092\n",
      "  batch 800 loss: 0.380169552564621\n",
      "  batch 850 loss: 0.4077716556191444\n",
      "  batch 900 loss: 0.39282498627901075\n",
      "LOSS train 0.39282 valid 0.99293, valid PER 26.26%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.31030650705099105\n",
      "  batch 100 loss: 0.33463876098394396\n",
      "  batch 150 loss: 0.3307109159231186\n",
      "  batch 200 loss: 0.3446583357453346\n",
      "  batch 250 loss: 0.35966183423995973\n",
      "  batch 300 loss: 0.31037368685007094\n",
      "  batch 350 loss: 0.34999666780233385\n",
      "  batch 400 loss: 0.35969180285930635\n",
      "  batch 450 loss: 0.3705568724870682\n",
      "  batch 500 loss: 0.369485564827919\n",
      "  batch 550 loss: 0.39466777235269546\n",
      "  batch 600 loss: 0.363199508190155\n",
      "  batch 650 loss: 0.3584749048948288\n",
      "  batch 700 loss: 0.3799092349410057\n",
      "  batch 750 loss: 0.37727746725082395\n",
      "  batch 800 loss: 0.36418500781059265\n",
      "  batch 850 loss: 0.3742553055286407\n",
      "  batch 900 loss: 0.366924911737442\n",
      "LOSS train 0.36692 valid 1.01026, valid PER 26.14%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.30962181627750396\n",
      "  batch 100 loss: 0.30131822764873506\n",
      "  batch 150 loss: 0.3325753653049469\n",
      "  batch 200 loss: 0.3326258456707001\n",
      "  batch 250 loss: 0.33214908063411713\n",
      "  batch 300 loss: 0.34642905861139295\n",
      "  batch 350 loss: 0.3370133164525032\n",
      "  batch 400 loss: 0.3387162432074547\n",
      "  batch 450 loss: 0.3702493071556091\n",
      "  batch 500 loss: 0.3544946587085724\n",
      "  batch 550 loss: 0.36365444630384447\n",
      "  batch 600 loss: 0.366566546857357\n",
      "  batch 650 loss: 0.3891857349872589\n",
      "  batch 700 loss: 0.3586993706226349\n",
      "  batch 750 loss: 0.34454174399375914\n",
      "  batch 800 loss: 0.35782170623540877\n",
      "  batch 850 loss: 0.36905778497457503\n",
      "  batch 900 loss: 0.37239127963781354\n",
      "LOSS train 0.37239 valid 1.02490, valid PER 26.13%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.2968020474910736\n",
      "  batch 100 loss: 0.294035297036171\n",
      "  batch 150 loss: 0.28362086564302447\n",
      "  batch 200 loss: 0.2943006110191345\n",
      "  batch 250 loss: 0.31590574473142624\n",
      "  batch 300 loss: 0.3264751920104027\n",
      "  batch 350 loss: 0.3210513305664062\n",
      "  batch 400 loss: 0.33619980186223986\n",
      "  batch 450 loss: 0.32910936683416364\n",
      "  batch 500 loss: 0.31783667862415316\n",
      "  batch 550 loss: 0.3606051233410835\n",
      "  batch 600 loss: 0.32871421217918395\n",
      "  batch 650 loss: 0.3373725691437721\n",
      "  batch 700 loss: 0.3332665619254112\n",
      "  batch 750 loss: 0.3407987228035927\n",
      "  batch 800 loss: 0.35487581431865695\n",
      "  batch 850 loss: 0.3428906574845314\n",
      "  batch 900 loss: 0.34385805934667585\n",
      "LOSS train 0.34386 valid 1.04430, valid PER 26.10%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_160247/model_9\n",
      "Currently using dropout rate of 0.3\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.76266450881958\n",
      "  batch 100 loss: 3.1144669437408448\n",
      "  batch 150 loss: 2.868456344604492\n",
      "  batch 200 loss: 2.6368102121353147\n",
      "  batch 250 loss: 2.3993393421173095\n",
      "  batch 300 loss: 2.1358787703514097\n",
      "  batch 350 loss: 1.9585790705680848\n",
      "  batch 400 loss: 1.9478890800476074\n",
      "  batch 450 loss: 1.876994140148163\n",
      "  batch 500 loss: 1.7462803435325622\n",
      "  batch 550 loss: 1.6955014085769653\n",
      "  batch 600 loss: 1.6650589752197265\n",
      "  batch 650 loss: 1.5563221335411073\n",
      "  batch 700 loss: 1.5841499471664429\n",
      "  batch 750 loss: 1.5048123025894165\n",
      "  batch 800 loss: 1.4893748545646668\n",
      "  batch 850 loss: 1.4528127312660217\n",
      "  batch 900 loss: 1.4230659866333009\n",
      "LOSS train 1.42307 valid 1.38377, valid PER 44.96%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.378766508102417\n",
      "  batch 100 loss: 1.3721169686317445\n",
      "  batch 150 loss: 1.3111352336406707\n",
      "  batch 200 loss: 1.3725836563110352\n",
      "  batch 250 loss: 1.3529176449775695\n",
      "  batch 300 loss: 1.3161222290992738\n",
      "  batch 350 loss: 1.2414685153961182\n",
      "  batch 400 loss: 1.301405246257782\n",
      "  batch 450 loss: 1.2511168086528779\n",
      "  batch 500 loss: 1.2733442330360412\n",
      "  batch 550 loss: 1.3032294011116028\n",
      "  batch 600 loss: 1.2360643613338471\n",
      "  batch 650 loss: 1.2467910087108611\n",
      "  batch 700 loss: 1.2340598237514495\n",
      "  batch 750 loss: 1.2072795164585113\n",
      "  batch 800 loss: 1.150849964618683\n",
      "  batch 850 loss: 1.185202499628067\n",
      "  batch 900 loss: 1.219105086326599\n",
      "LOSS train 1.21911 valid 1.17051, valid PER 38.12%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.1806833398342134\n",
      "  batch 100 loss: 1.1092923784255981\n",
      "  batch 150 loss: 1.1304605627059936\n",
      "  batch 200 loss: 1.1279950642585754\n",
      "  batch 250 loss: 1.1134891939163207\n",
      "  batch 300 loss: 1.084757786989212\n",
      "  batch 350 loss: 1.1346845638751983\n",
      "  batch 400 loss: 1.0946177518367768\n",
      "  batch 450 loss: 1.0926993584632874\n",
      "  batch 500 loss: 1.0874348127841948\n",
      "  batch 550 loss: 1.1193620789051055\n",
      "  batch 600 loss: 1.0712138533592224\n",
      "  batch 650 loss: 1.055373616218567\n",
      "  batch 700 loss: 1.086680703163147\n",
      "  batch 750 loss: 1.128462746143341\n",
      "  batch 800 loss: 1.0516920483112335\n",
      "  batch 850 loss: 1.1090886175632477\n",
      "  batch 900 loss: 1.0393323862552644\n",
      "LOSS train 1.03933 valid 1.06890, valid PER 34.66%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.9944739043712616\n",
      "  batch 100 loss: 1.0209029638767242\n",
      "  batch 150 loss: 1.0112345790863038\n",
      "  batch 200 loss: 1.0497659933567047\n",
      "  batch 250 loss: 1.0297927534580231\n",
      "  batch 300 loss: 1.0267313301563263\n",
      "  batch 350 loss: 0.9818013739585877\n",
      "  batch 400 loss: 1.0092368090152741\n",
      "  batch 450 loss: 0.9764965450763703\n",
      "  batch 500 loss: 0.9633052349090576\n",
      "  batch 550 loss: 1.0071043789386749\n",
      "  batch 600 loss: 1.0294756960868836\n",
      "  batch 650 loss: 0.987063764333725\n",
      "  batch 700 loss: 0.973154776096344\n",
      "  batch 750 loss: 0.9726105391979217\n",
      "  batch 800 loss: 0.9507734513282776\n",
      "  batch 850 loss: 0.9901484310626983\n",
      "  batch 900 loss: 1.0038615024089814\n",
      "LOSS train 1.00386 valid 0.99508, valid PER 32.62%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.8952547109127045\n",
      "  batch 100 loss: 0.9016934871673584\n",
      "  batch 150 loss: 0.9619491469860076\n",
      "  batch 200 loss: 0.8649284207820892\n",
      "  batch 250 loss: 0.9120981574058533\n",
      "  batch 300 loss: 0.9273272824287414\n",
      "  batch 350 loss: 0.9131313419342041\n",
      "  batch 400 loss: 0.9050042378902435\n",
      "  batch 450 loss: 0.9125738763809204\n",
      "  batch 500 loss: 0.9312186980247498\n",
      "  batch 550 loss: 0.8807785212993622\n",
      "  batch 600 loss: 0.9572601318359375\n",
      "  batch 650 loss: 0.9192567706108093\n",
      "  batch 700 loss: 0.9327450430393219\n",
      "  batch 750 loss: 0.8736139810085297\n",
      "  batch 800 loss: 0.8806688070297242\n",
      "  batch 850 loss: 0.9091286134719848\n",
      "  batch 900 loss: 0.9111992371082306\n",
      "LOSS train 0.91120 valid 0.95026, valid PER 30.26%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8605087637901306\n",
      "  batch 100 loss: 0.7992858016490936\n",
      "  batch 150 loss: 0.8081998884677887\n",
      "  batch 200 loss: 0.8453721988201142\n",
      "  batch 250 loss: 0.8603976655006409\n",
      "  batch 300 loss: 0.835865523815155\n",
      "  batch 350 loss: 0.8615698862075806\n",
      "  batch 400 loss: 0.8240002691745758\n",
      "  batch 450 loss: 0.8579862415790558\n",
      "  batch 500 loss: 0.8677706909179688\n",
      "  batch 550 loss: 0.872258929014206\n",
      "  batch 600 loss: 0.8182810235023499\n",
      "  batch 650 loss: 0.8631388139724732\n",
      "  batch 700 loss: 0.8426457464694976\n",
      "  batch 750 loss: 0.850579378604889\n",
      "  batch 800 loss: 0.8395974743366241\n",
      "  batch 850 loss: 0.8386248850822449\n",
      "  batch 900 loss: 0.860728143453598\n",
      "LOSS train 0.86073 valid 0.92255, valid PER 29.08%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.7944356369972229\n",
      "  batch 100 loss: 0.7942601549625397\n",
      "  batch 150 loss: 0.7633780193328857\n",
      "  batch 200 loss: 0.7633697110414505\n",
      "  batch 250 loss: 0.7594157826900482\n",
      "  batch 300 loss: 0.7869477701187134\n",
      "  batch 350 loss: 0.7852160835266113\n",
      "  batch 400 loss: 0.8057880568504333\n",
      "  batch 450 loss: 0.7944886368513108\n",
      "  batch 500 loss: 0.7646734321117401\n",
      "  batch 550 loss: 0.7922958028316498\n",
      "  batch 600 loss: 0.7855894458293915\n",
      "  batch 650 loss: 0.7754866504669189\n",
      "  batch 700 loss: 0.7948232090473175\n",
      "  batch 750 loss: 0.7949627900123596\n",
      "  batch 800 loss: 0.766501704454422\n",
      "  batch 850 loss: 0.7982725942134857\n",
      "  batch 900 loss: 0.8299465584754944\n",
      "LOSS train 0.82995 valid 0.90540, valid PER 28.64%\n",
      "EPOCH 8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 50 loss: 0.6987799018621444\n",
      "  batch 100 loss: 0.7084969627857208\n",
      "  batch 150 loss: 0.7397369956970214\n",
      "  batch 200 loss: 0.70780513048172\n",
      "  batch 250 loss: 0.7076957523822784\n",
      "  batch 300 loss: 0.7064252388477326\n",
      "  batch 350 loss: 0.7485427570343017\n",
      "  batch 400 loss: 0.7164519268274308\n",
      "  batch 450 loss: 0.7429992949962616\n",
      "  batch 500 loss: 0.7610198450088501\n",
      "  batch 550 loss: 0.7278652691841125\n",
      "  batch 600 loss: 0.7686130666732788\n",
      "  batch 650 loss: 0.768517084121704\n",
      "  batch 700 loss: 0.7251559239625931\n",
      "  batch 750 loss: 0.7248374319076538\n",
      "  batch 800 loss: 0.7374068689346314\n",
      "  batch 850 loss: 0.7361820858716964\n",
      "  batch 900 loss: 0.7340720927715302\n",
      "LOSS train 0.73407 valid 0.88831, valid PER 27.79%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.6423114579916\n",
      "  batch 100 loss: 0.646444879770279\n",
      "  batch 150 loss: 0.6832466477155685\n",
      "  batch 200 loss: 0.6380162060260772\n",
      "  batch 250 loss: 0.6831538832187652\n",
      "  batch 300 loss: 0.6860347223281861\n",
      "  batch 350 loss: 0.6958754801750183\n",
      "  batch 400 loss: 0.6891547906398773\n",
      "  batch 450 loss: 0.6776044082641601\n",
      "  batch 500 loss: 0.6640694814920426\n",
      "  batch 550 loss: 0.6836348593235015\n",
      "  batch 600 loss: 0.7107228356599807\n",
      "  batch 650 loss: 0.6939376717805863\n",
      "  batch 700 loss: 0.6653809428215027\n",
      "  batch 750 loss: 0.689848570227623\n",
      "  batch 800 loss: 0.699811623096466\n",
      "  batch 850 loss: 0.7049878323078156\n",
      "  batch 900 loss: 0.6823404282331467\n",
      "LOSS train 0.68234 valid 0.88678, valid PER 26.60%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6031732380390167\n",
      "  batch 100 loss: 0.6119769805669785\n",
      "  batch 150 loss: 0.6325393646955491\n",
      "  batch 200 loss: 0.6455138325691223\n",
      "  batch 250 loss: 0.6498453921079635\n",
      "  batch 300 loss: 0.6022510087490082\n",
      "  batch 350 loss: 0.6556469875574112\n",
      "  batch 400 loss: 0.6073008346557617\n",
      "  batch 450 loss: 0.6009257912635804\n",
      "  batch 500 loss: 0.6338928151130676\n",
      "  batch 550 loss: 0.659515146613121\n",
      "  batch 600 loss: 0.6413833087682724\n",
      "  batch 650 loss: 0.6187277740240097\n",
      "  batch 700 loss: 0.6522856307029724\n",
      "  batch 750 loss: 0.6261982208490372\n",
      "  batch 800 loss: 0.6467394667863846\n",
      "  batch 850 loss: 0.659272369146347\n",
      "  batch 900 loss: 0.6642208081483841\n",
      "LOSS train 0.66422 valid 0.90111, valid PER 27.06%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.5830007368326187\n",
      "  batch 100 loss: 0.54320909678936\n",
      "  batch 150 loss: 0.5555830705165863\n",
      "  batch 200 loss: 0.6064620900154114\n",
      "  batch 250 loss: 0.6083310490846634\n",
      "  batch 300 loss: 0.5550809037685395\n",
      "  batch 350 loss: 0.5775178229808807\n",
      "  batch 400 loss: 0.6023311525583267\n",
      "  batch 450 loss: 0.589909372329712\n",
      "  batch 500 loss: 0.5773071306943893\n",
      "  batch 550 loss: 0.5975240767002106\n",
      "  batch 600 loss: 0.5888868588209152\n",
      "  batch 650 loss: 0.6419917207956314\n",
      "  batch 700 loss: 0.5812302482128143\n",
      "  batch 750 loss: 0.5978076618909836\n",
      "  batch 800 loss: 0.6110577368736267\n",
      "  batch 850 loss: 0.6306335878372192\n",
      "  batch 900 loss: 0.6260525047779083\n",
      "LOSS train 0.62605 valid 0.89288, valid PER 26.45%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.5434897166490554\n",
      "  batch 100 loss: 0.5300147420167923\n",
      "  batch 150 loss: 0.48776562124490735\n",
      "  batch 200 loss: 0.5316577291488648\n",
      "  batch 250 loss: 0.5385201567411423\n",
      "  batch 300 loss: 0.527533233165741\n",
      "  batch 350 loss: 0.5223389172554016\n",
      "  batch 400 loss: 0.5573048979043961\n",
      "  batch 450 loss: 0.5644923126697541\n",
      "  batch 500 loss: 0.5801166415214538\n",
      "  batch 550 loss: 0.5396592089533806\n",
      "  batch 600 loss: 0.557426118850708\n",
      "  batch 650 loss: 0.5901595741510391\n",
      "  batch 700 loss: 0.5798180639743805\n",
      "  batch 750 loss: 0.5274370950460434\n",
      "  batch 800 loss: 0.5622313672304153\n",
      "  batch 850 loss: 0.6014270961284638\n",
      "  batch 900 loss: 0.5907699835300445\n",
      "LOSS train 0.59077 valid 0.89649, valid PER 26.50%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.5134523862600326\n",
      "  batch 100 loss: 0.47442191660404204\n",
      "  batch 150 loss: 0.474674791097641\n",
      "  batch 200 loss: 0.48846723198890685\n",
      "  batch 250 loss: 0.48295529961586\n",
      "  batch 300 loss: 0.4930855628848076\n",
      "  batch 350 loss: 0.5136822146177292\n",
      "  batch 400 loss: 0.5078561598062515\n",
      "  batch 450 loss: 0.5130812788009643\n",
      "  batch 500 loss: 0.4904159301519394\n",
      "  batch 550 loss: 0.5136251449584961\n",
      "  batch 600 loss: 0.5267222458124161\n",
      "  batch 650 loss: 0.5295056796073914\n",
      "  batch 700 loss: 0.5534589457511901\n",
      "  batch 750 loss: 0.5097554183006286\n",
      "  batch 800 loss: 0.5165127962827682\n",
      "  batch 850 loss: 0.5275842696428299\n",
      "  batch 900 loss: 0.5663165014982223\n",
      "LOSS train 0.56632 valid 0.91496, valid PER 26.26%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.45033479332923887\n",
      "  batch 100 loss: 0.4452367502450943\n",
      "  batch 150 loss: 0.4625707322359085\n",
      "  batch 200 loss: 0.43585645616054536\n",
      "  batch 250 loss: 0.4537279343605041\n",
      "  batch 300 loss: 0.5023993995785713\n",
      "  batch 350 loss: 0.4511647942662239\n",
      "  batch 400 loss: 0.46232156932353974\n",
      "  batch 450 loss: 0.47013860166072846\n",
      "  batch 500 loss: 0.5046796292066574\n",
      "  batch 550 loss: 0.50064393222332\n",
      "  batch 600 loss: 0.468294974565506\n",
      "  batch 650 loss: 0.49652350664138795\n",
      "  batch 700 loss: 0.4971666008234024\n",
      "  batch 750 loss: 0.49330242574214933\n",
      "  batch 800 loss: 0.47867474019527434\n",
      "  batch 850 loss: 0.5093456435203553\n",
      "  batch 900 loss: 0.5086377769708633\n",
      "LOSS train 0.50864 valid 0.93688, valid PER 26.36%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.4069019922614098\n",
      "  batch 100 loss: 0.4196113669872284\n",
      "  batch 150 loss: 0.43542580902576444\n",
      "  batch 200 loss: 0.4446718770265579\n",
      "  batch 250 loss: 0.45385591447353363\n",
      "  batch 300 loss: 0.4214306989312172\n",
      "  batch 350 loss: 0.44265328168869017\n",
      "  batch 400 loss: 0.4427178686857223\n",
      "  batch 450 loss: 0.44111969977617266\n",
      "  batch 500 loss: 0.44138560950756073\n",
      "  batch 550 loss: 0.458674418926239\n",
      "  batch 600 loss: 0.47865915060043335\n",
      "  batch 650 loss: 0.48600659728050233\n",
      "  batch 700 loss: 0.478279202580452\n",
      "  batch 750 loss: 0.47994571924209595\n",
      "  batch 800 loss: 0.4736264890432358\n",
      "  batch 850 loss: 0.44438603043556213\n",
      "  batch 900 loss: 0.4662208631634712\n",
      "LOSS train 0.46622 valid 0.95847, valid PER 26.05%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.4073682051897049\n",
      "  batch 100 loss: 0.4013087910413742\n",
      "  batch 150 loss: 0.41496184945106507\n",
      "  batch 200 loss: 0.3956476253271103\n",
      "  batch 250 loss: 0.43211339712142943\n",
      "  batch 300 loss: 0.41644820213317874\n",
      "  batch 350 loss: 0.43937102615833284\n",
      "  batch 400 loss: 0.44175331950187685\n",
      "  batch 450 loss: 0.4404539614915848\n",
      "  batch 500 loss: 0.3983465540409088\n",
      "  batch 550 loss: 0.41522967159748075\n",
      "  batch 600 loss: 0.41413348555564883\n",
      "  batch 650 loss: 0.44241724282503125\n",
      "  batch 700 loss: 0.43038533568382265\n",
      "  batch 750 loss: 0.4238401538133621\n",
      "  batch 800 loss: 0.4408409798145294\n",
      "  batch 850 loss: 0.4469631493091583\n",
      "  batch 900 loss: 0.452599002122879\n",
      "LOSS train 0.45260 valid 0.96619, valid PER 25.94%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.37928575307130813\n",
      "  batch 100 loss: 0.37700601041316983\n",
      "  batch 150 loss: 0.34999089360237123\n",
      "  batch 200 loss: 0.38333682745695113\n",
      "  batch 250 loss: 0.4121046230196953\n",
      "  batch 300 loss: 0.4006508579850197\n",
      "  batch 350 loss: 0.3967664855718613\n",
      "  batch 400 loss: 0.4142147609591484\n",
      "  batch 450 loss: 0.40989292442798614\n",
      "  batch 500 loss: 0.38828561663627625\n",
      "  batch 550 loss: 0.3971917191147804\n",
      "  batch 600 loss: 0.42280139058828353\n",
      "  batch 650 loss: 0.4094960543513298\n",
      "  batch 700 loss: 0.4162615239620209\n",
      "  batch 750 loss: 0.39593813866376876\n",
      "  batch 800 loss: 0.39618885397911074\n",
      "  batch 850 loss: 0.42657347589731215\n",
      "  batch 900 loss: 0.41523376196622847\n",
      "LOSS train 0.41523 valid 0.98443, valid PER 25.98%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.35895084500312807\n",
      "  batch 100 loss: 0.3497291734814644\n",
      "  batch 150 loss: 0.36918663650751116\n",
      "  batch 200 loss: 0.3670490548014641\n",
      "  batch 250 loss: 0.3723397934436798\n",
      "  batch 300 loss: 0.3427253118157387\n",
      "  batch 350 loss: 0.37548704862594606\n",
      "  batch 400 loss: 0.3561589285731316\n",
      "  batch 450 loss: 0.38360776245594025\n",
      "  batch 500 loss: 0.37604972422122956\n",
      "  batch 550 loss: 0.39270914494991305\n",
      "  batch 600 loss: 0.3760797527432442\n",
      "  batch 650 loss: 0.3613565796613693\n",
      "  batch 700 loss: 0.39635631889104844\n",
      "  batch 750 loss: 0.37296065390110017\n",
      "  batch 800 loss: 0.39020976692438125\n",
      "  batch 850 loss: 0.37700506299734116\n",
      "  batch 900 loss: 0.3964823049306869\n",
      "LOSS train 0.39648 valid 1.00166, valid PER 26.04%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.31196239799261094\n",
      "  batch 100 loss: 0.3125939166545868\n",
      "  batch 150 loss: 0.3213069707155228\n",
      "  batch 200 loss: 0.329311600625515\n",
      "  batch 250 loss: 0.3425802901387215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 0.3538187175989151\n",
      "  batch 350 loss: 0.34531709969043733\n",
      "  batch 400 loss: 0.34788135409355164\n",
      "  batch 450 loss: 0.36827865898609163\n",
      "  batch 500 loss: 0.366238369345665\n",
      "  batch 550 loss: 0.35347097992897036\n",
      "  batch 600 loss: 0.3500819593667984\n",
      "  batch 650 loss: 0.3854894223809242\n",
      "  batch 700 loss: 0.3600664559006691\n",
      "  batch 750 loss: 0.34053280889987947\n",
      "  batch 800 loss: 0.3782547268271446\n",
      "  batch 850 loss: 0.37736049354076384\n",
      "  batch 900 loss: 0.352489712536335\n",
      "LOSS train 0.35249 valid 1.02452, valid PER 25.75%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.30359938114881513\n",
      "  batch 100 loss: 0.2812565314769745\n",
      "  batch 150 loss: 0.30425172626972197\n",
      "  batch 200 loss: 0.30569772869348527\n",
      "  batch 250 loss: 0.3246734693646431\n",
      "  batch 300 loss: 0.3380791702866554\n",
      "  batch 350 loss: 0.3027036327123642\n",
      "  batch 400 loss: 0.3389632397890091\n",
      "  batch 450 loss: 0.33962378561496737\n",
      "  batch 500 loss: 0.3197706064581871\n",
      "  batch 550 loss: 0.35421653509140016\n",
      "  batch 600 loss: 0.3191822126507759\n",
      "  batch 650 loss: 0.3378339684009552\n",
      "  batch 700 loss: 0.35792272090911864\n",
      "  batch 750 loss: 0.3428885492682457\n",
      "  batch 800 loss: 0.37098493456840514\n",
      "  batch 850 loss: 0.3600010192394257\n",
      "  batch 900 loss: 0.36688762962818144\n",
      "LOSS train 0.36689 valid 1.03485, valid PER 26.20%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_161123/model_9\n",
      "Currently using dropout rate of 0.4\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.962759981155395\n",
      "  batch 100 loss: 3.2362321424484253\n",
      "  batch 150 loss: 3.038233337402344\n",
      "  batch 200 loss: 2.8086486721038817\n",
      "  batch 250 loss: 2.628515419960022\n",
      "  batch 300 loss: 2.391055245399475\n",
      "  batch 350 loss: 2.1485475754737853\n",
      "  batch 400 loss: 2.1798465251922607\n",
      "  batch 450 loss: 1.9923881888389587\n",
      "  batch 500 loss: 1.8984982514381408\n",
      "  batch 550 loss: 1.7921835446357728\n",
      "  batch 600 loss: 1.7521285843849181\n",
      "  batch 650 loss: 1.6599638319015504\n",
      "  batch 700 loss: 1.6729587078094483\n",
      "  batch 750 loss: 1.6372210049629212\n",
      "  batch 800 loss: 1.5955145788192748\n",
      "  batch 850 loss: 1.5628849172592163\n",
      "  batch 900 loss: 1.5243604636192323\n",
      "LOSS train 1.52436 valid 1.47922, valid PER 50.43%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.4487998914718627\n",
      "  batch 100 loss: 1.4233993840217591\n",
      "  batch 150 loss: 1.3797425436973572\n",
      "  batch 200 loss: 1.4098476886749267\n",
      "  batch 250 loss: 1.3880083656311035\n",
      "  batch 300 loss: 1.351210470199585\n",
      "  batch 350 loss: 1.275624601840973\n",
      "  batch 400 loss: 1.3046251559257507\n",
      "  batch 450 loss: 1.2533819305896758\n",
      "  batch 500 loss: 1.2905689549446107\n",
      "  batch 550 loss: 1.3048733043670655\n",
      "  batch 600 loss: 1.2514207315444947\n",
      "  batch 650 loss: 1.2740058505535126\n",
      "  batch 700 loss: 1.2583807969093324\n",
      "  batch 750 loss: 1.2347691142559052\n",
      "  batch 800 loss: 1.1924555373191834\n",
      "  batch 850 loss: 1.205938024520874\n",
      "  batch 900 loss: 1.2201810765266419\n",
      "LOSS train 1.22018 valid 1.18072, valid PER 38.14%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.1700377285480499\n",
      "  batch 100 loss: 1.1428139221668243\n",
      "  batch 150 loss: 1.1345044875144958\n",
      "  batch 200 loss: 1.1461961269378662\n",
      "  batch 250 loss: 1.1365438199043274\n",
      "  batch 300 loss: 1.1313193428516388\n",
      "  batch 350 loss: 1.1521819722652435\n",
      "  batch 400 loss: 1.1202470254898071\n",
      "  batch 450 loss: 1.0970641160011292\n",
      "  batch 500 loss: 1.1075944340229034\n",
      "  batch 550 loss: 1.1055949783325196\n",
      "  batch 600 loss: 1.0786051809787751\n",
      "  batch 650 loss: 1.0812461030483247\n",
      "  batch 700 loss: 1.0886044955253602\n",
      "  batch 750 loss: 1.1442120242118836\n",
      "  batch 800 loss: 1.068498330116272\n",
      "  batch 850 loss: 1.1047836923599244\n",
      "  batch 900 loss: 1.0363259637355804\n",
      "LOSS train 1.03633 valid 1.08696, valid PER 35.59%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0044681000709534\n",
      "  batch 100 loss: 1.014383555650711\n",
      "  batch 150 loss: 0.9980338633060455\n",
      "  batch 200 loss: 1.0213775360584259\n",
      "  batch 250 loss: 1.059844183921814\n",
      "  batch 300 loss: 1.0340867495536805\n",
      "  batch 350 loss: 0.975006228685379\n",
      "  batch 400 loss: 1.0170694029331206\n",
      "  batch 450 loss: 1.0008823704719543\n",
      "  batch 500 loss: 0.991075656414032\n",
      "  batch 550 loss: 1.0461134028434753\n",
      "  batch 600 loss: 1.0598941719532013\n",
      "  batch 650 loss: 0.9998192155361175\n",
      "  batch 700 loss: 0.9780240547657013\n",
      "  batch 750 loss: 0.9807279515266418\n",
      "  batch 800 loss: 0.9370394706726074\n",
      "  batch 850 loss: 0.9915974974632263\n",
      "  batch 900 loss: 1.001473968029022\n",
      "LOSS train 1.00147 valid 0.99571, valid PER 32.36%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9150149774551392\n",
      "  batch 100 loss: 0.8960717713832855\n",
      "  batch 150 loss: 0.9754423344135285\n",
      "  batch 200 loss: 0.8880617928504944\n",
      "  batch 250 loss: 0.8956134593486786\n",
      "  batch 300 loss: 0.9063294398784637\n",
      "  batch 350 loss: 0.941220018863678\n",
      "  batch 400 loss: 0.9152293801307678\n",
      "  batch 450 loss: 0.8986923420429229\n",
      "  batch 500 loss: 0.9357082736492157\n",
      "  batch 550 loss: 0.8778781247138977\n",
      "  batch 600 loss: 0.9402044129371643\n",
      "  batch 650 loss: 0.909680861234665\n",
      "  batch 700 loss: 0.9538739585876465\n",
      "  batch 750 loss: 0.8870629787445068\n",
      "  batch 800 loss: 0.904455155134201\n",
      "  batch 850 loss: 0.9050121653079987\n",
      "  batch 900 loss: 0.9208644413948059\n",
      "LOSS train 0.92086 valid 0.93462, valid PER 30.18%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8665783035755158\n",
      "  batch 100 loss: 0.8328184515237809\n",
      "  batch 150 loss: 0.8148764741420745\n",
      "  batch 200 loss: 0.8467858183383942\n",
      "  batch 250 loss: 0.8644589698314666\n",
      "  batch 300 loss: 0.8813308787345886\n",
      "  batch 350 loss: 0.848823344707489\n",
      "  batch 400 loss: 0.8688373136520385\n",
      "  batch 450 loss: 0.8611885011196136\n",
      "  batch 500 loss: 0.8690201997756958\n",
      "  batch 550 loss: 0.8851711392402649\n",
      "  batch 600 loss: 0.8454737806320191\n",
      "  batch 650 loss: 0.8766290378570557\n",
      "  batch 700 loss: 0.8603065371513366\n",
      "  batch 750 loss: 0.8458553969860076\n",
      "  batch 800 loss: 0.8470387303829193\n",
      "  batch 850 loss: 0.8536577451229096\n",
      "  batch 900 loss: 0.8585620844364166\n",
      "LOSS train 0.85856 valid 0.91493, valid PER 29.22%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8090875172615051\n",
      "  batch 100 loss: 0.7759977602958679\n",
      "  batch 150 loss: 0.7725958943367004\n",
      "  batch 200 loss: 0.7726129531860352\n",
      "  batch 250 loss: 0.7799631369113922\n",
      "  batch 300 loss: 0.785805629491806\n",
      "  batch 350 loss: 0.7909421813488007\n",
      "  batch 400 loss: 0.8103397464752198\n",
      "  batch 450 loss: 0.8053603452444077\n",
      "  batch 500 loss: 0.7812848722934723\n",
      "  batch 550 loss: 0.7786428856849671\n",
      "  batch 600 loss: 0.7932599079608917\n",
      "  batch 650 loss: 0.7805901908874512\n",
      "  batch 700 loss: 0.8030765199661255\n",
      "  batch 750 loss: 0.7875941741466522\n",
      "  batch 800 loss: 0.8086269247531891\n",
      "  batch 850 loss: 0.7886544072628021\n",
      "  batch 900 loss: 0.825534930229187\n",
      "LOSS train 0.82553 valid 0.90283, valid PER 28.83%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.711425096988678\n",
      "  batch 100 loss: 0.7144386976957321\n",
      "  batch 150 loss: 0.7078419595956802\n",
      "  batch 200 loss: 0.7019106227159501\n",
      "  batch 250 loss: 0.7219151818752289\n",
      "  batch 300 loss: 0.6827683556079864\n",
      "  batch 350 loss: 0.7539459300041199\n",
      "  batch 400 loss: 0.7119024556875229\n",
      "  batch 450 loss: 0.7407646083831787\n",
      "  batch 500 loss: 0.7739812982082367\n",
      "  batch 550 loss: 0.702435929775238\n",
      "  batch 600 loss: 0.7587059128284455\n",
      "  batch 650 loss: 0.7536479103565216\n",
      "  batch 700 loss: 0.7379562598466873\n",
      "  batch 750 loss: 0.7324148297309876\n",
      "  batch 800 loss: 0.7395617926120758\n",
      "  batch 850 loss: 0.7168892621994019\n",
      "  batch 900 loss: 0.750539340376854\n",
      "LOSS train 0.75054 valid 0.87706, valid PER 27.81%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.6627492654323578\n",
      "  batch 100 loss: 0.674291028380394\n",
      "  batch 150 loss: 0.6995234429836273\n",
      "  batch 200 loss: 0.6713396680355072\n",
      "  batch 250 loss: 0.7044451934099197\n",
      "  batch 300 loss: 0.7103389674425125\n",
      "  batch 350 loss: 0.7191694831848144\n",
      "  batch 400 loss: 0.6930890619754791\n",
      "  batch 450 loss: 0.6809610253572465\n",
      "  batch 500 loss: 0.6700695306062698\n",
      "  batch 550 loss: 0.7088991516828537\n",
      "  batch 600 loss: 0.7157061606645584\n",
      "  batch 650 loss: 0.7111624419689179\n",
      "  batch 700 loss: 0.6948469448089599\n",
      "  batch 750 loss: 0.6744591528177262\n",
      "  batch 800 loss: 0.6971587783098221\n",
      "  batch 850 loss: 0.7135055768489837\n",
      "  batch 900 loss: 0.6665569543838501\n",
      "LOSS train 0.66656 valid 0.87392, valid PER 27.66%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6143348282575607\n",
      "  batch 100 loss: 0.6076034134626389\n",
      "  batch 150 loss: 0.657031654715538\n",
      "  batch 200 loss: 0.6425561434030533\n",
      "  batch 250 loss: 0.657829858660698\n",
      "  batch 300 loss: 0.6090885984897614\n",
      "  batch 350 loss: 0.6336690813302994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 400 loss: 0.612696248292923\n",
      "  batch 450 loss: 0.6242217975854873\n",
      "  batch 500 loss: 0.6805233430862426\n",
      "  batch 550 loss: 0.6815624338388443\n",
      "  batch 600 loss: 0.6583178859949111\n",
      "  batch 650 loss: 0.6289494854211807\n",
      "  batch 700 loss: 0.6599508380889892\n",
      "  batch 750 loss: 0.6289027231931686\n",
      "  batch 800 loss: 0.6516576647758484\n",
      "  batch 850 loss: 0.6508586102724075\n",
      "  batch 900 loss: 0.6850400853157044\n",
      "LOSS train 0.68504 valid 0.86538, valid PER 26.86%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.555969905257225\n",
      "  batch 100 loss: 0.5475222170352936\n",
      "  batch 150 loss: 0.564431546330452\n",
      "  batch 200 loss: 0.590312522649765\n",
      "  batch 250 loss: 0.618030053973198\n",
      "  batch 300 loss: 0.5707537633180618\n",
      "  batch 350 loss: 0.5890253174304962\n",
      "  batch 400 loss: 0.621907993555069\n",
      "  batch 450 loss: 0.6177930468320847\n",
      "  batch 500 loss: 0.574165222644806\n",
      "  batch 550 loss: 0.5926416766643524\n",
      "  batch 600 loss: 0.6086423599720001\n",
      "  batch 650 loss: 0.6560548102855682\n",
      "  batch 700 loss: 0.5821153444051742\n",
      "  batch 750 loss: 0.5935155308246612\n",
      "  batch 800 loss: 0.6312389296293258\n",
      "  batch 850 loss: 0.6229225558042526\n",
      "  batch 900 loss: 0.6293835717439652\n",
      "LOSS train 0.62938 valid 0.87035, valid PER 26.54%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.5294274020195008\n",
      "  batch 100 loss: 0.5315709763765335\n",
      "  batch 150 loss: 0.5075456750392914\n",
      "  batch 200 loss: 0.5365297687053681\n",
      "  batch 250 loss: 0.5373982065916061\n",
      "  batch 300 loss: 0.5268100821971893\n",
      "  batch 350 loss: 0.5346642172336579\n",
      "  batch 400 loss: 0.5792700850963592\n",
      "  batch 450 loss: 0.5693591797351837\n",
      "  batch 500 loss: 0.6005558079481125\n",
      "  batch 550 loss: 0.5285905635356903\n",
      "  batch 600 loss: 0.5692649418115616\n",
      "  batch 650 loss: 0.580304024219513\n",
      "  batch 700 loss: 0.5877439361810685\n",
      "  batch 750 loss: 0.5509334820508957\n",
      "  batch 800 loss: 0.554986132979393\n",
      "  batch 850 loss: 0.5958809489011765\n",
      "  batch 900 loss: 0.5998583745956421\n",
      "LOSS train 0.59986 valid 0.87128, valid PER 26.14%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.497593811750412\n",
      "  batch 100 loss: 0.5021722167730331\n",
      "  batch 150 loss: 0.4975451415777206\n",
      "  batch 200 loss: 0.5078365904092789\n",
      "  batch 250 loss: 0.49191744446754454\n",
      "  batch 300 loss: 0.5005077022314072\n",
      "  batch 350 loss: 0.5259456181526184\n",
      "  batch 400 loss: 0.5068613958358764\n",
      "  batch 450 loss: 0.5128748106956482\n",
      "  batch 500 loss: 0.5109471875429153\n",
      "  batch 550 loss: 0.541413437128067\n",
      "  batch 600 loss: 0.5246400642395019\n",
      "  batch 650 loss: 0.5464765959978104\n",
      "  batch 700 loss: 0.5491017210483551\n",
      "  batch 750 loss: 0.5013094049692154\n",
      "  batch 800 loss: 0.5280149412155152\n",
      "  batch 850 loss: 0.5598866212368011\n",
      "  batch 900 loss: 0.5721528249979019\n",
      "LOSS train 0.57215 valid 0.88957, valid PER 26.42%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.4696681642532349\n",
      "  batch 100 loss: 0.4761469841003418\n",
      "  batch 150 loss: 0.48440615713596347\n",
      "  batch 200 loss: 0.4594100722670555\n",
      "  batch 250 loss: 0.46212750852108003\n",
      "  batch 300 loss: 0.5131154182553291\n",
      "  batch 350 loss: 0.46128801167011263\n",
      "  batch 400 loss: 0.48096480667591096\n",
      "  batch 450 loss: 0.46651422858238223\n",
      "  batch 500 loss: 0.4911980324983597\n",
      "  batch 550 loss: 0.497169748544693\n",
      "  batch 600 loss: 0.4713383582234383\n",
      "  batch 650 loss: 0.5023743659257889\n",
      "  batch 700 loss: 0.5276234251260757\n",
      "  batch 750 loss: 0.5073557484149933\n",
      "  batch 800 loss: 0.4919344985485077\n",
      "  batch 850 loss: 0.508695297241211\n",
      "  batch 900 loss: 0.5137809723615646\n",
      "LOSS train 0.51378 valid 0.89440, valid PER 26.27%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.41736588716506956\n",
      "  batch 100 loss: 0.4227404224872589\n",
      "  batch 150 loss: 0.4371705800294876\n",
      "  batch 200 loss: 0.45439394772052766\n",
      "  batch 250 loss: 0.4707601580023766\n",
      "  batch 300 loss: 0.42688501536846163\n",
      "  batch 350 loss: 0.442035847902298\n",
      "  batch 400 loss: 0.47674599826335906\n",
      "  batch 450 loss: 0.4418007296323776\n",
      "  batch 500 loss: 0.446482195854187\n",
      "  batch 550 loss: 0.46104001224040986\n",
      "  batch 600 loss: 0.4684446287155151\n",
      "  batch 650 loss: 0.4883670920133591\n",
      "  batch 700 loss: 0.4821882474422455\n",
      "  batch 750 loss: 0.49542389750480653\n",
      "  batch 800 loss: 0.46521701514720915\n",
      "  batch 850 loss: 0.4497702890634537\n",
      "  batch 900 loss: 0.4904418301582336\n",
      "LOSS train 0.49044 valid 0.91357, valid PER 25.84%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.4088826796412468\n",
      "  batch 100 loss: 0.4005593979358673\n",
      "  batch 150 loss: 0.41174099951982496\n",
      "  batch 200 loss: 0.4090108376741409\n",
      "  batch 250 loss: 0.4295375365018845\n",
      "  batch 300 loss: 0.41677975952625274\n",
      "  batch 350 loss: 0.4378073212504387\n",
      "  batch 400 loss: 0.4273474472761154\n",
      "  batch 450 loss: 0.43818179368972776\n",
      "  batch 500 loss: 0.3952997237443924\n",
      "  batch 550 loss: 0.4340766108036041\n",
      "  batch 600 loss: 0.4254764372110367\n",
      "  batch 650 loss: 0.4479630827903748\n",
      "  batch 700 loss: 0.4397183084487915\n",
      "  batch 750 loss: 0.450982146859169\n",
      "  batch 800 loss: 0.4575463420152664\n",
      "  batch 850 loss: 0.43905400335788725\n",
      "  batch 900 loss: 0.45796829760074614\n",
      "LOSS train 0.45797 valid 0.92794, valid PER 25.86%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.3686517748236656\n",
      "  batch 100 loss: 0.375264079272747\n",
      "  batch 150 loss: 0.371230728328228\n",
      "  batch 200 loss: 0.3802940824627876\n",
      "  batch 250 loss: 0.40044403582811355\n",
      "  batch 300 loss: 0.39173809826374056\n",
      "  batch 350 loss: 0.3910258787870407\n",
      "  batch 400 loss: 0.42244739294052125\n",
      "  batch 450 loss: 0.4099435651302338\n",
      "  batch 500 loss: 0.40452692687511443\n",
      "  batch 550 loss: 0.407762815952301\n",
      "  batch 600 loss: 0.42631745159626006\n",
      "  batch 650 loss: 0.39911176055669784\n",
      "  batch 700 loss: 0.4020305171608925\n",
      "  batch 750 loss: 0.39587507337331773\n",
      "  batch 800 loss: 0.40583118706941607\n",
      "  batch 850 loss: 0.41574073284864427\n",
      "  batch 900 loss: 0.42034789979457854\n",
      "LOSS train 0.42035 valid 0.94953, valid PER 25.74%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.3404250571131706\n",
      "  batch 100 loss: 0.35479863941669465\n",
      "  batch 150 loss: 0.35710165828466417\n",
      "  batch 200 loss: 0.3645256707072258\n",
      "  batch 250 loss: 0.37494388669729234\n",
      "  batch 300 loss: 0.34816327154636384\n",
      "  batch 350 loss: 0.3741014474630356\n",
      "  batch 400 loss: 0.3597965571284294\n",
      "  batch 450 loss: 0.3848583021759987\n",
      "  batch 500 loss: 0.3683704409003258\n",
      "  batch 550 loss: 0.39012564837932584\n",
      "  batch 600 loss: 0.386779220700264\n",
      "  batch 650 loss: 0.3665370228886604\n",
      "  batch 700 loss: 0.3993597251176834\n",
      "  batch 750 loss: 0.3891724732518196\n",
      "  batch 800 loss: 0.3827469190955162\n",
      "  batch 850 loss: 0.3989356699585915\n",
      "  batch 900 loss: 0.401443735063076\n",
      "LOSS train 0.40144 valid 0.95375, valid PER 25.66%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.33540129363536836\n",
      "  batch 100 loss: 0.33236000657081605\n",
      "  batch 150 loss: 0.33362199515104296\n",
      "  batch 200 loss: 0.33264096200466153\n",
      "  batch 250 loss: 0.35283158034086226\n",
      "  batch 300 loss: 0.35277952343225477\n",
      "  batch 350 loss: 0.3393892925977707\n",
      "  batch 400 loss: 0.351917604804039\n",
      "  batch 450 loss: 0.37375046491622926\n",
      "  batch 500 loss: 0.3543514785170555\n",
      "  batch 550 loss: 0.3557745859026909\n",
      "  batch 600 loss: 0.367060172855854\n",
      "  batch 650 loss: 0.39161611527204515\n",
      "  batch 700 loss: 0.36267062604427336\n",
      "  batch 750 loss: 0.35001797586679456\n",
      "  batch 800 loss: 0.3831824344396591\n",
      "  batch 850 loss: 0.3868734312057495\n",
      "  batch 900 loss: 0.38061675786972043\n",
      "LOSS train 0.38062 valid 0.99516, valid PER 26.02%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.3169322228431702\n",
      "  batch 100 loss: 0.3072634920477867\n",
      "  batch 150 loss: 0.2930848529934883\n",
      "  batch 200 loss: 0.3221435871720314\n",
      "  batch 250 loss: 0.31639036238193513\n",
      "  batch 300 loss: 0.33374862432479857\n",
      "  batch 350 loss: 0.3107901731133461\n",
      "  batch 400 loss: 0.3416826167702675\n",
      "  batch 450 loss: 0.34494464576244355\n",
      "  batch 500 loss: 0.3170182290673256\n",
      "  batch 550 loss: 0.3526264324784279\n",
      "  batch 600 loss: 0.3347698637843132\n",
      "  batch 650 loss: 0.35195940136909487\n",
      "  batch 700 loss: 0.344996837079525\n",
      "  batch 750 loss: 0.33193108171224595\n",
      "  batch 800 loss: 0.3693876060843468\n",
      "  batch 850 loss: 0.35260987251996995\n",
      "  batch 900 loss: 0.3450515329837799\n",
      "LOSS train 0.34505 valid 1.01843, valid PER 26.08%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_161958/model_10\n",
      "Currently using dropout rate of 0.5\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.746773796081543\n",
      "  batch 100 loss: 3.1486670446395872\n",
      "  batch 150 loss: 2.9416114044189454\n",
      "  batch 200 loss: 2.705732102394104\n",
      "  batch 250 loss: 2.5347864389419557\n",
      "  batch 300 loss: 2.269541311264038\n",
      "  batch 350 loss: 2.0665467619895934\n",
      "  batch 400 loss: 2.0536619806289673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 450 loss: 1.9406069827079773\n",
      "  batch 500 loss: 1.8331819343566895\n",
      "  batch 550 loss: 1.7778464007377623\n",
      "  batch 600 loss: 1.816505057811737\n",
      "  batch 650 loss: 1.6604608368873597\n",
      "  batch 700 loss: 1.6627582383155823\n",
      "  batch 750 loss: 1.632619755268097\n",
      "  batch 800 loss: 1.6283797478675843\n",
      "  batch 850 loss: 1.5945994782447814\n",
      "  batch 900 loss: 1.5660619926452637\n",
      "LOSS train 1.56606 valid 1.51303, valid PER 57.01%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.5128508257865905\n",
      "  batch 100 loss: 1.490769157409668\n",
      "  batch 150 loss: 1.4441561532020568\n",
      "  batch 200 loss: 1.474134659767151\n",
      "  batch 250 loss: 1.4800825500488282\n",
      "  batch 300 loss: 1.4431217575073243\n",
      "  batch 350 loss: 1.363006706237793\n",
      "  batch 400 loss: 1.4252169585227967\n",
      "  batch 450 loss: 1.3595135879516602\n",
      "  batch 500 loss: 1.372188549041748\n",
      "  batch 550 loss: 1.3990392017364501\n",
      "  batch 600 loss: 1.3513495683670045\n",
      "  batch 650 loss: 1.3680161499977113\n",
      "  batch 700 loss: 1.3381013298034667\n",
      "  batch 750 loss: 1.3187632942199707\n",
      "  batch 800 loss: 1.2817499554157257\n",
      "  batch 850 loss: 1.2893629634380341\n",
      "  batch 900 loss: 1.3214395213127137\n",
      "LOSS train 1.32144 valid 1.24385, valid PER 41.42%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.259475371837616\n",
      "  batch 100 loss: 1.2236846840381623\n",
      "  batch 150 loss: 1.2213197124004365\n",
      "  batch 200 loss: 1.2125891244411469\n",
      "  batch 250 loss: 1.2074943828582763\n",
      "  batch 300 loss: 1.1777032661437987\n",
      "  batch 350 loss: 1.257900629043579\n",
      "  batch 400 loss: 1.1989297699928283\n",
      "  batch 450 loss: 1.1916049385070802\n",
      "  batch 500 loss: 1.1760971808433534\n",
      "  batch 550 loss: 1.1863648867607117\n",
      "  batch 600 loss: 1.1635802328586577\n",
      "  batch 650 loss: 1.1527373147010804\n",
      "  batch 700 loss: 1.1593878626823426\n",
      "  batch 750 loss: 1.1922774422168732\n",
      "  batch 800 loss: 1.1523536908626557\n",
      "  batch 850 loss: 1.1836587905883789\n",
      "  batch 900 loss: 1.1366792297363282\n",
      "LOSS train 1.13668 valid 1.12589, valid PER 37.03%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.077913155555725\n",
      "  batch 100 loss: 1.093859122991562\n",
      "  batch 150 loss: 1.0706190705299377\n",
      "  batch 200 loss: 1.1068784558773042\n",
      "  batch 250 loss: 1.1106444370746613\n",
      "  batch 300 loss: 1.1059954535961152\n",
      "  batch 350 loss: 1.064474844932556\n",
      "  batch 400 loss: 1.1100946509838103\n",
      "  batch 450 loss: 1.0691927194595336\n",
      "  batch 500 loss: 1.0857383799552918\n",
      "  batch 550 loss: 1.1050155234336854\n",
      "  batch 600 loss: 1.0947026085853577\n",
      "  batch 650 loss: 1.0676716434955598\n",
      "  batch 700 loss: 1.0662123692035674\n",
      "  batch 750 loss: 1.0341755664348602\n",
      "  batch 800 loss: 1.0268903648853303\n",
      "  batch 850 loss: 1.0558397698402404\n",
      "  batch 900 loss: 1.079479604959488\n",
      "LOSS train 1.07948 valid 1.03476, valid PER 33.63%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.0055987012386323\n",
      "  batch 100 loss: 0.9931491553783417\n",
      "  batch 150 loss: 1.033081784248352\n",
      "  batch 200 loss: 0.9725868582725525\n",
      "  batch 250 loss: 0.9864058601856232\n",
      "  batch 300 loss: 0.9948875510692596\n",
      "  batch 350 loss: 0.9900349831581116\n",
      "  batch 400 loss: 0.9899416482448578\n",
      "  batch 450 loss: 0.9760474705696106\n",
      "  batch 500 loss: 0.9891850447654724\n",
      "  batch 550 loss: 0.9484331715106964\n",
      "  batch 600 loss: 1.0105938231945037\n",
      "  batch 650 loss: 0.9868539535999298\n",
      "  batch 700 loss: 1.0268862402439118\n",
      "  batch 750 loss: 0.9754994606971741\n",
      "  batch 800 loss: 0.9723395490646363\n",
      "  batch 850 loss: 0.9830443561077118\n",
      "  batch 900 loss: 1.0005439293384553\n",
      "LOSS train 1.00054 valid 0.97716, valid PER 32.22%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.949663645029068\n",
      "  batch 100 loss: 0.9007348990440369\n",
      "  batch 150 loss: 0.8808132135868072\n",
      "  batch 200 loss: 0.9353773140907288\n",
      "  batch 250 loss: 0.9388079082965851\n",
      "  batch 300 loss: 0.9326990628242493\n",
      "  batch 350 loss: 0.9275200235843658\n",
      "  batch 400 loss: 0.9157659864425659\n",
      "  batch 450 loss: 0.921118688583374\n",
      "  batch 500 loss: 0.9256989109516144\n",
      "  batch 550 loss: 0.950216908454895\n",
      "  batch 600 loss: 0.8983565163612366\n",
      "  batch 650 loss: 0.9193639588356018\n",
      "  batch 700 loss: 0.9239287495613098\n",
      "  batch 750 loss: 0.9176298367977143\n",
      "  batch 800 loss: 0.901871634721756\n",
      "  batch 850 loss: 0.922663893699646\n",
      "  batch 900 loss: 0.8952848279476165\n",
      "LOSS train 0.89528 valid 0.95197, valid PER 30.90%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8638604974746704\n",
      "  batch 100 loss: 0.8562949001789093\n",
      "  batch 150 loss: 0.8317077040672303\n",
      "  batch 200 loss: 0.8545756089687347\n",
      "  batch 250 loss: 0.8363102650642396\n",
      "  batch 300 loss: 0.8630554521083832\n",
      "  batch 350 loss: 0.8600101852416993\n",
      "  batch 400 loss: 0.871218535900116\n",
      "  batch 450 loss: 0.8934807002544403\n",
      "  batch 500 loss: 0.8626823198795318\n",
      "  batch 550 loss: 0.8676899886131286\n",
      "  batch 600 loss: 0.8766627657413483\n",
      "  batch 650 loss: 0.8650280737876892\n",
      "  batch 700 loss: 0.8809175097942352\n",
      "  batch 750 loss: 0.861617169380188\n",
      "  batch 800 loss: 0.8470257413387299\n",
      "  batch 850 loss: 0.8838006782531739\n",
      "  batch 900 loss: 0.8955175626277924\n",
      "LOSS train 0.89552 valid 0.94324, valid PER 30.14%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.7998375302553177\n",
      "  batch 100 loss: 0.7977359318733215\n",
      "  batch 150 loss: 0.8134268462657929\n",
      "  batch 200 loss: 0.7953076910972595\n",
      "  batch 250 loss: 0.8053035789728165\n",
      "  batch 300 loss: 0.7855433940887451\n",
      "  batch 350 loss: 0.8264324438571929\n",
      "  batch 400 loss: 0.7810821032524109\n",
      "  batch 450 loss: 0.8254996812343598\n",
      "  batch 500 loss: 0.8451966667175292\n",
      "  batch 550 loss: 0.7942192673683166\n",
      "  batch 600 loss: 0.833507730960846\n",
      "  batch 650 loss: 0.8446331882476806\n",
      "  batch 700 loss: 0.7945685517787934\n",
      "  batch 750 loss: 0.7970223200321197\n",
      "  batch 800 loss: 0.8196103978157043\n",
      "  batch 850 loss: 0.782177883386612\n",
      "  batch 900 loss: 0.8239924240112305\n",
      "LOSS train 0.82399 valid 0.90560, valid PER 28.81%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7028149378299713\n",
      "  batch 100 loss: 0.7412120425701141\n",
      "  batch 150 loss: 0.7501116526126862\n",
      "  batch 200 loss: 0.7218144810199738\n",
      "  batch 250 loss: 0.7714008033275604\n",
      "  batch 300 loss: 0.7846734833717346\n",
      "  batch 350 loss: 0.799929027557373\n",
      "  batch 400 loss: 0.7410469287633896\n",
      "  batch 450 loss: 0.7317531144618988\n",
      "  batch 500 loss: 0.7517260491847992\n",
      "  batch 550 loss: 0.7724972259998322\n",
      "  batch 600 loss: 0.7925024455785752\n",
      "  batch 650 loss: 0.7651443874835968\n",
      "  batch 700 loss: 0.7483085566759109\n",
      "  batch 750 loss: 0.7591597080230713\n",
      "  batch 800 loss: 0.7587253367900848\n",
      "  batch 850 loss: 0.7764656960964202\n",
      "  batch 900 loss: 0.7500149029493332\n",
      "LOSS train 0.75001 valid 0.90519, valid PER 27.94%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6681462866067887\n",
      "  batch 100 loss: 0.6813374334573745\n",
      "  batch 150 loss: 0.6901991021633148\n",
      "  batch 200 loss: 0.7084887194633483\n",
      "  batch 250 loss: 0.7220437443256378\n",
      "  batch 300 loss: 0.6792016708850861\n",
      "  batch 350 loss: 0.7228964424133301\n",
      "  batch 400 loss: 0.6835603618621826\n",
      "  batch 450 loss: 0.6935511314868927\n",
      "  batch 500 loss: 0.7330778539180756\n",
      "  batch 550 loss: 0.738287959098816\n",
      "  batch 600 loss: 0.7079135847091674\n",
      "  batch 650 loss: 0.7076756536960602\n",
      "  batch 700 loss: 0.7259408873319626\n",
      "  batch 750 loss: 0.7180023622512818\n",
      "  batch 800 loss: 0.7453844821453095\n",
      "  batch 850 loss: 0.757884007692337\n",
      "  batch 900 loss: 0.7456331574916839\n",
      "LOSS train 0.74563 valid 0.91050, valid PER 28.28%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6416336214542389\n",
      "  batch 100 loss: 0.6225735133886338\n",
      "  batch 150 loss: 0.6398234063386917\n",
      "  batch 200 loss: 0.6689097023010254\n",
      "  batch 250 loss: 0.7106959503889084\n",
      "  batch 300 loss: 0.6507251834869385\n",
      "  batch 350 loss: 0.6651515686511993\n",
      "  batch 400 loss: 0.6825674873590469\n",
      "  batch 450 loss: 0.677670042514801\n",
      "  batch 500 loss: 0.6712717312574387\n",
      "  batch 550 loss: 0.6960213148593902\n",
      "  batch 600 loss: 0.7000180751085281\n",
      "  batch 650 loss: 0.7621450865268707\n",
      "  batch 700 loss: 0.6655768698453903\n",
      "  batch 750 loss: 0.6951371067762375\n",
      "  batch 800 loss: 0.7227617198228836\n",
      "  batch 850 loss: 0.7159667026996612\n",
      "  batch 900 loss: 0.719796696305275\n",
      "LOSS train 0.71980 valid 0.89828, valid PER 27.48%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.647502948641777\n",
      "  batch 100 loss: 0.6194160729646683\n",
      "  batch 150 loss: 0.5729879379272461\n",
      "  batch 200 loss: 0.6340690392255783\n",
      "  batch 250 loss: 0.6356973719596862\n",
      "  batch 300 loss: 0.6275229918956756\n",
      "  batch 350 loss: 0.6234392589330673\n",
      "  batch 400 loss: 0.6365071952342987\n",
      "  batch 450 loss: 0.6445695579051971\n",
      "  batch 500 loss: 0.6547210073471069\n",
      "  batch 550 loss: 0.6068573236465454\n",
      "  batch 600 loss: 0.654949751496315\n",
      "  batch 650 loss: 0.6593287640810013\n",
      "  batch 700 loss: 0.6729483699798584\n",
      "  batch 750 loss: 0.6421737617254257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 800 loss: 0.6619769138097763\n",
      "  batch 850 loss: 0.6815523439645768\n",
      "  batch 900 loss: 0.6759602957963944\n",
      "LOSS train 0.67596 valid 0.90234, valid PER 27.19%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.586477101445198\n",
      "  batch 100 loss: 0.5866281563043594\n",
      "  batch 150 loss: 0.5955002319812774\n",
      "  batch 200 loss: 0.598739298582077\n",
      "  batch 250 loss: 0.6045553684234619\n",
      "  batch 300 loss: 0.5947940480709076\n",
      "  batch 350 loss: 0.5889270478487014\n",
      "  batch 400 loss: 0.59048230946064\n",
      "  batch 450 loss: 0.59545989215374\n",
      "  batch 500 loss: 0.5897282665967941\n",
      "  batch 550 loss: 0.6249653321504592\n",
      "  batch 600 loss: 0.6051031911373138\n",
      "  batch 650 loss: 0.6102239519357682\n",
      "  batch 700 loss: 0.6322624999284744\n",
      "  batch 750 loss: 0.5929705196619034\n",
      "  batch 800 loss: 0.6296602457761764\n",
      "  batch 850 loss: 0.6282240587472916\n",
      "  batch 900 loss: 0.6449568945169449\n",
      "LOSS train 0.64496 valid 0.90885, valid PER 26.85%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.5406775206327439\n",
      "  batch 100 loss: 0.5461617010831833\n",
      "  batch 150 loss: 0.5448284006118774\n",
      "  batch 200 loss: 0.533264828324318\n",
      "  batch 250 loss: 0.5521098744869232\n",
      "  batch 300 loss: 0.5821789175271987\n",
      "  batch 350 loss: 0.5340764373540878\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start tuning For wider 1 Layer LSTM\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 1,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 512,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 1,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 512,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_regularisation_dropout.BiLSTM(args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For Wider 1 Layer LSTM\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308cbda",
   "metadata": {},
   "source": [
    "###### continue Adam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e38725",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0.5]\n",
    "Optimiser = [\"Adam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ef17c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning For wider 1 Layer LSTM\n",
      "Currently using Adam optimiser\n",
      "Currently using dropout rate of 0.5\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.755533928871155\n",
      "  batch 100 loss: 3.1744091081619263\n",
      "  batch 150 loss: 2.7647918128967284\n",
      "  batch 200 loss: 2.4814211082458497\n",
      "  batch 250 loss: 2.2743773555755613\n",
      "  batch 300 loss: 2.10567253112793\n",
      "  batch 350 loss: 1.9760479307174683\n",
      "  batch 400 loss: 1.9497089934349061\n",
      "  batch 450 loss: 1.842070724964142\n",
      "  batch 500 loss: 1.772059552669525\n",
      "  batch 550 loss: 1.7214161491394042\n",
      "  batch 600 loss: 1.6797670388221742\n",
      "  batch 650 loss: 1.6148813414573668\n",
      "  batch 700 loss: 1.6050615429878234\n",
      "  batch 750 loss: 1.5479127597808837\n",
      "  batch 800 loss: 1.5451667237281799\n",
      "  batch 850 loss: 1.5153027558326722\n",
      "  batch 900 loss: 1.48150199174881\n",
      "LOSS train 1.48150 valid 1.43681, valid PER 47.21%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.4306887841224671\n",
      "  batch 100 loss: 1.3995813298225404\n",
      "  batch 150 loss: 1.3545600533485413\n",
      "  batch 200 loss: 1.3967129969596863\n",
      "  batch 250 loss: 1.3859964299201966\n",
      "  batch 300 loss: 1.3610330033302307\n",
      "  batch 350 loss: 1.2872321128845214\n",
      "  batch 400 loss: 1.2984967279434203\n",
      "  batch 450 loss: 1.2749803519248963\n",
      "  batch 500 loss: 1.3089092528820039\n",
      "  batch 550 loss: 1.3231044244766235\n",
      "  batch 600 loss: 1.2891336154937745\n",
      "  batch 650 loss: 1.2870443892478942\n",
      "  batch 700 loss: 1.2741839170455933\n",
      "  batch 750 loss: 1.2580807769298554\n",
      "  batch 800 loss: 1.215323464870453\n",
      "  batch 850 loss: 1.2123215210437774\n",
      "  batch 900 loss: 1.2439787220954894\n",
      "LOSS train 1.24398 valid 1.19192, valid PER 37.99%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.1984694504737854\n",
      "  batch 100 loss: 1.1773075759410858\n",
      "  batch 150 loss: 1.1513391613960267\n",
      "  batch 200 loss: 1.1498676311969758\n",
      "  batch 250 loss: 1.138309062719345\n",
      "  batch 300 loss: 1.1321345448493958\n",
      "  batch 350 loss: 1.1711575746536256\n",
      "  batch 400 loss: 1.1401156902313232\n",
      "  batch 450 loss: 1.134567985534668\n",
      "  batch 500 loss: 1.129588681459427\n",
      "  batch 550 loss: 1.1338300919532776\n",
      "  batch 600 loss: 1.1160264837741851\n",
      "  batch 650 loss: 1.1030620050430298\n",
      "  batch 700 loss: 1.121163455247879\n",
      "  batch 750 loss: 1.1728781068325043\n",
      "  batch 800 loss: 1.0932371759414672\n",
      "  batch 850 loss: 1.1196485221385957\n",
      "  batch 900 loss: 1.0635606455802917\n",
      "LOSS train 1.06356 valid 1.12721, valid PER 35.67%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0260354089736938\n",
      "  batch 100 loss: 1.0328508627414703\n",
      "  batch 150 loss: 1.0192622935771942\n",
      "  batch 200 loss: 1.051205267906189\n",
      "  batch 250 loss: 1.0799261701107026\n",
      "  batch 300 loss: 1.0484062790870667\n",
      "  batch 350 loss: 0.9848405230045318\n",
      "  batch 400 loss: 1.0205251908302306\n",
      "  batch 450 loss: 1.0254830050468444\n",
      "  batch 500 loss: 1.0063426291942597\n",
      "  batch 550 loss: 1.0405706202983855\n",
      "  batch 600 loss: 1.0644598054885863\n",
      "  batch 650 loss: 1.0172458505630493\n",
      "  batch 700 loss: 0.9854580295085907\n",
      "  batch 750 loss: 0.9731542944908143\n",
      "  batch 800 loss: 0.9596885573863984\n",
      "  batch 850 loss: 1.0037448501586914\n",
      "  batch 900 loss: 1.033747408390045\n",
      "LOSS train 1.03375 valid 0.99052, valid PER 31.77%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9472234928607941\n",
      "  batch 100 loss: 0.942877221107483\n",
      "  batch 150 loss: 1.0067047584056854\n",
      "  batch 200 loss: 0.9251686215400696\n",
      "  batch 250 loss: 0.922964928150177\n",
      "  batch 300 loss: 0.9530735313892365\n",
      "  batch 350 loss: 0.9520752763748169\n",
      "  batch 400 loss: 0.9467118036746979\n",
      "  batch 450 loss: 0.9336060023307801\n",
      "  batch 500 loss: 0.9717752885818481\n",
      "  batch 550 loss: 0.9091139948368072\n",
      "  batch 600 loss: 0.9684897208213806\n",
      "  batch 650 loss: 0.9500857543945312\n",
      "  batch 700 loss: 0.9614232790470123\n",
      "  batch 750 loss: 0.9109101235866547\n",
      "  batch 800 loss: 0.9703311467170715\n",
      "  batch 850 loss: 0.9558944284915925\n",
      "  batch 900 loss: 0.9419612491130829\n",
      "LOSS train 0.94196 valid 0.97989, valid PER 30.94%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9047225499153138\n",
      "  batch 100 loss: 0.8621476507186889\n",
      "  batch 150 loss: 0.8476280152797699\n",
      "  batch 200 loss: 0.8751406842470169\n",
      "  batch 250 loss: 0.9167543685436249\n",
      "  batch 300 loss: 0.8830860507488251\n",
      "  batch 350 loss: 0.8724551928043366\n",
      "  batch 400 loss: 0.8851809465885162\n",
      "  batch 450 loss: 0.9233882260322571\n",
      "  batch 500 loss: 0.8815481626987457\n",
      "  batch 550 loss: 0.880502690076828\n",
      "  batch 600 loss: 0.8483687460422515\n",
      "  batch 650 loss: 0.8990911448001861\n",
      "  batch 700 loss: 0.8799922287464141\n",
      "  batch 750 loss: 0.8566770267486572\n",
      "  batch 800 loss: 0.8774540889263153\n",
      "  batch 850 loss: 0.8616610848903656\n",
      "  batch 900 loss: 0.8953661930561065\n",
      "LOSS train 0.89537 valid 0.93271, valid PER 30.06%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8254260873794556\n",
      "  batch 100 loss: 0.828374627828598\n",
      "  batch 150 loss: 0.8010532116889953\n",
      "  batch 200 loss: 0.8173630046844482\n",
      "  batch 250 loss: 0.7964215135574341\n",
      "  batch 300 loss: 0.8067637121677399\n",
      "  batch 350 loss: 0.8120963990688324\n",
      "  batch 400 loss: 0.8130469691753387\n",
      "  batch 450 loss: 0.8133269262313843\n",
      "  batch 500 loss: 0.8215996670722961\n",
      "  batch 550 loss: 0.8139559710025788\n",
      "  batch 600 loss: 0.8408135747909546\n",
      "  batch 650 loss: 0.8268619072437287\n",
      "  batch 700 loss: 0.8345443779230117\n",
      "  batch 750 loss: 0.8143141353130341\n",
      "  batch 800 loss: 0.8239395499229432\n",
      "  batch 850 loss: 0.8625811767578125\n",
      "  batch 900 loss: 0.8694195818901062\n",
      "LOSS train 0.86942 valid 0.92600, valid PER 29.10%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.7589803338050842\n",
      "  batch 100 loss: 0.7360880935192108\n",
      "  batch 150 loss: 0.7774510097503662\n",
      "  batch 200 loss: 0.7539738357067108\n",
      "  batch 250 loss: 0.7604936456680298\n",
      "  batch 300 loss: 0.7161679166555405\n",
      "  batch 350 loss: 0.8007388591766358\n",
      "  batch 400 loss: 0.7444041872024536\n",
      "  batch 450 loss: 0.7808461689949036\n",
      "  batch 500 loss: 0.8130795407295227\n",
      "  batch 550 loss: 0.7369689691066742\n",
      "  batch 600 loss: 0.7830153703689575\n",
      "  batch 650 loss: 0.8020575916767121\n",
      "  batch 700 loss: 0.7669363325834274\n",
      "  batch 750 loss: 0.7801124542951584\n",
      "  batch 800 loss: 0.785577448606491\n",
      "  batch 850 loss: 0.7763530611991882\n",
      "  batch 900 loss: 0.80655020236969\n",
      "LOSS train 0.80655 valid 0.89749, valid PER 27.78%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.6832179707288742\n",
      "  batch 100 loss: 0.7065785956382752\n",
      "  batch 150 loss: 0.7173893105983734\n",
      "  batch 200 loss: 0.6936126893758774\n",
      "  batch 250 loss: 0.7121442914009094\n",
      "  batch 300 loss: 0.7335015952587127\n",
      "  batch 350 loss: 0.7513171905279159\n",
      "  batch 400 loss: 0.7322506141662598\n",
      "  batch 450 loss: 0.733228605389595\n",
      "  batch 500 loss: 0.7251716774702072\n",
      "  batch 550 loss: 0.7232231104373932\n",
      "  batch 600 loss: 0.7505903506278991\n",
      "  batch 650 loss: 0.7415724050998688\n",
      "  batch 700 loss: 0.7124608552455902\n",
      "  batch 750 loss: 0.740620509982109\n",
      "  batch 800 loss: 0.74156867146492\n",
      "  batch 850 loss: 0.7529543244838715\n",
      "  batch 900 loss: 0.7061130809783935\n",
      "LOSS train 0.70611 valid 0.89577, valid PER 27.56%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6503033643960953\n",
      "  batch 100 loss: 0.6419610762596131\n",
      "  batch 150 loss: 0.6716713768243789\n",
      "  batch 200 loss: 0.6892691576480865\n",
      "  batch 250 loss: 0.6860515081882477\n",
      "  batch 300 loss: 0.6572439479827881\n",
      "  batch 350 loss: 0.6816992461681366\n",
      "  batch 400 loss: 0.6483053863048553\n",
      "  batch 450 loss: 0.6678629320859909\n",
      "  batch 500 loss: 0.6954659926891327\n",
      "  batch 550 loss: 0.7044647812843323\n",
      "  batch 600 loss: 0.6750541794300079\n",
      "  batch 650 loss: 0.6753410947322845\n",
      "  batch 700 loss: 0.7225315588712692\n",
      "  batch 750 loss: 0.70277514398098\n",
      "  batch 800 loss: 0.6939848756790161\n",
      "  batch 850 loss: 0.6977039241790771\n",
      "  batch 900 loss: 0.7009156596660614\n",
      "LOSS train 0.70092 valid 0.87883, valid PER 26.84%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6029635721445084\n",
      "  batch 100 loss: 0.5678759413957596\n",
      "  batch 150 loss: 0.5931006175279617\n",
      "  batch 200 loss: 0.6509730345010758\n",
      "  batch 250 loss: 0.6586603140830993\n",
      "  batch 300 loss: 0.621762101650238\n",
      "  batch 350 loss: 0.6414694172143937\n",
      "  batch 400 loss: 0.65057632625103\n",
      "  batch 450 loss: 0.6290421438217163\n",
      "  batch 500 loss: 0.6206699740886689\n",
      "  batch 550 loss: 0.6286688369512557\n",
      "  batch 600 loss: 0.6289914321899414\n",
      "  batch 650 loss: 0.6840304118394852\n",
      "  batch 700 loss: 0.6295384705066681\n",
      "  batch 750 loss: 0.6294099843502045\n",
      "  batch 800 loss: 0.6771410530805588\n",
      "  batch 850 loss: 0.6800089681148529\n",
      "  batch 900 loss: 0.6638465929031372\n",
      "LOSS train 0.66385 valid 0.89914, valid PER 26.64%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.5755873227119446\n",
      "  batch 100 loss: 0.5883829796314239\n",
      "  batch 150 loss: 0.5287658226490021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.592981721162796\n",
      "  batch 250 loss: 0.5903254455327988\n",
      "  batch 300 loss: 0.5812612748146058\n",
      "  batch 350 loss: 0.5784489917755127\n",
      "  batch 400 loss: 0.624244077205658\n",
      "  batch 450 loss: 0.6205977416038513\n",
      "  batch 500 loss: 0.634345464706421\n",
      "  batch 550 loss: 0.5750998115539551\n",
      "  batch 600 loss: 0.6313319253921509\n",
      "  batch 650 loss: 0.6349232512712478\n",
      "  batch 700 loss: 0.6094418442249299\n",
      "  batch 750 loss: 0.6235235285758972\n",
      "  batch 800 loss: 0.6105699408054351\n",
      "  batch 850 loss: 0.6446928799152374\n",
      "  batch 900 loss: 0.6401231700181961\n",
      "LOSS train 0.64012 valid 0.88616, valid PER 26.05%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.5229670441150666\n",
      "  batch 100 loss: 0.5584414142370224\n",
      "  batch 150 loss: 0.5396608966588974\n",
      "  batch 200 loss: 0.5541112887859344\n",
      "  batch 250 loss: 0.5553472250699997\n",
      "  batch 300 loss: 0.5611824905872345\n",
      "  batch 350 loss: 0.5646594762802124\n",
      "  batch 400 loss: 0.5855917936563492\n",
      "  batch 450 loss: 0.5765502154827118\n",
      "  batch 500 loss: 0.5355299377441406\n",
      "  batch 550 loss: 0.5864468675851822\n",
      "  batch 600 loss: 0.5699326330423355\n",
      "  batch 650 loss: 0.5860847574472428\n",
      "  batch 700 loss: 0.6189549440145492\n",
      "  batch 750 loss: 0.5436816334724426\n",
      "  batch 800 loss: 0.5835973531007767\n",
      "  batch 850 loss: 0.600146604180336\n",
      "  batch 900 loss: 0.59370436668396\n",
      "LOSS train 0.59370 valid 0.91893, valid PER 26.40%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.4993665099143982\n",
      "  batch 100 loss: 0.5186583876609803\n",
      "  batch 150 loss: 0.5164870816469193\n",
      "  batch 200 loss: 0.5029182755947113\n",
      "  batch 250 loss: 0.534959186911583\n",
      "  batch 300 loss: 0.5461239168047904\n",
      "  batch 350 loss: 0.5005619424581528\n",
      "  batch 400 loss: 0.518092400431633\n",
      "  batch 450 loss: 0.5217805576324462\n",
      "  batch 500 loss: 0.5546317934989929\n",
      "  batch 550 loss: 0.5630963772535325\n",
      "  batch 600 loss: 0.5193531173467636\n",
      "  batch 650 loss: 0.5589277774095536\n",
      "  batch 700 loss: 0.5597409266233444\n",
      "  batch 750 loss: 0.5452484601736068\n",
      "  batch 800 loss: 0.5277196270227432\n",
      "  batch 850 loss: 0.55979776263237\n",
      "  batch 900 loss: 0.5692266488075256\n",
      "LOSS train 0.56923 valid 0.91639, valid PER 26.06%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.48267102122306826\n",
      "  batch 100 loss: 0.47126162707805636\n",
      "  batch 150 loss: 0.47718155562877657\n",
      "  batch 200 loss: 0.49830429553985595\n",
      "  batch 250 loss: 0.5176201891899109\n",
      "  batch 300 loss: 0.49102383375167846\n",
      "  batch 350 loss: 0.5084457671642304\n",
      "  batch 400 loss: 0.5161829322576523\n",
      "  batch 450 loss: 0.5037213706970215\n",
      "  batch 500 loss: 0.47264819979667666\n",
      "  batch 550 loss: 0.5172996348142624\n",
      "  batch 600 loss: 0.5287811386585236\n",
      "  batch 650 loss: 0.550724887251854\n",
      "  batch 700 loss: 0.5406557238101959\n",
      "  batch 750 loss: 0.5399433362483979\n",
      "  batch 800 loss: 0.5117171907424927\n",
      "  batch 850 loss: 0.510567963719368\n",
      "  batch 900 loss: 0.5512370026111603\n",
      "LOSS train 0.55124 valid 0.92187, valid PER 26.03%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.4713617253303528\n",
      "  batch 100 loss: 0.45515489459037783\n",
      "  batch 150 loss: 0.461374027132988\n",
      "  batch 200 loss: 0.454030254483223\n",
      "  batch 250 loss: 0.48436188995838164\n",
      "  batch 300 loss: 0.464763602912426\n",
      "  batch 350 loss: 0.477788051366806\n",
      "  batch 400 loss: 0.4926684641838074\n",
      "  batch 450 loss: 0.4826036822795868\n",
      "  batch 500 loss: 0.48864625751972196\n",
      "  batch 550 loss: 0.4712813875079155\n",
      "  batch 600 loss: 0.48761409044265747\n",
      "  batch 650 loss: 0.515645597577095\n",
      "  batch 700 loss: 0.4913244605064392\n",
      "  batch 750 loss: 0.49175056874752043\n",
      "  batch 800 loss: 0.5146035242080689\n",
      "  batch 850 loss: 0.4834811246395111\n",
      "  batch 900 loss: 0.48572240114212034\n",
      "LOSS train 0.48572 valid 0.94131, valid PER 25.80%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.4562970906496048\n",
      "  batch 100 loss: 0.4503846138715744\n",
      "  batch 150 loss: 0.42477326542139054\n",
      "  batch 200 loss: 0.43351726353168485\n",
      "  batch 250 loss: 0.4618172937631607\n",
      "  batch 300 loss: 0.46641273677349093\n",
      "  batch 350 loss: 0.426860491335392\n",
      "  batch 400 loss: 0.4627346724271774\n",
      "  batch 450 loss: 0.4518941915035248\n",
      "  batch 500 loss: 0.45787143588066104\n",
      "  batch 550 loss: 0.44703069001436235\n",
      "  batch 600 loss: 0.476637516617775\n",
      "  batch 650 loss: 0.4579815775156021\n",
      "  batch 700 loss: 0.456588848233223\n",
      "  batch 750 loss: 0.4663206881284714\n",
      "  batch 800 loss: 0.4630431580543518\n",
      "  batch 850 loss: 0.4814481148123741\n",
      "  batch 900 loss: 0.4663374376296997\n",
      "LOSS train 0.46634 valid 0.96663, valid PER 26.01%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.4270090699195862\n",
      "  batch 100 loss: 0.42783597320318223\n",
      "  batch 150 loss: 0.40779715657234195\n",
      "  batch 200 loss: 0.42655092060565947\n",
      "  batch 250 loss: 0.41541787326335905\n",
      "  batch 300 loss: 0.4091685685515404\n",
      "  batch 350 loss: 0.42776261299848556\n",
      "  batch 400 loss: 0.43411197513341904\n",
      "  batch 450 loss: 0.43751061379909517\n",
      "  batch 500 loss: 0.44005592226982115\n",
      "  batch 550 loss: 0.45191402822732923\n",
      "  batch 600 loss: 0.43167721927165986\n",
      "  batch 650 loss: 0.4248278024792671\n",
      "  batch 700 loss: 0.4622024631500244\n",
      "  batch 750 loss: 0.45222020387649536\n",
      "  batch 800 loss: 0.4568794649839401\n",
      "  batch 850 loss: 0.42125282049179075\n",
      "  batch 900 loss: 0.4458531978726387\n",
      "LOSS train 0.44585 valid 0.96923, valid PER 25.94%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.398470524251461\n",
      "  batch 100 loss: 0.3729608741402626\n",
      "  batch 150 loss: 0.38101008236408235\n",
      "  batch 200 loss: 0.4164481920003891\n",
      "  batch 250 loss: 0.3968603327870369\n",
      "  batch 300 loss: 0.4094747832417488\n",
      "  batch 350 loss: 0.41504338443279265\n",
      "  batch 400 loss: 0.3965939643979073\n",
      "  batch 450 loss: 0.4335357889533043\n",
      "  batch 500 loss: 0.4126466292142868\n",
      "  batch 550 loss: 0.4125692889094353\n",
      "  batch 600 loss: 0.4251753467321396\n",
      "  batch 650 loss: 0.4593529495596886\n",
      "  batch 700 loss: 0.409437372982502\n",
      "  batch 750 loss: 0.4321689411997795\n",
      "  batch 800 loss: 0.4586713981628418\n",
      "  batch 850 loss: 0.4410486650466919\n",
      "  batch 900 loss: 0.47021154910326\n",
      "LOSS train 0.47021 valid 0.97389, valid PER 25.71%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.3807257369160652\n",
      "  batch 100 loss: 0.3707485571503639\n",
      "  batch 150 loss: 0.36644286960363387\n",
      "  batch 200 loss: 0.3932941472530365\n",
      "  batch 250 loss: 0.3793841826915741\n",
      "  batch 300 loss: 0.4055058366060257\n",
      "  batch 350 loss: 0.3797733438014984\n",
      "  batch 400 loss: 0.40647358775138853\n",
      "  batch 450 loss: 0.41693867623806\n",
      "  batch 500 loss: 0.37947165429592133\n",
      "  batch 550 loss: 0.41091483473777773\n",
      "  batch 600 loss: 0.3800858497619629\n",
      "  batch 650 loss: 0.4080217233300209\n",
      "  batch 700 loss: 0.40565086513757703\n",
      "  batch 750 loss: 0.39128205478191375\n",
      "  batch 800 loss: 0.41860605239868165\n",
      "  batch 850 loss: 0.421832754611969\n",
      "  batch 900 loss: 0.4288508331775665\n",
      "LOSS train 0.42885 valid 1.01141, valid PER 26.29%\n",
      "Training finished in 10.0 minutes.\n",
      "Model saved to checkpoints/20231206_171249/model_10\n",
      "Finish Adam optimiser\n",
      "End tuning For Wider 1 Layer LSTM\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start tuning For wider 1 Layer LSTM\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 1,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 512,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 1,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 512,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_regularisation_dropout.BiLSTM(args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For Wider 1 Layer LSTM\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7116df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "Optimiser = [\"SGD_Scheduler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552e4362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning For wider 1 Layer LSTM\n",
      "Currently using SGD_Scheduler optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.112564177513122\n",
      "  batch 100 loss: 3.3055451250076295\n",
      "  batch 150 loss: 3.1033377838134766\n",
      "  batch 200 loss: 2.848377914428711\n",
      "  batch 250 loss: 2.6916692543029783\n",
      "  batch 300 loss: 2.5320166826248167\n",
      "  batch 350 loss: 2.4163118410110473\n",
      "  batch 400 loss: 2.3821061420440675\n",
      "  batch 450 loss: 2.298174934387207\n",
      "  batch 500 loss: 2.1950828647613525\n",
      "  batch 550 loss: 2.148702714443207\n",
      "  batch 600 loss: 2.0816590332984926\n",
      "  batch 650 loss: 1.9812079930305482\n",
      "  batch 700 loss: 1.978788387775421\n",
      "  batch 750 loss: 1.910346179008484\n",
      "  batch 800 loss: 1.8898598313331605\n",
      "  batch 850 loss: 1.855208122730255\n",
      "  batch 900 loss: 1.8303350925445556\n",
      "LOSS train 1.83034 valid 1.77394, valid PER 66.49%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7773738479614258\n",
      "  batch 100 loss: 1.7048520350456238\n",
      "  batch 150 loss: 1.687483582496643\n",
      "  batch 200 loss: 1.6931783413887025\n",
      "  batch 250 loss: 1.6754626035690308\n",
      "  batch 300 loss: 1.6452559566497802\n",
      "  batch 350 loss: 1.5632278394699097\n",
      "  batch 400 loss: 1.576799032688141\n",
      "  batch 450 loss: 1.5301846075057983\n",
      "  batch 500 loss: 1.5600339841842652\n",
      "  batch 550 loss: 1.5559243416786195\n",
      "  batch 600 loss: 1.4952423882484436\n",
      "  batch 650 loss: 1.522273802757263\n",
      "  batch 700 loss: 1.4733252453804015\n",
      "  batch 750 loss: 1.4641216063499451\n",
      "  batch 800 loss: 1.4099257159233094\n",
      "  batch 850 loss: 1.4313468909263611\n",
      "  batch 900 loss: 1.4379336833953857\n",
      "LOSS train 1.43793 valid 1.44767, valid PER 48.69%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.3886015653610229\n",
      "  batch 100 loss: 1.366029007434845\n",
      "  batch 150 loss: 1.353901002407074\n",
      "  batch 200 loss: 1.3304008269309997\n",
      "  batch 250 loss: 1.3267189931869507\n",
      "  batch 300 loss: 1.3228640174865722\n",
      "  batch 350 loss: 1.3839481925964356\n",
      "  batch 400 loss: 1.334188666343689\n",
      "  batch 450 loss: 1.3077604591846466\n",
      "  batch 500 loss: 1.290568778514862\n",
      "  batch 550 loss: 1.2944285047054291\n",
      "  batch 600 loss: 1.2713052749633789\n",
      "  batch 650 loss: 1.2518135142326354\n",
      "  batch 700 loss: 1.2684427165985108\n",
      "  batch 750 loss: 1.3198069202899934\n",
      "  batch 800 loss: 1.2508600914478303\n",
      "  batch 850 loss: 1.2698866534233093\n",
      "  batch 900 loss: 1.1993106985092163\n",
      "LOSS train 1.19931 valid 1.26060, valid PER 39.74%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.1938135206699372\n",
      "  batch 100 loss: 1.197511714696884\n",
      "  batch 150 loss: 1.164695006608963\n",
      "  batch 200 loss: 1.1833217561244964\n",
      "  batch 250 loss: 1.1948456144332886\n",
      "  batch 300 loss: 1.1923024773597717\n",
      "  batch 350 loss: 1.1201550626754762\n",
      "  batch 400 loss: 1.1700655913352966\n",
      "  batch 450 loss: 1.145487664937973\n",
      "  batch 500 loss: 1.134441145658493\n",
      "  batch 550 loss: 1.1683559763431548\n",
      "  batch 600 loss: 1.1706827306747436\n",
      "  batch 650 loss: 1.1335314583778382\n",
      "  batch 700 loss: 1.1234169209003448\n",
      "  batch 750 loss: 1.0969884610176086\n",
      "  batch 800 loss: 1.0693062508106232\n",
      "  batch 850 loss: 1.1205146169662477\n",
      "  batch 900 loss: 1.1409079265594482\n",
      "LOSS train 1.14091 valid 1.13056, valid PER 35.67%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.0551985549926757\n",
      "  batch 100 loss: 1.0477689242362975\n",
      "  batch 150 loss: 1.1075274574756622\n",
      "  batch 200 loss: 1.028916883468628\n",
      "  batch 250 loss: 1.0343690288066865\n",
      "  batch 300 loss: 1.0501510751247407\n",
      "  batch 350 loss: 1.0434362983703613\n",
      "  batch 400 loss: 1.042139152288437\n",
      "  batch 450 loss: 1.0287963271141052\n",
      "  batch 500 loss: 1.0454744875431061\n",
      "  batch 550 loss: 1.001128795146942\n",
      "  batch 600 loss: 1.0683081543445587\n",
      "  batch 650 loss: 1.0356646192073822\n",
      "  batch 700 loss: 1.0605861163139343\n",
      "  batch 750 loss: 0.9879935896396637\n",
      "  batch 800 loss: 1.0149254941940307\n",
      "  batch 850 loss: 1.0126247811317444\n",
      "  batch 900 loss: 1.0190848553180694\n",
      "LOSS train 1.01908 valid 1.06063, valid PER 33.17%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.984426795244217\n",
      "  batch 100 loss: 0.9326104617118836\n",
      "  batch 150 loss: 0.9283128976821899\n",
      "  batch 200 loss: 0.9509677362442016\n",
      "  batch 250 loss: 0.9676533114910125\n",
      "  batch 300 loss: 0.9535315269231797\n",
      "  batch 350 loss: 0.9516485071182251\n",
      "  batch 400 loss: 0.9436843633651734\n",
      "  batch 450 loss: 0.963204950094223\n",
      "  batch 500 loss: 0.9296079587936401\n",
      "  batch 550 loss: 0.9663208937644958\n",
      "  batch 600 loss: 0.9182919299602509\n",
      "  batch 650 loss: 0.9449067974090576\n",
      "  batch 700 loss: 0.9381703996658325\n",
      "  batch 750 loss: 0.9260595524311066\n",
      "  batch 800 loss: 0.9270554125308991\n",
      "  batch 850 loss: 0.9064816439151764\n",
      "  batch 900 loss: 0.930480319261551\n",
      "LOSS train 0.93048 valid 1.03309, valid PER 31.98%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8851507139205933\n",
      "  batch 100 loss: 0.8708521819114685\n",
      "  batch 150 loss: 0.8660313522815705\n",
      "  batch 200 loss: 0.8504735755920411\n",
      "  batch 250 loss: 0.8460403978824615\n",
      "  batch 300 loss: 0.8448071706295014\n",
      "  batch 350 loss: 0.8693743646144867\n",
      "  batch 400 loss: 0.8647735631465912\n",
      "  batch 450 loss: 0.870691938996315\n",
      "  batch 500 loss: 0.8545606672763825\n",
      "  batch 550 loss: 0.844929485321045\n",
      "  batch 600 loss: 0.8619939351081848\n",
      "  batch 650 loss: 0.8452461898326874\n",
      "  batch 700 loss: 0.877537395954132\n",
      "  batch 750 loss: 0.8517371070384979\n",
      "  batch 800 loss: 0.8431842982769012\n",
      "  batch 850 loss: 0.8637402176856994\n",
      "  batch 900 loss: 0.871930673122406\n",
      "LOSS train 0.87193 valid 0.97884, valid PER 30.22%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.7729248142242432\n",
      "  batch 100 loss: 0.7717534625530242\n",
      "  batch 150 loss: 0.7762844049930573\n",
      "  batch 200 loss: 0.7832789826393127\n",
      "  batch 250 loss: 0.7831273007392884\n",
      "  batch 300 loss: 0.7627836394309998\n",
      "  batch 350 loss: 0.833757221698761\n",
      "  batch 400 loss: 0.779658123254776\n",
      "  batch 450 loss: 0.798713207244873\n",
      "  batch 500 loss: 0.8204682183265686\n",
      "  batch 550 loss: 0.7592017096281052\n",
      "  batch 600 loss: 0.8150299596786499\n",
      "  batch 650 loss: 0.8303065431118012\n",
      "  batch 700 loss: 0.7865476167201996\n",
      "  batch 750 loss: 0.7971019303798675\n",
      "  batch 800 loss: 0.8055996406078338\n",
      "  batch 850 loss: 0.7743537724018097\n",
      "  batch 900 loss: 0.8161766916513443\n",
      "LOSS train 0.81618 valid 0.96927, valid PER 29.75%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.6824681222438812\n",
      "  batch 100 loss: 0.7268907362222672\n",
      "  batch 150 loss: 0.735295969247818\n",
      "  batch 200 loss: 0.7128906589746475\n",
      "  batch 250 loss: 0.7419603133201599\n",
      "  batch 300 loss: 0.7434264612197876\n",
      "  batch 350 loss: 0.7435945928096771\n",
      "  batch 400 loss: 0.7344870305061341\n",
      "  batch 450 loss: 0.7281754207611084\n",
      "  batch 500 loss: 0.7217341297864914\n",
      "  batch 550 loss: 0.7305391198396682\n",
      "  batch 600 loss: 0.7647449046373367\n",
      "  batch 650 loss: 0.7371530461311341\n",
      "  batch 700 loss: 0.710121278166771\n",
      "  batch 750 loss: 0.7301660692691803\n",
      "  batch 800 loss: 0.7375584095716476\n",
      "  batch 850 loss: 0.761021363735199\n",
      "  batch 900 loss: 0.7065119212865829\n",
      "LOSS train 0.70651 valid 0.94559, valid PER 28.24%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6293934869766236\n",
      "  batch 100 loss: 0.6492951971292495\n",
      "  batch 150 loss: 0.6803255808353424\n",
      "  batch 200 loss: 0.688374547958374\n",
      "  batch 250 loss: 0.6876283246278763\n",
      "  batch 300 loss: 0.6514671766757965\n",
      "  batch 350 loss: 0.6684878325462341\n",
      "  batch 400 loss: 0.6263963901996612\n",
      "  batch 450 loss: 0.6470406222343444\n",
      "  batch 500 loss: 0.6788906425237655\n",
      "  batch 550 loss: 0.7001681888103485\n",
      "  batch 600 loss: 0.678470778465271\n",
      "  batch 650 loss: 0.6747050720453263\n",
      "  batch 700 loss: 0.6971954250335693\n",
      "  batch 750 loss: 0.6601184558868408\n",
      "  batch 800 loss: 0.6907635724544525\n",
      "  batch 850 loss: 0.6947097635269165\n",
      "  batch 900 loss: 0.6996808832883835\n",
      "LOSS train 0.69968 valid 0.94218, valid PER 28.62%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.580042433142662\n",
      "  batch 100 loss: 0.5523619687557221\n",
      "  batch 150 loss: 0.5903399723768235\n",
      "  batch 200 loss: 0.6354758155345916\n",
      "  batch 250 loss: 0.610354990363121\n",
      "  batch 300 loss: 0.5724534803628921\n",
      "  batch 350 loss: 0.6171360218524933\n",
      "  batch 400 loss: 0.6405189299583435\n",
      "  batch 450 loss: 0.6279274773597717\n",
      "  batch 500 loss: 0.6149204647541047\n",
      "  batch 550 loss: 0.6259889703989029\n",
      "  batch 600 loss: 0.6189589762687683\n",
      "  batch 650 loss: 0.6587277317047119\n",
      "  batch 700 loss: 0.6025292617082596\n",
      "  batch 750 loss: 0.6097808063030243\n",
      "  batch 800 loss: 0.6484034532308578\n",
      "  batch 850 loss: 0.6495812273025513\n",
      "  batch 900 loss: 0.6579419684410095\n",
      "LOSS train 0.65794 valid 0.92993, valid PER 27.11%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.5526751780509949\n",
      "  batch 100 loss: 0.5339941757917405\n",
      "  batch 150 loss: 0.5088021945953369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.545132417678833\n",
      "  batch 250 loss: 0.5553662419319153\n",
      "  batch 300 loss: 0.5499298286437988\n",
      "  batch 350 loss: 0.5465474557876587\n",
      "  batch 400 loss: 0.5794289118051529\n",
      "  batch 450 loss: 0.5699537533521652\n",
      "  batch 500 loss: 0.5821452504396438\n",
      "  batch 550 loss: 0.5427112525701523\n",
      "  batch 600 loss: 0.5712779611349106\n",
      "  batch 650 loss: 0.5996668606996536\n",
      "  batch 700 loss: 0.5973357647657395\n",
      "  batch 750 loss: 0.5629798120260239\n",
      "  batch 800 loss: 0.5807257044315338\n",
      "  batch 850 loss: 0.6155434507131576\n",
      "  batch 900 loss: 0.6034255754947663\n",
      "Epoch 00012: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.60343 valid 0.95061, valid PER 27.24%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.46423712879419327\n",
      "  batch 100 loss: 0.44658171236515043\n",
      "  batch 150 loss: 0.43232550263404845\n",
      "  batch 200 loss: 0.4347713911533356\n",
      "  batch 250 loss: 0.43242357224225997\n",
      "  batch 300 loss: 0.42777034670114517\n",
      "  batch 350 loss: 0.4211980152130127\n",
      "  batch 400 loss: 0.4294757318496704\n",
      "  batch 450 loss: 0.41160573542118073\n",
      "  batch 500 loss: 0.42630651652812956\n",
      "  batch 550 loss: 0.4440526926517487\n",
      "  batch 600 loss: 0.4372387284040451\n",
      "  batch 650 loss: 0.4505490857362747\n",
      "  batch 700 loss: 0.45496014326810835\n",
      "  batch 750 loss: 0.40160915374755857\n",
      "  batch 800 loss: 0.41972089976072313\n",
      "  batch 850 loss: 0.4438100975751877\n",
      "  batch 900 loss: 0.46207444489002225\n",
      "LOSS train 0.46207 valid 0.91159, valid PER 25.66%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.3688629424571991\n",
      "  batch 100 loss: 0.35172668159008025\n",
      "  batch 150 loss: 0.37101113617420195\n",
      "  batch 200 loss: 0.35300591111183166\n",
      "  batch 250 loss: 0.3722654068470001\n",
      "  batch 300 loss: 0.4046605497598648\n",
      "  batch 350 loss: 0.36757213532924654\n",
      "  batch 400 loss: 0.3813613086938858\n",
      "  batch 450 loss: 0.3779592594504356\n",
      "  batch 500 loss: 0.39118860006332395\n",
      "  batch 550 loss: 0.4014094510674477\n",
      "  batch 600 loss: 0.361442865729332\n",
      "  batch 650 loss: 0.39022053927183153\n",
      "  batch 700 loss: 0.4078977510333061\n",
      "  batch 750 loss: 0.373178668320179\n",
      "  batch 800 loss: 0.3845518487691879\n",
      "  batch 850 loss: 0.4005386620759964\n",
      "  batch 900 loss: 0.3961195382475853\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.39612 valid 0.94751, valid PER 26.07%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.30700753659009933\n",
      "  batch 100 loss: 0.2963498482108116\n",
      "  batch 150 loss: 0.3071575513482094\n",
      "  batch 200 loss: 0.30915922820568087\n",
      "  batch 250 loss: 0.3239104571938515\n",
      "  batch 300 loss: 0.2978770872950554\n",
      "  batch 350 loss: 0.30412226855754854\n",
      "  batch 400 loss: 0.31093407809734347\n",
      "  batch 450 loss: 0.29062905073165896\n",
      "  batch 500 loss: 0.2852650412917137\n",
      "  batch 550 loss: 0.2982392531633377\n",
      "  batch 600 loss: 0.3089665946364403\n",
      "  batch 650 loss: 0.31742177069187166\n",
      "  batch 700 loss: 0.32111311435699463\n",
      "  batch 750 loss: 0.3139719051122665\n",
      "  batch 800 loss: 0.2992732262611389\n",
      "  batch 850 loss: 0.28333999902009965\n",
      "  batch 900 loss: 0.3189569169282913\n",
      "Epoch 00015: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.31896 valid 0.94633, valid PER 25.36%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.26379950761795046\n",
      "  batch 100 loss: 0.2514354455471039\n",
      "  batch 150 loss: 0.27014009416103363\n",
      "  batch 200 loss: 0.24973928600549697\n",
      "  batch 250 loss: 0.2632292267680168\n",
      "  batch 300 loss: 0.25295740261673927\n",
      "  batch 350 loss: 0.253836427628994\n",
      "  batch 400 loss: 0.2678093223273754\n",
      "  batch 450 loss: 0.2623051860928535\n",
      "  batch 500 loss: 0.2468320022523403\n",
      "  batch 550 loss: 0.2331287968158722\n",
      "  batch 600 loss: 0.2524254569411278\n",
      "  batch 650 loss: 0.26880249217152596\n",
      "  batch 700 loss: 0.25613393023610115\n",
      "  batch 750 loss: 0.2585108834505081\n",
      "  batch 800 loss: 0.2676171852648258\n",
      "  batch 850 loss: 0.2534250625967979\n",
      "  batch 900 loss: 0.27763109147548676\n",
      "Epoch 00016: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.27763 valid 0.95545, valid PER 25.26%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.24241580352187156\n",
      "  batch 100 loss: 0.24070551201701165\n",
      "  batch 150 loss: 0.23235012724995613\n",
      "  batch 200 loss: 0.22610395953059195\n",
      "  batch 250 loss: 0.2514528851211071\n",
      "  batch 300 loss: 0.23240997284650802\n",
      "  batch 350 loss: 0.22696606665849686\n",
      "  batch 400 loss: 0.24167746394872666\n",
      "  batch 450 loss: 0.23260773211717606\n",
      "  batch 500 loss: 0.21996693849563598\n",
      "  batch 550 loss: 0.23273147106170655\n",
      "  batch 600 loss: 0.23236196458339692\n",
      "  batch 650 loss: 0.23064779356122017\n",
      "  batch 700 loss: 0.23813448041677476\n",
      "  batch 750 loss: 0.23334945484995842\n",
      "  batch 800 loss: 0.22013763219118118\n",
      "  batch 850 loss: 0.23991164669394494\n",
      "  batch 900 loss: 0.2199006849527359\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.21990 valid 0.96492, valid PER 25.02%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.21730943351984025\n",
      "  batch 100 loss: 0.22575555264949798\n",
      "  batch 150 loss: 0.23777425587177276\n",
      "  batch 200 loss: 0.22143128171563148\n",
      "  batch 250 loss: 0.24054772689938544\n",
      "  batch 300 loss: 0.21628524407744407\n",
      "  batch 350 loss: 0.21903482034802438\n",
      "  batch 400 loss: 0.21286755606532096\n",
      "  batch 450 loss: 0.2231994503736496\n",
      "  batch 500 loss: 0.21892363652586938\n",
      "  batch 550 loss: 0.2358031864464283\n",
      "  batch 600 loss: 0.2146834161877632\n",
      "  batch 650 loss: 0.20593488171696664\n",
      "  batch 700 loss: 0.22172111093997957\n",
      "  batch 750 loss: 0.20748504281044006\n",
      "  batch 800 loss: 0.2241765323281288\n",
      "  batch 850 loss: 0.21955844327807428\n",
      "  batch 900 loss: 0.223114797770977\n",
      "Epoch 00018: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.22311 valid 0.97111, valid PER 25.10%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.2198069792985916\n",
      "  batch 100 loss: 0.21231642961502076\n",
      "  batch 150 loss: 0.20398356273770332\n",
      "  batch 200 loss: 0.21673046082258224\n",
      "  batch 250 loss: 0.21329168379306793\n",
      "  batch 300 loss: 0.22558940470218658\n",
      "  batch 350 loss: 0.207104434967041\n",
      "  batch 400 loss: 0.21421679586172104\n",
      "  batch 450 loss: 0.22286327064037323\n",
      "  batch 500 loss: 0.20957924649119378\n",
      "  batch 550 loss: 0.2023581138253212\n",
      "  batch 600 loss: 0.20293382614850997\n",
      "  batch 650 loss: 0.22976516753435136\n",
      "  batch 700 loss: 0.21321707963943481\n",
      "  batch 750 loss: 0.2145444656908512\n",
      "  batch 800 loss: 0.22342797487974167\n",
      "  batch 850 loss: 0.22462444975972176\n",
      "  batch 900 loss: 0.2182474072277546\n",
      "Epoch 00019: reducing learning rate of group 0 to 3.9062e-03.\n",
      "LOSS train 0.21825 valid 0.97292, valid PER 25.09%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.2298302437365055\n",
      "  batch 100 loss: 0.20752591982483864\n",
      "  batch 150 loss: 0.20170023545622826\n",
      "  batch 200 loss: 0.20248486638069152\n",
      "  batch 250 loss: 0.20566473811864852\n",
      "  batch 300 loss: 0.21662146255373954\n",
      "  batch 350 loss: 0.2044629693031311\n",
      "  batch 400 loss: 0.21796327963471412\n",
      "  batch 450 loss: 0.2205374740064144\n",
      "  batch 500 loss: 0.20350728094577789\n",
      "  batch 550 loss: 0.23239262148737908\n",
      "  batch 600 loss: 0.203534834086895\n",
      "  batch 650 loss: 0.2091583226621151\n",
      "  batch 700 loss: 0.21322521090507507\n",
      "  batch 750 loss: 0.19999765530228614\n",
      "  batch 800 loss: 0.22754083082079887\n",
      "  batch 850 loss: 0.21565019994974136\n",
      "  batch 900 loss: 0.2080000199377537\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.9531e-03.\n",
      "LOSS train 0.20800 valid 0.97465, valid PER 25.07%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_181727/model_13\n",
      "Currently using dropout rate of 0.1\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.083596773147583\n",
      "  batch 100 loss: 3.2707718276977538\n",
      "  batch 150 loss: 3.093993787765503\n",
      "  batch 200 loss: 2.830637464523315\n",
      "  batch 250 loss: 2.683871989250183\n",
      "  batch 300 loss: 2.513520178794861\n",
      "  batch 350 loss: 2.406189727783203\n",
      "  batch 400 loss: 2.371434798240662\n",
      "  batch 450 loss: 2.2922443389892577\n",
      "  batch 500 loss: 2.1908040690422057\n",
      "  batch 550 loss: 2.1405355310440064\n",
      "  batch 600 loss: 2.0773284077644347\n",
      "  batch 650 loss: 1.9832226490974427\n",
      "  batch 700 loss: 1.9752568364143372\n",
      "  batch 750 loss: 1.9171182322502136\n",
      "  batch 800 loss: 1.895899429321289\n",
      "  batch 850 loss: 1.8581232810020447\n",
      "  batch 900 loss: 1.8302151083946228\n",
      "LOSS train 1.83022 valid 1.77937, valid PER 67.34%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7859054207801819\n",
      "  batch 100 loss: 1.7168671870231629\n",
      "  batch 150 loss: 1.6916190791130066\n",
      "  batch 200 loss: 1.6975223517417908\n",
      "  batch 250 loss: 1.6988643431663513\n",
      "  batch 300 loss: 1.656251118183136\n",
      "  batch 350 loss: 1.5778555941581727\n",
      "  batch 400 loss: 1.5739039754867554\n",
      "  batch 450 loss: 1.5409926891326904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 500 loss: 1.573147301673889\n",
      "  batch 550 loss: 1.5530430817604064\n",
      "  batch 600 loss: 1.5183616781234741\n",
      "  batch 650 loss: 1.537800328731537\n",
      "  batch 700 loss: 1.4968914556503297\n",
      "  batch 750 loss: 1.4851606011390686\n",
      "  batch 800 loss: 1.4263258004188537\n",
      "  batch 850 loss: 1.432520112991333\n",
      "  batch 900 loss: 1.440121932029724\n",
      "LOSS train 1.44012 valid 1.40190, valid PER 46.74%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4149198865890502\n",
      "  batch 100 loss: 1.3668462562561035\n",
      "  batch 150 loss: 1.3523076891899108\n",
      "  batch 200 loss: 1.337201819419861\n",
      "  batch 250 loss: 1.314195649623871\n",
      "  batch 300 loss: 1.3120716691017151\n",
      "  batch 350 loss: 1.361608247756958\n",
      "  batch 400 loss: 1.326514518260956\n",
      "  batch 450 loss: 1.282817393541336\n",
      "  batch 500 loss: 1.2617322516441345\n",
      "  batch 550 loss: 1.2751185286045075\n",
      "  batch 600 loss: 1.2507956182956697\n",
      "  batch 650 loss: 1.2217898857593537\n",
      "  batch 700 loss: 1.240902339220047\n",
      "  batch 750 loss: 1.3006118083000182\n",
      "  batch 800 loss: 1.2163991916179657\n",
      "  batch 850 loss: 1.2367117941379546\n",
      "  batch 900 loss: 1.1754445517063141\n",
      "LOSS train 1.17544 valid 1.20106, valid PER 36.42%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.169426896572113\n",
      "  batch 100 loss: 1.2022577691078187\n",
      "  batch 150 loss: 1.1602061414718627\n",
      "  batch 200 loss: 1.1896561419963836\n",
      "  batch 250 loss: 1.174954618215561\n",
      "  batch 300 loss: 1.1721675384044647\n",
      "  batch 350 loss: 1.1012169444561004\n",
      "  batch 400 loss: 1.1792925548553468\n",
      "  batch 450 loss: 1.1262908363342286\n",
      "  batch 500 loss: 1.1327537620067596\n",
      "  batch 550 loss: 1.162437824010849\n",
      "  batch 600 loss: 1.1569162857532502\n",
      "  batch 650 loss: 1.1296345245838166\n",
      "  batch 700 loss: 1.1088785970211028\n",
      "  batch 750 loss: 1.0893674623966216\n",
      "  batch 800 loss: 1.0658759129047395\n",
      "  batch 850 loss: 1.1016595780849456\n",
      "  batch 900 loss: 1.1357369446754455\n",
      "LOSS train 1.13574 valid 1.08807, valid PER 34.48%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.0686357641220092\n",
      "  batch 100 loss: 1.0524998378753663\n",
      "  batch 150 loss: 1.1024083530902862\n",
      "  batch 200 loss: 1.0384752023220063\n",
      "  batch 250 loss: 1.0454812049865723\n",
      "  batch 300 loss: 1.057286559343338\n",
      "  batch 350 loss: 1.0377236700057983\n",
      "  batch 400 loss: 1.052811042070389\n",
      "  batch 450 loss: 1.0241069090366364\n",
      "  batch 500 loss: 1.0454435765743255\n",
      "  batch 550 loss: 1.0057292187213898\n",
      "  batch 600 loss: 1.070066545009613\n",
      "  batch 650 loss: 1.0540933907032013\n",
      "  batch 700 loss: 1.0716911113262177\n",
      "  batch 750 loss: 1.0000856482982636\n",
      "  batch 800 loss: 1.0466362929344177\n",
      "  batch 850 loss: 1.0325178825855255\n",
      "  batch 900 loss: 1.0467010092735292\n",
      "LOSS train 1.04670 valid 1.02523, valid PER 32.34%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.017935037612915\n",
      "  batch 100 loss: 0.9630692481994629\n",
      "  batch 150 loss: 0.9653973042964935\n",
      "  batch 200 loss: 0.9576567900180817\n",
      "  batch 250 loss: 1.0124943733215332\n",
      "  batch 300 loss: 0.9857143914699554\n",
      "  batch 350 loss: 0.9913080906867981\n",
      "  batch 400 loss: 0.9614781332015991\n",
      "  batch 450 loss: 0.9826139450073242\n",
      "  batch 500 loss: 0.9692331171035766\n",
      "  batch 550 loss: 0.9876970183849335\n",
      "  batch 600 loss: 0.9566951990127563\n",
      "  batch 650 loss: 0.9620580184459686\n",
      "  batch 700 loss: 0.964599415063858\n",
      "  batch 750 loss: 0.9470082592964172\n",
      "  batch 800 loss: 0.9524272811412812\n",
      "  batch 850 loss: 0.9403882467746735\n",
      "  batch 900 loss: 0.9575069332122803\n",
      "LOSS train 0.95751 valid 1.00990, valid PER 31.16%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.924492279291153\n",
      "  batch 100 loss: 0.9221464252471924\n",
      "  batch 150 loss: 0.887698370218277\n",
      "  batch 200 loss: 0.8995325696468354\n",
      "  batch 250 loss: 0.90339204788208\n",
      "  batch 300 loss: 0.8894159400463104\n",
      "  batch 350 loss: 0.9038765823841095\n",
      "  batch 400 loss: 0.9061978888511658\n",
      "  batch 450 loss: 0.921933114528656\n",
      "  batch 500 loss: 0.9012926304340363\n",
      "  batch 550 loss: 0.8977492451667786\n",
      "  batch 600 loss: 0.923948894739151\n",
      "  batch 650 loss: 0.8964930152893067\n",
      "  batch 700 loss: 0.9181118392944336\n",
      "  batch 750 loss: 0.8849667131900787\n",
      "  batch 800 loss: 0.9108203637599945\n",
      "  batch 850 loss: 0.9189879357814789\n",
      "  batch 900 loss: 0.9341876184940339\n",
      "LOSS train 0.93419 valid 0.95924, valid PER 30.02%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8454601526260376\n",
      "  batch 100 loss: 0.8337744653224946\n",
      "  batch 150 loss: 0.8466602039337158\n",
      "  batch 200 loss: 0.8304103541374207\n",
      "  batch 250 loss: 0.8507315522432327\n",
      "  batch 300 loss: 0.8045012164115906\n",
      "  batch 350 loss: 0.8772883331775665\n",
      "  batch 400 loss: 0.8345097875595093\n",
      "  batch 450 loss: 0.8468039667606354\n",
      "  batch 500 loss: 0.8887357175350189\n",
      "  batch 550 loss: 0.8218383407592773\n",
      "  batch 600 loss: 0.8781940710544586\n",
      "  batch 650 loss: 0.8905014300346374\n",
      "  batch 700 loss: 0.8422486400604248\n",
      "  batch 750 loss: 0.8490350937843323\n",
      "  batch 800 loss: 0.866201697587967\n",
      "  batch 850 loss: 0.8511989605426789\n",
      "  batch 900 loss: 0.8807178449630737\n",
      "LOSS train 0.88072 valid 0.94842, valid PER 29.45%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7675485467910766\n",
      "  batch 100 loss: 0.7990129387378693\n",
      "  batch 150 loss: 0.7899481439590454\n",
      "  batch 200 loss: 0.7711837589740753\n",
      "  batch 250 loss: 0.805744743347168\n",
      "  batch 300 loss: 0.8194502937793732\n",
      "  batch 350 loss: 0.8504327774047852\n",
      "  batch 400 loss: 0.8269271534681321\n",
      "  batch 450 loss: 0.7864867711067199\n",
      "  batch 500 loss: 0.7929147899150848\n",
      "  batch 550 loss: 0.8079703831672669\n",
      "  batch 600 loss: 0.8162561571598053\n",
      "  batch 650 loss: 0.7949206674098969\n",
      "  batch 700 loss: 0.7721343982219696\n",
      "  batch 750 loss: 0.7832317137718201\n",
      "  batch 800 loss: 0.8118465280532837\n",
      "  batch 850 loss: 0.8283883202075958\n",
      "  batch 900 loss: 0.7762779033184052\n",
      "LOSS train 0.77628 valid 0.91284, valid PER 27.84%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7177064853906632\n",
      "  batch 100 loss: 0.734853338599205\n",
      "  batch 150 loss: 0.7637405002117157\n",
      "  batch 200 loss: 0.764507343173027\n",
      "  batch 250 loss: 0.7712044537067413\n",
      "  batch 300 loss: 0.7321739864349365\n",
      "  batch 350 loss: 0.7497459775209427\n",
      "  batch 400 loss: 0.7256962913274765\n",
      "  batch 450 loss: 0.7312338209152222\n",
      "  batch 500 loss: 0.7729661393165589\n",
      "  batch 550 loss: 0.7903577387332916\n",
      "  batch 600 loss: 0.7623645287752151\n",
      "  batch 650 loss: 0.738389219045639\n",
      "  batch 700 loss: 0.7738620913028718\n",
      "  batch 750 loss: 0.7608254158496857\n",
      "  batch 800 loss: 0.7624061000347138\n",
      "  batch 850 loss: 0.7704907858371735\n",
      "  batch 900 loss: 0.7716580760478974\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.77166 valid 0.91389, valid PER 28.94%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.6482671493291855\n",
      "  batch 100 loss: 0.6028855711221695\n",
      "  batch 150 loss: 0.6306106561422348\n",
      "  batch 200 loss: 0.6635649985074997\n",
      "  batch 250 loss: 0.650287355184555\n",
      "  batch 300 loss: 0.6152953863143921\n",
      "  batch 350 loss: 0.6434477603435517\n",
      "  batch 400 loss: 0.6563133877515793\n",
      "  batch 450 loss: 0.6492882215976715\n",
      "  batch 500 loss: 0.6329059553146362\n",
      "  batch 550 loss: 0.6442043846845626\n",
      "  batch 600 loss: 0.6128197783231735\n",
      "  batch 650 loss: 0.6686706972122193\n",
      "  batch 700 loss: 0.6156509101390839\n",
      "  batch 750 loss: 0.6244197762012482\n",
      "  batch 800 loss: 0.6540033298730851\n",
      "  batch 850 loss: 0.6524022197723389\n",
      "  batch 900 loss: 0.6468136739730835\n",
      "LOSS train 0.64681 valid 0.84724, valid PER 25.59%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.5944065302610397\n",
      "  batch 100 loss: 0.59838957965374\n",
      "  batch 150 loss: 0.5522900438308715\n",
      "  batch 200 loss: 0.5909866940975189\n",
      "  batch 250 loss: 0.5950771623849869\n",
      "  batch 300 loss: 0.5914005166292191\n",
      "  batch 350 loss: 0.5763247257471085\n",
      "  batch 400 loss: 0.5997657918930054\n",
      "  batch 450 loss: 0.6099363428354263\n",
      "  batch 500 loss: 0.6027268272638321\n",
      "  batch 550 loss: 0.5643691289424896\n",
      "  batch 600 loss: 0.5896617877483368\n",
      "  batch 650 loss: 0.625832250714302\n",
      "  batch 700 loss: 0.6135012239217759\n",
      "  batch 750 loss: 0.5944991338253022\n",
      "  batch 800 loss: 0.5850201427936554\n",
      "  batch 850 loss: 0.6504392182826996\n",
      "  batch 900 loss: 0.6361968916654587\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.63620 valid 0.85905, valid PER 25.78%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.5332745307683945\n",
      "  batch 100 loss: 0.5427258026599884\n",
      "  batch 150 loss: 0.5213497346639633\n",
      "  batch 200 loss: 0.5367037063837051\n",
      "  batch 250 loss: 0.5333043098449707\n",
      "  batch 300 loss: 0.5287865298986435\n",
      "  batch 350 loss: 0.5083520090579987\n",
      "  batch 400 loss: 0.5416731160879135\n",
      "  batch 450 loss: 0.5258543866872788\n",
      "  batch 500 loss: 0.5138013547658921\n",
      "  batch 550 loss: 0.5499275875091553\n",
      "  batch 600 loss: 0.5162224942445754\n",
      "  batch 650 loss: 0.5439413779973984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 700 loss: 0.5481157433986664\n",
      "  batch 750 loss: 0.4892977702617645\n",
      "  batch 800 loss: 0.5161862432956695\n",
      "  batch 850 loss: 0.5497252070903778\n",
      "  batch 900 loss: 0.5532179367542267\n",
      "LOSS train 0.55322 valid 0.84588, valid PER 24.99%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.49431702971458436\n",
      "  batch 100 loss: 0.4992279362678528\n",
      "  batch 150 loss: 0.49597338676452635\n",
      "  batch 200 loss: 0.48906121760606763\n",
      "  batch 250 loss: 0.49046292662620544\n",
      "  batch 300 loss: 0.5274491453170777\n",
      "  batch 350 loss: 0.4772061192989349\n",
      "  batch 400 loss: 0.5012496158480644\n",
      "  batch 450 loss: 0.5106979823112487\n",
      "  batch 500 loss: 0.5080923062562942\n",
      "  batch 550 loss: 0.5258022391796112\n",
      "  batch 600 loss: 0.4810876148939133\n",
      "  batch 650 loss: 0.507489197254181\n",
      "  batch 700 loss: 0.5288402992486954\n",
      "  batch 750 loss: 0.48455176293849944\n",
      "  batch 800 loss: 0.4786686950922012\n",
      "  batch 850 loss: 0.5238566321134567\n",
      "  batch 900 loss: 0.5154296189546586\n",
      "Epoch 00014: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.51543 valid 0.85045, valid PER 25.30%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.47556118488311766\n",
      "  batch 100 loss: 0.46512976944446566\n",
      "  batch 150 loss: 0.4675068646669388\n",
      "  batch 200 loss: 0.4660798096656799\n",
      "  batch 250 loss: 0.49593341469764707\n",
      "  batch 300 loss: 0.4529270839691162\n",
      "  batch 350 loss: 0.4710083031654358\n",
      "  batch 400 loss: 0.4642534017562866\n",
      "  batch 450 loss: 0.45418456822633746\n",
      "  batch 500 loss: 0.4321743994951248\n",
      "  batch 550 loss: 0.47005907118320467\n",
      "  batch 600 loss: 0.47676329374313353\n",
      "  batch 650 loss: 0.47921963810920715\n",
      "  batch 700 loss: 0.48315929651260375\n",
      "  batch 750 loss: 0.4570542061328888\n",
      "  batch 800 loss: 0.45707746922969816\n",
      "  batch 850 loss: 0.4354309517145157\n",
      "  batch 900 loss: 0.46580488920211793\n",
      "Epoch 00015: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.46580 valid 0.85177, valid PER 24.94%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.4553050398826599\n",
      "  batch 100 loss: 0.42596394777297975\n",
      "  batch 150 loss: 0.43709541141986846\n",
      "  batch 200 loss: 0.4350321125984192\n",
      "  batch 250 loss: 0.44476780146360395\n",
      "  batch 300 loss: 0.42985422909259796\n",
      "  batch 350 loss: 0.4420398569107056\n",
      "  batch 400 loss: 0.44524185329675675\n",
      "  batch 450 loss: 0.45138793528079985\n",
      "  batch 500 loss: 0.4159690481424332\n",
      "  batch 550 loss: 0.43102398097515104\n",
      "  batch 600 loss: 0.4328415447473526\n",
      "  batch 650 loss: 0.4465601420402527\n",
      "  batch 700 loss: 0.4487480792403221\n",
      "  batch 750 loss: 0.44785006701946256\n",
      "  batch 800 loss: 0.44635482609272004\n",
      "  batch 850 loss: 0.4313549533486366\n",
      "  batch 900 loss: 0.44817030608654024\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.44817 valid 0.85252, valid PER 24.62%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.4258276212215424\n",
      "  batch 100 loss: 0.43580836057662964\n",
      "  batch 150 loss: 0.42304049551486966\n",
      "  batch 200 loss: 0.4183096626400948\n",
      "  batch 250 loss: 0.43517679870128634\n",
      "  batch 300 loss: 0.43224143862724307\n",
      "  batch 350 loss: 0.40532444208860396\n",
      "  batch 400 loss: 0.45384819626808165\n",
      "  batch 450 loss: 0.4294040310382843\n",
      "  batch 500 loss: 0.41317317247390745\n",
      "  batch 550 loss: 0.42444095253944397\n",
      "  batch 600 loss: 0.44104627907276156\n",
      "  batch 650 loss: 0.41492250204086306\n",
      "  batch 700 loss: 0.41592929989099503\n",
      "  batch 750 loss: 0.4251182723045349\n",
      "  batch 800 loss: 0.4033741733431816\n",
      "  batch 850 loss: 0.436966712474823\n",
      "  batch 900 loss: 0.4126134997606277\n",
      "Epoch 00017: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.41261 valid 0.85543, valid PER 24.60%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.4217999851703644\n",
      "  batch 100 loss: 0.4216600769758225\n",
      "  batch 150 loss: 0.44859475135803223\n",
      "  batch 200 loss: 0.41476773351430896\n",
      "  batch 250 loss: 0.4372314512729645\n",
      "  batch 300 loss: 0.4033119669556618\n",
      "  batch 350 loss: 0.41530336856842043\n",
      "  batch 400 loss: 0.41002594828605654\n",
      "  batch 450 loss: 0.43120244741439817\n",
      "  batch 500 loss: 0.4191299369931221\n",
      "  batch 550 loss: 0.4312361305952072\n",
      "  batch 600 loss: 0.40179874539375304\n",
      "  batch 650 loss: 0.40229501396417616\n",
      "  batch 700 loss: 0.4264885997772217\n",
      "  batch 750 loss: 0.4004470431804657\n",
      "  batch 800 loss: 0.4083756282925606\n",
      "  batch 850 loss: 0.4100147259235382\n",
      "  batch 900 loss: 0.4294126629829407\n",
      "Epoch 00018: reducing learning rate of group 0 to 3.9062e-03.\n",
      "LOSS train 0.42941 valid 0.85640, valid PER 24.64%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.413121198117733\n",
      "  batch 100 loss: 0.41322693318128584\n",
      "  batch 150 loss: 0.4131433814764023\n",
      "  batch 200 loss: 0.43032463371753693\n",
      "  batch 250 loss: 0.4035185265541077\n",
      "  batch 300 loss: 0.4130734345316887\n",
      "  batch 350 loss: 0.40178870528936383\n",
      "  batch 400 loss: 0.4161302477121353\n",
      "  batch 450 loss: 0.4285677444934845\n",
      "  batch 500 loss: 0.4099954971671104\n",
      "  batch 550 loss: 0.4009388095140457\n",
      "  batch 600 loss: 0.3973641854524612\n",
      "  batch 650 loss: 0.4463552266359329\n",
      "  batch 700 loss: 0.41357459098100663\n",
      "  batch 750 loss: 0.4038673359155655\n",
      "  batch 800 loss: 0.43174475848674776\n",
      "  batch 850 loss: 0.41435841023921965\n",
      "  batch 900 loss: 0.4099121072888374\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.9531e-03.\n",
      "LOSS train 0.40991 valid 0.85674, valid PER 24.68%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.4209131062030792\n",
      "  batch 100 loss: 0.4114421960711479\n",
      "  batch 150 loss: 0.41362394392490387\n",
      "  batch 200 loss: 0.4201638329029083\n",
      "  batch 250 loss: 0.4134537225961685\n",
      "  batch 300 loss: 0.41706652730703353\n",
      "  batch 350 loss: 0.3896288055181503\n",
      "  batch 400 loss: 0.4152184438705444\n",
      "  batch 450 loss: 0.4110553288459778\n",
      "  batch 500 loss: 0.3954735165834427\n",
      "  batch 550 loss: 0.42885119616985323\n",
      "  batch 600 loss: 0.39519241034984587\n",
      "  batch 650 loss: 0.40691336393356325\n",
      "  batch 700 loss: 0.41063880294561383\n",
      "  batch 750 loss: 0.39026698768138884\n",
      "  batch 800 loss: 0.43318680763244627\n",
      "  batch 850 loss: 0.41872183591127393\n",
      "  batch 900 loss: 0.43960018873214723\n",
      "Epoch 00020: reducing learning rate of group 0 to 9.7656e-04.\n",
      "LOSS train 0.43960 valid 0.85744, valid PER 24.60%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_182550/model_13\n",
      "Currently using dropout rate of 0.2\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.085046625137329\n",
      "  batch 100 loss: 3.3125234270095825\n",
      "  batch 150 loss: 3.087031350135803\n",
      "  batch 200 loss: 2.836699757575989\n",
      "  batch 250 loss: 2.687627954483032\n",
      "  batch 300 loss: 2.5270652532577516\n",
      "  batch 350 loss: 2.4150728797912597\n",
      "  batch 400 loss: 2.3841252946853637\n",
      "  batch 450 loss: 2.3148409128189087\n",
      "  batch 500 loss: 2.20307115316391\n",
      "  batch 550 loss: 2.150293242931366\n",
      "  batch 600 loss: 2.092309601306915\n",
      "  batch 650 loss: 1.999490306377411\n",
      "  batch 700 loss: 1.9905320358276368\n",
      "  batch 750 loss: 1.9248139595985412\n",
      "  batch 800 loss: 1.908957929611206\n",
      "  batch 850 loss: 1.8705404281616211\n",
      "  batch 900 loss: 1.841537094116211\n",
      "LOSS train 1.84154 valid 1.79648, valid PER 68.14%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.8001688647270202\n",
      "  batch 100 loss: 1.7302380180358887\n",
      "  batch 150 loss: 1.7093550944328308\n",
      "  batch 200 loss: 1.7138652539253234\n",
      "  batch 250 loss: 1.7121184802055358\n",
      "  batch 300 loss: 1.673713903427124\n",
      "  batch 350 loss: 1.5863016486167907\n",
      "  batch 400 loss: 1.5899533748626709\n",
      "  batch 450 loss: 1.554180109500885\n",
      "  batch 500 loss: 1.5870913505554198\n",
      "  batch 550 loss: 1.564224157333374\n",
      "  batch 600 loss: 1.534714915752411\n",
      "  batch 650 loss: 1.5539824509620666\n",
      "  batch 700 loss: 1.5114398050308226\n",
      "  batch 750 loss: 1.5033383202552795\n",
      "  batch 800 loss: 1.4417610812187194\n",
      "  batch 850 loss: 1.4487879157066346\n",
      "  batch 900 loss: 1.4609267783164979\n",
      "LOSS train 1.46093 valid 1.38838, valid PER 46.64%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4256187510490417\n",
      "  batch 100 loss: 1.3853577184677124\n",
      "  batch 150 loss: 1.365262417793274\n",
      "  batch 200 loss: 1.3564940524101257\n",
      "  batch 250 loss: 1.3300903177261352\n",
      "  batch 300 loss: 1.3252015042304992\n",
      "  batch 350 loss: 1.3796381759643555\n",
      "  batch 400 loss: 1.3452082777023315\n",
      "  batch 450 loss: 1.2954949474334716\n",
      "  batch 500 loss: 1.2918782925605774\n",
      "  batch 550 loss: 1.292940583229065\n",
      "  batch 600 loss: 1.2688444590568542\n",
      "  batch 650 loss: 1.2476918292045593\n",
      "  batch 700 loss: 1.2713473999500275\n",
      "  batch 750 loss: 1.321095585823059\n",
      "  batch 800 loss: 1.245825436115265\n",
      "  batch 850 loss: 1.2571493768692017\n",
      "  batch 900 loss: 1.1979848265647888\n",
      "LOSS train 1.19798 valid 1.19921, valid PER 37.64%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.2020991849899292\n",
      "  batch 100 loss: 1.2125112235546112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 150 loss: 1.1743949592113494\n",
      "  batch 200 loss: 1.2062722384929656\n",
      "  batch 250 loss: 1.1994979894161224\n",
      "  batch 300 loss: 1.2017689144611359\n",
      "  batch 350 loss: 1.1245360219478606\n",
      "  batch 400 loss: 1.1951877510547637\n",
      "  batch 450 loss: 1.1523856115341187\n",
      "  batch 500 loss: 1.1503411555290222\n",
      "  batch 550 loss: 1.187881201505661\n",
      "  batch 600 loss: 1.1676056838035584\n",
      "  batch 650 loss: 1.145479860305786\n",
      "  batch 700 loss: 1.1362339019775392\n",
      "  batch 750 loss: 1.117742292881012\n",
      "  batch 800 loss: 1.0737801373004914\n",
      "  batch 850 loss: 1.1173320603370667\n",
      "  batch 900 loss: 1.1547860181331635\n",
      "LOSS train 1.15479 valid 1.11181, valid PER 35.18%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.079086879491806\n",
      "  batch 100 loss: 1.0637152349948884\n",
      "  batch 150 loss: 1.121157455444336\n",
      "  batch 200 loss: 1.0494711124897003\n",
      "  batch 250 loss: 1.0627468037605285\n",
      "  batch 300 loss: 1.078684856891632\n",
      "  batch 350 loss: 1.0575164866447448\n",
      "  batch 400 loss: 1.0783769500255584\n",
      "  batch 450 loss: 1.0348233842849732\n",
      "  batch 500 loss: 1.0734421956539153\n",
      "  batch 550 loss: 1.0283681488037109\n",
      "  batch 600 loss: 1.0912262463569642\n",
      "  batch 650 loss: 1.0621917188167571\n",
      "  batch 700 loss: 1.0857629573345184\n",
      "  batch 750 loss: 1.0194895005226134\n",
      "  batch 800 loss: 1.0527698838710784\n",
      "  batch 850 loss: 1.0443626427650452\n",
      "  batch 900 loss: 1.0518524026870728\n",
      "LOSS train 1.05185 valid 1.02574, valid PER 32.96%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.0346035850048065\n",
      "  batch 100 loss: 0.9861235082149505\n",
      "  batch 150 loss: 0.9760527467727661\n",
      "  batch 200 loss: 0.9769883668422699\n",
      "  batch 250 loss: 1.022274216413498\n",
      "  batch 300 loss: 0.9948643529415131\n",
      "  batch 350 loss: 1.0144209170341492\n",
      "  batch 400 loss: 0.9732411301136017\n",
      "  batch 450 loss: 1.0004647374153137\n",
      "  batch 500 loss: 0.9747202014923095\n",
      "  batch 550 loss: 1.0178807365894318\n",
      "  batch 600 loss: 0.987239978313446\n",
      "  batch 650 loss: 0.9871723449230194\n",
      "  batch 700 loss: 0.9889153003692627\n",
      "  batch 750 loss: 0.9660464406013489\n",
      "  batch 800 loss: 0.9660750603675843\n",
      "  batch 850 loss: 0.9598016345500946\n",
      "  batch 900 loss: 0.9721601104736328\n",
      "LOSS train 0.97216 valid 1.01310, valid PER 31.51%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.9513873708248138\n",
      "  batch 100 loss: 0.9498625862598419\n",
      "  batch 150 loss: 0.9172491455078124\n",
      "  batch 200 loss: 0.9224578416347504\n",
      "  batch 250 loss: 0.9275325632095337\n",
      "  batch 300 loss: 0.9131894445419312\n",
      "  batch 350 loss: 0.9325192677974701\n",
      "  batch 400 loss: 0.9397326099872589\n",
      "  batch 450 loss: 0.9424926674365998\n",
      "  batch 500 loss: 0.9273175883293152\n",
      "  batch 550 loss: 0.9086172962188721\n",
      "  batch 600 loss: 0.9440862286090851\n",
      "  batch 650 loss: 0.9131745254993439\n",
      "  batch 700 loss: 0.9403146880865098\n",
      "  batch 750 loss: 0.9028455495834351\n",
      "  batch 800 loss: 0.9226836323738098\n",
      "  batch 850 loss: 0.9480091607570649\n",
      "  batch 900 loss: 0.9695314919948578\n",
      "LOSS train 0.96953 valid 0.97050, valid PER 30.73%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.8636276602745057\n",
      "  batch 100 loss: 0.8635770165920258\n",
      "  batch 150 loss: 0.8710359632968903\n",
      "  batch 200 loss: 0.8627481126785278\n",
      "  batch 250 loss: 0.8836953723430634\n",
      "  batch 300 loss: 0.8349526965618134\n",
      "  batch 350 loss: 0.9133342897891998\n",
      "  batch 400 loss: 0.8725270962715149\n",
      "  batch 450 loss: 0.8684687829017639\n",
      "  batch 500 loss: 0.9163486659526825\n",
      "  batch 550 loss: 0.8524270379543304\n",
      "  batch 600 loss: 0.8982042121887207\n",
      "  batch 650 loss: 0.9049544394016266\n",
      "  batch 700 loss: 0.8680259931087494\n",
      "  batch 750 loss: 0.8738910460472107\n",
      "  batch 800 loss: 0.8981242918968201\n",
      "  batch 850 loss: 0.8759493637084961\n",
      "  batch 900 loss: 0.8958519744873047\n",
      "LOSS train 0.89585 valid 0.93895, valid PER 29.65%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7923465740680694\n",
      "  batch 100 loss: 0.8334621453285217\n",
      "  batch 150 loss: 0.8134868323802948\n",
      "  batch 200 loss: 0.8048151576519013\n",
      "  batch 250 loss: 0.8346651989221573\n",
      "  batch 300 loss: 0.8471295344829559\n",
      "  batch 350 loss: 0.8597033476829529\n",
      "  batch 400 loss: 0.8405092799663544\n",
      "  batch 450 loss: 0.8083485531806945\n",
      "  batch 500 loss: 0.8286868846416473\n",
      "  batch 550 loss: 0.8433929169178009\n",
      "  batch 600 loss: 0.8413752746582032\n",
      "  batch 650 loss: 0.826752426624298\n",
      "  batch 700 loss: 0.8152462327480317\n",
      "  batch 750 loss: 0.8244044148921966\n",
      "  batch 800 loss: 0.8693915402889252\n",
      "  batch 850 loss: 0.8517333137989044\n",
      "  batch 900 loss: 0.8077141916751862\n",
      "LOSS train 0.80771 valid 0.92910, valid PER 28.27%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.7556626015901565\n",
      "  batch 100 loss: 0.7680678170919418\n",
      "  batch 150 loss: 0.803839852809906\n",
      "  batch 200 loss: 0.8037063610553742\n",
      "  batch 250 loss: 0.8012470364570617\n",
      "  batch 300 loss: 0.7735164749622345\n",
      "  batch 350 loss: 0.7900778198242188\n",
      "  batch 400 loss: 0.7522461760044098\n",
      "  batch 450 loss: 0.7678928899765015\n",
      "  batch 500 loss: 0.8035500705242157\n",
      "  batch 550 loss: 0.8306789767742156\n",
      "  batch 600 loss: 0.7929026055335998\n",
      "  batch 650 loss: 0.7721874743700028\n",
      "  batch 700 loss: 0.8211254119873047\n",
      "  batch 750 loss: 0.7867724597454071\n",
      "  batch 800 loss: 0.8040189909934997\n",
      "  batch 850 loss: 0.7929262673854828\n",
      "  batch 900 loss: 0.8073649382591248\n",
      "LOSS train 0.80736 valid 0.92466, valid PER 29.28%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.7126861119270325\n",
      "  batch 100 loss: 0.693153036236763\n",
      "  batch 150 loss: 0.7246366941928863\n",
      "  batch 200 loss: 0.7741018986701965\n",
      "  batch 250 loss: 0.7501218414306641\n",
      "  batch 300 loss: 0.724329537153244\n",
      "  batch 350 loss: 0.7415317296981812\n",
      "  batch 400 loss: 0.7806686329841613\n",
      "  batch 450 loss: 0.7573085355758667\n",
      "  batch 500 loss: 0.7456029891967774\n",
      "  batch 550 loss: 0.7478366696834564\n",
      "  batch 600 loss: 0.7224745017290115\n",
      "  batch 650 loss: 0.7793249773979187\n",
      "  batch 700 loss: 0.7321300971508026\n",
      "  batch 750 loss: 0.7341474509239196\n",
      "  batch 800 loss: 0.7770643866062165\n",
      "  batch 850 loss: 0.7805612719058991\n",
      "  batch 900 loss: 0.7667152082920075\n",
      "LOSS train 0.76672 valid 0.89355, valid PER 27.58%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.7102468222379684\n",
      "  batch 100 loss: 0.7158337688446045\n",
      "  batch 150 loss: 0.6690518695116043\n",
      "  batch 200 loss: 0.6877448236942292\n",
      "  batch 250 loss: 0.7140134716033936\n",
      "  batch 300 loss: 0.692475865483284\n",
      "  batch 350 loss: 0.6931723690032959\n",
      "  batch 400 loss: 0.7238903832435608\n",
      "  batch 450 loss: 0.7277310407161712\n",
      "  batch 500 loss: 0.7196132230758667\n",
      "  batch 550 loss: 0.6732327431440354\n",
      "  batch 600 loss: 0.6954589831829071\n",
      "  batch 650 loss: 0.7321888411045074\n",
      "  batch 700 loss: 0.728797225356102\n",
      "  batch 750 loss: 0.6992289656400681\n",
      "  batch 800 loss: 0.7007593107223511\n",
      "  batch 850 loss: 0.7637411683797837\n",
      "  batch 900 loss: 0.738957422375679\n",
      "LOSS train 0.73896 valid 0.87292, valid PER 26.71%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.6359772402048111\n",
      "  batch 100 loss: 0.6644436031579971\n",
      "  batch 150 loss: 0.6397760170698166\n",
      "  batch 200 loss: 0.6737213027477265\n",
      "  batch 250 loss: 0.6536263787746429\n",
      "  batch 300 loss: 0.6663542902469635\n",
      "  batch 350 loss: 0.654059681892395\n",
      "  batch 400 loss: 0.6807910436391831\n",
      "  batch 450 loss: 0.6600103944540023\n",
      "  batch 500 loss: 0.6566967272758484\n",
      "  batch 550 loss: 0.7067693281173706\n",
      "  batch 600 loss: 0.6575600510835647\n",
      "  batch 650 loss: 0.6960179114341736\n",
      "  batch 700 loss: 0.7100593143701553\n",
      "  batch 750 loss: 0.6369820529222489\n",
      "  batch 800 loss: 0.667610793709755\n",
      "  batch 850 loss: 0.707812551856041\n",
      "  batch 900 loss: 0.6925140476226807\n",
      "Epoch 00013: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.69251 valid 0.91068, valid PER 26.75%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.6005123418569565\n",
      "  batch 100 loss: 0.5872346353530884\n",
      "  batch 150 loss: 0.5692817330360412\n",
      "  batch 200 loss: 0.5638437902927399\n",
      "  batch 250 loss: 0.560503585934639\n",
      "  batch 300 loss: 0.5921784096956253\n",
      "  batch 350 loss: 0.5344489997625351\n",
      "  batch 400 loss: 0.5485556930303573\n",
      "  batch 450 loss: 0.5612921178340912\n",
      "  batch 500 loss: 0.5568870621919632\n",
      "  batch 550 loss: 0.590194930434227\n",
      "  batch 600 loss: 0.5330222481489182\n",
      "  batch 650 loss: 0.5708042812347413\n",
      "  batch 700 loss: 0.5881188029050827\n",
      "  batch 750 loss: 0.5459281992912293\n",
      "  batch 800 loss: 0.5392510896921158\n",
      "  batch 850 loss: 0.5839797598123551\n",
      "  batch 900 loss: 0.5849200046062469\n",
      "LOSS train 0.58492 valid 0.85038, valid PER 25.79%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.5183145934343338\n",
      "  batch 100 loss: 0.5100307589769364\n",
      "  batch 150 loss: 0.5125107473134994\n",
      "  batch 200 loss: 0.5325410753488541\n",
      "  batch 250 loss: 0.5454818952083588\n",
      "  batch 300 loss: 0.5129146248102188\n",
      "  batch 350 loss: 0.5305718225240708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 400 loss: 0.5244093930721283\n",
      "  batch 450 loss: 0.5191688078641892\n",
      "  batch 500 loss: 0.48099903285503387\n",
      "  batch 550 loss: 0.5264372289180755\n",
      "  batch 600 loss: 0.5361028283834457\n",
      "  batch 650 loss: 0.5361140978336334\n",
      "  batch 700 loss: 0.5496016174554825\n",
      "  batch 750 loss: 0.5291775131225586\n",
      "  batch 800 loss: 0.5205046576261521\n",
      "  batch 850 loss: 0.5081042695045471\n",
      "  batch 900 loss: 0.5370255672931671\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.53703 valid 0.85936, valid PER 25.17%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.4806993353366852\n",
      "  batch 100 loss: 0.44433277636766433\n",
      "  batch 150 loss: 0.45800987422466277\n",
      "  batch 200 loss: 0.46047449827194215\n",
      "  batch 250 loss: 0.4648360592126846\n",
      "  batch 300 loss: 0.4610844373703003\n",
      "  batch 350 loss: 0.46602651953697205\n",
      "  batch 400 loss: 0.4715236622095108\n",
      "  batch 450 loss: 0.4781614488363266\n",
      "  batch 500 loss: 0.4500925183296204\n",
      "  batch 550 loss: 0.45829766392707827\n",
      "  batch 600 loss: 0.43891619503498075\n",
      "  batch 650 loss: 0.48290381371974944\n",
      "  batch 700 loss: 0.46354605436325075\n",
      "  batch 750 loss: 0.4635003620386124\n",
      "  batch 800 loss: 0.4669212782382965\n",
      "  batch 850 loss: 0.4553825953602791\n",
      "  batch 900 loss: 0.47086910367012025\n",
      "Epoch 00016: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.47087 valid 0.85322, valid PER 24.44%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.42073872685432434\n",
      "  batch 100 loss: 0.43151408195495605\n",
      "  batch 150 loss: 0.42125608086585997\n",
      "  batch 200 loss: 0.4056734099984169\n",
      "  batch 250 loss: 0.4282973802089691\n",
      "  batch 300 loss: 0.42657654941082\n",
      "  batch 350 loss: 0.4032532995939255\n",
      "  batch 400 loss: 0.441438290476799\n",
      "  batch 450 loss: 0.42723657190799713\n",
      "  batch 500 loss: 0.4072146707773209\n",
      "  batch 550 loss: 0.41158074140548706\n",
      "  batch 600 loss: 0.4340533885359764\n",
      "  batch 650 loss: 0.4115244311094284\n",
      "  batch 700 loss: 0.4165543496608734\n",
      "  batch 750 loss: 0.412645246386528\n",
      "  batch 800 loss: 0.39621948957443237\n",
      "  batch 850 loss: 0.4262819591164589\n",
      "  batch 900 loss: 0.4115974286198616\n",
      "Epoch 00017: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.41160 valid 0.85266, valid PER 24.31%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.3987995707988739\n",
      "  batch 100 loss: 0.40064954936504366\n",
      "  batch 150 loss: 0.4257835713028908\n",
      "  batch 200 loss: 0.3984989795088768\n",
      "  batch 250 loss: 0.4135070359706879\n",
      "  batch 300 loss: 0.38276193886995313\n",
      "  batch 350 loss: 0.3936278420686722\n",
      "  batch 400 loss: 0.38611345648765566\n",
      "  batch 450 loss: 0.40744457364082337\n",
      "  batch 500 loss: 0.4005317631363869\n",
      "  batch 550 loss: 0.41165869623422624\n",
      "  batch 600 loss: 0.3817288303375244\n",
      "  batch 650 loss: 0.37806139409542083\n",
      "  batch 700 loss: 0.4050138646364212\n",
      "  batch 750 loss: 0.38800107210874557\n",
      "  batch 800 loss: 0.38179179847240446\n",
      "  batch 850 loss: 0.3945705994963646\n",
      "  batch 900 loss: 0.4058792245388031\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.40588 valid 0.85770, valid PER 24.10%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.3777406021952629\n",
      "  batch 100 loss: 0.37879619032144546\n",
      "  batch 150 loss: 0.3826360735297203\n",
      "  batch 200 loss: 0.3950056380033493\n",
      "  batch 250 loss: 0.3790022790431976\n",
      "  batch 300 loss: 0.38898140013217924\n",
      "  batch 350 loss: 0.37404727190732956\n",
      "  batch 400 loss: 0.3813654792308807\n",
      "  batch 450 loss: 0.39075984686613086\n",
      "  batch 500 loss: 0.3939347690343857\n",
      "  batch 550 loss: 0.3714678058028221\n",
      "  batch 600 loss: 0.36397442638874056\n",
      "  batch 650 loss: 0.4107150515913963\n",
      "  batch 700 loss: 0.3811971864104271\n",
      "  batch 750 loss: 0.37255027532577517\n",
      "  batch 800 loss: 0.38943803161382673\n",
      "  batch 850 loss: 0.3795936009287834\n",
      "  batch 900 loss: 0.390115992128849\n",
      "Epoch 00019: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.39012 valid 0.86116, valid PER 24.32%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.3939046108722687\n",
      "  batch 100 loss: 0.3688593351840973\n",
      "  batch 150 loss: 0.38082524985074995\n",
      "  batch 200 loss: 0.38196936100721357\n",
      "  batch 250 loss: 0.3760916373133659\n",
      "  batch 300 loss: 0.3757777866721153\n",
      "  batch 350 loss: 0.34951069205999374\n",
      "  batch 400 loss: 0.38383050858974455\n",
      "  batch 450 loss: 0.38105581283569334\n",
      "  batch 500 loss: 0.3580650299787521\n",
      "  batch 550 loss: 0.3934344017505646\n",
      "  batch 600 loss: 0.36424833923578265\n",
      "  batch 650 loss: 0.37976754039525984\n",
      "  batch 700 loss: 0.3740513452887535\n",
      "  batch 750 loss: 0.35917695224285123\n",
      "  batch 800 loss: 0.3936848372220993\n",
      "  batch 850 loss: 0.37853905469179155\n",
      "  batch 900 loss: 0.3918547949194908\n",
      "Epoch 00020: reducing learning rate of group 0 to 3.9062e-03.\n",
      "LOSS train 0.39185 valid 0.86420, valid PER 24.26%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_183419/model_14\n",
      "Currently using dropout rate of 0.3\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.088354768753052\n",
      "  batch 100 loss: 3.3106798839569094\n",
      "  batch 150 loss: 3.1136553192138674\n",
      "  batch 200 loss: 2.853907895088196\n",
      "  batch 250 loss: 2.683180890083313\n",
      "  batch 300 loss: 2.535752878189087\n",
      "  batch 350 loss: 2.426166744232178\n",
      "  batch 400 loss: 2.3921919202804567\n",
      "  batch 450 loss: 2.3109256076812743\n",
      "  batch 500 loss: 2.208707950115204\n",
      "  batch 550 loss: 2.1586089396476744\n",
      "  batch 600 loss: 2.092938852310181\n",
      "  batch 650 loss: 2.0072317004203795\n",
      "  batch 700 loss: 1.9958546018600465\n",
      "  batch 750 loss: 1.935198531150818\n",
      "  batch 800 loss: 1.9130743312835694\n",
      "  batch 850 loss: 1.8831582713127135\n",
      "  batch 900 loss: 1.853959710597992\n",
      "LOSS train 1.85396 valid 1.81179, valid PER 68.01%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.8142696881294251\n",
      "  batch 100 loss: 1.7413671159744262\n",
      "  batch 150 loss: 1.7158447217941284\n",
      "  batch 200 loss: 1.732230966091156\n",
      "  batch 250 loss: 1.7215426206588744\n",
      "  batch 300 loss: 1.6807607436180114\n",
      "  batch 350 loss: 1.6045073342323304\n",
      "  batch 400 loss: 1.6089899039268494\n",
      "  batch 450 loss: 1.57538147687912\n",
      "  batch 500 loss: 1.60647456407547\n",
      "  batch 550 loss: 1.5869943833351134\n",
      "  batch 600 loss: 1.5487003684043885\n",
      "  batch 650 loss: 1.5658001160621644\n",
      "  batch 700 loss: 1.5394979882240296\n",
      "  batch 750 loss: 1.5213603281974792\n",
      "  batch 800 loss: 1.4551337361335754\n",
      "  batch 850 loss: 1.4686284613609315\n",
      "  batch 900 loss: 1.4834388947486878\n",
      "LOSS train 1.48344 valid 1.39115, valid PER 47.82%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4471456623077392\n",
      "  batch 100 loss: 1.4110120844841003\n",
      "  batch 150 loss: 1.3968647336959839\n",
      "  batch 200 loss: 1.374905095100403\n",
      "  batch 250 loss: 1.356815185546875\n",
      "  batch 300 loss: 1.3604553079605102\n",
      "  batch 350 loss: 1.40545086145401\n",
      "  batch 400 loss: 1.364925503730774\n",
      "  batch 450 loss: 1.3260648369789123\n",
      "  batch 500 loss: 1.3091352844238282\n",
      "  batch 550 loss: 1.3260335445404052\n",
      "  batch 600 loss: 1.2915545070171357\n",
      "  batch 650 loss: 1.2707210350036622\n",
      "  batch 700 loss: 1.2873926520347596\n",
      "  batch 750 loss: 1.33181960105896\n",
      "  batch 800 loss: 1.2616936826705933\n",
      "  batch 850 loss: 1.288708987236023\n",
      "  batch 900 loss: 1.230796616077423\n",
      "LOSS train 1.23080 valid 1.25143, valid PER 39.42%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.2319375491142273\n",
      "  batch 100 loss: 1.2423381447792052\n",
      "  batch 150 loss: 1.1965504229068755\n",
      "  batch 200 loss: 1.233551217317581\n",
      "  batch 250 loss: 1.2232387566566467\n",
      "  batch 300 loss: 1.2340053868293763\n",
      "  batch 350 loss: 1.1503440308570863\n",
      "  batch 400 loss: 1.2171065294742585\n",
      "  batch 450 loss: 1.1778976118564606\n",
      "  batch 500 loss: 1.1735581743717194\n",
      "  batch 550 loss: 1.1968545961380004\n",
      "  batch 600 loss: 1.2027724921703338\n",
      "  batch 650 loss: 1.1720409798622131\n",
      "  batch 700 loss: 1.15310120344162\n",
      "  batch 750 loss: 1.1415633797645568\n",
      "  batch 800 loss: 1.0972417950630189\n",
      "  batch 850 loss: 1.1387656211853028\n",
      "  batch 900 loss: 1.1716225218772889\n",
      "LOSS train 1.17162 valid 1.10349, valid PER 35.56%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.107678439617157\n",
      "  batch 100 loss: 1.1022323966026306\n",
      "  batch 150 loss: 1.151023312807083\n",
      "  batch 200 loss: 1.0861064016819\n",
      "  batch 250 loss: 1.0881630730628968\n",
      "  batch 300 loss: 1.118007868528366\n",
      "  batch 350 loss: 1.0795450150966643\n",
      "  batch 400 loss: 1.1066871404647827\n",
      "  batch 450 loss: 1.0724388706684111\n",
      "  batch 500 loss: 1.0928528690338135\n",
      "  batch 550 loss: 1.0472871530056\n",
      "  batch 600 loss: 1.110901049375534\n",
      "  batch 650 loss: 1.0871000945568086\n",
      "  batch 700 loss: 1.1136151146888733\n",
      "  batch 750 loss: 1.0458114409446717\n",
      "  batch 800 loss: 1.0801370930671692\n",
      "  batch 850 loss: 1.0738892889022826\n",
      "  batch 900 loss: 1.070947184562683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 1.07095 valid 1.03832, valid PER 33.73%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.0630870580673217\n",
      "  batch 100 loss: 1.001206921339035\n",
      "  batch 150 loss: 1.0120127773284913\n",
      "  batch 200 loss: 1.0071174788475037\n",
      "  batch 250 loss: 1.0457606875896455\n",
      "  batch 300 loss: 1.0292725598812102\n",
      "  batch 350 loss: 1.042575750350952\n",
      "  batch 400 loss: 1.0068545746803284\n",
      "  batch 450 loss: 1.0401436078548432\n",
      "  batch 500 loss: 1.0014583003520965\n",
      "  batch 550 loss: 1.0362426841259003\n",
      "  batch 600 loss: 1.0132618129253388\n",
      "  batch 650 loss: 1.0154386484622955\n",
      "  batch 700 loss: 1.0191441416740417\n",
      "  batch 750 loss: 1.0014268600940703\n",
      "  batch 800 loss: 0.9972923946380615\n",
      "  batch 850 loss: 0.9896293890476227\n",
      "  batch 900 loss: 1.0055956959724426\n",
      "LOSS train 1.00560 valid 1.02327, valid PER 32.30%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.9692187523841858\n",
      "  batch 100 loss: 0.9912802934646606\n",
      "  batch 150 loss: 0.9543732559680939\n",
      "  batch 200 loss: 0.945831533074379\n",
      "  batch 250 loss: 0.9593860542774201\n",
      "  batch 300 loss: 0.9465419125556945\n",
      "  batch 350 loss: 0.9698902046680451\n",
      "  batch 400 loss: 0.968582261800766\n",
      "  batch 450 loss: 0.9603104674816132\n",
      "  batch 500 loss: 0.9684510898590087\n",
      "  batch 550 loss: 0.9507203674316407\n",
      "  batch 600 loss: 0.9706968784332275\n",
      "  batch 650 loss: 0.9426015973091125\n",
      "  batch 700 loss: 0.9769072341918945\n",
      "  batch 750 loss: 0.9346572148799897\n",
      "  batch 800 loss: 0.9546830034255982\n",
      "  batch 850 loss: 0.9756349813938141\n",
      "  batch 900 loss: 0.9758973932266235\n",
      "LOSS train 0.97590 valid 0.97790, valid PER 30.92%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.9127102267742156\n",
      "  batch 100 loss: 0.9040086090564727\n",
      "  batch 150 loss: 0.9102846384048462\n",
      "  batch 200 loss: 0.8916624677181244\n",
      "  batch 250 loss: 0.9119136559963227\n",
      "  batch 300 loss: 0.8638487565517425\n",
      "  batch 350 loss: 0.9453981077671051\n",
      "  batch 400 loss: 0.8942609691619873\n",
      "  batch 450 loss: 0.9101085662841797\n",
      "  batch 500 loss: 0.9474989151954651\n",
      "  batch 550 loss: 0.8787373316287994\n",
      "  batch 600 loss: 0.9317454397678375\n",
      "  batch 650 loss: 0.9313388705253601\n",
      "  batch 700 loss: 0.8908574771881104\n",
      "  batch 750 loss: 0.9033450829982758\n",
      "  batch 800 loss: 0.9116148269176483\n",
      "  batch 850 loss: 0.9187502956390381\n",
      "  batch 900 loss: 0.9228690302371979\n",
      "LOSS train 0.92287 valid 0.95733, valid PER 29.97%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.836039605140686\n",
      "  batch 100 loss: 0.8623130929470062\n",
      "  batch 150 loss: 0.852916716337204\n",
      "  batch 200 loss: 0.8387819039821625\n",
      "  batch 250 loss: 0.8732778704166413\n",
      "  batch 300 loss: 0.884244874715805\n",
      "  batch 350 loss: 0.9071308267116547\n",
      "  batch 400 loss: 0.8747899007797241\n",
      "  batch 450 loss: 0.8512648928165436\n",
      "  batch 500 loss: 0.854460586309433\n",
      "  batch 550 loss: 0.8605481481552124\n",
      "  batch 600 loss: 0.88259934425354\n",
      "  batch 650 loss: 0.8630936419963837\n",
      "  batch 700 loss: 0.8385772711038589\n",
      "  batch 750 loss: 0.8483870303630829\n",
      "  batch 800 loss: 0.9011692309379578\n",
      "  batch 850 loss: 0.8862061846256256\n",
      "  batch 900 loss: 0.8344083738327026\n",
      "LOSS train 0.83441 valid 0.92791, valid PER 28.66%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.772545667886734\n",
      "  batch 100 loss: 0.7930676239728928\n",
      "  batch 150 loss: 0.8214545488357544\n",
      "  batch 200 loss: 0.8406885933876037\n",
      "  batch 250 loss: 0.8379380512237549\n",
      "  batch 300 loss: 0.8081360447406769\n",
      "  batch 350 loss: 0.8242879390716553\n",
      "  batch 400 loss: 0.7815414237976074\n",
      "  batch 450 loss: 0.8095878207683563\n",
      "  batch 500 loss: 0.8306085252761841\n",
      "  batch 550 loss: 0.8456074631214142\n",
      "  batch 600 loss: 0.8211937719583511\n",
      "  batch 650 loss: 0.8039076244831085\n",
      "  batch 700 loss: 0.8402656948566437\n",
      "  batch 750 loss: 0.8192228877544403\n",
      "  batch 800 loss: 0.8287545049190521\n",
      "  batch 850 loss: 0.8318609988689423\n",
      "  batch 900 loss: 0.8387179458141327\n",
      "LOSS train 0.83872 valid 0.92537, valid PER 29.50%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.7375163042545319\n",
      "  batch 100 loss: 0.7209623169898987\n",
      "  batch 150 loss: 0.7546850383281708\n",
      "  batch 200 loss: 0.8025142562389374\n",
      "  batch 250 loss: 0.7932436615228653\n",
      "  batch 300 loss: 0.7412426471710205\n",
      "  batch 350 loss: 0.7745452165603638\n",
      "  batch 400 loss: 0.8039730906486511\n",
      "  batch 450 loss: 0.7897147464752198\n",
      "  batch 500 loss: 0.7803143060207367\n",
      "  batch 550 loss: 0.7807725739479064\n",
      "  batch 600 loss: 0.7599344182014466\n",
      "  batch 650 loss: 0.8104478931427002\n",
      "  batch 700 loss: 0.7634601843357086\n",
      "  batch 750 loss: 0.7613362348079682\n",
      "  batch 800 loss: 0.800003410577774\n",
      "  batch 850 loss: 0.8131444418430328\n",
      "  batch 900 loss: 0.8017541682720184\n",
      "LOSS train 0.80175 valid 0.88976, valid PER 27.68%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.7357218372821808\n",
      "  batch 100 loss: 0.7486898738145829\n",
      "  batch 150 loss: 0.7021068942546844\n",
      "  batch 200 loss: 0.7207773303985596\n",
      "  batch 250 loss: 0.7515803694725036\n",
      "  batch 300 loss: 0.7262993943691254\n",
      "  batch 350 loss: 0.7268831610679627\n",
      "  batch 400 loss: 0.7421048259735108\n",
      "  batch 450 loss: 0.7519347560405731\n",
      "  batch 500 loss: 0.7599332332611084\n",
      "  batch 550 loss: 0.7037400913238525\n",
      "  batch 600 loss: 0.7251901841163635\n",
      "  batch 650 loss: 0.7598293554782868\n",
      "  batch 700 loss: 0.7568796908855439\n",
      "  batch 750 loss: 0.7391999316215515\n",
      "  batch 800 loss: 0.724115863442421\n",
      "  batch 850 loss: 0.8039855790138245\n",
      "  batch 900 loss: 0.7723111808300018\n",
      "LOSS train 0.77231 valid 0.88172, valid PER 27.15%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.6673071718215943\n",
      "  batch 100 loss: 0.7182009398937226\n",
      "  batch 150 loss: 0.6782148277759552\n",
      "  batch 200 loss: 0.7062323850393295\n",
      "  batch 250 loss: 0.6915525323152543\n",
      "  batch 300 loss: 0.6948843324184417\n",
      "  batch 350 loss: 0.6925205254554748\n",
      "  batch 400 loss: 0.7174317336082459\n",
      "  batch 450 loss: 0.7049655467271805\n",
      "  batch 500 loss: 0.6841345888376236\n",
      "  batch 550 loss: 0.7440054023265839\n",
      "  batch 600 loss: 0.6906354361772538\n",
      "  batch 650 loss: 0.7302965950965882\n",
      "  batch 700 loss: 0.729040025472641\n",
      "  batch 750 loss: 0.6689037197828293\n",
      "  batch 800 loss: 0.7097196179628372\n",
      "  batch 850 loss: 0.7295573514699936\n",
      "  batch 900 loss: 0.725688054561615\n",
      "Epoch 00013: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.72569 valid 0.89459, valid PER 27.41%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.626363594532013\n",
      "  batch 100 loss: 0.6105227220058441\n",
      "  batch 150 loss: 0.6081027293205261\n",
      "  batch 200 loss: 0.601170911192894\n",
      "  batch 250 loss: 0.59932182431221\n",
      "  batch 300 loss: 0.631713193655014\n",
      "  batch 350 loss: 0.5728983950614929\n",
      "  batch 400 loss: 0.5912929958105088\n",
      "  batch 450 loss: 0.5961699378490448\n",
      "  batch 500 loss: 0.6074047428369522\n",
      "  batch 550 loss: 0.6284281313419342\n",
      "  batch 600 loss: 0.5687955766916275\n",
      "  batch 650 loss: 0.6016724580526351\n",
      "  batch 700 loss: 0.6339911127090454\n",
      "  batch 750 loss: 0.5899351948499679\n",
      "  batch 800 loss: 0.5695899283885956\n",
      "  batch 850 loss: 0.6205655312538148\n",
      "  batch 900 loss: 0.6032581549882888\n",
      "LOSS train 0.60326 valid 0.84953, valid PER 26.00%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.5566234177350998\n",
      "  batch 100 loss: 0.5540292221307754\n",
      "  batch 150 loss: 0.5549955409765244\n",
      "  batch 200 loss: 0.5664955794811248\n",
      "  batch 250 loss: 0.5857266455888748\n",
      "  batch 300 loss: 0.5555976486206055\n",
      "  batch 350 loss: 0.5629634487628937\n",
      "  batch 400 loss: 0.5709720641374588\n",
      "  batch 450 loss: 0.567727409005165\n",
      "  batch 500 loss: 0.5403342008590698\n",
      "  batch 550 loss: 0.5648290532827377\n",
      "  batch 600 loss: 0.5917385864257813\n",
      "  batch 650 loss: 0.5801225066184997\n",
      "  batch 700 loss: 0.5909697645902634\n",
      "  batch 750 loss: 0.5733286261558532\n",
      "  batch 800 loss: 0.566281104683876\n",
      "  batch 850 loss: 0.5409791362285614\n",
      "  batch 900 loss: 0.5670111602544785\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.56701 valid 0.85144, valid PER 25.24%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.5282771980762482\n",
      "  batch 100 loss: 0.48930066436529157\n",
      "  batch 150 loss: 0.49794340014457705\n",
      "  batch 200 loss: 0.49625130236148834\n",
      "  batch 250 loss: 0.511840608716011\n",
      "  batch 300 loss: 0.49922205626964566\n",
      "  batch 350 loss: 0.51277596950531\n",
      "  batch 400 loss: 0.5205821943283081\n",
      "  batch 450 loss: 0.5082027661800385\n",
      "  batch 500 loss: 0.49149383723735807\n",
      "  batch 550 loss: 0.48999862611293793\n",
      "  batch 600 loss: 0.48680525600910185\n",
      "  batch 650 loss: 0.5054331886768341\n",
      "  batch 700 loss: 0.4869437766075134\n",
      "  batch 750 loss: 0.5009803473949432\n",
      "  batch 800 loss: 0.5085837656259536\n",
      "  batch 850 loss: 0.4899749982357025\n",
      "  batch 900 loss: 0.5098043018579483\n",
      "LOSS train 0.50980 valid 0.84923, valid PER 24.87%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.4719114798307419\n",
      "  batch 100 loss: 0.4869678318500519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 150 loss: 0.4695447093248367\n",
      "  batch 200 loss: 0.4596595650911331\n",
      "  batch 250 loss: 0.47629631996154786\n",
      "  batch 300 loss: 0.4864446094632149\n",
      "  batch 350 loss: 0.45120300590991974\n",
      "  batch 400 loss: 0.5016255861520768\n",
      "  batch 450 loss: 0.4818764042854309\n",
      "  batch 500 loss: 0.47168093144893647\n",
      "  batch 550 loss: 0.47665621876716613\n",
      "  batch 600 loss: 0.5030154162645339\n",
      "  batch 650 loss: 0.4715789979696274\n",
      "  batch 700 loss: 0.47409560799598693\n",
      "  batch 750 loss: 0.47063573718070983\n",
      "  batch 800 loss: 0.4678617638349533\n",
      "  batch 850 loss: 0.48902686417102814\n",
      "  batch 900 loss: 0.4754356372356415\n",
      "Epoch 00017: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.47544 valid 0.85536, valid PER 24.83%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.4526678740978241\n",
      "  batch 100 loss: 0.4507301950454712\n",
      "  batch 150 loss: 0.4690092611312866\n",
      "  batch 200 loss: 0.44686767280101775\n",
      "  batch 250 loss: 0.4540708392858505\n",
      "  batch 300 loss: 0.42259536027908323\n",
      "  batch 350 loss: 0.43620520800352097\n",
      "  batch 400 loss: 0.4253639602661133\n",
      "  batch 450 loss: 0.45773694932460784\n",
      "  batch 500 loss: 0.4424322599172592\n",
      "  batch 550 loss: 0.4485077798366547\n",
      "  batch 600 loss: 0.4259941881895065\n",
      "  batch 650 loss: 0.42323580086231233\n",
      "  batch 700 loss: 0.44756891250610353\n",
      "  batch 750 loss: 0.4316033798456192\n",
      "  batch 800 loss: 0.4303079944849014\n",
      "  batch 850 loss: 0.43646331429481505\n",
      "  batch 900 loss: 0.4587458294630051\n",
      "Epoch 00018: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.45875 valid 0.85662, valid PER 24.58%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.4048847988247871\n",
      "  batch 100 loss: 0.4122382199764252\n",
      "  batch 150 loss: 0.41142339408397677\n",
      "  batch 200 loss: 0.42993513494729996\n",
      "  batch 250 loss: 0.4139224782586098\n",
      "  batch 300 loss: 0.4171741819381714\n",
      "  batch 350 loss: 0.40886527717113497\n",
      "  batch 400 loss: 0.4187960171699524\n",
      "  batch 450 loss: 0.4304269805550575\n",
      "  batch 500 loss: 0.42403747856616975\n",
      "  batch 550 loss: 0.41566681206226347\n",
      "  batch 600 loss: 0.400325286090374\n",
      "  batch 650 loss: 0.45486914575099946\n",
      "  batch 700 loss: 0.41739934593439104\n",
      "  batch 750 loss: 0.40826621174812316\n",
      "  batch 800 loss: 0.43248265981674194\n",
      "  batch 850 loss: 0.42212688237428664\n",
      "  batch 900 loss: 0.4294472110271454\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.42945 valid 0.86465, valid PER 24.68%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.4184035935997963\n",
      "  batch 100 loss: 0.4073417150974274\n",
      "  batch 150 loss: 0.4083710420131683\n",
      "  batch 200 loss: 0.40418088138103486\n",
      "  batch 250 loss: 0.4134202364087105\n",
      "  batch 300 loss: 0.4141820335388184\n",
      "  batch 350 loss: 0.3817263826727867\n",
      "  batch 400 loss: 0.41090087860822677\n",
      "  batch 450 loss: 0.40930742770433426\n",
      "  batch 500 loss: 0.38990776270627975\n",
      "  batch 550 loss: 0.43113082319498064\n",
      "  batch 600 loss: 0.3900667482614517\n",
      "  batch 650 loss: 0.4043402799963951\n",
      "  batch 700 loss: 0.4058500194549561\n",
      "  batch 750 loss: 0.38430107742547986\n",
      "  batch 800 loss: 0.41781283646821976\n",
      "  batch 850 loss: 0.41604187548160554\n",
      "  batch 900 loss: 0.4246035733819008\n",
      "Epoch 00020: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.42460 valid 0.86598, valid PER 24.54%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_184229/model_16\n",
      "Currently using dropout rate of 0.4\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.0827366161346434\n",
      "  batch 100 loss: 3.3117276525497434\n",
      "  batch 150 loss: 3.089013295173645\n",
      "  batch 200 loss: 2.8515174913406374\n",
      "  batch 250 loss: 2.68836624622345\n",
      "  batch 300 loss: 2.5419102334976196\n",
      "  batch 350 loss: 2.4295863914489746\n",
      "  batch 400 loss: 2.3931494784355163\n",
      "  batch 450 loss: 2.319454593658447\n",
      "  batch 500 loss: 2.2121402645111083\n",
      "  batch 550 loss: 2.1658383989334107\n",
      "  batch 600 loss: 2.0972308492660523\n",
      "  batch 650 loss: 2.0235915040969847\n",
      "  batch 700 loss: 2.006009397506714\n",
      "  batch 750 loss: 1.949101824760437\n",
      "  batch 800 loss: 1.9326641654968262\n",
      "  batch 850 loss: 1.897056632041931\n",
      "  batch 900 loss: 1.8609757494926453\n",
      "LOSS train 1.86098 valid 1.82524, valid PER 68.05%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.8259012722969055\n",
      "  batch 100 loss: 1.7510694193840026\n",
      "  batch 150 loss: 1.735411901473999\n",
      "  batch 200 loss: 1.7487694454193115\n",
      "  batch 250 loss: 1.739296109676361\n",
      "  batch 300 loss: 1.708085105419159\n",
      "  batch 350 loss: 1.6245357942581178\n",
      "  batch 400 loss: 1.6387435245513915\n",
      "  batch 450 loss: 1.5888694906234742\n",
      "  batch 500 loss: 1.6260793447494506\n",
      "  batch 550 loss: 1.6095322585105896\n",
      "  batch 600 loss: 1.5730538010597228\n",
      "  batch 650 loss: 1.5936073565483093\n",
      "  batch 700 loss: 1.5613316917419433\n",
      "  batch 750 loss: 1.5501270484924317\n",
      "  batch 800 loss: 1.4821480751037597\n",
      "  batch 850 loss: 1.4881625771522522\n",
      "  batch 900 loss: 1.5112322068214417\n",
      "LOSS train 1.51123 valid 1.42063, valid PER 50.31%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4811918616294861\n",
      "  batch 100 loss: 1.4489505195617676\n",
      "  batch 150 loss: 1.4260136890411377\n",
      "  batch 200 loss: 1.401995871067047\n",
      "  batch 250 loss: 1.3947185444831849\n",
      "  batch 300 loss: 1.3884142112731934\n",
      "  batch 350 loss: 1.4433976626396179\n",
      "  batch 400 loss: 1.4088495016098022\n",
      "  batch 450 loss: 1.354752402305603\n",
      "  batch 500 loss: 1.3437356019020081\n",
      "  batch 550 loss: 1.3543671035766602\n",
      "  batch 600 loss: 1.3347045695781707\n",
      "  batch 650 loss: 1.2985940885543823\n",
      "  batch 700 loss: 1.3177397894859313\n",
      "  batch 750 loss: 1.3725876986980439\n",
      "  batch 800 loss: 1.294001966714859\n",
      "  batch 850 loss: 1.319322338104248\n",
      "  batch 900 loss: 1.2592251765727998\n",
      "LOSS train 1.25923 valid 1.24426, valid PER 38.89%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.2590175402164459\n",
      "  batch 100 loss: 1.2741050946712493\n",
      "  batch 150 loss: 1.2222332167625427\n",
      "  batch 200 loss: 1.2622950065135956\n",
      "  batch 250 loss: 1.2640241122245788\n",
      "  batch 300 loss: 1.2608846974372865\n",
      "  batch 350 loss: 1.1832772278785706\n",
      "  batch 400 loss: 1.239931890964508\n",
      "  batch 450 loss: 1.2079626381397248\n",
      "  batch 500 loss: 1.195625615119934\n",
      "  batch 550 loss: 1.2261467063426972\n",
      "  batch 600 loss: 1.227540030479431\n",
      "  batch 650 loss: 1.1999982917308807\n",
      "  batch 700 loss: 1.1824700784683229\n",
      "  batch 750 loss: 1.1716216099262238\n",
      "  batch 800 loss: 1.1355469381809236\n",
      "  batch 850 loss: 1.171011383533478\n",
      "  batch 900 loss: 1.2081395971775055\n",
      "LOSS train 1.20814 valid 1.12241, valid PER 36.04%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.1409867703914642\n",
      "  batch 100 loss: 1.1275166404247283\n",
      "  batch 150 loss: 1.1880222058296204\n",
      "  batch 200 loss: 1.1237470316886902\n",
      "  batch 250 loss: 1.1300912976264954\n",
      "  batch 300 loss: 1.1419568383693695\n",
      "  batch 350 loss: 1.1265903806686401\n",
      "  batch 400 loss: 1.1302745735645294\n",
      "  batch 450 loss: 1.1001706230640411\n",
      "  batch 500 loss: 1.1347505474090576\n",
      "  batch 550 loss: 1.0811327064037324\n",
      "  batch 600 loss: 1.1419818425178527\n",
      "  batch 650 loss: 1.1110733878612518\n",
      "  batch 700 loss: 1.1345861625671387\n",
      "  batch 750 loss: 1.079939512014389\n",
      "  batch 800 loss: 1.1193674564361573\n",
      "  batch 850 loss: 1.0905151462554932\n",
      "  batch 900 loss: 1.1009232187271119\n",
      "LOSS train 1.10092 valid 1.04787, valid PER 34.12%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.0995187115669252\n",
      "  batch 100 loss: 1.0443145644664764\n",
      "  batch 150 loss: 1.0524235844612122\n",
      "  batch 200 loss: 1.0350548541545868\n",
      "  batch 250 loss: 1.0757513058185577\n",
      "  batch 300 loss: 1.0738981175422668\n",
      "  batch 350 loss: 1.0657507491111755\n",
      "  batch 400 loss: 1.0326143658161164\n",
      "  batch 450 loss: 1.0514744246006011\n",
      "  batch 500 loss: 1.043470584154129\n",
      "  batch 550 loss: 1.0648056638240815\n",
      "  batch 600 loss: 1.0355879700183868\n",
      "  batch 650 loss: 1.0402736842632294\n",
      "  batch 700 loss: 1.0455977630615234\n",
      "  batch 750 loss: 1.0211496365070343\n",
      "  batch 800 loss: 1.0350915265083314\n",
      "  batch 850 loss: 1.0226273024082184\n",
      "  batch 900 loss: 1.0409380686283112\n",
      "LOSS train 1.04094 valid 1.03040, valid PER 32.86%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.0088499224185943\n",
      "  batch 100 loss: 1.0228217816352845\n",
      "  batch 150 loss: 0.9813300049304963\n",
      "  batch 200 loss: 0.9962348610162735\n",
      "  batch 250 loss: 0.9954080832004547\n",
      "  batch 300 loss: 0.9719011342525482\n",
      "  batch 350 loss: 0.9960380065441131\n",
      "  batch 400 loss: 1.0138592946529388\n",
      "  batch 450 loss: 1.0043769466876984\n",
      "  batch 500 loss: 0.9940360057353973\n",
      "  batch 550 loss: 0.9778656661510468\n",
      "  batch 600 loss: 1.0174605572223663\n",
      "  batch 650 loss: 0.9825751936435699\n",
      "  batch 700 loss: 1.0186922824382783\n",
      "  batch 750 loss: 0.9674612319469452\n",
      "  batch 800 loss: 0.9860194098949432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 850 loss: 1.01988254904747\n",
      "  batch 900 loss: 1.0243774235248566\n",
      "LOSS train 1.02438 valid 0.99432, valid PER 31.84%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.953800493478775\n",
      "  batch 100 loss: 0.9404949700832367\n",
      "  batch 150 loss: 0.9542383754253387\n",
      "  batch 200 loss: 0.9253785753250122\n",
      "  batch 250 loss: 0.9431506836414337\n",
      "  batch 300 loss: 0.8929647672176361\n",
      "  batch 350 loss: 0.9687912595272065\n",
      "  batch 400 loss: 0.928209685087204\n",
      "  batch 450 loss: 0.9316632425785065\n",
      "  batch 500 loss: 0.9783075845241547\n",
      "  batch 550 loss: 0.9084952092170715\n",
      "  batch 600 loss: 0.9683585619926453\n",
      "  batch 650 loss: 0.9679429543018341\n",
      "  batch 700 loss: 0.9253095698356628\n",
      "  batch 750 loss: 0.9478265035152436\n",
      "  batch 800 loss: 0.9472755694389343\n",
      "  batch 850 loss: 0.9495887982845307\n",
      "  batch 900 loss: 0.951854737997055\n",
      "LOSS train 0.95185 valid 0.97683, valid PER 31.12%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.8739570188522339\n",
      "  batch 100 loss: 0.9037275898456574\n",
      "  batch 150 loss: 0.8901740682125091\n",
      "  batch 200 loss: 0.8723218142986298\n",
      "  batch 250 loss: 0.9070873212814331\n",
      "  batch 300 loss: 0.915061264038086\n",
      "  batch 350 loss: 0.9364378464221954\n",
      "  batch 400 loss: 0.90197558760643\n",
      "  batch 450 loss: 0.8901735079288483\n",
      "  batch 500 loss: 0.8998702228069305\n",
      "  batch 550 loss: 0.899404548406601\n",
      "  batch 600 loss: 0.9295885646343232\n",
      "  batch 650 loss: 0.8842625570297241\n",
      "  batch 700 loss: 0.8760292994976043\n",
      "  batch 750 loss: 0.8666233038902282\n",
      "  batch 800 loss: 0.913157365322113\n",
      "  batch 850 loss: 0.9168847930431366\n",
      "  batch 900 loss: 0.8699427223205567\n",
      "LOSS train 0.86994 valid 0.93675, valid PER 29.16%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.8207318472862244\n",
      "  batch 100 loss: 0.8520166838169098\n",
      "  batch 150 loss: 0.8616346383094787\n",
      "  batch 200 loss: 0.8759863162040711\n",
      "  batch 250 loss: 0.8768539965152741\n",
      "  batch 300 loss: 0.8360825157165528\n",
      "  batch 350 loss: 0.866870390176773\n",
      "  batch 400 loss: 0.8219996845722198\n",
      "  batch 450 loss: 0.8290915954113006\n",
      "  batch 500 loss: 0.8725990545749664\n",
      "  batch 550 loss: 0.8881188440322876\n",
      "  batch 600 loss: 0.8580988311767578\n",
      "  batch 650 loss: 0.8369365286827087\n",
      "  batch 700 loss: 0.8720600247383118\n",
      "  batch 750 loss: 0.8473999357223511\n",
      "  batch 800 loss: 0.854788658618927\n",
      "  batch 850 loss: 0.8609926056861877\n",
      "  batch 900 loss: 0.8727272176742553\n",
      "LOSS train 0.87273 valid 0.93402, valid PER 29.56%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.7916606080532074\n",
      "  batch 100 loss: 0.7743244802951813\n",
      "  batch 150 loss: 0.793685348033905\n",
      "  batch 200 loss: 0.8489214837551117\n",
      "  batch 250 loss: 0.8225340485572815\n",
      "  batch 300 loss: 0.7875405406951904\n",
      "  batch 350 loss: 0.8198495483398438\n",
      "  batch 400 loss: 0.841349744796753\n",
      "  batch 450 loss: 0.826196163892746\n",
      "  batch 500 loss: 0.8069669771194458\n",
      "  batch 550 loss: 0.8191657567024231\n",
      "  batch 600 loss: 0.8028292965888977\n",
      "  batch 650 loss: 0.8543205749988556\n",
      "  batch 700 loss: 0.7870738756656647\n",
      "  batch 750 loss: 0.8040098857879638\n",
      "  batch 800 loss: 0.8289353251457214\n",
      "  batch 850 loss: 0.8579948413372039\n",
      "  batch 900 loss: 0.8370176804065704\n",
      "LOSS train 0.83702 valid 0.92058, valid PER 28.28%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.7938841140270233\n",
      "  batch 100 loss: 0.7816670334339142\n",
      "  batch 150 loss: 0.7442400205135346\n",
      "  batch 200 loss: 0.7786938136816025\n",
      "  batch 250 loss: 0.8035560691356659\n",
      "  batch 300 loss: 0.7686216580867767\n",
      "  batch 350 loss: 0.7486993396282196\n",
      "  batch 400 loss: 0.7850034952163696\n",
      "  batch 450 loss: 0.7957946348190308\n",
      "  batch 500 loss: 0.8012428951263427\n",
      "  batch 550 loss: 0.7459982460737229\n",
      "  batch 600 loss: 0.7685590553283691\n",
      "  batch 650 loss: 0.799734354019165\n",
      "  batch 700 loss: 0.8033346962928772\n",
      "  batch 750 loss: 0.775407595038414\n",
      "  batch 800 loss: 0.7781538927555084\n",
      "  batch 850 loss: 0.8433823823928833\n",
      "  batch 900 loss: 0.8184939384460449\n",
      "LOSS train 0.81849 valid 0.88673, valid PER 27.77%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7255098032951355\n",
      "  batch 100 loss: 0.7553128123283386\n",
      "  batch 150 loss: 0.7348227453231811\n",
      "  batch 200 loss: 0.7687877690792084\n",
      "  batch 250 loss: 0.7437418162822723\n",
      "  batch 300 loss: 0.7389849376678467\n",
      "  batch 350 loss: 0.7402085769176483\n",
      "  batch 400 loss: 0.754297342300415\n",
      "  batch 450 loss: 0.7642405581474304\n",
      "  batch 500 loss: 0.7374699991941452\n",
      "  batch 550 loss: 0.7783184683322907\n",
      "  batch 600 loss: 0.729931824207306\n",
      "  batch 650 loss: 0.7750418448448181\n",
      "  batch 700 loss: 0.7820066487789155\n",
      "  batch 750 loss: 0.7133343732357025\n",
      "  batch 800 loss: 0.7464319968223572\n",
      "  batch 850 loss: 0.7729005891084672\n",
      "  batch 900 loss: 0.7785889554023743\n",
      "Epoch 00013: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.77859 valid 0.90304, valid PER 27.94%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.67503324508667\n",
      "  batch 100 loss: 0.6620505678653718\n",
      "  batch 150 loss: 0.653576226234436\n",
      "  batch 200 loss: 0.6512318575382232\n",
      "  batch 250 loss: 0.6548151952028275\n",
      "  batch 300 loss: 0.6728840410709381\n",
      "  batch 350 loss: 0.6150845724344254\n",
      "  batch 400 loss: 0.632739064693451\n",
      "  batch 450 loss: 0.634902937412262\n",
      "  batch 500 loss: 0.6641128438711167\n",
      "  batch 550 loss: 0.6665536403656006\n",
      "  batch 600 loss: 0.618062081336975\n",
      "  batch 650 loss: 0.6572109586000443\n",
      "  batch 700 loss: 0.6815205824375152\n",
      "  batch 750 loss: 0.632736514210701\n",
      "  batch 800 loss: 0.6135093826055527\n",
      "  batch 850 loss: 0.6550197505950928\n",
      "  batch 900 loss: 0.6607176578044891\n",
      "LOSS train 0.66072 valid 0.85819, valid PER 26.06%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6045847064256669\n",
      "  batch 100 loss: 0.609409247636795\n",
      "  batch 150 loss: 0.6292850428819656\n",
      "  batch 200 loss: 0.6200100100040435\n",
      "  batch 250 loss: 0.6310420501232147\n",
      "  batch 300 loss: 0.6131096941232681\n",
      "  batch 350 loss: 0.6088556915521621\n",
      "  batch 400 loss: 0.6160473477840424\n",
      "  batch 450 loss: 0.6083211463689804\n",
      "  batch 500 loss: 0.579876424074173\n",
      "  batch 550 loss: 0.6188292652368546\n",
      "  batch 600 loss: 0.6395480644702911\n",
      "  batch 650 loss: 0.6367053562402725\n",
      "  batch 700 loss: 0.642205719947815\n",
      "  batch 750 loss: 0.6204905873537063\n",
      "  batch 800 loss: 0.5995214265584946\n",
      "  batch 850 loss: 0.6017698007822037\n",
      "  batch 900 loss: 0.6094959104061126\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.60950 valid 0.86725, valid PER 25.74%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.5835783821344376\n",
      "  batch 100 loss: 0.5407790726423264\n",
      "  batch 150 loss: 0.5609611636400222\n",
      "  batch 200 loss: 0.5556823164224625\n",
      "  batch 250 loss: 0.5636664956808091\n",
      "  batch 300 loss: 0.5457851970195771\n",
      "  batch 350 loss: 0.5522552037239075\n",
      "  batch 400 loss: 0.5648093193769455\n",
      "  batch 450 loss: 0.5677681547403336\n",
      "  batch 500 loss: 0.5442837053537368\n",
      "  batch 550 loss: 0.5544024562835693\n",
      "  batch 600 loss: 0.5302883851528167\n",
      "  batch 650 loss: 0.5716078615188599\n",
      "  batch 700 loss: 0.545853271484375\n",
      "  batch 750 loss: 0.5587226855754852\n",
      "  batch 800 loss: 0.5655615574121475\n",
      "  batch 850 loss: 0.548077380657196\n",
      "  batch 900 loss: 0.5498419433832169\n",
      "Epoch 00016: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.54984 valid 0.86129, valid PER 24.88%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.52279421210289\n",
      "  batch 100 loss: 0.5259068435430527\n",
      "  batch 150 loss: 0.5199829000234604\n",
      "  batch 200 loss: 0.5112760266661645\n",
      "  batch 250 loss: 0.5184343808889389\n",
      "  batch 300 loss: 0.5190675753355026\n",
      "  batch 350 loss: 0.4931391030550003\n",
      "  batch 400 loss: 0.5352216666936874\n",
      "  batch 450 loss: 0.527453071475029\n",
      "  batch 500 loss: 0.500478520989418\n",
      "  batch 550 loss: 0.5217834895849228\n",
      "  batch 600 loss: 0.5333222955465317\n",
      "  batch 650 loss: 0.5013717538118363\n",
      "  batch 700 loss: 0.49863775074481964\n",
      "  batch 750 loss: 0.4997857940196991\n",
      "  batch 800 loss: 0.5023618799448013\n",
      "  batch 850 loss: 0.5178460699319839\n",
      "  batch 900 loss: 0.5141695857048034\n",
      "LOSS train 0.51417 valid 0.85000, valid PER 24.46%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.5060026931762696\n",
      "  batch 100 loss: 0.5026335257291794\n",
      "  batch 150 loss: 0.5353750711679459\n",
      "  batch 200 loss: 0.5055455616116524\n",
      "  batch 250 loss: 0.5115348607301712\n",
      "  batch 300 loss: 0.4835765326023102\n",
      "  batch 350 loss: 0.5011678218841553\n",
      "  batch 400 loss: 0.48240873992443084\n",
      "  batch 450 loss: 0.519817778468132\n",
      "  batch 500 loss: 0.5061629486083984\n",
      "  batch 550 loss: 0.5152665716409683\n",
      "  batch 600 loss: 0.48550919592380526\n",
      "  batch 650 loss: 0.4806625747680664\n",
      "  batch 700 loss: 0.522045059800148\n",
      "  batch 750 loss: 0.47871002554893494\n",
      "  batch 800 loss: 0.4839097970724106\n",
      "  batch 850 loss: 0.5013205355405808\n",
      "  batch 900 loss: 0.5137967491149902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00018: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.51380 valid 0.85656, valid PER 24.66%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.4814688217639923\n",
      "  batch 100 loss: 0.47496295273303984\n",
      "  batch 150 loss: 0.47737405449151993\n",
      "  batch 200 loss: 0.48728883266448975\n",
      "  batch 250 loss: 0.47549105405807496\n",
      "  batch 300 loss: 0.48086371898651126\n",
      "  batch 350 loss: 0.47651096403598786\n",
      "  batch 400 loss: 0.47891995012760163\n",
      "  batch 450 loss: 0.4965215283632278\n",
      "  batch 500 loss: 0.4944687402248383\n",
      "  batch 550 loss: 0.4642008376121521\n",
      "  batch 600 loss: 0.4668017953634262\n",
      "  batch 650 loss: 0.5261361753940582\n",
      "  batch 700 loss: 0.48003258645534513\n",
      "  batch 750 loss: 0.4717707180976868\n",
      "  batch 800 loss: 0.49638315916061404\n",
      "  batch 850 loss: 0.4817576628923416\n",
      "  batch 900 loss: 0.48720639288425444\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.48721 valid 0.86012, valid PER 24.62%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.4817079544067383\n",
      "  batch 100 loss: 0.4687881565093994\n",
      "  batch 150 loss: 0.47189588129520416\n",
      "  batch 200 loss: 0.47680860340595244\n",
      "  batch 250 loss: 0.47476799428462985\n",
      "  batch 300 loss: 0.4778368353843689\n",
      "  batch 350 loss: 0.4392829674482346\n",
      "  batch 400 loss: 0.47126549780368804\n",
      "  batch 450 loss: 0.4736448112130165\n",
      "  batch 500 loss: 0.44012288749217987\n",
      "  batch 550 loss: 0.4969862198829651\n",
      "  batch 600 loss: 0.4581529128551483\n",
      "  batch 650 loss: 0.4752431154251099\n",
      "  batch 700 loss: 0.4705064016580582\n",
      "  batch 750 loss: 0.4517733433842659\n",
      "  batch 800 loss: 0.4870270484685898\n",
      "  batch 850 loss: 0.4816942936182022\n",
      "  batch 900 loss: 0.48649252533912657\n",
      "Epoch 00020: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.48649 valid 0.86208, valid PER 24.47%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_185048/model_17\n",
      "Currently using dropout rate of 0.5\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.08173855304718\n",
      "  batch 100 loss: 3.315403709411621\n",
      "  batch 150 loss: 3.0999908590316774\n",
      "  batch 200 loss: 2.869555492401123\n",
      "  batch 250 loss: 2.6880698204040527\n",
      "  batch 300 loss: 2.551163921356201\n",
      "  batch 350 loss: 2.4418137741088866\n",
      "  batch 400 loss: 2.397132325172424\n",
      "  batch 450 loss: 2.3300556135177612\n",
      "  batch 500 loss: 2.2275108337402343\n",
      "  batch 550 loss: 2.181959342956543\n",
      "  batch 600 loss: 2.1167226672172545\n",
      "  batch 650 loss: 2.0456332898139955\n",
      "  batch 700 loss: 2.0281445288658144\n",
      "  batch 750 loss: 1.974630742073059\n",
      "  batch 800 loss: 1.9552316999435424\n",
      "  batch 850 loss: 1.9169714665412902\n",
      "  batch 900 loss: 1.8904907369613648\n",
      "LOSS train 1.89049 valid 1.84698, valid PER 71.08%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.853653106689453\n",
      "  batch 100 loss: 1.7887450242042542\n",
      "  batch 150 loss: 1.7642257404327393\n",
      "  batch 200 loss: 1.7650327014923095\n",
      "  batch 250 loss: 1.7685769939422606\n",
      "  batch 300 loss: 1.7387320375442505\n",
      "  batch 350 loss: 1.660928933620453\n",
      "  batch 400 loss: 1.6622274541854858\n",
      "  batch 450 loss: 1.6278115129470825\n",
      "  batch 500 loss: 1.656682825088501\n",
      "  batch 550 loss: 1.6433143568038941\n",
      "  batch 600 loss: 1.59816162109375\n",
      "  batch 650 loss: 1.6244804453849793\n",
      "  batch 700 loss: 1.5877375483512879\n",
      "  batch 750 loss: 1.5847920203208923\n",
      "  batch 800 loss: 1.5138009524345397\n",
      "  batch 850 loss: 1.525955126285553\n",
      "  batch 900 loss: 1.549559359550476\n",
      "LOSS train 1.54956 valid 1.49429, valid PER 53.11%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.520210757255554\n",
      "  batch 100 loss: 1.4892908120155335\n",
      "  batch 150 loss: 1.474145016670227\n",
      "  batch 200 loss: 1.4472748112678528\n",
      "  batch 250 loss: 1.436994616985321\n",
      "  batch 300 loss: 1.437295627593994\n",
      "  batch 350 loss: 1.4967052173614501\n",
      "  batch 400 loss: 1.454710533618927\n",
      "  batch 450 loss: 1.4061510515213014\n",
      "  batch 500 loss: 1.3953008341789246\n",
      "  batch 550 loss: 1.399341449737549\n",
      "  batch 600 loss: 1.3710568833351136\n",
      "  batch 650 loss: 1.3429288744926453\n",
      "  batch 700 loss: 1.3673920261859893\n",
      "  batch 750 loss: 1.4075776982307433\n",
      "  batch 800 loss: 1.3340145754814148\n",
      "  batch 850 loss: 1.3551506352424623\n",
      "  batch 900 loss: 1.2955559468269349\n",
      "LOSS train 1.29556 valid 1.26621, valid PER 39.83%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.3073399710655211\n",
      "  batch 100 loss: 1.3120406186580658\n",
      "  batch 150 loss: 1.2623371744155885\n",
      "  batch 200 loss: 1.301901400089264\n",
      "  batch 250 loss: 1.290972729921341\n",
      "  batch 300 loss: 1.2975643289089203\n",
      "  batch 350 loss: 1.225346953868866\n",
      "  batch 400 loss: 1.2869049489498139\n",
      "  batch 450 loss: 1.2400476789474488\n",
      "  batch 500 loss: 1.241829344034195\n",
      "  batch 550 loss: 1.2596312165260315\n",
      "  batch 600 loss: 1.2793105959892273\n",
      "  batch 650 loss: 1.2397357153892516\n",
      "  batch 700 loss: 1.2237207651138307\n",
      "  batch 750 loss: 1.2121130776405336\n",
      "  batch 800 loss: 1.162445306777954\n",
      "  batch 850 loss: 1.2045910251140595\n",
      "  batch 900 loss: 1.2407625758647918\n",
      "LOSS train 1.24076 valid 1.15033, valid PER 36.62%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.1956169545650481\n",
      "  batch 100 loss: 1.1597885620594024\n",
      "  batch 150 loss: 1.2250253772735595\n",
      "  batch 200 loss: 1.1551865887641908\n",
      "  batch 250 loss: 1.1631486904621124\n",
      "  batch 300 loss: 1.1777538895606994\n",
      "  batch 350 loss: 1.1488691473007202\n",
      "  batch 400 loss: 1.1744406139850616\n",
      "  batch 450 loss: 1.1465353965759277\n",
      "  batch 500 loss: 1.161693754196167\n",
      "  batch 550 loss: 1.1225527894496918\n",
      "  batch 600 loss: 1.196816989183426\n",
      "  batch 650 loss: 1.1649581432342528\n",
      "  batch 700 loss: 1.1882632422447204\n",
      "  batch 750 loss: 1.1177968871593476\n",
      "  batch 800 loss: 1.1488586688041686\n",
      "  batch 850 loss: 1.1379839563369751\n",
      "  batch 900 loss: 1.1526324331760407\n",
      "LOSS train 1.15263 valid 1.07626, valid PER 34.90%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.1362453043460845\n",
      "  batch 100 loss: 1.0870430755615235\n",
      "  batch 150 loss: 1.0905017113685609\n",
      "  batch 200 loss: 1.0875173532962799\n",
      "  batch 250 loss: 1.1164070069789886\n",
      "  batch 300 loss: 1.1096232199668885\n",
      "  batch 350 loss: 1.1071215844154358\n",
      "  batch 400 loss: 1.078882279396057\n",
      "  batch 450 loss: 1.1043733322620393\n",
      "  batch 500 loss: 1.0743171966075897\n",
      "  batch 550 loss: 1.0988054013252258\n",
      "  batch 600 loss: 1.0749516367912293\n",
      "  batch 650 loss: 1.0789149224758148\n",
      "  batch 700 loss: 1.0864886677265166\n",
      "  batch 750 loss: 1.0696086537837983\n",
      "  batch 800 loss: 1.0583372008800507\n",
      "  batch 850 loss: 1.0483786523342133\n",
      "  batch 900 loss: 1.065580242872238\n",
      "LOSS train 1.06558 valid 1.04655, valid PER 33.22%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.0455134832859039\n",
      "  batch 100 loss: 1.0615285396575929\n",
      "  batch 150 loss: 1.0191218078136444\n",
      "  batch 200 loss: 1.0356757140159607\n",
      "  batch 250 loss: 1.0260829508304596\n",
      "  batch 300 loss: 1.0189558446407319\n",
      "  batch 350 loss: 1.0309745228290559\n",
      "  batch 400 loss: 1.0330121695995331\n",
      "  batch 450 loss: 1.0412831568717957\n",
      "  batch 500 loss: 1.021627712249756\n",
      "  batch 550 loss: 1.0225269782543183\n",
      "  batch 600 loss: 1.0420012819766997\n",
      "  batch 650 loss: 1.0197463166713714\n",
      "  batch 700 loss: 1.0553993344306947\n",
      "  batch 750 loss: 0.9945904815196991\n",
      "  batch 800 loss: 1.0204150772094727\n",
      "  batch 850 loss: 1.030541605949402\n",
      "  batch 900 loss: 1.0708364868164062\n",
      "LOSS train 1.07084 valid 1.00418, valid PER 32.74%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.9829653167724609\n",
      "  batch 100 loss: 0.9749187779426575\n",
      "  batch 150 loss: 0.9811431288719177\n",
      "  batch 200 loss: 0.9616776573657989\n",
      "  batch 250 loss: 0.9815879213809967\n",
      "  batch 300 loss: 0.937942076921463\n",
      "  batch 350 loss: 1.012672382593155\n",
      "  batch 400 loss: 0.979605861902237\n",
      "  batch 450 loss: 0.972785165309906\n",
      "  batch 500 loss: 1.0096660256385803\n",
      "  batch 550 loss: 0.9592522132396698\n",
      "  batch 600 loss: 1.0100084173679351\n",
      "  batch 650 loss: 0.9991162467002869\n",
      "  batch 700 loss: 0.9686157214641571\n",
      "  batch 750 loss: 0.9702135264873505\n",
      "  batch 800 loss: 0.9876748466491699\n",
      "  batch 850 loss: 0.9822047567367553\n",
      "  batch 900 loss: 0.9799603188037872\n",
      "LOSS train 0.97996 valid 0.96875, valid PER 31.10%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.9016651976108551\n",
      "  batch 100 loss: 0.9393878996372222\n",
      "  batch 150 loss: 0.9306099963188171\n",
      "  batch 200 loss: 0.9208319163322449\n",
      "  batch 250 loss: 0.9395543169975281\n",
      "  batch 300 loss: 0.9652367448806762\n",
      "  batch 350 loss: 0.9739735329151153\n",
      "  batch 400 loss: 0.9504098284244538\n",
      "  batch 450 loss: 0.931048938035965\n",
      "  batch 500 loss: 0.9357863640785218\n",
      "  batch 550 loss: 0.9338368201255798\n",
      "  batch 600 loss: 0.9616936910152435\n",
      "  batch 650 loss: 0.9269485330581665\n",
      "  batch 700 loss: 0.9095072960853576\n",
      "  batch 750 loss: 0.9021880292892456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 800 loss: 0.9691673612594605\n",
      "  batch 850 loss: 0.9623625814914704\n",
      "  batch 900 loss: 0.9038423025608062\n",
      "LOSS train 0.90384 valid 0.96019, valid PER 30.21%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.8670732367038727\n",
      "  batch 100 loss: 0.8978613340854644\n",
      "  batch 150 loss: 0.8891121423244477\n",
      "  batch 200 loss: 0.9206712174415589\n",
      "  batch 250 loss: 0.9210029590129852\n",
      "  batch 300 loss: 0.8882624566555023\n",
      "  batch 350 loss: 0.9017565858364105\n",
      "  batch 400 loss: 0.8563500785827637\n",
      "  batch 450 loss: 0.8770472311973572\n",
      "  batch 500 loss: 0.9008853209018707\n",
      "  batch 550 loss: 0.9373248088359832\n",
      "  batch 600 loss: 0.8856323778629303\n",
      "  batch 650 loss: 0.8803555023670196\n",
      "  batch 700 loss: 0.9089763569831848\n",
      "  batch 750 loss: 0.8930118119716645\n",
      "  batch 800 loss: 0.8999055337905884\n",
      "  batch 850 loss: 0.9025368964672089\n",
      "  batch 900 loss: 0.8960216772556305\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.89602 valid 0.98559, valid PER 31.58%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.816790611743927\n",
      "  batch 100 loss: 0.7591241210699081\n",
      "  batch 150 loss: 0.7820914518833161\n",
      "  batch 200 loss: 0.836763676404953\n",
      "  batch 250 loss: 0.8057857835292817\n",
      "  batch 300 loss: 0.7711745119094848\n",
      "  batch 350 loss: 0.8006298947334289\n",
      "  batch 400 loss: 0.8208775985240936\n",
      "  batch 450 loss: 0.7955539619922638\n",
      "  batch 500 loss: 0.7744714331626892\n",
      "  batch 550 loss: 0.7886535048484802\n",
      "  batch 600 loss: 0.7752460086345673\n",
      "  batch 650 loss: 0.8204035842418671\n",
      "  batch 700 loss: 0.7657684755325317\n",
      "  batch 750 loss: 0.7763278722763062\n",
      "  batch 800 loss: 0.7985636579990387\n",
      "  batch 850 loss: 0.8215856575965881\n",
      "  batch 900 loss: 0.8049335777759552\n",
      "LOSS train 0.80493 valid 0.88214, valid PER 27.36%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.7735734701156616\n",
      "  batch 100 loss: 0.7722028255462646\n",
      "  batch 150 loss: 0.716444855928421\n",
      "  batch 200 loss: 0.7613005197048187\n",
      "  batch 250 loss: 0.7723640716075897\n",
      "  batch 300 loss: 0.7523220932483673\n",
      "  batch 350 loss: 0.7388843321800231\n",
      "  batch 400 loss: 0.7657473540306091\n",
      "  batch 450 loss: 0.7700509119033814\n",
      "  batch 500 loss: 0.781471883058548\n",
      "  batch 550 loss: 0.7226441150903702\n",
      "  batch 600 loss: 0.7460829496383667\n",
      "  batch 650 loss: 0.7845337986946106\n",
      "  batch 700 loss: 0.7698650741577149\n",
      "  batch 750 loss: 0.741397168636322\n",
      "  batch 800 loss: 0.7461532354354858\n",
      "  batch 850 loss: 0.8160998702049256\n",
      "  batch 900 loss: 0.7943179595470429\n",
      "LOSS train 0.79432 valid 0.86703, valid PER 27.32%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7081205081939698\n",
      "  batch 100 loss: 0.7389513635635376\n",
      "  batch 150 loss: 0.7189129149913788\n",
      "  batch 200 loss: 0.7422855019569397\n",
      "  batch 250 loss: 0.7340470188856125\n",
      "  batch 300 loss: 0.7145959591865539\n",
      "  batch 350 loss: 0.7125769031047821\n",
      "  batch 400 loss: 0.7412106221914292\n",
      "  batch 450 loss: 0.7378180873394012\n",
      "  batch 500 loss: 0.6993400198221207\n",
      "  batch 550 loss: 0.7577324444055558\n",
      "  batch 600 loss: 0.7163129734992981\n",
      "  batch 650 loss: 0.7456047439575195\n",
      "  batch 700 loss: 0.7531076270341873\n",
      "  batch 750 loss: 0.6977706754207611\n",
      "  batch 800 loss: 0.7297340059280395\n",
      "  batch 850 loss: 0.7575196349620819\n",
      "  batch 900 loss: 0.7491555881500244\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.74916 valid 0.88242, valid PER 27.56%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.6796164643764496\n",
      "  batch 100 loss: 0.6828632330894471\n",
      "  batch 150 loss: 0.6632689225673676\n",
      "  batch 200 loss: 0.6691279870271682\n",
      "  batch 250 loss: 0.665732336640358\n",
      "  batch 300 loss: 0.7045481711626053\n",
      "  batch 350 loss: 0.6437479054927826\n",
      "  batch 400 loss: 0.647288697361946\n",
      "  batch 450 loss: 0.6638358771800995\n",
      "  batch 500 loss: 0.6836897534132004\n",
      "  batch 550 loss: 0.6983729672431945\n",
      "  batch 600 loss: 0.6517887359857559\n",
      "  batch 650 loss: 0.6759197974205017\n",
      "  batch 700 loss: 0.7139578968286514\n",
      "  batch 750 loss: 0.6541300809383392\n",
      "  batch 800 loss: 0.6423861241340637\n",
      "  batch 850 loss: 0.6773692667484283\n",
      "  batch 900 loss: 0.6858705765008927\n",
      "Epoch 00014: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.68587 valid 0.86830, valid PER 26.76%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6534393471479416\n",
      "  batch 100 loss: 0.6342999696731567\n",
      "  batch 150 loss: 0.6510352301597595\n",
      "  batch 200 loss: 0.6576770365238189\n",
      "  batch 250 loss: 0.6530049300193786\n",
      "  batch 300 loss: 0.633696049451828\n",
      "  batch 350 loss: 0.6440433382987976\n",
      "  batch 400 loss: 0.6328170877695084\n",
      "  batch 450 loss: 0.6323069214820862\n",
      "  batch 500 loss: 0.5962534785270691\n",
      "  batch 550 loss: 0.6349035680294037\n",
      "  batch 600 loss: 0.6627320909500122\n",
      "  batch 650 loss: 0.6545324015617371\n",
      "  batch 700 loss: 0.6604828870296479\n",
      "  batch 750 loss: 0.6416717725992203\n",
      "  batch 800 loss: 0.624087450504303\n",
      "  batch 850 loss: 0.6099451524019242\n",
      "  batch 900 loss: 0.6316545200347901\n",
      "LOSS train 0.63165 valid 0.85812, valid PER 26.48%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.6368801933526993\n",
      "  batch 100 loss: 0.5881747829914094\n",
      "  batch 150 loss: 0.6239054733514786\n",
      "  batch 200 loss: 0.6197005516290665\n",
      "  batch 250 loss: 0.6413314783573151\n",
      "  batch 300 loss: 0.6082984793186188\n",
      "  batch 350 loss: 0.6337963032722473\n",
      "  batch 400 loss: 0.638560419678688\n",
      "  batch 450 loss: 0.6401220661401749\n",
      "  batch 500 loss: 0.6059137815237046\n",
      "  batch 550 loss: 0.6251890999078751\n",
      "  batch 600 loss: 0.6006786674261093\n",
      "  batch 650 loss: 0.6362896466255188\n",
      "  batch 700 loss: 0.6164218312501908\n",
      "  batch 750 loss: 0.6202108842134476\n",
      "  batch 800 loss: 0.6304467517137528\n",
      "  batch 850 loss: 0.6153136837482452\n",
      "  batch 900 loss: 0.6169489634037018\n",
      "LOSS train 0.61695 valid 0.85789, valid PER 25.88%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.6115743142366409\n",
      "  batch 100 loss: 0.6295339196920395\n",
      "  batch 150 loss: 0.6107972514629364\n",
      "  batch 200 loss: 0.598717520236969\n",
      "  batch 250 loss: 0.6186430382728577\n",
      "  batch 300 loss: 0.6178592544794083\n",
      "  batch 350 loss: 0.5850552833080291\n",
      "  batch 400 loss: 0.6434292227029801\n",
      "  batch 450 loss: 0.6200400614738464\n",
      "  batch 500 loss: 0.6054021859169006\n",
      "  batch 550 loss: 0.6153500807285309\n",
      "  batch 600 loss: 0.6382857567071915\n",
      "  batch 650 loss: 0.6149493420124054\n",
      "  batch 700 loss: 0.599174730181694\n",
      "  batch 750 loss: 0.6036726146936416\n",
      "  batch 800 loss: 0.5902283430099488\n",
      "  batch 850 loss: 0.6123719030618667\n",
      "  batch 900 loss: 0.6060545200109482\n",
      "Epoch 00017: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.60605 valid 0.86576, valid PER 26.16%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.6073100191354751\n",
      "  batch 100 loss: 0.6011904138326645\n",
      "  batch 150 loss: 0.6224356138706207\n",
      "  batch 200 loss: 0.5963504695892334\n",
      "  batch 250 loss: 0.6050134348869324\n",
      "  batch 300 loss: 0.5738619548082352\n",
      "  batch 350 loss: 0.5871118259429932\n",
      "  batch 400 loss: 0.5699423629045487\n",
      "  batch 450 loss: 0.6143718409538269\n",
      "  batch 500 loss: 0.6010980248451233\n",
      "  batch 550 loss: 0.6206229585409164\n",
      "  batch 600 loss: 0.5693859440088272\n",
      "  batch 650 loss: 0.5875990116596221\n",
      "  batch 700 loss: 0.6116192001104355\n",
      "  batch 750 loss: 0.5626386475563049\n",
      "  batch 800 loss: 0.5743256455659866\n",
      "  batch 850 loss: 0.574764524102211\n",
      "  batch 900 loss: 0.6028099644184113\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.60281 valid 0.86296, valid PER 25.91%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5763806718587875\n",
      "  batch 100 loss: 0.5690356314182281\n",
      "  batch 150 loss: 0.5706634587049484\n",
      "  batch 200 loss: 0.5866002851724624\n",
      "  batch 250 loss: 0.5804032945632934\n",
      "  batch 300 loss: 0.5784206449985504\n",
      "  batch 350 loss: 0.5679420590400696\n",
      "  batch 400 loss: 0.5921127724647522\n",
      "  batch 450 loss: 0.5966739255189896\n",
      "  batch 500 loss: 0.5774781614542007\n",
      "  batch 550 loss: 0.5728713703155518\n",
      "  batch 600 loss: 0.5631808167696\n",
      "  batch 650 loss: 0.622671731710434\n",
      "  batch 700 loss: 0.5707987821102143\n",
      "  batch 750 loss: 0.5688055372238159\n",
      "  batch 800 loss: 0.5981403702497482\n",
      "  batch 850 loss: 0.5934339779615402\n",
      "  batch 900 loss: 0.588220004439354\n",
      "Epoch 00019: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.58822 valid 0.86171, valid PER 25.89%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.5754673159122468\n",
      "  batch 100 loss: 0.5769635581970215\n",
      "  batch 150 loss: 0.577852519750595\n",
      "  batch 200 loss: 0.5725652009248734\n",
      "  batch 250 loss: 0.5813965111970901\n",
      "  batch 300 loss: 0.5691703623533249\n",
      "  batch 350 loss: 0.5511602550745011\n",
      "  batch 400 loss: 0.5768209290504456\n",
      "  batch 450 loss: 0.5769490498304367\n",
      "  batch 500 loss: 0.5502900809049607\n",
      "  batch 550 loss: 0.6053442561626434\n",
      "  batch 600 loss: 0.5542991364002228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 650 loss: 0.5779419898986816\n",
      "  batch 700 loss: 0.5749426466226578\n",
      "  batch 750 loss: 0.5585409581661225\n",
      "  batch 800 loss: 0.5949688035249711\n",
      "  batch 850 loss: 0.5723340636491776\n",
      "  batch 900 loss: 0.5915161174535751\n",
      "Epoch 00020: reducing learning rate of group 0 to 3.9062e-03.\n",
      "LOSS train 0.59152 valid 0.86209, valid PER 25.73%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231206_185908/model_16\n",
      "Finish SGD_Scheduler optimiser\n",
      "End tuning For Wider 1 Layer LSTM\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start tuning For wider 1 Layer LSTM\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 1,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 512,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 1,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 512,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_regularisation_dropout.BiLSTM(args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For Wider 1 Layer LSTM\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8fdeed",
   "metadata": {},
   "source": [
    "## 3. Uni-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ff2ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "Optimiser = [\"Adam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0af70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning For uni directional 1 Layer LSTM\n",
      "Currently using Adam optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 7.068644766807556\n",
      "  batch 100 loss: 3.2868471431732176\n",
      "  batch 150 loss: 3.2423207855224607\n",
      "  batch 200 loss: 3.1164698123931887\n",
      "  batch 250 loss: 2.9227201080322267\n",
      "  batch 300 loss: 2.772088499069214\n",
      "  batch 350 loss: 2.657766065597534\n",
      "  batch 400 loss: 2.5803119564056396\n",
      "  batch 450 loss: 2.4692926168441773\n",
      "  batch 500 loss: 2.329936728477478\n",
      "  batch 550 loss: 2.230182764530182\n",
      "  batch 600 loss: 2.1393247032165528\n",
      "  batch 650 loss: 2.01820960521698\n",
      "  batch 700 loss: 1.9971644401550293\n",
      "  batch 750 loss: 1.8908081555366516\n",
      "  batch 800 loss: 1.8770714354515077\n",
      "  batch 850 loss: 1.8051597023010253\n",
      "  batch 900 loss: 1.7391205859184264\n",
      "LOSS train 1.73912 valid 1.72848, valid PER 56.36%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.670622899532318\n",
      "  batch 100 loss: 1.6264806747436524\n",
      "  batch 150 loss: 1.576259949207306\n",
      "  batch 200 loss: 1.6083553528785706\n",
      "  batch 250 loss: 1.584986572265625\n",
      "  batch 300 loss: 1.552129843235016\n",
      "  batch 350 loss: 1.465038812160492\n",
      "  batch 400 loss: 1.4651889514923095\n",
      "  batch 450 loss: 1.4236950969696045\n",
      "  batch 500 loss: 1.4642541527748107\n",
      "  batch 550 loss: 1.4803500151634217\n",
      "  batch 600 loss: 1.4326500988006592\n",
      "  batch 650 loss: 1.4570019578933715\n",
      "  batch 700 loss: 1.4154643392562867\n",
      "  batch 750 loss: 1.394677336215973\n",
      "  batch 800 loss: 1.3483277034759522\n",
      "  batch 850 loss: 1.3598758268356324\n",
      "  batch 900 loss: 1.379592583179474\n",
      "LOSS train 1.37959 valid 1.37460, valid PER 44.49%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.335208010673523\n",
      "  batch 100 loss: 1.3015903067588805\n",
      "  batch 150 loss: 1.3006390380859374\n",
      "  batch 200 loss: 1.3034715509414674\n",
      "  batch 250 loss: 1.3011990475654602\n",
      "  batch 300 loss: 1.2719933319091796\n",
      "  batch 350 loss: 1.3012064790725708\n",
      "  batch 400 loss: 1.312837505340576\n",
      "  batch 450 loss: 1.2630063056945802\n",
      "  batch 500 loss: 1.2907443928718567\n",
      "  batch 550 loss: 1.266069848537445\n",
      "  batch 600 loss: 1.2322459125518799\n",
      "  batch 650 loss: 1.2220951092243195\n",
      "  batch 700 loss: 1.2510917043685914\n",
      "  batch 750 loss: 1.2696038150787354\n",
      "  batch 800 loss: 1.2072360742092132\n",
      "  batch 850 loss: 1.2499184334278106\n",
      "  batch 900 loss: 1.1865980672836303\n",
      "LOSS train 1.18660 valid 1.24401, valid PER 38.49%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.1598005878925324\n",
      "  batch 100 loss: 1.1819101345539094\n",
      "  batch 150 loss: 1.1539783871173859\n",
      "  batch 200 loss: 1.1942668092250823\n",
      "  batch 250 loss: 1.1998521077632904\n",
      "  batch 300 loss: 1.2125716638565063\n",
      "  batch 350 loss: 1.1642724335193635\n",
      "  batch 400 loss: 1.194402905702591\n",
      "  batch 450 loss: 1.164987018108368\n",
      "  batch 500 loss: 1.1940469098091127\n",
      "  batch 550 loss: 1.1869079518318175\n",
      "  batch 600 loss: 1.1978963172435761\n",
      "  batch 650 loss: 1.183083757162094\n",
      "  batch 700 loss: 1.118866151571274\n",
      "  batch 750 loss: 1.1316696035861968\n",
      "  batch 800 loss: 1.1043113362789154\n",
      "  batch 850 loss: 1.141761919260025\n",
      "  batch 900 loss: 1.1818186700344087\n",
      "LOSS train 1.18182 valid 1.17342, valid PER 36.41%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.1137153601646423\n",
      "  batch 100 loss: 1.1012929916381835\n",
      "  batch 150 loss: 1.1389188134670258\n",
      "  batch 200 loss: 1.0651588320732117\n",
      "  batch 250 loss: 1.0573254120349884\n",
      "  batch 300 loss: 1.1042277991771698\n",
      "  batch 350 loss: 1.1065285360813142\n",
      "  batch 400 loss: 1.078280668258667\n",
      "  batch 450 loss: 1.0845943903923034\n",
      "  batch 500 loss: 1.1063911056518554\n",
      "  batch 550 loss: 1.0452877163887024\n",
      "  batch 600 loss: 1.1162134110927582\n",
      "  batch 650 loss: 1.0521504664421082\n",
      "  batch 700 loss: 1.1059522008895875\n",
      "  batch 750 loss: 1.0385875165462495\n",
      "  batch 800 loss: 1.0721537566184998\n",
      "  batch 850 loss: 1.0714156436920166\n",
      "  batch 900 loss: 1.0886353492736816\n",
      "LOSS train 1.08864 valid 1.10595, valid PER 34.72%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.0543953168392182\n",
      "  batch 100 loss: 1.0163820242881776\n",
      "  batch 150 loss: 0.9947409510612488\n",
      "  batch 200 loss: 1.023366768360138\n",
      "  batch 250 loss: 1.017626065015793\n",
      "  batch 300 loss: 1.0207496690750122\n",
      "  batch 350 loss: 1.0262259078025817\n",
      "  batch 400 loss: 1.0006272137165069\n",
      "  batch 450 loss: 1.0344511318206786\n",
      "  batch 500 loss: 1.015410145521164\n",
      "  batch 550 loss: 1.043784716129303\n",
      "  batch 600 loss: 1.0139132356643676\n",
      "  batch 650 loss: 1.0215795981884002\n",
      "  batch 700 loss: 1.020803349018097\n",
      "  batch 750 loss: 0.9932135939598083\n",
      "  batch 800 loss: 0.9859827303886414\n",
      "  batch 850 loss: 1.0163528180122376\n",
      "  batch 900 loss: 1.0149862945079804\n",
      "LOSS train 1.01499 valid 1.10109, valid PER 35.44%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.9979414689540863\n",
      "  batch 100 loss: 1.018039857149124\n",
      "  batch 150 loss: 0.9996258234977722\n",
      "  batch 200 loss: 0.9871176302433013\n",
      "  batch 250 loss: 0.970961297750473\n",
      "  batch 300 loss: 0.9728958582878113\n",
      "  batch 350 loss: 0.9818431842327118\n",
      "  batch 400 loss: 0.9797121095657348\n",
      "  batch 450 loss: 0.9853893351554871\n",
      "  batch 500 loss: 0.9571168041229248\n",
      "  batch 550 loss: 0.9459839081764221\n",
      "  batch 600 loss: 0.9969097125530243\n",
      "  batch 650 loss: 0.9686670076847076\n",
      "  batch 700 loss: 0.9747189629077911\n",
      "  batch 750 loss: 0.9576841723918915\n",
      "  batch 800 loss: 0.9480301463603973\n",
      "  batch 850 loss: 0.98074853181839\n",
      "  batch 900 loss: 0.988048335313797\n",
      "LOSS train 0.98805 valid 1.03505, valid PER 33.08%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.9602860403060913\n",
      "  batch 100 loss: 0.9226946496963501\n",
      "  batch 150 loss: 0.920884712934494\n",
      "  batch 200 loss: 0.9165509533882141\n",
      "  batch 250 loss: 0.9227345216274262\n",
      "  batch 300 loss: 0.8751224637031555\n",
      "  batch 350 loss: 0.9481487810611725\n",
      "  batch 400 loss: 0.9020341551303863\n",
      "  batch 450 loss: 0.9406134295463562\n",
      "  batch 500 loss: 0.9650888180732727\n",
      "  batch 550 loss: 0.9046826958656311\n",
      "  batch 600 loss: 0.9256650018692016\n",
      "  batch 650 loss: 0.9604149973392486\n",
      "  batch 700 loss: 0.9433156776428223\n",
      "  batch 750 loss: 0.9261534178256988\n",
      "  batch 800 loss: 0.9255373382568359\n",
      "  batch 850 loss: 0.9018730235099792\n",
      "  batch 900 loss: 0.9181161248683929\n",
      "LOSS train 0.91812 valid 0.98750, valid PER 30.98%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.8428902900218964\n",
      "  batch 100 loss: 0.8838197946548462\n",
      "  batch 150 loss: 0.913024308681488\n",
      "  batch 200 loss: 0.8560689496994018\n",
      "  batch 250 loss: 0.9016823029518127\n",
      "  batch 300 loss: 0.8973133528232574\n",
      "  batch 350 loss: 0.8887895405292511\n",
      "  batch 400 loss: 0.8961134803295135\n",
      "  batch 450 loss: 0.9358089184761047\n",
      "  batch 500 loss: 0.8737283217906952\n",
      "  batch 550 loss: 0.9258619618415832\n",
      "  batch 600 loss: 0.9237309312820434\n",
      "  batch 650 loss: 0.8684682381153107\n",
      "  batch 700 loss: 0.8714815485477447\n",
      "  batch 750 loss: 0.884230215549469\n",
      "  batch 800 loss: 0.8939981997013092\n",
      "  batch 850 loss: 0.9405206286907196\n",
      "  batch 900 loss: 0.8505442154407501\n",
      "LOSS train 0.85054 valid 0.96426, valid PER 30.40%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.817679568529129\n",
      "  batch 100 loss: 0.8345559060573577\n",
      "  batch 150 loss: 0.869944920539856\n",
      "  batch 200 loss: 0.8751274287700653\n",
      "  batch 250 loss: 0.856759501695633\n",
      "  batch 300 loss: 0.8192145538330078\n",
      "  batch 350 loss: 0.8711650049686432\n",
      "  batch 400 loss: 0.8229284882545471\n",
      "  batch 450 loss: 0.8286061358451843\n",
      "  batch 500 loss: 0.886351215839386\n",
      "  batch 550 loss: 0.8726705205440521\n",
      "  batch 600 loss: 0.8290855467319489\n",
      "  batch 650 loss: 0.8495573544502258\n",
      "  batch 700 loss: 0.8635757243633271\n",
      "  batch 750 loss: 0.8195369756221771\n",
      "  batch 800 loss: 0.8562920439243317\n",
      "  batch 850 loss: 0.8945140337944031\n",
      "  batch 900 loss: 0.897001850605011\n",
      "LOSS train 0.89700 valid 0.96142, valid PER 30.52%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.8212915134429931\n",
      "  batch 100 loss: 0.7810209512710571\n",
      "  batch 150 loss: 0.7918180048465728\n",
      "  batch 200 loss: 0.848923756480217\n",
      "  batch 250 loss: 0.8164677989482879\n",
      "  batch 300 loss: 0.7927232658863068\n",
      "  batch 350 loss: 0.8145115578174591\n",
      "  batch 400 loss: 0.8362581300735473\n",
      "  batch 450 loss: 0.8424662566184997\n",
      "  batch 500 loss: 0.8248132979869842\n",
      "  batch 550 loss: 0.8547319829463959\n",
      "  batch 600 loss: 0.799426691532135\n",
      "  batch 650 loss: 0.8515037488937378\n",
      "  batch 700 loss: 0.8267527282238006\n",
      "  batch 750 loss: 0.8308653414249421\n",
      "  batch 800 loss: 0.8649492275714874\n",
      "  batch 850 loss: 0.8619876265525818\n",
      "  batch 900 loss: 0.847342301607132\n",
      "LOSS train 0.84734 valid 0.95577, valid PER 29.98%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.792335633635521\n",
      "  batch 100 loss: 0.7867039215564727\n",
      "  batch 150 loss: 0.7957654237747193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.8000277239084244\n",
      "  batch 250 loss: 0.8315515196323395\n",
      "  batch 300 loss: 0.7757016468048096\n",
      "  batch 350 loss: 0.7851963746547699\n",
      "  batch 400 loss: 0.8257591187953949\n",
      "  batch 450 loss: 0.8080403232574462\n",
      "  batch 500 loss: 0.814664855003357\n",
      "  batch 550 loss: 0.7656943237781525\n",
      "  batch 600 loss: 0.7739217472076416\n",
      "  batch 650 loss: 0.8383294546604156\n",
      "  batch 700 loss: 0.8056134557724\n",
      "  batch 750 loss: 0.7768097698688508\n",
      "  batch 800 loss: 0.7624419391155243\n",
      "  batch 850 loss: 0.816886100769043\n",
      "  batch 900 loss: 0.8003061127662658\n",
      "LOSS train 0.80031 valid 0.91316, valid PER 29.25%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7305805402994155\n",
      "  batch 100 loss: 0.7535984486341476\n",
      "  batch 150 loss: 0.7388305532932281\n",
      "  batch 200 loss: 0.780689868927002\n",
      "  batch 250 loss: 0.7537158250808715\n",
      "  batch 300 loss: 0.7357066428661346\n",
      "  batch 350 loss: 0.7261322766542435\n",
      "  batch 400 loss: 0.7850597286224366\n",
      "  batch 450 loss: 0.7965561699867248\n",
      "  batch 500 loss: 0.754290064573288\n",
      "  batch 550 loss: 0.7761531245708465\n",
      "  batch 600 loss: 0.7655996143817901\n",
      "  batch 650 loss: 0.7790931212902069\n",
      "  batch 700 loss: 0.7683189606666565\n",
      "  batch 750 loss: 0.7446370899677277\n",
      "  batch 800 loss: 0.7704523932933808\n",
      "  batch 850 loss: 0.8130098938941955\n",
      "  batch 900 loss: 0.7907680296897888\n",
      "LOSS train 0.79077 valid 0.94824, valid PER 29.64%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.7511371719837189\n",
      "  batch 100 loss: 0.7807657146453857\n",
      "  batch 150 loss: 0.7056765621900558\n",
      "  batch 200 loss: 0.7450035816431045\n",
      "  batch 250 loss: 0.7364878344535828\n",
      "  batch 300 loss: 0.7777359682321549\n",
      "  batch 350 loss: 0.7348151963949203\n",
      "  batch 400 loss: 0.7268950015306472\n",
      "  batch 450 loss: 0.7232023239135742\n",
      "  batch 500 loss: 0.7622604721784592\n",
      "  batch 550 loss: 0.7513404417037964\n",
      "  batch 600 loss: 0.7128105342388154\n",
      "  batch 650 loss: 0.7732897269725799\n",
      "  batch 700 loss: 0.7701059556007386\n",
      "  batch 750 loss: 0.7075959432125092\n",
      "  batch 800 loss: 0.7163495516777039\n",
      "  batch 850 loss: 0.7712064158916473\n",
      "  batch 900 loss: 0.7536717486381531\n",
      "LOSS train 0.75367 valid 0.91602, valid PER 28.89%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.7050716698169708\n",
      "  batch 100 loss: 0.6934995156526566\n",
      "  batch 150 loss: 0.7136578106880188\n",
      "  batch 200 loss: 0.7272833943367004\n",
      "  batch 250 loss: 0.710129873752594\n",
      "  batch 300 loss: 0.7085448849201202\n",
      "  batch 350 loss: 0.7112523460388184\n",
      "  batch 400 loss: 0.706000257730484\n",
      "  batch 450 loss: 0.6953206074237823\n",
      "  batch 500 loss: 0.6947454714775085\n",
      "  batch 550 loss: 0.7142002362012864\n",
      "  batch 600 loss: 0.7141712009906769\n",
      "  batch 650 loss: 0.7276144689321518\n",
      "  batch 700 loss: 0.7436132794618606\n",
      "  batch 750 loss: 0.7222444230318069\n",
      "  batch 800 loss: 0.6991520380973816\n",
      "  batch 850 loss: 0.7025083351135254\n",
      "  batch 900 loss: 0.7250690573453903\n",
      "LOSS train 0.72507 valid 0.90236, valid PER 27.67%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.6992909955978394\n",
      "  batch 100 loss: 0.6624878799915314\n",
      "  batch 150 loss: 0.6579875248670578\n",
      "  batch 200 loss: 0.6643198186159134\n",
      "  batch 250 loss: 0.7102222216129302\n",
      "  batch 300 loss: 0.6802079558372498\n",
      "  batch 350 loss: 0.6831145966053009\n",
      "  batch 400 loss: 0.6741569954156875\n",
      "  batch 450 loss: 0.6940443766117096\n",
      "  batch 500 loss: 0.6621636301279068\n",
      "  batch 550 loss: 0.670455875992775\n",
      "  batch 600 loss: 0.6881743848323822\n",
      "  batch 650 loss: 0.7036630034446716\n",
      "  batch 700 loss: 0.6741656655073166\n",
      "  batch 750 loss: 0.6990254294872283\n",
      "  batch 800 loss: 0.6918384915590287\n",
      "  batch 850 loss: 0.6688579404354096\n",
      "  batch 900 loss: 0.6903866231441498\n",
      "LOSS train 0.69039 valid 0.90090, valid PER 28.16%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.6544180071353912\n",
      "  batch 100 loss: 0.6676362603902817\n",
      "  batch 150 loss: 0.6435527324676513\n",
      "  batch 200 loss: 0.6343542414903641\n",
      "  batch 250 loss: 0.6782494127750397\n",
      "  batch 300 loss: 0.6745955300331116\n",
      "  batch 350 loss: 0.6580054205656052\n",
      "  batch 400 loss: 0.7215249454975128\n",
      "  batch 450 loss: 0.6781273877620697\n",
      "  batch 500 loss: 0.6910881757736206\n",
      "  batch 550 loss: 0.6678333801031112\n",
      "  batch 600 loss: 0.706599263548851\n",
      "  batch 650 loss: 0.6686540204286575\n",
      "  batch 700 loss: 0.6623980718851089\n",
      "  batch 750 loss: 0.6529873216152191\n",
      "  batch 800 loss: 0.6324492090940476\n",
      "  batch 850 loss: 0.6719748550653457\n",
      "  batch 900 loss: 0.6426988518238068\n",
      "LOSS train 0.64270 valid 0.88751, valid PER 27.49%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.6329757553339005\n",
      "  batch 100 loss: 0.6419899958372116\n",
      "  batch 150 loss: 0.6264004093408585\n",
      "  batch 200 loss: 0.633689569234848\n",
      "  batch 250 loss: 0.6423690789937972\n",
      "  batch 300 loss: 0.650249252319336\n",
      "  batch 350 loss: 0.6373990064859391\n",
      "  batch 400 loss: 0.6282187002897263\n",
      "  batch 450 loss: 0.6524942117929459\n",
      "  batch 500 loss: 0.6563335293531418\n",
      "  batch 550 loss: 0.6508155888319016\n",
      "  batch 600 loss: 0.6244371652603149\n",
      "  batch 650 loss: 0.6326843529939652\n",
      "  batch 700 loss: 0.6572631895542145\n",
      "  batch 750 loss: 0.6541858899593354\n",
      "  batch 800 loss: 0.6188751709461212\n",
      "  batch 850 loss: 0.6463266623020172\n",
      "  batch 900 loss: 0.6631831389665603\n",
      "LOSS train 0.66318 valid 0.88478, valid PER 26.88%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5812704688310624\n",
      "  batch 100 loss: 0.5684966558218002\n",
      "  batch 150 loss: 0.5991100811958313\n",
      "  batch 200 loss: 0.6067674767971039\n",
      "  batch 250 loss: 0.6061977243423462\n",
      "  batch 300 loss: 0.6129369300603866\n",
      "  batch 350 loss: 0.6089818584918976\n",
      "  batch 400 loss: 0.6273235559463501\n",
      "  batch 450 loss: 0.6393553423881531\n",
      "  batch 500 loss: 0.6358651214838028\n",
      "  batch 550 loss: 0.6005042785406113\n",
      "  batch 600 loss: 0.6367245000600815\n",
      "  batch 650 loss: 0.67485970556736\n",
      "  batch 700 loss: 0.6099990683794022\n",
      "  batch 750 loss: 0.6117176949977875\n",
      "  batch 800 loss: 0.6529204505681991\n",
      "  batch 850 loss: 0.6532458490133286\n",
      "  batch 900 loss: 0.6401972323656082\n",
      "LOSS train 0.64020 valid 0.90563, valid PER 27.28%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.6039322495460511\n",
      "  batch 100 loss: 0.603977975845337\n",
      "  batch 150 loss: 0.5813536196947098\n",
      "  batch 200 loss: 0.5954029697179795\n",
      "  batch 250 loss: 0.5842946100234986\n",
      "  batch 300 loss: 0.5944287693500518\n",
      "  batch 350 loss: 0.5738484984636307\n",
      "  batch 400 loss: 0.6003070259094239\n",
      "  batch 450 loss: 0.6092615532875061\n",
      "  batch 500 loss: 0.6300353193283081\n",
      "  batch 550 loss: 0.6584073346853256\n",
      "  batch 600 loss: 0.5955015259981156\n",
      "  batch 650 loss: 0.6116337800025939\n",
      "  batch 700 loss: 0.603522565960884\n",
      "  batch 750 loss: 0.5936818528175354\n",
      "  batch 800 loss: 0.6372223263978958\n",
      "  batch 850 loss: 0.6249338883161545\n",
      "  batch 900 loss: 0.6114126694202423\n",
      "LOSS train 0.61141 valid 0.89087, valid PER 27.46%\n",
      "Training finished in 11.0 minutes.\n",
      "Model saved to checkpoints/20231209_173428/model_18\n",
      "Currently using dropout rate of 0.1\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.788117480278015\n",
      "  batch 100 loss: 3.2861213636398316\n",
      "  batch 150 loss: 3.239734830856323\n",
      "  batch 200 loss: 3.0926074314117433\n",
      "  batch 250 loss: 2.8775467824935914\n",
      "  batch 300 loss: 2.716394748687744\n",
      "  batch 350 loss: 2.593983540534973\n",
      "  batch 400 loss: 2.519478888511658\n",
      "  batch 450 loss: 2.4179141330718994\n",
      "  batch 500 loss: 2.2807554745674135\n",
      "  batch 550 loss: 2.174637849330902\n",
      "  batch 600 loss: 2.0895737862586974\n",
      "  batch 650 loss: 1.9902128052711487\n",
      "  batch 700 loss: 1.9691655445098877\n",
      "  batch 750 loss: 1.8996828603744507\n",
      "  batch 800 loss: 1.8655815291404725\n",
      "  batch 850 loss: 1.799680323600769\n",
      "  batch 900 loss: 1.7423810148239136\n",
      "LOSS train 1.74238 valid 1.71351, valid PER 56.35%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.6886166548728943\n",
      "  batch 100 loss: 1.6380735278129577\n",
      "  batch 150 loss: 1.5728446221351624\n",
      "  batch 200 loss: 1.6373389530181885\n",
      "  batch 250 loss: 1.5983117461204528\n",
      "  batch 300 loss: 1.5801001477241516\n",
      "  batch 350 loss: 1.4637331032752992\n",
      "  batch 400 loss: 1.5000472164154053\n",
      "  batch 450 loss: 1.444504771232605\n",
      "  batch 500 loss: 1.4648394036293029\n",
      "  batch 550 loss: 1.4999492073059082\n",
      "  batch 600 loss: 1.44744136095047\n",
      "  batch 650 loss: 1.4935621285438538\n",
      "  batch 700 loss: 1.473387885093689\n",
      "  batch 750 loss: 1.4220125937461854\n",
      "  batch 800 loss: 1.3830365920066834\n",
      "  batch 850 loss: 1.3944335174560547\n",
      "  batch 900 loss: 1.413442530632019\n",
      "LOSS train 1.41344 valid 1.37382, valid PER 43.60%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.3837124395370484\n",
      "  batch 100 loss: 1.345580174922943\n",
      "  batch 150 loss: 1.3278730201721192\n",
      "  batch 200 loss: 1.3207787704467773\n",
      "  batch 250 loss: 1.3109790861606598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 1.3024451661109924\n",
      "  batch 350 loss: 1.3586368155479431\n",
      "  batch 400 loss: 1.3236861062049865\n",
      "  batch 450 loss: 1.3044300782680511\n",
      "  batch 500 loss: 1.303516070842743\n",
      "  batch 550 loss: 1.3055153918266296\n",
      "  batch 600 loss: 1.2570006799697877\n",
      "  batch 650 loss: 1.2530989241600037\n",
      "  batch 700 loss: 1.2610418105125427\n",
      "  batch 750 loss: 1.3029775571823121\n",
      "  batch 800 loss: 1.251253912448883\n",
      "  batch 850 loss: 1.2910902321338653\n",
      "  batch 900 loss: 1.238579704761505\n",
      "LOSS train 1.23858 valid 1.27636, valid PER 40.29%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.2058268535137175\n",
      "  batch 100 loss: 1.216104918718338\n",
      "  batch 150 loss: 1.1927248644828796\n",
      "  batch 200 loss: 1.1997068107128144\n",
      "  batch 250 loss: 1.206126594543457\n",
      "  batch 300 loss: 1.2462512385845184\n",
      "  batch 350 loss: 1.1721721875667572\n",
      "  batch 400 loss: 1.199078425168991\n",
      "  batch 450 loss: 1.1843448555469513\n",
      "  batch 500 loss: 1.1510384511947631\n",
      "  batch 550 loss: 1.2062680447101592\n",
      "  batch 600 loss: 1.207136217355728\n",
      "  batch 650 loss: 1.1848440182209015\n",
      "  batch 700 loss: 1.1383923649787904\n",
      "  batch 750 loss: 1.134726300239563\n",
      "  batch 800 loss: 1.1426446092128755\n",
      "  batch 850 loss: 1.143090455532074\n",
      "  batch 900 loss: 1.1857463085651399\n",
      "LOSS train 1.18575 valid 1.16899, valid PER 37.33%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.1148594069480895\n",
      "  batch 100 loss: 1.0968695962429047\n",
      "  batch 150 loss: 1.1526987552642822\n",
      "  batch 200 loss: 1.0784061563014984\n",
      "  batch 250 loss: 1.1102065277099609\n",
      "  batch 300 loss: 1.1067669916152953\n",
      "  batch 350 loss: 1.1123714888095855\n",
      "  batch 400 loss: 1.1229162752628326\n",
      "  batch 450 loss: 1.1064248645305634\n",
      "  batch 500 loss: 1.1071572196483612\n",
      "  batch 550 loss: 1.0824933016300202\n",
      "  batch 600 loss: 1.1379850602149963\n",
      "  batch 650 loss: 1.0930246770381928\n",
      "  batch 700 loss: 1.1347423124313354\n",
      "  batch 750 loss: 1.0621630764007568\n",
      "  batch 800 loss: 1.0751233184337616\n",
      "  batch 850 loss: 1.1019368469715118\n",
      "  batch 900 loss: 1.0853462290763856\n",
      "LOSS train 1.08535 valid 1.12895, valid PER 35.32%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.0830645537376404\n",
      "  batch 100 loss: 1.0557886707782744\n",
      "  batch 150 loss: 1.0222165095806122\n",
      "  batch 200 loss: 1.029702410697937\n",
      "  batch 250 loss: 1.041287488937378\n",
      "  batch 300 loss: 1.03316180229187\n",
      "  batch 350 loss: 1.034861341714859\n",
      "  batch 400 loss: 1.035459088087082\n",
      "  batch 450 loss: 1.056059250831604\n",
      "  batch 500 loss: 1.044196206331253\n",
      "  batch 550 loss: 1.049387115240097\n",
      "  batch 600 loss: 1.0359786581993102\n",
      "  batch 650 loss: 1.0419339191913606\n",
      "  batch 700 loss: 1.0269779121875764\n",
      "  batch 750 loss: 1.0185084998607636\n",
      "  batch 800 loss: 1.0155700087547301\n",
      "  batch 850 loss: 1.011287978887558\n",
      "  batch 900 loss: 1.0140420377254487\n",
      "LOSS train 1.01404 valid 1.04153, valid PER 33.61%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.9779800164699555\n",
      "  batch 100 loss: 1.0505340373516083\n",
      "  batch 150 loss: 0.9936335706710815\n",
      "  batch 200 loss: 0.9805715227127075\n",
      "  batch 250 loss: 0.9726838755607605\n",
      "  batch 300 loss: 0.9749867498874665\n",
      "  batch 350 loss: 0.9616803121566773\n",
      "  batch 400 loss: 0.9911944019794464\n",
      "  batch 450 loss: 0.9652102267742158\n",
      "  batch 500 loss: 0.9813254606723786\n",
      "  batch 550 loss: 0.961498053073883\n",
      "  batch 600 loss: 0.9696773099899292\n",
      "  batch 650 loss: 0.9856969082355499\n",
      "  batch 700 loss: 0.9776792693138122\n",
      "  batch 750 loss: 0.9687218356132508\n",
      "  batch 800 loss: 0.9682627201080323\n",
      "  batch 850 loss: 0.9942845726013183\n",
      "  batch 900 loss: 0.9952490162849427\n",
      "LOSS train 0.99525 valid 1.02782, valid PER 33.01%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.9492912411689758\n",
      "  batch 100 loss: 0.9252694189548493\n",
      "  batch 150 loss: 0.9447800350189209\n",
      "  batch 200 loss: 0.9291296637058258\n",
      "  batch 250 loss: 0.9768052446842194\n",
      "  batch 300 loss: 0.896317503452301\n",
      "  batch 350 loss: 0.9634333407878876\n",
      "  batch 400 loss: 0.9040288341045379\n",
      "  batch 450 loss: 0.9394025456905365\n",
      "  batch 500 loss: 0.9829478716850281\n",
      "  batch 550 loss: 0.9257012867927551\n",
      "  batch 600 loss: 0.9605084526538848\n",
      "  batch 650 loss: 0.9584322094917297\n",
      "  batch 700 loss: 0.9090864205360413\n",
      "  batch 750 loss: 0.9461434531211853\n",
      "  batch 800 loss: 0.9196046769618988\n",
      "  batch 850 loss: 0.9227930045127869\n",
      "  batch 900 loss: 0.9266437840461731\n",
      "LOSS train 0.92664 valid 0.99322, valid PER 31.14%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.86215008020401\n",
      "  batch 100 loss: 0.8903242242336273\n",
      "  batch 150 loss: 0.9180769813060761\n",
      "  batch 200 loss: 0.8641479921340942\n",
      "  batch 250 loss: 0.8950748884677887\n",
      "  batch 300 loss: 0.8982289779186249\n",
      "  batch 350 loss: 0.9085884737968445\n",
      "  batch 400 loss: 0.8976170873641968\n",
      "  batch 450 loss: 0.9007936370372772\n",
      "  batch 500 loss: 0.8564261865615844\n",
      "  batch 550 loss: 0.917189564704895\n",
      "  batch 600 loss: 0.9214258754253387\n",
      "  batch 650 loss: 0.8821296465396881\n",
      "  batch 700 loss: 0.8770062673091888\n",
      "  batch 750 loss: 0.8819320642948151\n",
      "  batch 800 loss: 0.9151494884490967\n",
      "  batch 850 loss: 0.9278715801239014\n",
      "  batch 900 loss: 0.8533577799797059\n",
      "LOSS train 0.85336 valid 0.98093, valid PER 30.50%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.8441208004951477\n",
      "  batch 100 loss: 0.8329695451259613\n",
      "  batch 150 loss: 0.8460195243358613\n",
      "  batch 200 loss: 0.8767176616191864\n",
      "  batch 250 loss: 0.8761775064468383\n",
      "  batch 300 loss: 0.8423910331726074\n",
      "  batch 350 loss: 0.856067168712616\n",
      "  batch 400 loss: 0.8219527685642243\n",
      "  batch 450 loss: 0.8205764913558959\n",
      "  batch 500 loss: 0.8720462846755982\n",
      "  batch 550 loss: 0.8827636969089508\n",
      "  batch 600 loss: 0.8382508087158204\n",
      "  batch 650 loss: 0.8554854309558868\n",
      "  batch 700 loss: 0.882885389328003\n",
      "  batch 750 loss: 0.8566731178760528\n",
      "  batch 800 loss: 0.8533147251605988\n",
      "  batch 850 loss: 0.868191795349121\n",
      "  batch 900 loss: 0.8912115895748138\n",
      "LOSS train 0.89121 valid 0.97230, valid PER 30.61%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.8322555506229401\n",
      "  batch 100 loss: 0.7678719234466552\n",
      "  batch 150 loss: 0.7871462345123291\n",
      "  batch 200 loss: 0.8351538944244384\n",
      "  batch 250 loss: 0.8355510920286179\n",
      "  batch 300 loss: 0.799456934928894\n",
      "  batch 350 loss: 0.8439768540859223\n",
      "  batch 400 loss: 0.851235933303833\n",
      "  batch 450 loss: 0.8651452600955963\n",
      "  batch 500 loss: 0.8049612236022949\n",
      "  batch 550 loss: 0.8277184236049652\n",
      "  batch 600 loss: 0.8161456513404847\n",
      "  batch 650 loss: 0.8739396750926971\n",
      "  batch 700 loss: 0.7928752672672271\n",
      "  batch 750 loss: 0.7996501350402831\n",
      "  batch 800 loss: 0.8359584784507752\n",
      "  batch 850 loss: 0.8498688304424286\n",
      "  batch 900 loss: 0.8535653984546662\n",
      "LOSS train 0.85357 valid 0.92871, valid PER 28.77%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.777724815607071\n",
      "  batch 100 loss: 0.7610619854927063\n",
      "  batch 150 loss: 0.743152865767479\n",
      "  batch 200 loss: 0.7844045615196228\n",
      "  batch 250 loss: 0.7981464385986328\n",
      "  batch 300 loss: 0.770492742061615\n",
      "  batch 350 loss: 0.7808589142560959\n",
      "  batch 400 loss: 0.821586002111435\n",
      "  batch 450 loss: 0.7989379680156707\n",
      "  batch 500 loss: 0.8214450240135193\n",
      "  batch 550 loss: 0.7357597404718399\n",
      "  batch 600 loss: 0.7880871319770812\n",
      "  batch 650 loss: 0.8128615552186966\n",
      "  batch 700 loss: 0.7732061386108399\n",
      "  batch 750 loss: 0.7665414381027221\n",
      "  batch 800 loss: 0.7828259956836701\n",
      "  batch 850 loss: 0.8344287586212158\n",
      "  batch 900 loss: 0.8332405054569244\n",
      "LOSS train 0.83324 valid 0.91785, valid PER 29.42%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7467258250713349\n",
      "  batch 100 loss: 0.7895715248584747\n",
      "  batch 150 loss: 0.7546357548236847\n",
      "  batch 200 loss: 0.7700673067569732\n",
      "  batch 250 loss: 0.7539535754919052\n",
      "  batch 300 loss: 0.7397203671932221\n",
      "  batch 350 loss: 0.7346336448192596\n",
      "  batch 400 loss: 0.7626020348072052\n",
      "  batch 450 loss: 0.7941853427886962\n",
      "  batch 500 loss: 0.7508411455154419\n",
      "  batch 550 loss: 0.7796425771713257\n",
      "  batch 600 loss: 0.7581608301401138\n",
      "  batch 650 loss: 0.7852888864278793\n",
      "  batch 700 loss: 0.7679489493370056\n",
      "  batch 750 loss: 0.7272632455825806\n",
      "  batch 800 loss: 0.7728359401226044\n",
      "  batch 850 loss: 0.8506291615962982\n",
      "  batch 900 loss: 0.8003308868408203\n",
      "LOSS train 0.80033 valid 0.94071, valid PER 29.34%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.7378759181499481\n",
      "  batch 100 loss: 0.7331918632984161\n",
      "  batch 150 loss: 0.7302186095714569\n",
      "  batch 200 loss: 0.7223628294467926\n",
      "  batch 250 loss: 0.7445922082662583\n",
      "  batch 300 loss: 0.7524361312389374\n",
      "  batch 350 loss: 0.7327152073383332\n",
      "  batch 400 loss: 0.7387862098217011\n",
      "  batch 450 loss: 0.7337143254280091\n",
      "  batch 500 loss: 0.7523702359199524\n",
      "  batch 550 loss: 0.7488169330358505\n",
      "  batch 600 loss: 0.7168696618080139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 650 loss: 0.7455121546983718\n",
      "  batch 700 loss: 0.7831431257724762\n",
      "  batch 750 loss: 0.7258519268035889\n",
      "  batch 800 loss: 0.7119515711069107\n",
      "  batch 850 loss: 0.7583411639928818\n",
      "  batch 900 loss: 0.7272481453418732\n",
      "LOSS train 0.72725 valid 0.90672, valid PER 28.26%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6962152087688446\n",
      "  batch 100 loss: 0.6889090347290039\n",
      "  batch 150 loss: 0.6834739708900451\n",
      "  batch 200 loss: 0.7156212919950485\n",
      "  batch 250 loss: 0.7065595763921738\n",
      "  batch 300 loss: 0.6894778108596802\n",
      "  batch 350 loss: 0.7029316341876983\n",
      "  batch 400 loss: 0.730038970708847\n",
      "  batch 450 loss: 0.7253419208526611\n",
      "  batch 500 loss: 0.6908002245426178\n",
      "  batch 550 loss: 0.7369162499904632\n",
      "  batch 600 loss: 0.7416010630130768\n",
      "  batch 650 loss: 0.7327955377101898\n",
      "  batch 700 loss: 0.728225474357605\n",
      "  batch 750 loss: 0.7357064861059189\n",
      "  batch 800 loss: 0.7078442579507828\n",
      "  batch 850 loss: 0.7039832293987274\n",
      "  batch 900 loss: 0.7311299717426301\n",
      "LOSS train 0.73113 valid 0.89539, valid PER 27.70%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.7005435246229171\n",
      "  batch 100 loss: 0.6602392685413361\n",
      "  batch 150 loss: 0.6781146717071533\n",
      "  batch 200 loss: 0.6741939854621887\n",
      "  batch 250 loss: 0.7226655972003937\n",
      "  batch 300 loss: 0.6830074858665466\n",
      "  batch 350 loss: 0.6861664384603501\n",
      "  batch 400 loss: 0.7006127780675888\n",
      "  batch 450 loss: 0.7092473810911178\n",
      "  batch 500 loss: 0.69034334897995\n",
      "  batch 550 loss: 0.6634584373235702\n",
      "  batch 600 loss: 0.680627692937851\n",
      "  batch 650 loss: 0.6803035914897919\n",
      "  batch 700 loss: 0.6776523041725159\n",
      "  batch 750 loss: 0.6862048596143723\n",
      "  batch 800 loss: 0.6987788558006287\n",
      "  batch 850 loss: 0.6888670110702515\n",
      "  batch 900 loss: 0.694821412563324\n",
      "LOSS train 0.69482 valid 0.90304, valid PER 27.73%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.6689748591184617\n",
      "  batch 100 loss: 0.6588638991117477\n",
      "  batch 150 loss: 0.6415105468034744\n",
      "  batch 200 loss: 0.6616034567356109\n",
      "  batch 250 loss: 0.6679384326934814\n",
      "  batch 300 loss: 0.6603897231817245\n",
      "  batch 350 loss: 0.6500088411569596\n",
      "  batch 400 loss: 0.7058378541469574\n",
      "  batch 450 loss: 0.6751336175203323\n",
      "  batch 500 loss: 0.6642138761281967\n",
      "  batch 550 loss: 0.6490328288078309\n",
      "  batch 600 loss: 0.6937183868885041\n",
      "  batch 650 loss: 0.6630515480041503\n",
      "  batch 700 loss: 0.6552344596385956\n",
      "  batch 750 loss: 0.6533721977472305\n",
      "  batch 800 loss: 0.6552585232257843\n",
      "  batch 850 loss: 0.664677317738533\n",
      "  batch 900 loss: 0.6390065968036651\n",
      "LOSS train 0.63901 valid 0.88566, valid PER 26.56%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.6259932124614715\n",
      "  batch 100 loss: 0.6214068377017975\n",
      "  batch 150 loss: 0.6264998877048492\n",
      "  batch 200 loss: 0.6292690515518189\n",
      "  batch 250 loss: 0.6479630571603775\n",
      "  batch 300 loss: 0.6284411805868149\n",
      "  batch 350 loss: 0.6297103625535965\n",
      "  batch 400 loss: 0.6211214917898178\n",
      "  batch 450 loss: 0.6553292053937912\n",
      "  batch 500 loss: 0.627600393295288\n",
      "  batch 550 loss: 0.6299267053604126\n",
      "  batch 600 loss: 0.6252770537137985\n",
      "  batch 650 loss: 0.6476828509569168\n",
      "  batch 700 loss: 0.6595980477333069\n",
      "  batch 750 loss: 0.6653583687543869\n",
      "  batch 800 loss: 0.6300769245624542\n",
      "  batch 850 loss: 0.6283052444458008\n",
      "  batch 900 loss: 0.6674699783325195\n",
      "LOSS train 0.66747 valid 0.87844, valid PER 26.50%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5738907206058502\n",
      "  batch 100 loss: 0.5689954721927643\n",
      "  batch 150 loss: 0.5846063673496247\n",
      "  batch 200 loss: 0.612203021645546\n",
      "  batch 250 loss: 0.6240700167417527\n",
      "  batch 300 loss: 0.5979298204183578\n",
      "  batch 350 loss: 0.5994930034875869\n",
      "  batch 400 loss: 0.608524215221405\n",
      "  batch 450 loss: 0.631487347483635\n",
      "  batch 500 loss: 0.6312675583362579\n",
      "  batch 550 loss: 0.594208310842514\n",
      "  batch 600 loss: 0.6180834829807281\n",
      "  batch 650 loss: 0.6550101089477539\n",
      "  batch 700 loss: 0.5997855186462402\n",
      "  batch 750 loss: 0.5925081920623779\n",
      "  batch 800 loss: 0.6460703754425049\n",
      "  batch 850 loss: 0.6310217750072479\n",
      "  batch 900 loss: 0.6151218229532242\n",
      "LOSS train 0.61512 valid 0.89583, valid PER 27.10%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.5675903904438019\n",
      "  batch 100 loss: 0.5694952946901322\n",
      "  batch 150 loss: 0.5606589275598526\n",
      "  batch 200 loss: 0.5696642112731933\n",
      "  batch 250 loss: 0.5772114592790604\n",
      "  batch 300 loss: 0.6151991218328476\n",
      "  batch 350 loss: 0.5585375046730041\n",
      "  batch 400 loss: 0.5868755757808686\n",
      "  batch 450 loss: 0.6326352876424789\n",
      "  batch 500 loss: 0.57374904692173\n",
      "  batch 550 loss: 0.6164587306976318\n",
      "  batch 600 loss: 0.5635943466424942\n",
      "  batch 650 loss: 0.607529639005661\n",
      "  batch 700 loss: 0.6066695117950439\n",
      "  batch 750 loss: 0.5866471010446549\n",
      "  batch 800 loss: 0.6251372188329697\n",
      "  batch 850 loss: 0.6235438770055771\n",
      "  batch 900 loss: 0.611386775970459\n",
      "LOSS train 0.61139 valid 0.88284, valid PER 26.92%\n",
      "Training finished in 9.0 minutes.\n",
      "Model saved to checkpoints/20231209_174535/model_18\n",
      "Currently using dropout rate of 0.2\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.794919576644897\n",
      "  batch 100 loss: 3.286378107070923\n",
      "  batch 150 loss: 3.2419281244277953\n",
      "  batch 200 loss: 3.0990497970581057\n",
      "  batch 250 loss: 2.87869695186615\n",
      "  batch 300 loss: 2.7248454666137696\n",
      "  batch 350 loss: 2.6049508571624758\n",
      "  batch 400 loss: 2.539790449142456\n",
      "  batch 450 loss: 2.4467822265625\n",
      "  batch 500 loss: 2.310268268585205\n",
      "  batch 550 loss: 2.2034408378601076\n",
      "  batch 600 loss: 2.122035143375397\n",
      "  batch 650 loss: 2.0187648010253905\n",
      "  batch 700 loss: 2.0068633699417116\n",
      "  batch 750 loss: 1.9233268308639526\n",
      "  batch 800 loss: 1.8884750270843507\n",
      "  batch 850 loss: 1.8341220569610597\n",
      "  batch 900 loss: 1.78993262052536\n",
      "LOSS train 1.78993 valid 1.70266, valid PER 55.69%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7142188572883605\n",
      "  batch 100 loss: 1.6719526410102845\n",
      "  batch 150 loss: 1.6067211818695069\n",
      "  batch 200 loss: 1.6674683499336242\n",
      "  batch 250 loss: 1.6123411130905152\n",
      "  batch 300 loss: 1.6064553308486937\n",
      "  batch 350 loss: 1.4997361755371095\n",
      "  batch 400 loss: 1.5584266567230225\n",
      "  batch 450 loss: 1.4775818634033202\n",
      "  batch 500 loss: 1.5011921286582948\n",
      "  batch 550 loss: 1.5169475746154786\n",
      "  batch 600 loss: 1.478588593006134\n",
      "  batch 650 loss: 1.521617021560669\n",
      "  batch 700 loss: 1.4725480461120606\n",
      "  batch 750 loss: 1.4440786957740783\n",
      "  batch 800 loss: 1.3898480725288391\n",
      "  batch 850 loss: 1.4056550765037537\n",
      "  batch 900 loss: 1.4333938884735107\n",
      "LOSS train 1.43339 valid 1.39381, valid PER 45.40%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.3988906645774841\n",
      "  batch 100 loss: 1.358260028362274\n",
      "  batch 150 loss: 1.3404338717460633\n",
      "  batch 200 loss: 1.3491909074783326\n",
      "  batch 250 loss: 1.3347158312797547\n",
      "  batch 300 loss: 1.345677571296692\n",
      "  batch 350 loss: 1.382728910446167\n",
      "  batch 400 loss: 1.3361397624015807\n",
      "  batch 450 loss: 1.3054184448719024\n",
      "  batch 500 loss: 1.3229762649536132\n",
      "  batch 550 loss: 1.326089472770691\n",
      "  batch 600 loss: 1.2908122551441192\n",
      "  batch 650 loss: 1.2560470843315124\n",
      "  batch 700 loss: 1.2977854585647584\n",
      "  batch 750 loss: 1.338859521150589\n",
      "  batch 800 loss: 1.2788226401805878\n",
      "  batch 850 loss: 1.294562965631485\n",
      "  batch 900 loss: 1.225169689655304\n",
      "LOSS train 1.22517 valid 1.28945, valid PER 40.35%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.2316431844234466\n",
      "  batch 100 loss: 1.2441951298713685\n",
      "  batch 150 loss: 1.2041797232627869\n",
      "  batch 200 loss: 1.2374931120872497\n",
      "  batch 250 loss: 1.2561816775798798\n",
      "  batch 300 loss: 1.2593734073638916\n",
      "  batch 350 loss: 1.2022048497200013\n",
      "  batch 400 loss: 1.209116255044937\n",
      "  batch 450 loss: 1.204123752117157\n",
      "  batch 500 loss: 1.1770047390460967\n",
      "  batch 550 loss: 1.2359612679481506\n",
      "  batch 600 loss: 1.2353293979167939\n",
      "  batch 650 loss: 1.1961128056049346\n",
      "  batch 700 loss: 1.1604340195655822\n",
      "  batch 750 loss: 1.1682109832763672\n",
      "  batch 800 loss: 1.1537517309188843\n",
      "  batch 850 loss: 1.1770588064193725\n",
      "  batch 900 loss: 1.2136870837211609\n",
      "LOSS train 1.21369 valid 1.16800, valid PER 37.93%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.1503446209430694\n",
      "  batch 100 loss: 1.125614631175995\n",
      "  batch 150 loss: 1.1767974662780762\n",
      "  batch 200 loss: 1.10110369682312\n",
      "  batch 250 loss: 1.1043317675590516\n",
      "  batch 300 loss: 1.1462134075164796\n",
      "  batch 350 loss: 1.1216794884204864\n",
      "  batch 400 loss: 1.1260944187641144\n",
      "  batch 450 loss: 1.1441822516918183\n",
      "  batch 500 loss: 1.1272549951076507\n",
      "  batch 550 loss: 1.120835394859314\n",
      "  batch 600 loss: 1.2013422083854675\n",
      "  batch 650 loss: 1.1374829840660095\n",
      "  batch 700 loss: 1.1718696916103364\n",
      "  batch 750 loss: 1.0760758984088898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 800 loss: 1.1242998957633972\n",
      "  batch 850 loss: 1.0978145229816436\n",
      "  batch 900 loss: 1.1208236336708068\n",
      "LOSS train 1.12082 valid 1.14701, valid PER 36.21%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.1000030469894408\n",
      "  batch 100 loss: 1.0552160215377808\n",
      "  batch 150 loss: 1.0262992525100707\n",
      "  batch 200 loss: 1.0740707695484162\n",
      "  batch 250 loss: 1.1154062533378601\n",
      "  batch 300 loss: 1.080606083869934\n",
      "  batch 350 loss: 1.0602339291572571\n",
      "  batch 400 loss: 1.0352595889568328\n",
      "  batch 450 loss: 1.073378438949585\n",
      "  batch 500 loss: 1.0503549551963807\n",
      "  batch 550 loss: 1.093051769733429\n",
      "  batch 600 loss: 1.0373570501804352\n",
      "  batch 650 loss: 1.0649416732788086\n",
      "  batch 700 loss: 1.0573769342899322\n",
      "  batch 750 loss: 1.0252625465393066\n",
      "  batch 800 loss: 1.0270174825191498\n",
      "  batch 850 loss: 1.0170249295234681\n",
      "  batch 900 loss: 1.0483681952953339\n",
      "LOSS train 1.04837 valid 1.08995, valid PER 34.42%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.0447641813755035\n",
      "  batch 100 loss: 1.0460060167312621\n",
      "  batch 150 loss: 1.0051160275936126\n",
      "  batch 200 loss: 1.0040782558918\n",
      "  batch 250 loss: 1.0421925783157349\n",
      "  batch 300 loss: 0.9882168602943421\n",
      "  batch 350 loss: 0.9900047075748444\n",
      "  batch 400 loss: 1.011530705690384\n",
      "  batch 450 loss: 1.0111332142353058\n",
      "  batch 500 loss: 1.0022501492500304\n",
      "  batch 550 loss: 0.9881078898906708\n",
      "  batch 600 loss: 1.0054446411132814\n",
      "  batch 650 loss: 1.0009778916835785\n",
      "  batch 700 loss: 0.9933887648582459\n",
      "  batch 750 loss: 0.9715096652507782\n",
      "  batch 800 loss: 0.9870665812492371\n",
      "  batch 850 loss: 1.0232378613948823\n",
      "  batch 900 loss: 1.0158310842514038\n",
      "LOSS train 1.01583 valid 1.04001, valid PER 32.53%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.9872888779640198\n",
      "  batch 100 loss: 0.9385576140880585\n",
      "  batch 150 loss: 0.95139852643013\n",
      "  batch 200 loss: 0.9655227792263031\n",
      "  batch 250 loss: 0.9766768205165863\n",
      "  batch 300 loss: 0.9088091671466827\n",
      "  batch 350 loss: 0.9882627069950104\n",
      "  batch 400 loss: 0.9605628669261932\n",
      "  batch 450 loss: 0.9841751194000244\n",
      "  batch 500 loss: 1.0107264769077302\n",
      "  batch 550 loss: 0.9624732100963592\n",
      "  batch 600 loss: 0.968798017501831\n",
      "  batch 650 loss: 0.9903359234333038\n",
      "  batch 700 loss: 0.9453852498531341\n",
      "  batch 750 loss: 0.9619070565700532\n",
      "  batch 800 loss: 0.970095534324646\n",
      "  batch 850 loss: 0.9435185265541076\n",
      "  batch 900 loss: 0.9597903454303741\n",
      "LOSS train 0.95979 valid 1.00573, valid PER 31.96%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.8845845627784729\n",
      "  batch 100 loss: 0.9257474672794342\n",
      "  batch 150 loss: 0.9542223513126373\n",
      "  batch 200 loss: 0.888101089000702\n",
      "  batch 250 loss: 0.9292058455944061\n",
      "  batch 300 loss: 0.9133449375629425\n",
      "  batch 350 loss: 0.9501738893985748\n",
      "  batch 400 loss: 0.9351839983463287\n",
      "  batch 450 loss: 0.9288742864131927\n",
      "  batch 500 loss: 0.8808761429786682\n",
      "  batch 550 loss: 0.9346357047557831\n",
      "  batch 600 loss: 0.9225421071052551\n",
      "  batch 650 loss: 0.912972936630249\n",
      "  batch 700 loss: 0.9062186074256897\n",
      "  batch 750 loss: 0.931074435710907\n",
      "  batch 800 loss: 0.9374794888496399\n",
      "  batch 850 loss: 0.9504844963550567\n",
      "  batch 900 loss: 0.9009710848331451\n",
      "LOSS train 0.90097 valid 0.97313, valid PER 30.57%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.8695176589488983\n",
      "  batch 100 loss: 0.8640607559680938\n",
      "  batch 150 loss: 0.8986002337932587\n",
      "  batch 200 loss: 0.9046237373352051\n",
      "  batch 250 loss: 0.9037233054637909\n",
      "  batch 300 loss: 0.858899929523468\n",
      "  batch 350 loss: 0.8912094962596894\n",
      "  batch 400 loss: 0.8575624597072601\n",
      "  batch 450 loss: 0.849205139875412\n",
      "  batch 500 loss: 0.9207447373867035\n",
      "  batch 550 loss: 0.927939692735672\n",
      "  batch 600 loss: 0.8713141214847565\n",
      "  batch 650 loss: 0.8748315501213074\n",
      "  batch 700 loss: 0.904745055437088\n",
      "  batch 750 loss: 0.8840085244178773\n",
      "  batch 800 loss: 0.8933633708953858\n",
      "  batch 850 loss: 0.8907427203655243\n",
      "  batch 900 loss: 0.9046691286563874\n",
      "LOSS train 0.90467 valid 0.96955, valid PER 30.58%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.8304924821853638\n",
      "  batch 100 loss: 0.8212106323242188\n",
      "  batch 150 loss: 0.830864304304123\n",
      "  batch 200 loss: 0.9067083311080932\n",
      "  batch 250 loss: 0.904289653301239\n",
      "  batch 300 loss: 0.842942761182785\n",
      "  batch 350 loss: 0.8630222553014755\n",
      "  batch 400 loss: 0.8864649021625519\n",
      "  batch 450 loss: 0.8998947489261627\n",
      "  batch 500 loss: 0.8593044304847717\n",
      "  batch 550 loss: 0.8882603883743286\n",
      "  batch 600 loss: 0.8618028676509857\n",
      "  batch 650 loss: 0.9206818854808807\n",
      "  batch 700 loss: 0.839072847366333\n",
      "  batch 750 loss: 0.8292436254024506\n",
      "  batch 800 loss: 0.8654452884197235\n",
      "  batch 850 loss: 0.8602231550216675\n",
      "  batch 900 loss: 0.8811031353473663\n",
      "LOSS train 0.88110 valid 0.95541, valid PER 29.42%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.8370426273345948\n",
      "  batch 100 loss: 0.8192583775520325\n",
      "  batch 150 loss: 0.8023217105865479\n",
      "  batch 200 loss: 0.8192072546482086\n",
      "  batch 250 loss: 0.8600998246669769\n",
      "  batch 300 loss: 0.8258094274997712\n",
      "  batch 350 loss: 0.8316231501102448\n",
      "  batch 400 loss: 0.8803760623931884\n",
      "  batch 450 loss: 0.8595506370067596\n",
      "  batch 500 loss: 0.8692979753017426\n",
      "  batch 550 loss: 0.7853133749961853\n",
      "  batch 600 loss: 0.85739466547966\n",
      "  batch 650 loss: 0.8847023785114289\n",
      "  batch 700 loss: 0.8465179944038391\n",
      "  batch 750 loss: 0.8157904386520386\n",
      "  batch 800 loss: 0.7963902080059051\n",
      "  batch 850 loss: 0.8530938625335693\n",
      "  batch 900 loss: 0.848841301202774\n",
      "LOSS train 0.84884 valid 0.93708, valid PER 29.78%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7780255603790284\n",
      "  batch 100 loss: 0.8058258426189423\n",
      "  batch 150 loss: 0.7770185911655426\n",
      "  batch 200 loss: 0.8337841403484344\n",
      "  batch 250 loss: 0.8122909593582154\n",
      "  batch 300 loss: 0.7995551896095275\n",
      "  batch 350 loss: 0.7751957583427429\n",
      "  batch 400 loss: 0.8083744287490845\n",
      "  batch 450 loss: 0.8288471210002899\n",
      "  batch 500 loss: 0.7728513300418853\n",
      "  batch 550 loss: 0.8018884909152985\n",
      "  batch 600 loss: 0.7987940955162048\n",
      "  batch 650 loss: 0.8086247527599335\n",
      "  batch 700 loss: 0.8126210141181945\n",
      "  batch 750 loss: 0.7643383133411408\n",
      "  batch 800 loss: 0.7927692639827728\n",
      "  batch 850 loss: 0.8793297553062439\n",
      "  batch 900 loss: 0.8357160151004791\n",
      "LOSS train 0.83572 valid 0.92207, valid PER 29.15%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.7510449671745301\n",
      "  batch 100 loss: 0.7704643321037292\n",
      "  batch 150 loss: 0.7593964529037476\n",
      "  batch 200 loss: 0.743880609869957\n",
      "  batch 250 loss: 0.7706474769115448\n",
      "  batch 300 loss: 0.7892296278476715\n",
      "  batch 350 loss: 0.7420245635509491\n",
      "  batch 400 loss: 0.7655220186710358\n",
      "  batch 450 loss: 0.7568545174598694\n",
      "  batch 500 loss: 0.8071540856361389\n",
      "  batch 550 loss: 0.7971763384342193\n",
      "  batch 600 loss: 0.7550195878744126\n",
      "  batch 650 loss: 0.7944963073730469\n",
      "  batch 700 loss: 0.7852250123023987\n",
      "  batch 750 loss: 0.7664099407196044\n",
      "  batch 800 loss: 0.7282594478130341\n",
      "  batch 850 loss: 0.8001010715961456\n",
      "  batch 900 loss: 0.7630307936668396\n",
      "LOSS train 0.76303 valid 0.91989, valid PER 28.70%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.7493264132738113\n",
      "  batch 100 loss: 0.7334548336267471\n",
      "  batch 150 loss: 0.7092005890607834\n",
      "  batch 200 loss: 0.7517348146438598\n",
      "  batch 250 loss: 0.7526321828365325\n",
      "  batch 300 loss: 0.727625687122345\n",
      "  batch 350 loss: 0.7405529892444611\n",
      "  batch 400 loss: 0.7531035071611405\n",
      "  batch 450 loss: 0.7486514961719513\n",
      "  batch 500 loss: 0.7161641156673432\n",
      "  batch 550 loss: 0.7473175257444382\n",
      "  batch 600 loss: 0.7807112467288971\n",
      "  batch 650 loss: 0.7695768713951111\n",
      "  batch 700 loss: 0.7905285531282424\n",
      "  batch 750 loss: 0.7707998496294022\n",
      "  batch 800 loss: 0.7282655555009842\n",
      "  batch 850 loss: 0.736979849934578\n",
      "  batch 900 loss: 0.7624551153182983\n",
      "LOSS train 0.76246 valid 0.89383, valid PER 27.59%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.7172082799673081\n",
      "  batch 100 loss: 0.6870880007743836\n",
      "  batch 150 loss: 0.7291520822048188\n",
      "  batch 200 loss: 0.7113495969772339\n",
      "  batch 250 loss: 0.7451739680767059\n",
      "  batch 300 loss: 0.7261434108018875\n",
      "  batch 350 loss: 0.7207423889636994\n",
      "  batch 400 loss: 0.7471457481384277\n",
      "  batch 450 loss: 0.7497310394048691\n",
      "  batch 500 loss: 0.722198622226715\n",
      "  batch 550 loss: 0.7044636660814285\n",
      "  batch 600 loss: 0.7201246249675751\n",
      "  batch 650 loss: 0.7383932757377625\n",
      "  batch 700 loss: 0.7161957061290741\n",
      "  batch 750 loss: 0.7129307973384857\n",
      "  batch 800 loss: 0.7211315536499023\n",
      "  batch 850 loss: 0.6922256082296372\n",
      "  batch 900 loss: 0.7355184864997864\n",
      "LOSS train 0.73552 valid 0.88971, valid PER 27.88%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.6862600922584534\n",
      "  batch 100 loss: 0.6778667497634888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 150 loss: 0.6742611169815064\n",
      "  batch 200 loss: 0.698261433839798\n",
      "  batch 250 loss: 0.6916088259220123\n",
      "  batch 300 loss: 0.7112164133787156\n",
      "  batch 350 loss: 0.6833381915092468\n",
      "  batch 400 loss: 0.7471463686227798\n",
      "  batch 450 loss: 0.7001542127132416\n",
      "  batch 500 loss: 0.6981944358348846\n",
      "  batch 550 loss: 0.6958925664424896\n",
      "  batch 600 loss: 0.7441695201396942\n",
      "  batch 650 loss: 0.6949682486057281\n",
      "  batch 700 loss: 0.692716081738472\n",
      "  batch 750 loss: 0.6755614966154099\n",
      "  batch 800 loss: 0.6777801024913788\n",
      "  batch 850 loss: 0.7058265697956085\n",
      "  batch 900 loss: 0.692073946595192\n",
      "LOSS train 0.69207 valid 0.89086, valid PER 27.42%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.6679976832866669\n",
      "  batch 100 loss: 0.6528937137126922\n",
      "  batch 150 loss: 0.6850659197568894\n",
      "  batch 200 loss: 0.6646606969833374\n",
      "  batch 250 loss: 0.6823423361778259\n",
      "  batch 300 loss: 0.6630653411149978\n",
      "  batch 350 loss: 0.6932913535833358\n",
      "  batch 400 loss: 0.6619552934169769\n",
      "  batch 450 loss: 0.6991228318214416\n",
      "  batch 500 loss: 0.6670075309276581\n",
      "  batch 550 loss: 0.6822357976436615\n",
      "  batch 600 loss: 0.6654832553863526\n",
      "  batch 650 loss: 0.6762617903947831\n",
      "  batch 700 loss: 0.69100481569767\n",
      "  batch 750 loss: 0.6766201150417328\n",
      "  batch 800 loss: 0.6483258551359177\n",
      "  batch 850 loss: 0.6683486175537109\n",
      "  batch 900 loss: 0.6995620441436767\n",
      "LOSS train 0.69956 valid 0.89673, valid PER 27.20%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.6144439536333084\n",
      "  batch 100 loss: 0.624123250246048\n",
      "  batch 150 loss: 0.6336188220977783\n",
      "  batch 200 loss: 0.6356481826305389\n",
      "  batch 250 loss: 0.6624604457616806\n",
      "  batch 300 loss: 0.6700972455739975\n",
      "  batch 350 loss: 0.659974353313446\n",
      "  batch 400 loss: 0.6593643355369568\n",
      "  batch 450 loss: 0.6630339419841766\n",
      "  batch 500 loss: 0.6907667601108551\n",
      "  batch 550 loss: 0.6329116427898407\n",
      "  batch 600 loss: 0.6595294934511184\n",
      "  batch 650 loss: 0.7209380555152893\n",
      "  batch 700 loss: 0.6262933999300003\n",
      "  batch 750 loss: 0.6437504929304123\n",
      "  batch 800 loss: 0.6962608993053436\n",
      "  batch 850 loss: 0.6766278177499772\n",
      "  batch 900 loss: 0.685044624209404\n",
      "LOSS train 0.68504 valid 0.89141, valid PER 27.02%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.6152315837144852\n",
      "  batch 100 loss: 0.6173658156394959\n",
      "  batch 150 loss: 0.636973946094513\n",
      "  batch 200 loss: 0.642533740401268\n",
      "  batch 250 loss: 0.5981571191549301\n",
      "  batch 300 loss: 0.6428470873832702\n",
      "  batch 350 loss: 0.6034663766622543\n",
      "  batch 400 loss: 0.638567391037941\n",
      "  batch 450 loss: 0.6334475129842758\n",
      "  batch 500 loss: 0.6139527237415314\n",
      "  batch 550 loss: 0.6580232352018356\n",
      "  batch 600 loss: 0.6206093537807464\n",
      "  batch 650 loss: 0.6393964511156082\n",
      "  batch 700 loss: 0.6400475823879241\n",
      "  batch 750 loss: 0.6177935945987701\n",
      "  batch 800 loss: 0.667150838971138\n",
      "  batch 850 loss: 0.6660042780637742\n",
      "  batch 900 loss: 0.673548834323883\n",
      "LOSS train 0.67355 valid 0.89645, valid PER 27.08%\n",
      "Training finished in 9.0 minutes.\n",
      "Model saved to checkpoints/20231209_175446/model_16\n",
      "Currently using dropout rate of 0.3\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.7968504428863525\n",
      "  batch 100 loss: 3.283512649536133\n",
      "  batch 150 loss: 3.238341941833496\n",
      "  batch 200 loss: 3.105666785240173\n",
      "  batch 250 loss: 2.8860037755966186\n",
      "  batch 300 loss: 2.73650119304657\n",
      "  batch 350 loss: 2.6212311458587645\n",
      "  batch 400 loss: 2.5512893390655518\n",
      "  batch 450 loss: 2.455622673034668\n",
      "  batch 500 loss: 2.3223818254470827\n",
      "  batch 550 loss: 2.212673089504242\n",
      "  batch 600 loss: 2.1325522828102113\n",
      "  batch 650 loss: 2.0284000873565673\n",
      "  batch 700 loss: 2.0249642181396483\n",
      "  batch 750 loss: 1.9450614619255067\n",
      "  batch 800 loss: 1.9192776036262513\n",
      "  batch 850 loss: 1.8609219002723694\n",
      "  batch 900 loss: 1.805011818408966\n",
      "LOSS train 1.80501 valid 1.72415, valid PER 58.21%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.736344220638275\n",
      "  batch 100 loss: 1.7042345643043517\n",
      "  batch 150 loss: 1.6284329271316529\n",
      "  batch 200 loss: 1.707309401035309\n",
      "  batch 250 loss: 1.6738549971580505\n",
      "  batch 300 loss: 1.637461462020874\n",
      "  batch 350 loss: 1.5328632736206054\n",
      "  batch 400 loss: 1.5472185349464416\n",
      "  batch 450 loss: 1.5118827390670777\n",
      "  batch 500 loss: 1.5404925155639648\n",
      "  batch 550 loss: 1.5566015791893006\n",
      "  batch 600 loss: 1.521173086166382\n",
      "  batch 650 loss: 1.5619002056121827\n",
      "  batch 700 loss: 1.49314293384552\n",
      "  batch 750 loss: 1.4784179735183716\n",
      "  batch 800 loss: 1.4533618903160095\n",
      "  batch 850 loss: 1.446844539642334\n",
      "  batch 900 loss: 1.4776221251487731\n",
      "LOSS train 1.47762 valid 1.42098, valid PER 46.27%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4366395330429078\n",
      "  batch 100 loss: 1.4246179223060609\n",
      "  batch 150 loss: 1.4157715296745301\n",
      "  batch 200 loss: 1.4050787949562074\n",
      "  batch 250 loss: 1.3690424382686615\n",
      "  batch 300 loss: 1.3735745668411254\n",
      "  batch 350 loss: 1.4095399713516235\n",
      "  batch 400 loss: 1.3608078360557556\n",
      "  batch 450 loss: 1.3549465262889862\n",
      "  batch 500 loss: 1.3848703861236573\n",
      "  batch 550 loss: 1.3770528554916381\n",
      "  batch 600 loss: 1.3390316796302795\n",
      "  batch 650 loss: 1.2912932658195495\n",
      "  batch 700 loss: 1.3269226956367492\n",
      "  batch 750 loss: 1.3969165778160095\n",
      "  batch 800 loss: 1.313136065006256\n",
      "  batch 850 loss: 1.3300622606277466\n",
      "  batch 900 loss: 1.288269147872925\n",
      "LOSS train 1.28827 valid 1.31582, valid PER 41.18%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.273095760345459\n",
      "  batch 100 loss: 1.298393588066101\n",
      "  batch 150 loss: 1.2439245367050171\n",
      "  batch 200 loss: 1.265209172964096\n",
      "  batch 250 loss: 1.2832298636436463\n",
      "  batch 300 loss: 1.3013091063499451\n",
      "  batch 350 loss: 1.221488255262375\n",
      "  batch 400 loss: 1.255881245136261\n",
      "  batch 450 loss: 1.244194964170456\n",
      "  batch 500 loss: 1.228746576309204\n",
      "  batch 550 loss: 1.2576193010807037\n",
      "  batch 600 loss: 1.259592933654785\n",
      "  batch 650 loss: 1.247975172996521\n",
      "  batch 700 loss: 1.1968033134937286\n",
      "  batch 750 loss: 1.2066493654251098\n",
      "  batch 800 loss: 1.1905022931098939\n",
      "  batch 850 loss: 1.2459571862220764\n",
      "  batch 900 loss: 1.2383183109760285\n",
      "LOSS train 1.23832 valid 1.23418, valid PER 37.93%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.1921881532669067\n",
      "  batch 100 loss: 1.1750198960304261\n",
      "  batch 150 loss: 1.2133465111255646\n",
      "  batch 200 loss: 1.146855547428131\n",
      "  batch 250 loss: 1.1487092661857605\n",
      "  batch 300 loss: 1.1743771159648895\n",
      "  batch 350 loss: 1.174794294834137\n",
      "  batch 400 loss: 1.1811225128173828\n",
      "  batch 450 loss: 1.1800207495689392\n",
      "  batch 500 loss: 1.1945825362205504\n",
      "  batch 550 loss: 1.1407669126987456\n",
      "  batch 600 loss: 1.179360362291336\n",
      "  batch 650 loss: 1.1343450808525086\n",
      "  batch 700 loss: 1.1855205738544463\n",
      "  batch 750 loss: 1.1218595612049103\n",
      "  batch 800 loss: 1.1458096969127656\n",
      "  batch 850 loss: 1.136752539873123\n",
      "  batch 900 loss: 1.1454365468025207\n",
      "LOSS train 1.14544 valid 1.14100, valid PER 35.88%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.1993907630443572\n",
      "  batch 100 loss: 1.1063672852516175\n",
      "  batch 150 loss: 1.0740404653549194\n",
      "  batch 200 loss: 1.0916724610328674\n",
      "  batch 250 loss: 1.1138274335861207\n",
      "  batch 300 loss: 1.1042429065704347\n",
      "  batch 350 loss: 1.1015569698810577\n",
      "  batch 400 loss: 1.0862121772766113\n",
      "  batch 450 loss: 1.1027703809738159\n",
      "  batch 500 loss: 1.0872799170017242\n",
      "  batch 550 loss: 1.1044159531593323\n",
      "  batch 600 loss: 1.0847651910781861\n",
      "  batch 650 loss: 1.0991795670986175\n",
      "  batch 700 loss: 1.0967342793941497\n",
      "  batch 750 loss: 1.039040116071701\n",
      "  batch 800 loss: 1.0674531745910645\n",
      "  batch 850 loss: 1.0618248403072357\n",
      "  batch 900 loss: 1.0842565083503723\n",
      "LOSS train 1.08426 valid 1.07687, valid PER 34.81%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.0401688969135285\n",
      "  batch 100 loss: 1.0618310797214507\n",
      "  batch 150 loss: 1.0169208693504332\n",
      "  batch 200 loss: 1.0078360068798065\n",
      "  batch 250 loss: 1.027104080915451\n",
      "  batch 300 loss: 1.0038077521324158\n",
      "  batch 350 loss: 1.0266410255432128\n",
      "  batch 400 loss: 1.0328564846515655\n",
      "  batch 450 loss: 1.0483428037166596\n",
      "  batch 500 loss: 1.032651323080063\n",
      "  batch 550 loss: 0.9994442391395569\n",
      "  batch 600 loss: 1.032836972475052\n",
      "  batch 650 loss: 1.0325453805923461\n",
      "  batch 700 loss: 1.036338016986847\n",
      "  batch 750 loss: 1.02094064950943\n",
      "  batch 800 loss: 1.019290852546692\n",
      "  batch 850 loss: 1.057412873506546\n",
      "  batch 900 loss: 1.0505685377120972\n",
      "LOSS train 1.05057 valid 1.07175, valid PER 33.92%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.0024848699569702\n",
      "  batch 100 loss: 0.9666902363300324\n",
      "  batch 150 loss: 0.991392410993576\n",
      "  batch 200 loss: 0.981084269285202\n",
      "  batch 250 loss: 1.0019588315486907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 0.9427116703987122\n",
      "  batch 350 loss: 1.0354172229766845\n",
      "  batch 400 loss: 0.977017011642456\n",
      "  batch 450 loss: 1.0054696369171143\n",
      "  batch 500 loss: 1.00422119140625\n",
      "  batch 550 loss: 0.9683794724941254\n",
      "  batch 600 loss: 1.0250252628326415\n",
      "  batch 650 loss: 1.0454233229160308\n",
      "  batch 700 loss: 0.9544640362262726\n",
      "  batch 750 loss: 0.9876099634170532\n",
      "  batch 800 loss: 0.9923493635654449\n",
      "  batch 850 loss: 0.9833037805557251\n",
      "  batch 900 loss: 0.9803268277645111\n",
      "LOSS train 0.98033 valid 1.04263, valid PER 32.13%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.9125952112674713\n",
      "  batch 100 loss: 0.9464164209365845\n",
      "  batch 150 loss: 0.9530799508094787\n",
      "  batch 200 loss: 0.9162597060203552\n",
      "  batch 250 loss: 0.9521110844612122\n",
      "  batch 300 loss: 0.9422731041908264\n",
      "  batch 350 loss: 0.9801238977909088\n",
      "  batch 400 loss: 0.9429351842403412\n",
      "  batch 450 loss: 0.9554661011695862\n",
      "  batch 500 loss: 0.9178290092945098\n",
      "  batch 550 loss: 0.9766494119167328\n",
      "  batch 600 loss: 0.9622375333309173\n",
      "  batch 650 loss: 0.961494220495224\n",
      "  batch 700 loss: 0.940850876569748\n",
      "  batch 750 loss: 0.9484409308433532\n",
      "  batch 800 loss: 0.9773686933517456\n",
      "  batch 850 loss: 0.9871593344211579\n",
      "  batch 900 loss: 0.9111129081249237\n",
      "LOSS train 0.91111 valid 0.98150, valid PER 31.11%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.8974206566810607\n",
      "  batch 100 loss: 0.9041336035728454\n",
      "  batch 150 loss: 0.9112035524845123\n",
      "  batch 200 loss: 0.9399942564964294\n",
      "  batch 250 loss: 0.935829176902771\n",
      "  batch 300 loss: 0.8718219304084778\n",
      "  batch 350 loss: 0.9128631317615509\n",
      "  batch 400 loss: 0.8923014605045319\n",
      "  batch 450 loss: 0.8777659440040588\n",
      "  batch 500 loss: 0.9382043051719665\n",
      "  batch 550 loss: 0.9527675092220307\n",
      "  batch 600 loss: 0.9269536590576172\n",
      "  batch 650 loss: 0.9773867952823639\n",
      "  batch 700 loss: 0.9412705028057098\n",
      "  batch 750 loss: 0.9061770939826965\n",
      "  batch 800 loss: 0.9278182089328766\n",
      "  batch 850 loss: 0.9185586416721344\n",
      "  batch 900 loss: 0.9396720969676972\n",
      "LOSS train 0.93967 valid 0.95579, valid PER 29.92%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.8505218982696533\n",
      "  batch 100 loss: 0.8503359782695771\n",
      "  batch 150 loss: 0.8496555507183075\n",
      "  batch 200 loss: 0.8963666844367981\n",
      "  batch 250 loss: 0.91082155585289\n",
      "  batch 300 loss: 0.8483390593528748\n",
      "  batch 350 loss: 0.9061909580230713\n",
      "  batch 400 loss: 0.9090702497959137\n",
      "  batch 450 loss: 0.907033303976059\n",
      "  batch 500 loss: 0.8649492835998536\n",
      "  batch 550 loss: 0.8913313460350036\n",
      "  batch 600 loss: 0.8726594614982605\n",
      "  batch 650 loss: 0.9761730754375457\n",
      "  batch 700 loss: 0.8598045217990875\n",
      "  batch 750 loss: 0.8504745233058929\n",
      "  batch 800 loss: 0.904795982837677\n",
      "  batch 850 loss: 0.9275401616096497\n",
      "  batch 900 loss: 0.9136236357688904\n",
      "LOSS train 0.91362 valid 0.94519, valid PER 29.74%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.8459685671329499\n",
      "  batch 100 loss: 0.8174935078620911\n",
      "  batch 150 loss: 0.8522546273469925\n",
      "  batch 200 loss: 0.8363512372970581\n",
      "  batch 250 loss: 0.8774244654178619\n",
      "  batch 300 loss: 0.8434127008914948\n",
      "  batch 350 loss: 0.8507247030735016\n",
      "  batch 400 loss: 0.8831019294261933\n",
      "  batch 450 loss: 0.8757491540908814\n",
      "  batch 500 loss: 0.9060081350803375\n",
      "  batch 550 loss: 0.8097348725795745\n",
      "  batch 600 loss: 0.8292382788658142\n",
      "  batch 650 loss: 0.8785838627815247\n",
      "  batch 700 loss: 0.8400476753711701\n",
      "  batch 750 loss: 0.8399255591630935\n",
      "  batch 800 loss: 0.8251955211162567\n",
      "  batch 850 loss: 0.8606524777412414\n",
      "  batch 900 loss: 0.8686083567142486\n",
      "LOSS train 0.86861 valid 0.93440, valid PER 29.39%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.8129360115528107\n",
      "  batch 100 loss: 0.8370932900905609\n",
      "  batch 150 loss: 0.7934690392017365\n",
      "  batch 200 loss: 0.8493415856361389\n",
      "  batch 250 loss: 0.8223409295082093\n",
      "  batch 300 loss: 0.8051004064083099\n",
      "  batch 350 loss: 0.8011787927150726\n",
      "  batch 400 loss: 0.8376050209999084\n",
      "  batch 450 loss: 0.8681888008117675\n",
      "  batch 500 loss: 0.8214461898803711\n",
      "  batch 550 loss: 0.8449184691905975\n",
      "  batch 600 loss: 0.8150583016872406\n",
      "  batch 650 loss: 0.8367353308200837\n",
      "  batch 700 loss: 0.8231403094530105\n",
      "  batch 750 loss: 0.7955623888969421\n",
      "  batch 800 loss: 0.8154237532615661\n",
      "  batch 850 loss: 0.8609853446483612\n",
      "  batch 900 loss: 0.8324503219127655\n",
      "LOSS train 0.83245 valid 0.93832, valid PER 29.60%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.7841387569904328\n",
      "  batch 100 loss: 0.7966897368431092\n",
      "  batch 150 loss: 0.7733716452121735\n",
      "  batch 200 loss: 0.798580772280693\n",
      "  batch 250 loss: 0.7879395270347596\n",
      "  batch 300 loss: 0.8127388548851013\n",
      "  batch 350 loss: 0.7607155466079711\n",
      "  batch 400 loss: 0.781470000743866\n",
      "  batch 450 loss: 0.7887496507167816\n",
      "  batch 500 loss: 0.7860941338539124\n",
      "  batch 550 loss: 0.8027627420425415\n",
      "  batch 600 loss: 0.7727253699302673\n",
      "  batch 650 loss: 0.8311463534832001\n",
      "  batch 700 loss: 0.800385200381279\n",
      "  batch 750 loss: 0.8022318649291992\n",
      "  batch 800 loss: 0.7743523919582367\n",
      "  batch 850 loss: 0.8176713991165161\n",
      "  batch 900 loss: 0.8183606421947479\n",
      "LOSS train 0.81836 valid 0.90812, valid PER 28.48%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.7486220222711563\n",
      "  batch 100 loss: 0.7496208894252777\n",
      "  batch 150 loss: 0.7355884027481079\n",
      "  batch 200 loss: 0.7810909020900726\n",
      "  batch 250 loss: 0.8202609062194824\n",
      "  batch 300 loss: 0.7633296811580658\n",
      "  batch 350 loss: 0.7694260060787201\n",
      "  batch 400 loss: 0.7650868058204651\n",
      "  batch 450 loss: 0.7851211309432984\n",
      "  batch 500 loss: 0.7521993935108184\n",
      "  batch 550 loss: 0.7727640581130981\n",
      "  batch 600 loss: 0.7876915776729584\n",
      "  batch 650 loss: 0.8360014116764068\n",
      "  batch 700 loss: 0.8059459924697876\n",
      "  batch 750 loss: 0.7861068749427795\n",
      "  batch 800 loss: 0.7629795718193054\n",
      "  batch 850 loss: 0.7564909785985947\n",
      "  batch 900 loss: 0.7950916540622711\n",
      "LOSS train 0.79509 valid 0.90367, valid PER 28.31%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.752646131515503\n",
      "  batch 100 loss: 0.7030632495880127\n",
      "  batch 150 loss: 0.7444963526725769\n",
      "  batch 200 loss: 0.7128022754192352\n",
      "  batch 250 loss: 0.7739670324325562\n",
      "  batch 300 loss: 0.7315566694736481\n",
      "  batch 350 loss: 0.7640292054414749\n",
      "  batch 400 loss: 0.7585751783847808\n",
      "  batch 450 loss: 0.7762684011459351\n",
      "  batch 500 loss: 0.7199513471126556\n",
      "  batch 550 loss: 0.7322465795278549\n",
      "  batch 600 loss: 0.7448408770561218\n",
      "  batch 650 loss: 0.7852909409999848\n",
      "  batch 700 loss: 0.742087551355362\n",
      "  batch 750 loss: 0.7373946285247803\n",
      "  batch 800 loss: 0.7409826326370239\n",
      "  batch 850 loss: 0.7431174623966217\n",
      "  batch 900 loss: 0.7547858691215515\n",
      "LOSS train 0.75479 valid 0.90688, valid PER 28.54%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.7282475054264068\n",
      "  batch 100 loss: 0.7182314944267273\n",
      "  batch 150 loss: 0.6865941125154496\n",
      "  batch 200 loss: 0.7052082937955856\n",
      "  batch 250 loss: 0.705621263384819\n",
      "  batch 300 loss: 0.7290809452533722\n",
      "  batch 350 loss: 0.7113827353715897\n",
      "  batch 400 loss: 0.7846932423114776\n",
      "  batch 450 loss: 0.7355619066953659\n",
      "  batch 500 loss: 0.7222173964977264\n",
      "  batch 550 loss: 0.7502971303462982\n",
      "  batch 600 loss: 0.8280391025543213\n",
      "  batch 650 loss: 0.7524145317077636\n",
      "  batch 700 loss: 0.7279735368490219\n",
      "  batch 750 loss: 0.7142359846830368\n",
      "  batch 800 loss: 0.7217862796783447\n",
      "  batch 850 loss: 0.7348674309253692\n",
      "  batch 900 loss: 0.7232953655719757\n",
      "LOSS train 0.72330 valid 0.89870, valid PER 27.78%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.7121573209762573\n",
      "  batch 100 loss: 0.7293448269367218\n",
      "  batch 150 loss: 0.7129768610000611\n",
      "  batch 200 loss: 0.6975778585672379\n",
      "  batch 250 loss: 0.7076465356349945\n",
      "  batch 300 loss: 0.71040363073349\n",
      "  batch 350 loss: 0.7110432612895966\n",
      "  batch 400 loss: 0.7086580562591552\n",
      "  batch 450 loss: 0.7452314531803131\n",
      "  batch 500 loss: 0.7064503455162048\n",
      "  batch 550 loss: 0.708499922156334\n",
      "  batch 600 loss: 0.6728628259897232\n",
      "  batch 650 loss: 0.6942627191543579\n",
      "  batch 700 loss: 0.7164105701446534\n",
      "  batch 750 loss: 0.707962155342102\n",
      "  batch 800 loss: 0.6953412580490113\n",
      "  batch 850 loss: 0.7005311870574951\n",
      "  batch 900 loss: 0.7364112794399261\n",
      "LOSS train 0.73641 valid 0.89624, valid PER 27.91%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.6692799454927445\n",
      "  batch 100 loss: 0.6466828453540802\n",
      "  batch 150 loss: 0.687804291844368\n",
      "  batch 200 loss: 0.6839962416887283\n",
      "  batch 250 loss: 0.702609891295433\n",
      "  batch 300 loss: 0.6658743017911911\n",
      "  batch 350 loss: 0.6901765197515488\n",
      "  batch 400 loss: 0.6910812693834305\n",
      "  batch 450 loss: 0.7042352992296219\n",
      "  batch 500 loss: 0.7183088111877441\n",
      "  batch 550 loss: 0.663628352880478\n",
      "  batch 600 loss: 0.6772782647609711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 650 loss: 0.736552112698555\n",
      "  batch 700 loss: 0.6551927828788757\n",
      "  batch 750 loss: 0.6649380666017533\n",
      "  batch 800 loss: 0.7096665036678315\n",
      "  batch 850 loss: 0.7125500464439392\n",
      "  batch 900 loss: 0.7164711463451385\n",
      "LOSS train 0.71647 valid 0.88072, valid PER 26.93%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.6325559604167938\n",
      "  batch 100 loss: 0.6451751148700714\n",
      "  batch 150 loss: 0.6496818941831589\n",
      "  batch 200 loss: 0.6569957983493805\n",
      "  batch 250 loss: 0.6548853641748429\n",
      "  batch 300 loss: 0.6984907245635986\n",
      "  batch 350 loss: 0.6450159102678299\n",
      "  batch 400 loss: 0.660501571893692\n",
      "  batch 450 loss: 0.6663973706960679\n",
      "  batch 500 loss: 0.6446354824304581\n",
      "  batch 550 loss: 0.6997564119100571\n",
      "  batch 600 loss: 0.6417151600122452\n",
      "  batch 650 loss: 0.685376780629158\n",
      "  batch 700 loss: 0.690725759267807\n",
      "  batch 750 loss: 0.6581242877244949\n",
      "  batch 800 loss: 0.6892657709121705\n",
      "  batch 850 loss: 0.6989101016521454\n",
      "  batch 900 loss: 0.6753958594799042\n",
      "LOSS train 0.67540 valid 0.88265, valid PER 27.25%\n",
      "Training finished in 9.0 minutes.\n",
      "Model saved to checkpoints/20231209_180355/model_19\n",
      "Currently using dropout rate of 0.4\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.80450336933136\n",
      "  batch 100 loss: 3.283140306472778\n",
      "  batch 150 loss: 3.2383489179611207\n",
      "  batch 200 loss: 3.1064887142181394\n",
      "  batch 250 loss: 2.8924641275405882\n",
      "  batch 300 loss: 2.7431233644485475\n",
      "  batch 350 loss: 2.6341855049133303\n",
      "  batch 400 loss: 2.578898868560791\n",
      "  batch 450 loss: 2.485662245750427\n",
      "  batch 500 loss: 2.367798147201538\n",
      "  batch 550 loss: 2.250630760192871\n",
      "  batch 600 loss: 2.1699496030807497\n",
      "  batch 650 loss: 2.069870228767395\n",
      "  batch 700 loss: 2.064520583152771\n",
      "  batch 750 loss: 1.9810661458969117\n",
      "  batch 800 loss: 1.9588257765769959\n",
      "  batch 850 loss: 1.8868051624298097\n",
      "  batch 900 loss: 1.8406221127510072\n",
      "LOSS train 1.84062 valid 1.74675, valid PER 60.61%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7931800389289856\n",
      "  batch 100 loss: 1.733557834625244\n",
      "  batch 150 loss: 1.6768060517311096\n",
      "  batch 200 loss: 1.7140208745002747\n",
      "  batch 250 loss: 1.6919659233093263\n",
      "  batch 300 loss: 1.652916395664215\n",
      "  batch 350 loss: 1.5753030180931091\n",
      "  batch 400 loss: 1.5919311809539796\n",
      "  batch 450 loss: 1.5346175408363343\n",
      "  batch 500 loss: 1.5821748542785645\n",
      "  batch 550 loss: 1.5730949878692626\n",
      "  batch 600 loss: 1.535523693561554\n",
      "  batch 650 loss: 1.5684428191184998\n",
      "  batch 700 loss: 1.5170284175872804\n",
      "  batch 750 loss: 1.4962296295166015\n",
      "  batch 800 loss: 1.471259641647339\n",
      "  batch 850 loss: 1.455409379005432\n",
      "  batch 900 loss: 1.4961011052131652\n",
      "LOSS train 1.49610 valid 1.45420, valid PER 46.83%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4513283061981201\n",
      "  batch 100 loss: 1.4272171092033386\n",
      "  batch 150 loss: 1.4140051054954528\n",
      "  batch 200 loss: 1.4160683417320252\n",
      "  batch 250 loss: 1.401680474281311\n",
      "  batch 300 loss: 1.386031277179718\n",
      "  batch 350 loss: 1.430160264968872\n",
      "  batch 400 loss: 1.4049903678894042\n",
      "  batch 450 loss: 1.3625123143196105\n",
      "  batch 500 loss: 1.380487837791443\n",
      "  batch 550 loss: 1.3866885232925414\n",
      "  batch 600 loss: 1.365538775920868\n",
      "  batch 650 loss: 1.358687126636505\n",
      "  batch 700 loss: 1.3736074709892272\n",
      "  batch 750 loss: 1.4084155106544494\n",
      "  batch 800 loss: 1.330386700630188\n",
      "  batch 850 loss: 1.3453845620155334\n",
      "  batch 900 loss: 1.3211829447746277\n",
      "LOSS train 1.32118 valid 1.27690, valid PER 41.47%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.286763277053833\n",
      "  batch 100 loss: 1.293525002002716\n",
      "  batch 150 loss: 1.272196433544159\n",
      "  batch 200 loss: 1.3049565935134888\n",
      "  batch 250 loss: 1.3059256768226624\n",
      "  batch 300 loss: 1.2968559753894806\n",
      "  batch 350 loss: 1.2329849684238434\n",
      "  batch 400 loss: 1.276110838651657\n",
      "  batch 450 loss: 1.2553599560260773\n",
      "  batch 500 loss: 1.257220287322998\n",
      "  batch 550 loss: 1.3041245973110198\n",
      "  batch 600 loss: 1.2739014327526093\n",
      "  batch 650 loss: 1.278852572441101\n",
      "  batch 700 loss: 1.2382643818855286\n",
      "  batch 750 loss: 1.220600243806839\n",
      "  batch 800 loss: 1.2174176955223084\n",
      "  batch 850 loss: 1.2347215449810027\n",
      "  batch 900 loss: 1.2816176414489746\n",
      "LOSS train 1.28162 valid 1.17272, valid PER 38.36%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.2020316696166993\n",
      "  batch 100 loss: 1.1865431034564973\n",
      "  batch 150 loss: 1.2344922018051148\n",
      "  batch 200 loss: 1.1659241676330567\n",
      "  batch 250 loss: 1.165987821817398\n",
      "  batch 300 loss: 1.2757208788394927\n",
      "  batch 350 loss: 1.2495101976394654\n",
      "  batch 400 loss: 1.2096334099769592\n",
      "  batch 450 loss: 1.1908754789829255\n",
      "  batch 500 loss: 1.2200913691520692\n",
      "  batch 550 loss: 1.1833780086040497\n",
      "  batch 600 loss: 1.2060385978221893\n",
      "  batch 650 loss: 1.16536869764328\n",
      "  batch 700 loss: 1.1979509174823761\n",
      "  batch 750 loss: 1.1532729494571685\n",
      "  batch 800 loss: 1.1757506036758423\n",
      "  batch 850 loss: 1.2039767694473267\n",
      "  batch 900 loss: 1.2006546914577485\n",
      "LOSS train 1.20065 valid 1.15007, valid PER 36.86%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.1832728385925293\n",
      "  batch 100 loss: 1.136093031167984\n",
      "  batch 150 loss: 1.1071139097213745\n",
      "  batch 200 loss: 1.1365249633789063\n",
      "  batch 250 loss: 1.1494883406162262\n",
      "  batch 300 loss: 1.1325953245162963\n",
      "  batch 350 loss: 1.1195579433441163\n",
      "  batch 400 loss: 1.1269571030139922\n",
      "  batch 450 loss: 1.1540940475463868\n",
      "  batch 500 loss: 1.1679224908351897\n",
      "  batch 550 loss: 1.1709407222270967\n",
      "  batch 600 loss: 1.1412940871715547\n",
      "  batch 650 loss: 1.1434669697284698\n",
      "  batch 700 loss: 1.1277615308761597\n",
      "  batch 750 loss: 1.0938963496685028\n",
      "  batch 800 loss: 1.0852029895782471\n",
      "  batch 850 loss: 1.0808147859573365\n",
      "  batch 900 loss: 1.1137043738365173\n",
      "LOSS train 1.11370 valid 1.08506, valid PER 35.38%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.0937127768993378\n",
      "  batch 100 loss: 1.099077469110489\n",
      "  batch 150 loss: 1.0728572154045104\n",
      "  batch 200 loss: 1.0809456944465636\n",
      "  batch 250 loss: 1.0750779974460603\n",
      "  batch 300 loss: 1.0524948334693909\n",
      "  batch 350 loss: 1.0528058683872223\n",
      "  batch 400 loss: 1.06238872051239\n",
      "  batch 450 loss: 1.0644672846794128\n",
      "  batch 500 loss: 1.0689989125728607\n",
      "  batch 550 loss: 1.0830379486083985\n",
      "  batch 600 loss: 1.0872389090061187\n",
      "  batch 650 loss: 1.0657132053375244\n",
      "  batch 700 loss: 1.0676378643512725\n",
      "  batch 750 loss: 1.0599502444267273\n",
      "  batch 800 loss: 1.0618863272666932\n",
      "  batch 850 loss: 1.0991273283958436\n",
      "  batch 900 loss: 1.1019098806381225\n",
      "LOSS train 1.10191 valid 1.08817, valid PER 34.78%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.0603395843505858\n",
      "  batch 100 loss: 1.0368285644054414\n",
      "  batch 150 loss: 1.063805078268051\n",
      "  batch 200 loss: 1.0334234845638275\n",
      "  batch 250 loss: 1.052659363746643\n",
      "  batch 300 loss: 0.966563012599945\n",
      "  batch 350 loss: 1.0832479524612426\n",
      "  batch 400 loss: 1.0113222563266755\n",
      "  batch 450 loss: 1.0825808441638947\n",
      "  batch 500 loss: 1.0976322162151337\n",
      "  batch 550 loss: 1.0431645321846008\n",
      "  batch 600 loss: 1.0264609634876252\n",
      "  batch 650 loss: 1.0694176733493805\n",
      "  batch 700 loss: 1.026741051673889\n",
      "  batch 750 loss: 1.0204847049713135\n",
      "  batch 800 loss: 1.0471817779541015\n",
      "  batch 850 loss: 1.023288586139679\n",
      "  batch 900 loss: 1.0352283656597137\n",
      "LOSS train 1.03523 valid 1.04586, valid PER 33.42%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.948651933670044\n",
      "  batch 100 loss: 0.9937713372707367\n",
      "  batch 150 loss: 1.011116453409195\n",
      "  batch 200 loss: 0.9363577473163605\n",
      "  batch 250 loss: 1.0061839890480042\n",
      "  batch 300 loss: 1.0244298326969146\n",
      "  batch 350 loss: 0.9943998062610626\n",
      "  batch 400 loss: 0.9893137621879577\n",
      "  batch 450 loss: 1.0021005463600159\n",
      "  batch 500 loss: 0.9598635244369507\n",
      "  batch 550 loss: 1.0271065104007722\n",
      "  batch 600 loss: 1.0068154871463775\n",
      "  batch 650 loss: 1.0054926347732545\n",
      "  batch 700 loss: 0.9624658310413361\n",
      "  batch 750 loss: 1.0148728048801423\n",
      "  batch 800 loss: 1.007468012571335\n",
      "  batch 850 loss: 1.0281980443000793\n",
      "  batch 900 loss: 0.9739094507694245\n",
      "LOSS train 0.97391 valid 1.01362, valid PER 32.38%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.9348573839664459\n",
      "  batch 100 loss: 0.9550203466415406\n",
      "  batch 150 loss: 0.9627707529067994\n",
      "  batch 200 loss: 0.9924473452568054\n",
      "  batch 250 loss: 0.9850779831409454\n",
      "  batch 300 loss: 0.9497089970111847\n",
      "  batch 350 loss: 0.9643006539344787\n",
      "  batch 400 loss: 0.9333366441726685\n",
      "  batch 450 loss: 0.924903119802475\n",
      "  batch 500 loss: 0.9909975242614746\n",
      "  batch 550 loss: 0.9827028346061707\n",
      "  batch 600 loss: 0.937992069721222\n",
      "  batch 650 loss: 0.9404913794994354\n",
      "  batch 700 loss: 0.9677562892436982\n",
      "  batch 750 loss: 0.9325590324401856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 800 loss: 0.9495143926143647\n",
      "  batch 850 loss: 0.9757311034202576\n",
      "  batch 900 loss: 0.9828205323219299\n",
      "LOSS train 0.98282 valid 1.00373, valid PER 31.06%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.9207299494743347\n",
      "  batch 100 loss: 0.9087458670139312\n",
      "  batch 150 loss: 0.9106666398048401\n",
      "  batch 200 loss: 0.964024885892868\n",
      "  batch 250 loss: 0.9544546425342559\n",
      "  batch 300 loss: 0.8976307284832\n",
      "  batch 350 loss: 0.9504489779472352\n",
      "  batch 400 loss: 1.0083463442325593\n",
      "  batch 450 loss: 0.9764277517795563\n",
      "  batch 500 loss: 0.9129799127578735\n",
      "  batch 550 loss: 0.9160435926914215\n",
      "  batch 600 loss: 0.9348890674114227\n",
      "  batch 650 loss: 0.9621075403690338\n",
      "  batch 700 loss: 0.9044822025299072\n",
      "  batch 750 loss: 0.9123932433128357\n",
      "  batch 800 loss: 0.9511575508117676\n",
      "  batch 850 loss: 0.9684589076042175\n",
      "  batch 900 loss: 0.9541422259807587\n",
      "LOSS train 0.95414 valid 0.97881, valid PER 30.90%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.9104132771492004\n",
      "  batch 100 loss: 0.9145445728302002\n",
      "  batch 150 loss: 0.8560525524616241\n",
      "  batch 200 loss: 0.8932366812229157\n",
      "  batch 250 loss: 0.897752537727356\n",
      "  batch 300 loss: 0.8620900928974151\n",
      "  batch 350 loss: 0.8836006009578705\n",
      "  batch 400 loss: 0.9209509313106536\n",
      "  batch 450 loss: 0.9049556422233581\n",
      "  batch 500 loss: 0.923403297662735\n",
      "  batch 550 loss: 0.8356773054599762\n",
      "  batch 600 loss: 0.8735809379816055\n",
      "  batch 650 loss: 0.9232804524898529\n",
      "  batch 700 loss: 0.9149615323543548\n",
      "  batch 750 loss: 0.8955921900272369\n",
      "  batch 800 loss: 0.8949199771881103\n",
      "  batch 850 loss: 0.9561229526996613\n",
      "  batch 900 loss: 0.9303105437755584\n",
      "LOSS train 0.93031 valid 0.94812, valid PER 30.51%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.8305232322216034\n",
      "  batch 100 loss: 0.8868128407001495\n",
      "  batch 150 loss: 0.8460755658149719\n",
      "  batch 200 loss: 0.8858013105392456\n",
      "  batch 250 loss: 0.8815915644168854\n",
      "  batch 300 loss: 0.8703481113910675\n",
      "  batch 350 loss: 0.8661390924453736\n",
      "  batch 400 loss: 0.8642764949798584\n",
      "  batch 450 loss: 0.8926033794879913\n",
      "  batch 500 loss: 0.8445019960403443\n",
      "  batch 550 loss: 0.8825602281093597\n",
      "  batch 600 loss: 0.8732097744941711\n",
      "  batch 650 loss: 0.8932740616798401\n",
      "  batch 700 loss: 0.8889051759243012\n",
      "  batch 750 loss: 0.8951404953002929\n",
      "  batch 800 loss: 0.8786066782474518\n",
      "  batch 850 loss: 0.918543154001236\n",
      "  batch 900 loss: 0.8913990473747253\n",
      "LOSS train 0.89140 valid 0.94322, valid PER 29.90%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.8235051679611206\n",
      "  batch 100 loss: 0.843967182636261\n",
      "  batch 150 loss: 0.8384316372871399\n",
      "  batch 200 loss: 0.8515137887001037\n",
      "  batch 250 loss: 0.8504642105102539\n",
      "  batch 300 loss: 0.8801207041740418\n",
      "  batch 350 loss: 0.826169661283493\n",
      "  batch 400 loss: 0.835232036113739\n",
      "  batch 450 loss: 0.8302088701725006\n",
      "  batch 500 loss: 0.8728552901744843\n",
      "  batch 550 loss: 0.8577171778678894\n",
      "  batch 600 loss: 0.827941448688507\n",
      "  batch 650 loss: 0.8498851132392883\n",
      "  batch 700 loss: 0.8623244619369507\n",
      "  batch 750 loss: 0.8167041063308715\n",
      "  batch 800 loss: 0.798550683259964\n",
      "  batch 850 loss: 0.8731090307235718\n",
      "  batch 900 loss: 0.8618495070934296\n",
      "LOSS train 0.86185 valid 0.93466, valid PER 29.29%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.8090230214595795\n",
      "  batch 100 loss: 0.8066821455955505\n",
      "  batch 150 loss: 0.8108718359470367\n",
      "  batch 200 loss: 0.8446825861930847\n",
      "  batch 250 loss: 0.8276633656024933\n",
      "  batch 300 loss: 0.7975715863704681\n",
      "  batch 350 loss: 0.8011641907691955\n",
      "  batch 400 loss: 0.8082117676734925\n",
      "  batch 450 loss: 0.8133657717704773\n",
      "  batch 500 loss: 0.7721709215641022\n",
      "  batch 550 loss: 0.829669634103775\n",
      "  batch 600 loss: 0.8445442295074463\n",
      "  batch 650 loss: 0.828650393486023\n",
      "  batch 700 loss: 0.8484035074710846\n",
      "  batch 750 loss: 0.8588602888584137\n",
      "  batch 800 loss: 0.8418000364303588\n",
      "  batch 850 loss: 0.8163442134857177\n",
      "  batch 900 loss: 0.862950222492218\n",
      "LOSS train 0.86295 valid 0.93385, valid PER 29.65%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.8329365932941437\n",
      "  batch 100 loss: 0.7923811411857605\n",
      "  batch 150 loss: 0.8040166163444519\n",
      "  batch 200 loss: 0.81032035946846\n",
      "  batch 250 loss: 0.8190831768512726\n",
      "  batch 300 loss: 0.8028341615200043\n",
      "  batch 350 loss: 0.8209516680240632\n",
      "  batch 400 loss: 0.8030015087127685\n",
      "  batch 450 loss: 0.8370776855945588\n",
      "  batch 500 loss: 0.8157623589038849\n",
      "  batch 550 loss: 0.7892947828769684\n",
      "  batch 600 loss: 0.7925235116481781\n",
      "  batch 650 loss: 0.8429054474830627\n",
      "  batch 700 loss: 0.797938928604126\n",
      "  batch 750 loss: 0.8122420537471772\n",
      "  batch 800 loss: 0.7956731331348419\n",
      "  batch 850 loss: 0.7987240952253342\n",
      "  batch 900 loss: 0.8008828389644623\n",
      "LOSS train 0.80088 valid 0.91799, valid PER 28.88%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.7749338328838349\n",
      "  batch 100 loss: 0.7562146192789078\n",
      "  batch 150 loss: 0.7594768929481507\n",
      "  batch 200 loss: 0.7622045087814331\n",
      "  batch 250 loss: 0.7717062485218048\n",
      "  batch 300 loss: 0.7928718912601471\n",
      "  batch 350 loss: 0.7510838657617569\n",
      "  batch 400 loss: 0.8130197238922119\n",
      "  batch 450 loss: 0.7837997889518737\n",
      "  batch 500 loss: 0.7694277650117874\n",
      "  batch 550 loss: 0.7771732467412948\n",
      "  batch 600 loss: 0.8381165814399719\n",
      "  batch 650 loss: 0.7885422646999359\n",
      "  batch 700 loss: 0.7899301809072494\n",
      "  batch 750 loss: 0.7861868453025818\n",
      "  batch 800 loss: 0.7646085226535797\n",
      "  batch 850 loss: 0.8050932234525681\n",
      "  batch 900 loss: 0.7704128551483155\n",
      "LOSS train 0.77041 valid 0.89999, valid PER 27.46%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.7577933895587922\n",
      "  batch 100 loss: 0.7405837333202362\n",
      "  batch 150 loss: 0.7638603436946869\n",
      "  batch 200 loss: 0.7632928162813186\n",
      "  batch 250 loss: 0.7922368943691254\n",
      "  batch 300 loss: 0.8147423493862153\n",
      "  batch 350 loss: 0.8334908664226532\n",
      "  batch 400 loss: 0.7607496166229248\n",
      "  batch 450 loss: 0.800221530199051\n",
      "  batch 500 loss: 0.7984488141536713\n",
      "  batch 550 loss: 0.8105564963817596\n",
      "  batch 600 loss: 0.7443226516246796\n",
      "  batch 650 loss: 0.7524122619628906\n",
      "  batch 700 loss: 0.779752470254898\n",
      "  batch 750 loss: 0.7816349160671234\n",
      "  batch 800 loss: 0.7418716633319855\n",
      "  batch 850 loss: 0.7426550740003586\n",
      "  batch 900 loss: 0.7887346827983857\n",
      "LOSS train 0.78873 valid 0.91020, valid PER 28.03%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.7026091992855072\n",
      "  batch 100 loss: 0.6846753984689713\n",
      "  batch 150 loss: 0.7244810807704926\n",
      "  batch 200 loss: 0.7543440020084381\n",
      "  batch 250 loss: 0.7693316411972045\n",
      "  batch 300 loss: 0.7372813028097153\n",
      "  batch 350 loss: 0.7450784695148468\n",
      "  batch 400 loss: 0.7528361213207245\n",
      "  batch 450 loss: 0.7462135148048401\n",
      "  batch 500 loss: 0.7616336190700531\n",
      "  batch 550 loss: 0.7058840268850326\n",
      "  batch 600 loss: 0.7466722726821899\n",
      "  batch 650 loss: 0.7871161472797393\n",
      "  batch 700 loss: 0.7133225899934769\n",
      "  batch 750 loss: 0.7269078522920609\n",
      "  batch 800 loss: 0.7490832149982453\n",
      "  batch 850 loss: 0.7448176473379136\n",
      "  batch 900 loss: 0.7395350396633148\n",
      "LOSS train 0.73954 valid 0.88442, valid PER 27.13%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.7045456558465958\n",
      "  batch 100 loss: 0.7096179437637329\n",
      "  batch 150 loss: 0.7030170118808746\n",
      "  batch 200 loss: 0.6959616434574127\n",
      "  batch 250 loss: 0.6991294461488724\n",
      "  batch 300 loss: 0.7364731973409653\n",
      "  batch 350 loss: 0.6811290580034256\n",
      "  batch 400 loss: 0.7034461909532547\n",
      "  batch 450 loss: 0.7027005761861801\n",
      "  batch 500 loss: 0.6960743129253387\n",
      "  batch 550 loss: 0.7469508075714111\n",
      "  batch 600 loss: 0.6931329083442688\n",
      "  batch 650 loss: 0.7393486368656158\n",
      "  batch 700 loss: 0.71566126704216\n",
      "  batch 750 loss: 0.7035074526071549\n",
      "  batch 800 loss: 0.7406851774454117\n",
      "  batch 850 loss: 0.7591057693958283\n",
      "  batch 900 loss: 0.7363567745685577\n",
      "LOSS train 0.73636 valid 0.88798, valid PER 27.34%\n",
      "Training finished in 9.0 minutes.\n",
      "Model saved to checkpoints/20231209_181304/model_19\n",
      "Currently using dropout rate of 0.5\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.808479108810425\n",
      "  batch 100 loss: 3.284929723739624\n",
      "  batch 150 loss: 3.2394107818603515\n",
      "  batch 200 loss: 3.121606011390686\n",
      "  batch 250 loss: 2.9085700035095217\n",
      "  batch 300 loss: 2.752601728439331\n",
      "  batch 350 loss: 2.6452973604202272\n",
      "  batch 400 loss: 2.587904920578003\n",
      "  batch 450 loss: 2.5035421323776244\n",
      "  batch 500 loss: 2.3861169815063477\n",
      "  batch 550 loss: 2.28459792137146\n",
      "  batch 600 loss: 2.206964991092682\n",
      "  batch 650 loss: 2.108897626399994\n",
      "  batch 700 loss: 2.100326430797577\n",
      "  batch 750 loss: 2.0207440090179443\n",
      "  batch 800 loss: 2.009223074913025\n",
      "  batch 850 loss: 1.9491660833358764\n",
      "  batch 900 loss: 1.9072019362449646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 1.90720 valid 1.79373, valid PER 62.94%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.8301711058616639\n",
      "  batch 100 loss: 1.7926499128341675\n",
      "  batch 150 loss: 1.7303179717063903\n",
      "  batch 200 loss: 1.769501256942749\n",
      "  batch 250 loss: 1.744200382232666\n",
      "  batch 300 loss: 1.718890097141266\n",
      "  batch 350 loss: 1.6274459266662598\n",
      "  batch 400 loss: 1.6443031048774719\n",
      "  batch 450 loss: 1.6095751333236694\n",
      "  batch 500 loss: 1.6112739849090576\n",
      "  batch 550 loss: 1.6410032892227173\n",
      "  batch 600 loss: 1.5927890825271607\n",
      "  batch 650 loss: 1.616264054775238\n",
      "  batch 700 loss: 1.5981450653076172\n",
      "  batch 750 loss: 1.5490399503707886\n",
      "  batch 800 loss: 1.5195410609245301\n",
      "  batch 850 loss: 1.532885057926178\n",
      "  batch 900 loss: 1.5352124953269959\n",
      "LOSS train 1.53521 valid 1.42954, valid PER 46.23%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4949604225158692\n",
      "  batch 100 loss: 1.4676747131347656\n",
      "  batch 150 loss: 1.4578939700126647\n",
      "  batch 200 loss: 1.4793711972236634\n",
      "  batch 250 loss: 1.4336978101730347\n",
      "  batch 300 loss: 1.431298966407776\n",
      "  batch 350 loss: 1.4890352821350097\n",
      "  batch 400 loss: 1.4726216793060303\n",
      "  batch 450 loss: 1.4164917373657226\n",
      "  batch 500 loss: 1.4469507551193237\n",
      "  batch 550 loss: 1.4184608459472656\n",
      "  batch 600 loss: 1.3845890176296234\n",
      "  batch 650 loss: 1.3676670336723327\n",
      "  batch 700 loss: 1.3800991249084473\n",
      "  batch 750 loss: 1.4364898538589477\n",
      "  batch 800 loss: 1.3635784435272216\n",
      "  batch 850 loss: 1.401409330368042\n",
      "  batch 900 loss: 1.3651769161224365\n",
      "LOSS train 1.36518 valid 1.34572, valid PER 42.75%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.3610117483139037\n",
      "  batch 100 loss: 1.342631757259369\n",
      "  batch 150 loss: 1.3039619874954225\n",
      "  batch 200 loss: 1.3486074829101562\n",
      "  batch 250 loss: 1.3520271682739258\n",
      "  batch 300 loss: 1.3593720769882203\n",
      "  batch 350 loss: 1.2880880260467529\n",
      "  batch 400 loss: 1.3201759338378907\n",
      "  batch 450 loss: 1.3108371472358704\n",
      "  batch 500 loss: 1.293563494682312\n",
      "  batch 550 loss: 1.328806482553482\n",
      "  batch 600 loss: 1.3414138102531432\n",
      "  batch 650 loss: 1.3056408548355103\n",
      "  batch 700 loss: 1.2688072681427003\n",
      "  batch 750 loss: 1.2477472019195557\n",
      "  batch 800 loss: 1.2668706941604615\n",
      "  batch 850 loss: 1.2753196215629579\n",
      "  batch 900 loss: 1.3021867060661316\n",
      "LOSS train 1.30219 valid 1.22668, valid PER 39.53%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.251583058834076\n",
      "  batch 100 loss: 1.225789783000946\n",
      "  batch 150 loss: 1.2660524976253509\n",
      "  batch 200 loss: 1.2139723992347717\n",
      "  batch 250 loss: 1.231963336467743\n",
      "  batch 300 loss: 1.2448650240898131\n",
      "  batch 350 loss: 1.2257521092891692\n",
      "  batch 400 loss: 1.2384351563453675\n",
      "  batch 450 loss: 1.2477282881736755\n",
      "  batch 500 loss: 1.2566393625736236\n",
      "  batch 550 loss: 1.2278973698616027\n",
      "  batch 600 loss: 1.2502838897705078\n",
      "  batch 650 loss: 1.2285281240940094\n",
      "  batch 700 loss: 1.2590644943714142\n",
      "  batch 750 loss: 1.1880587005615235\n",
      "  batch 800 loss: 1.2081953978538513\n",
      "  batch 850 loss: 1.2399150729179382\n",
      "  batch 900 loss: 1.2259968781471253\n",
      "LOSS train 1.22600 valid 1.16185, valid PER 37.40%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.2033009767532348\n",
      "  batch 100 loss: 1.1525465941429138\n",
      "  batch 150 loss: 1.1374449169635772\n",
      "  batch 200 loss: 1.1785388696193695\n",
      "  batch 250 loss: 1.2476104271411896\n",
      "  batch 300 loss: 1.2013260626792908\n",
      "  batch 350 loss: 1.1809998524188996\n",
      "  batch 400 loss: 1.166003452539444\n",
      "  batch 450 loss: 1.2023220467567444\n",
      "  batch 500 loss: 1.1845419991016388\n",
      "  batch 550 loss: 1.187671982049942\n",
      "  batch 600 loss: 1.1516777169704437\n",
      "  batch 650 loss: 1.1979266178607941\n",
      "  batch 700 loss: 1.164419493675232\n",
      "  batch 750 loss: 1.129578824043274\n",
      "  batch 800 loss: 1.1617856180667878\n",
      "  batch 850 loss: 1.126171407699585\n",
      "  batch 900 loss: 1.1582214891910554\n",
      "LOSS train 1.15822 valid 1.13814, valid PER 36.82%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.1474453711509705\n",
      "  batch 100 loss: 1.1647718381881713\n",
      "  batch 150 loss: 1.115646893978119\n",
      "  batch 200 loss: 1.0916840052604675\n",
      "  batch 250 loss: 1.110908603668213\n",
      "  batch 300 loss: 1.0987371218204498\n",
      "  batch 350 loss: 1.095116513967514\n",
      "  batch 400 loss: 1.1340553760528564\n",
      "  batch 450 loss: 1.116787415742874\n",
      "  batch 500 loss: 1.1165511965751649\n",
      "  batch 550 loss: 1.110453598499298\n",
      "  batch 600 loss: 1.1290491461753844\n",
      "  batch 650 loss: 1.110720660686493\n",
      "  batch 700 loss: 1.1219917047023773\n",
      "  batch 750 loss: 1.0899673569202424\n",
      "  batch 800 loss: 1.090490471124649\n",
      "  batch 850 loss: 1.1255421018600464\n",
      "  batch 900 loss: 1.1380812275409697\n",
      "LOSS train 1.13808 valid 1.11846, valid PER 36.11%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.1146020424365997\n",
      "  batch 100 loss: 1.127990584373474\n",
      "  batch 150 loss: 1.0826082706451416\n",
      "  batch 200 loss: 1.0667496240139007\n",
      "  batch 250 loss: 1.0883196091651917\n",
      "  batch 300 loss: 1.0166613042354584\n",
      "  batch 350 loss: 1.1116532862186432\n",
      "  batch 400 loss: 1.046207596063614\n",
      "  batch 450 loss: 1.0914048850536346\n",
      "  batch 500 loss: 1.1073163115978242\n",
      "  batch 550 loss: 1.089456113576889\n",
      "  batch 600 loss: 1.0797225213050843\n",
      "  batch 650 loss: 1.0948475360870362\n",
      "  batch 700 loss: 1.0650277268886565\n",
      "  batch 750 loss: 1.052500355243683\n",
      "  batch 800 loss: 1.0841460490226746\n",
      "  batch 850 loss: 1.0588591706752777\n",
      "  batch 900 loss: 1.0368144834041595\n",
      "LOSS train 1.03681 valid 1.07959, valid PER 34.28%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.9945772850513458\n",
      "  batch 100 loss: 1.0511649966239929\n",
      "  batch 150 loss: 1.0408341240882875\n",
      "  batch 200 loss: 0.9940622460842132\n",
      "  batch 250 loss: 1.0649789798259734\n",
      "  batch 300 loss: 1.0565827369689942\n",
      "  batch 350 loss: 1.0730964291095733\n",
      "  batch 400 loss: 1.0441618394851684\n",
      "  batch 450 loss: 1.0468790543079376\n",
      "  batch 500 loss: 0.9977618277072906\n",
      "  batch 550 loss: 1.0767073619365692\n",
      "  batch 600 loss: 1.0821385204792022\n",
      "  batch 650 loss: 1.048648052215576\n",
      "  batch 700 loss: 1.0306076562404634\n",
      "  batch 750 loss: 1.0257806420326232\n",
      "  batch 800 loss: 1.0395456957817077\n",
      "  batch 850 loss: 1.0638897180557252\n",
      "  batch 900 loss: 1.0235932970046997\n",
      "LOSS train 1.02359 valid 1.13050, valid PER 35.72%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 1.0147735476493835\n",
      "  batch 100 loss: 1.0065818560123443\n",
      "  batch 150 loss: 1.0454442155361177\n",
      "  batch 200 loss: 1.0422974121570587\n",
      "  batch 250 loss: 1.0416031086444855\n",
      "  batch 300 loss: 1.0121208548545837\n",
      "  batch 350 loss: 1.0375256264209747\n",
      "  batch 400 loss: 0.9844556093215943\n",
      "  batch 450 loss: 0.992270120382309\n",
      "  batch 500 loss: 1.0269044387340545\n",
      "  batch 550 loss: 1.0600063860416413\n",
      "  batch 600 loss: 1.0293289804458619\n",
      "  batch 650 loss: 1.036422290802002\n",
      "  batch 700 loss: 1.023367269039154\n",
      "  batch 750 loss: 0.9900680708885193\n",
      "  batch 800 loss: 0.9943541836738586\n",
      "  batch 850 loss: 1.009051284790039\n",
      "  batch 900 loss: 1.0375470697879792\n",
      "LOSS train 1.03755 valid 1.03959, valid PER 32.47%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.9830742287635803\n",
      "  batch 100 loss: 0.9320720326900482\n",
      "  batch 150 loss: 0.9443729770183563\n",
      "  batch 200 loss: 1.0231450021266937\n",
      "  batch 250 loss: 0.9994670963287353\n",
      "  batch 300 loss: 0.940737578868866\n",
      "  batch 350 loss: 0.9684587287902832\n",
      "  batch 400 loss: 0.9965348446369171\n",
      "  batch 450 loss: 0.997339836359024\n",
      "  batch 500 loss: 0.9500505447387695\n",
      "  batch 550 loss: 0.9683001959323883\n",
      "  batch 600 loss: 0.9688624596595764\n",
      "  batch 650 loss: 1.0237208104133606\n",
      "  batch 700 loss: 0.9484284424781799\n",
      "  batch 750 loss: 0.9601306629180908\n",
      "  batch 800 loss: 0.9942175245285034\n",
      "  batch 850 loss: 0.9985811269283295\n",
      "  batch 900 loss: 0.9930104565620422\n",
      "LOSS train 0.99301 valid 1.00060, valid PER 31.06%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.9498198068141938\n",
      "  batch 100 loss: 0.936641457080841\n",
      "  batch 150 loss: 0.9399190640449524\n",
      "  batch 200 loss: 0.9594178593158722\n",
      "  batch 250 loss: 0.9951180136203766\n",
      "  batch 300 loss: 0.9472775363922119\n",
      "  batch 350 loss: 0.9686799931526184\n",
      "  batch 400 loss: 1.0005432784557342\n",
      "  batch 450 loss: 0.9734041595458984\n",
      "  batch 500 loss: 0.9713450825214386\n",
      "  batch 550 loss: 0.9154058837890625\n",
      "  batch 600 loss: 0.9329544746875763\n",
      "  batch 650 loss: 0.9767423009872437\n",
      "  batch 700 loss: 0.9301319563388825\n",
      "  batch 750 loss: 0.9255322384834289\n",
      "  batch 800 loss: 0.9205178129673004\n",
      "  batch 850 loss: 0.9726415169239044\n",
      "  batch 900 loss: 0.9527358496189118\n",
      "LOSS train 0.95274 valid 1.00106, valid PER 31.33%\n",
      "EPOCH 13:\n"
     ]
    }
   ],
   "source": [
    "import model_uni_directional_LSTM\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start tuning For uni directional 1 Layer LSTM\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 210,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 210,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_uni_directional_LSTM.BiLSTM(args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For uni directional 1 Layer LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c047abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0.5]\n",
    "Optimiser = [\"Adam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e20caed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning For uni directional 1 Layer LSTM\n",
      "Currently using Adam optimiser\n",
      "Currently using dropout rate of 0.5\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 6.976097302436829\n",
      "  batch 100 loss: 3.2917252731323243\n",
      "  batch 150 loss: 3.2514602851867678\n",
      "  batch 200 loss: 3.1707065677642823\n",
      "  batch 250 loss: 2.978942756652832\n",
      "  batch 300 loss: 2.822084379196167\n",
      "  batch 350 loss: 2.7268634033203125\n",
      "  batch 400 loss: 2.666108479499817\n",
      "  batch 450 loss: 2.5964044094085694\n",
      "  batch 500 loss: 2.478789610862732\n",
      "  batch 550 loss: 2.3846352005004885\n",
      "  batch 600 loss: 2.299918322563171\n",
      "  batch 650 loss: 2.193564465045929\n",
      "  batch 700 loss: 2.166175916194916\n",
      "  batch 750 loss: 2.0769609427452087\n",
      "  batch 800 loss: 2.0440055990219115\n",
      "  batch 850 loss: 1.983420639038086\n",
      "  batch 900 loss: 1.9359222722053528\n",
      "LOSS train 1.93592 valid 1.81685, valid PER 63.62%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.8677888631820678\n",
      "  batch 100 loss: 1.8118211460113525\n",
      "  batch 150 loss: 1.748062677383423\n",
      "  batch 200 loss: 1.7899343729019166\n",
      "  batch 250 loss: 1.771130473613739\n",
      "  batch 300 loss: 1.7528658652305602\n",
      "  batch 350 loss: 1.6640821981430054\n",
      "  batch 400 loss: 1.6513428068161011\n",
      "  batch 450 loss: 1.6077882075309753\n",
      "  batch 500 loss: 1.632379767894745\n",
      "  batch 550 loss: 1.646877019405365\n",
      "  batch 600 loss: 1.5825892162322999\n",
      "  batch 650 loss: 1.6389853596687316\n",
      "  batch 700 loss: 1.5739524674415588\n",
      "  batch 750 loss: 1.5494316220283508\n",
      "  batch 800 loss: 1.5318048596382141\n",
      "  batch 850 loss: 1.5465845251083374\n",
      "  batch 900 loss: 1.5615261793136597\n",
      "LOSS train 1.56153 valid 1.46705, valid PER 47.10%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.515527710914612\n",
      "  batch 100 loss: 1.4757487654685975\n",
      "  batch 150 loss: 1.4887644243240357\n",
      "  batch 200 loss: 1.4756956553459168\n",
      "  batch 250 loss: 1.4361920762062073\n",
      "  batch 300 loss: 1.4468487048149108\n",
      "  batch 350 loss: 1.4652597522735595\n",
      "  batch 400 loss: 1.4415501856803894\n",
      "  batch 450 loss: 1.4093897604942323\n",
      "  batch 500 loss: 1.4442113161087036\n",
      "  batch 550 loss: 1.4097402703762054\n",
      "  batch 600 loss: 1.3971804225444793\n",
      "  batch 650 loss: 1.3823960208892823\n",
      "  batch 700 loss: 1.3888547801971436\n",
      "  batch 750 loss: 1.4490338635444642\n",
      "  batch 800 loss: 1.3779205513000488\n",
      "  batch 850 loss: 1.4312507176399232\n",
      "  batch 900 loss: 1.357920479774475\n",
      "LOSS train 1.35792 valid 1.34306, valid PER 42.31%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.3472322940826416\n",
      "  batch 100 loss: 1.3572760534286499\n",
      "  batch 150 loss: 1.3411309933662414\n",
      "  batch 200 loss: 1.3572796714305877\n",
      "  batch 250 loss: 1.3629311800003052\n",
      "  batch 300 loss: 1.3818773245811462\n",
      "  batch 350 loss: 1.2995769202709198\n",
      "  batch 400 loss: 1.3273859119415283\n",
      "  batch 450 loss: 1.3091418552398681\n",
      "  batch 500 loss: 1.29118870139122\n",
      "  batch 550 loss: 1.3302696919441224\n",
      "  batch 600 loss: 1.3223459553718566\n",
      "  batch 650 loss: 1.315813845396042\n",
      "  batch 700 loss: 1.2874136555194855\n",
      "  batch 750 loss: 1.2732548367977143\n",
      "  batch 800 loss: 1.2567911446094513\n",
      "  batch 850 loss: 1.298610302209854\n",
      "  batch 900 loss: 1.3020861196517943\n",
      "LOSS train 1.30209 valid 1.23886, valid PER 38.88%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.2741322636604309\n",
      "  batch 100 loss: 1.2443821489810944\n",
      "  batch 150 loss: 1.2852614426612854\n",
      "  batch 200 loss: 1.2218568110466004\n",
      "  batch 250 loss: 1.2251765131950378\n",
      "  batch 300 loss: 1.2382044911384582\n",
      "  batch 350 loss: 1.2479070806503296\n",
      "  batch 400 loss: 1.240313663482666\n",
      "  batch 450 loss: 1.2429624319076538\n",
      "  batch 500 loss: 1.257066583633423\n",
      "  batch 550 loss: 1.1984863805770873\n",
      "  batch 600 loss: 1.249195386171341\n",
      "  batch 650 loss: 1.2233451437950134\n",
      "  batch 700 loss: 1.2614135217666627\n",
      "  batch 750 loss: 1.208948665857315\n",
      "  batch 800 loss: 1.2307079887390138\n",
      "  batch 850 loss: 1.2201057267189026\n",
      "  batch 900 loss: 1.2625871431827544\n",
      "LOSS train 1.26259 valid 1.19669, valid PER 37.81%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.2382530331611634\n",
      "  batch 100 loss: 1.1876309859752654\n",
      "  batch 150 loss: 1.1533557510375976\n",
      "  batch 200 loss: 1.2083887255191803\n",
      "  batch 250 loss: 1.201445106267929\n",
      "  batch 300 loss: 1.163555895090103\n",
      "  batch 350 loss: 1.2007287383079528\n",
      "  batch 400 loss: 1.1624246919155121\n",
      "  batch 450 loss: 1.2014187788963318\n",
      "  batch 500 loss: 1.1754963839054107\n",
      "  batch 550 loss: 1.1789716744422913\n",
      "  batch 600 loss: 1.1805855059623718\n",
      "  batch 650 loss: 1.1809958136081695\n",
      "  batch 700 loss: 1.1831027925014497\n",
      "  batch 750 loss: 1.1609614264965058\n",
      "  batch 800 loss: 1.166710366010666\n",
      "  batch 850 loss: 1.1454972755908965\n",
      "  batch 900 loss: 1.155743064880371\n",
      "LOSS train 1.15574 valid 1.13949, valid PER 36.03%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.1437329757213592\n",
      "  batch 100 loss: 1.1451666843891144\n",
      "  batch 150 loss: 1.130730812549591\n",
      "  batch 200 loss: 1.1240806579589844\n",
      "  batch 250 loss: 1.1309780776500702\n",
      "  batch 300 loss: 1.1030036389827729\n",
      "  batch 350 loss: 1.1151619732379914\n",
      "  batch 400 loss: 1.1280630564689635\n",
      "  batch 450 loss: 1.1128546786308289\n",
      "  batch 500 loss: 1.1189934456348418\n",
      "  batch 550 loss: 1.1060125708580018\n",
      "  batch 600 loss: 1.1329361248016356\n",
      "  batch 650 loss: 1.1243888175487518\n",
      "  batch 700 loss: 1.1201872646808624\n",
      "  batch 750 loss: 1.0924043202400207\n",
      "  batch 800 loss: 1.0946261477470398\n",
      "  batch 850 loss: 1.1357271254062653\n",
      "  batch 900 loss: 1.1347499513626098\n",
      "LOSS train 1.13475 valid 1.10956, valid PER 35.51%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.091131615638733\n",
      "  batch 100 loss: 1.0819217085838317\n",
      "  batch 150 loss: 1.0809216392040253\n",
      "  batch 200 loss: 1.0582150185108186\n",
      "  batch 250 loss: 1.092231150865555\n",
      "  batch 300 loss: 1.0206370043754578\n",
      "  batch 350 loss: 1.1036979448795319\n",
      "  batch 400 loss: 1.0437575769424439\n",
      "  batch 450 loss: 1.096699674129486\n",
      "  batch 500 loss: 1.1463280773162843\n",
      "  batch 550 loss: 1.0786470210552215\n",
      "  batch 600 loss: 1.0897361934185028\n",
      "  batch 650 loss: 1.1261630058288574\n",
      "  batch 700 loss: 1.0750850546360016\n",
      "  batch 750 loss: 1.0761649441719054\n",
      "  batch 800 loss: 1.0679794013500215\n",
      "  batch 850 loss: 1.0632795655727387\n",
      "  batch 900 loss: 1.0511380302906037\n",
      "LOSS train 1.05114 valid 1.07148, valid PER 34.24%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.9945454370975494\n",
      "  batch 100 loss: 1.0702479565143586\n",
      "  batch 150 loss: 1.0659668600559236\n",
      "  batch 200 loss: 1.0138457250595092\n",
      "  batch 250 loss: 1.0513701987266542\n",
      "  batch 300 loss: 1.0515916645526886\n",
      "  batch 350 loss: 1.0530237925052643\n",
      "  batch 400 loss: 1.0957769131660462\n",
      "  batch 450 loss: 1.1221085953712464\n",
      "  batch 500 loss: 1.02257639169693\n",
      "  batch 550 loss: 1.0689606618881227\n",
      "  batch 600 loss: 1.0682254374027251\n",
      "  batch 650 loss: 1.0898247349262238\n",
      "  batch 700 loss: 1.0287437617778779\n",
      "  batch 750 loss: 1.0415955138206483\n",
      "  batch 800 loss: 1.0723361957073212\n",
      "  batch 850 loss: 1.079797545671463\n",
      "  batch 900 loss: 1.0122557711601257\n",
      "LOSS train 1.01226 valid 1.05386, valid PER 32.98%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.9983521020412445\n",
      "  batch 100 loss: 0.9908041965961456\n",
      "  batch 150 loss: 1.015771210193634\n",
      "  batch 200 loss: 1.0253052449226379\n",
      "  batch 250 loss: 1.0262523531913756\n",
      "  batch 300 loss: 0.987135306596756\n",
      "  batch 350 loss: 1.0466125094890595\n",
      "  batch 400 loss: 0.9940100240707398\n",
      "  batch 450 loss: 0.9667124164104461\n",
      "  batch 500 loss: 1.0022614812850952\n",
      "  batch 550 loss: 1.0122825348377227\n",
      "  batch 600 loss: 1.0105278015136718\n",
      "  batch 650 loss: 1.0122927796840668\n",
      "  batch 700 loss: 1.0108158743381501\n",
      "  batch 750 loss: 1.003798178434372\n",
      "  batch 800 loss: 1.018593018054962\n",
      "  batch 850 loss: 1.0320136272907257\n",
      "  batch 900 loss: 1.0318816685676575\n",
      "LOSS train 1.03188 valid 1.04789, valid PER 33.21%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.9423173022270203\n",
      "  batch 100 loss: 0.9122829222679139\n",
      "  batch 150 loss: 0.947845755815506\n",
      "  batch 200 loss: 1.004490373134613\n",
      "  batch 250 loss: 0.9871337962150574\n",
      "  batch 300 loss: 0.9739933907985687\n",
      "  batch 350 loss: 0.9638329696655273\n",
      "  batch 400 loss: 0.9821485900878906\n",
      "  batch 450 loss: 0.9707600617408753\n",
      "  batch 500 loss: 0.9517929041385651\n",
      "  batch 550 loss: 0.9715345442295075\n",
      "  batch 600 loss: 0.9377876985073089\n",
      "  batch 650 loss: 1.026853038072586\n",
      "  batch 700 loss: 0.9718006801605225\n",
      "  batch 750 loss: 0.9645847463607788\n",
      "  batch 800 loss: 0.9875024545192719\n",
      "  batch 850 loss: 1.0465424036979676\n",
      "  batch 900 loss: 1.0289578974246978\n",
      "LOSS train 1.02896 valid 1.01775, valid PER 32.86%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.9429871833324432\n",
      "  batch 100 loss: 0.9277903246879577\n",
      "  batch 150 loss: 0.9155270552635193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.9350296974182128\n",
      "  batch 250 loss: 0.9640889620780945\n",
      "  batch 300 loss: 0.9281706213951111\n",
      "  batch 350 loss: 0.9329260003566742\n",
      "  batch 400 loss: 0.9653176987171173\n",
      "  batch 450 loss: 0.9725274789333344\n",
      "  batch 500 loss: 0.9895003342628479\n",
      "  batch 550 loss: 0.9076323556900024\n",
      "  batch 600 loss: 0.9635228085517883\n",
      "  batch 650 loss: 0.9959461188316345\n",
      "  batch 700 loss: 0.964137521982193\n",
      "  batch 750 loss: 0.94458944439888\n",
      "  batch 800 loss: 0.9345377123355866\n",
      "  batch 850 loss: 0.9751678979396821\n",
      "  batch 900 loss: 0.9743003284931183\n",
      "LOSS train 0.97430 valid 0.97837, valid PER 31.46%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.9073807609081268\n",
      "  batch 100 loss: 0.9450461864471436\n",
      "  batch 150 loss: 0.8791310358047485\n",
      "  batch 200 loss: 0.9613366317749024\n",
      "  batch 250 loss: 0.9270772182941437\n",
      "  batch 300 loss: 0.8997859239578248\n",
      "  batch 350 loss: 0.9119932532310486\n",
      "  batch 400 loss: 0.9332650446891785\n",
      "  batch 450 loss: 0.941560308933258\n",
      "  batch 500 loss: 0.8999531078338623\n",
      "  batch 550 loss: 0.921404881477356\n",
      "  batch 600 loss: 0.9628321957588196\n",
      "  batch 650 loss: 0.9304552590847015\n",
      "  batch 700 loss: 0.9372209668159485\n",
      "  batch 750 loss: 0.8860179579257965\n",
      "  batch 800 loss: 0.9309745275974274\n",
      "  batch 850 loss: 0.9577683818340301\n",
      "  batch 900 loss: 0.9539519274234771\n",
      "LOSS train 0.95395 valid 0.97741, valid PER 31.05%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.8872079479694367\n",
      "  batch 100 loss: 0.934542704820633\n",
      "  batch 150 loss: 0.894868597984314\n",
      "  batch 200 loss: 0.8713136029243469\n",
      "  batch 250 loss: 0.9010243713855743\n",
      "  batch 300 loss: 0.9162192225456238\n",
      "  batch 350 loss: 0.8607121419906616\n",
      "  batch 400 loss: 0.8684240698814392\n",
      "  batch 450 loss: 0.8860156810283661\n",
      "  batch 500 loss: 0.9126138305664062\n",
      "  batch 550 loss: 0.9196481096744538\n",
      "  batch 600 loss: 0.8738668632507324\n",
      "  batch 650 loss: 0.9183975565433502\n",
      "  batch 700 loss: 0.914311032295227\n",
      "  batch 750 loss: 0.8726645970344543\n",
      "  batch 800 loss: 0.8522439920902252\n",
      "  batch 850 loss: 0.9126096391677856\n",
      "  batch 900 loss: 0.8979903078079223\n",
      "LOSS train 0.89799 valid 0.97470, valid PER 30.48%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.8756698274612427\n",
      "  batch 100 loss: 0.86515105843544\n",
      "  batch 150 loss: 0.8494816040992736\n",
      "  batch 200 loss: 0.8891060864925384\n",
      "  batch 250 loss: 0.8833649790287018\n",
      "  batch 300 loss: 0.8658673536777496\n",
      "  batch 350 loss: 0.8655293440818786\n",
      "  batch 400 loss: 0.8579908347129822\n",
      "  batch 450 loss: 0.8763878726959229\n",
      "  batch 500 loss: 0.8410394859313964\n",
      "  batch 550 loss: 0.8622017586231232\n",
      "  batch 600 loss: 0.8812835168838501\n",
      "  batch 650 loss: 0.8848862540721893\n",
      "  batch 700 loss: 0.8891040980815887\n",
      "  batch 750 loss: 0.8847701835632324\n",
      "  batch 800 loss: 0.8554714906215668\n",
      "  batch 850 loss: 0.8466682481765747\n",
      "  batch 900 loss: 0.8775648903846741\n",
      "LOSS train 0.87756 valid 0.95417, valid PER 29.96%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.8462629103660584\n",
      "  batch 100 loss: 0.805233187675476\n",
      "  batch 150 loss: 0.8438670742511749\n",
      "  batch 200 loss: 0.8429134130477905\n",
      "  batch 250 loss: 0.883460054397583\n",
      "  batch 300 loss: 0.868841038942337\n",
      "  batch 350 loss: 0.8737798285484314\n",
      "  batch 400 loss: 0.8500110363960266\n",
      "  batch 450 loss: 0.8647713112831116\n",
      "  batch 500 loss: 0.8082241594791413\n",
      "  batch 550 loss: 0.8428880167007446\n",
      "  batch 600 loss: 0.8494988715648651\n",
      "  batch 650 loss: 0.8542252027988434\n",
      "  batch 700 loss: 0.8320290434360504\n",
      "  batch 750 loss: 0.851535587310791\n",
      "  batch 800 loss: 0.876727546453476\n",
      "  batch 850 loss: 0.833482072353363\n",
      "  batch 900 loss: 0.8516696393489838\n",
      "LOSS train 0.85167 valid 0.96896, valid PER 29.56%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.8567324018478394\n",
      "  batch 100 loss: 0.8308352190256119\n",
      "  batch 150 loss: 0.8192978894710541\n",
      "  batch 200 loss: 0.8159986078739166\n",
      "  batch 250 loss: 0.8245965123176575\n",
      "  batch 300 loss: 0.8368162512779236\n",
      "  batch 350 loss: 0.7911947989463806\n",
      "  batch 400 loss: 0.8624034333229065\n",
      "  batch 450 loss: 0.8318528044223785\n",
      "  batch 500 loss: 0.8205486977100372\n",
      "  batch 550 loss: 0.8299454498291016\n",
      "  batch 600 loss: 0.8774590194225311\n",
      "  batch 650 loss: 0.8437589704990387\n",
      "  batch 700 loss: 0.8454213583469391\n",
      "  batch 750 loss: 0.7992720937728882\n",
      "  batch 800 loss: 0.8228243839740753\n",
      "  batch 850 loss: 0.8339933860301971\n",
      "  batch 900 loss: 0.8237012898921967\n",
      "LOSS train 0.82370 valid 0.94185, valid PER 28.90%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.7953919470310211\n",
      "  batch 100 loss: 0.8163937556743622\n",
      "  batch 150 loss: 0.8559252953529358\n",
      "  batch 200 loss: 0.8044963908195496\n",
      "  batch 250 loss: 0.8201318860054017\n",
      "  batch 300 loss: 0.8277200973033905\n",
      "  batch 350 loss: 0.8153691208362579\n",
      "  batch 400 loss: 0.8219523525238037\n",
      "  batch 450 loss: 0.8552569913864135\n",
      "  batch 500 loss: 0.8551892101764679\n",
      "  batch 550 loss: 0.822449916601181\n",
      "  batch 600 loss: 0.79836008310318\n",
      "  batch 650 loss: 0.8052261984348297\n",
      "  batch 700 loss: 0.8252865624427795\n",
      "  batch 750 loss: 0.8343600440025329\n",
      "  batch 800 loss: 0.7810973811149597\n",
      "  batch 850 loss: 0.7943243682384491\n",
      "  batch 900 loss: 0.8702149844169617\n",
      "LOSS train 0.87021 valid 0.92656, valid PER 28.53%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.7463785338401795\n",
      "  batch 100 loss: 0.764806696176529\n",
      "  batch 150 loss: 0.8012109112739563\n",
      "  batch 200 loss: 0.7985199463367462\n",
      "  batch 250 loss: 0.8027138364315033\n",
      "  batch 300 loss: 0.7860262060165405\n",
      "  batch 350 loss: 0.8045800960063935\n",
      "  batch 400 loss: 0.8002699530124664\n",
      "  batch 450 loss: 0.8081883907318115\n",
      "  batch 500 loss: 0.8364795219898223\n",
      "  batch 550 loss: 0.8035110807418824\n",
      "  batch 600 loss: 0.8123721945285797\n",
      "  batch 650 loss: 0.8659890854358673\n",
      "  batch 700 loss: 0.76668871819973\n",
      "  batch 750 loss: 0.7920688307285308\n",
      "  batch 800 loss: 0.8405150651931763\n",
      "  batch 850 loss: 0.8111725795269012\n",
      "  batch 900 loss: 0.8150597071647644\n",
      "LOSS train 0.81506 valid 0.93174, valid PER 28.81%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.7644685685634613\n",
      "  batch 100 loss: 0.7740287232398987\n",
      "  batch 150 loss: 0.7634678888320923\n",
      "  batch 200 loss: 0.8243830680847168\n",
      "  batch 250 loss: 0.7747256064414978\n",
      "  batch 300 loss: 0.7882336688041687\n",
      "  batch 350 loss: 0.7697749829292297\n",
      "  batch 400 loss: 0.7806924027204514\n",
      "  batch 450 loss: 0.8008234751224518\n",
      "  batch 500 loss: 0.758109039068222\n",
      "  batch 550 loss: 0.8005798888206482\n",
      "  batch 600 loss: 0.7469605815410614\n",
      "  batch 650 loss: 0.7843193483352661\n",
      "  batch 700 loss: 0.7708333039283752\n",
      "  batch 750 loss: 0.7688915783166885\n",
      "  batch 800 loss: 0.792230134010315\n",
      "  batch 850 loss: 0.7874098193645477\n",
      "  batch 900 loss: 0.7948142611980438\n",
      "LOSS train 0.79481 valid 0.91128, valid PER 27.88%\n",
      "Training finished in 10.0 minutes.\n",
      "Model saved to checkpoints/20231209_192234/model_20\n",
      "Finish Adam optimiser\n",
      "End tuning For uni directional 1 Layer LSTM\n"
     ]
    }
   ],
   "source": [
    "import model_uni_directional_LSTM\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start tuning For uni directional 1 Layer LSTM\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 210,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 210,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_uni_directional_LSTM.BiLSTM(args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For uni directional 1 Layer LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57201ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "Optimiser = [\"SGD_Scheduler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe66269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning For uni directional 1 Layer LSTM\n",
      "Currently using SGD_Scheduler optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.239828119277954\n",
      "  batch 100 loss: 3.409066710472107\n",
      "  batch 150 loss: 3.30587975025177\n",
      "  batch 200 loss: 3.249364848136902\n",
      "  batch 250 loss: 3.191646680831909\n",
      "  batch 300 loss: 3.0570363903045656\n",
      "  batch 350 loss: 2.901274256706238\n",
      "  batch 400 loss: 2.745630340576172\n",
      "  batch 450 loss: 2.66358802318573\n",
      "  batch 500 loss: 2.528423981666565\n",
      "  batch 550 loss: 2.431274275779724\n",
      "  batch 600 loss: 2.360090832710266\n",
      "  batch 650 loss: 2.2537232780456544\n",
      "  batch 700 loss: 2.2244571018218995\n",
      "  batch 750 loss: 2.1404535603523254\n",
      "  batch 800 loss: 2.1029496955871583\n",
      "  batch 850 loss: 2.0288353419303893\n",
      "  batch 900 loss: 1.9828188681602479\n",
      "LOSS train 1.98282 valid 1.91523, valid PER 70.89%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.9027397084236144\n",
      "  batch 100 loss: 1.8303321027755737\n",
      "  batch 150 loss: 1.7563483500480652\n",
      "  batch 200 loss: 1.7785113263130188\n",
      "  batch 250 loss: 1.755724813938141\n",
      "  batch 300 loss: 1.7061360478401184\n",
      "  batch 350 loss: 1.603251214027405\n",
      "  batch 400 loss: 1.6054558229446412\n",
      "  batch 450 loss: 1.544611268043518\n",
      "  batch 500 loss: 1.557103374004364\n",
      "  batch 550 loss: 1.573563849925995\n",
      "  batch 600 loss: 1.5053742504119874\n",
      "  batch 650 loss: 1.5543523001670838\n",
      "  batch 700 loss: 1.4803789114952088\n",
      "  batch 750 loss: 1.4462475991249084\n",
      "  batch 800 loss: 1.411460084915161\n",
      "  batch 850 loss: 1.3933128118515015\n",
      "  batch 900 loss: 1.4102815866470337\n",
      "LOSS train 1.41028 valid 1.39149, valid PER 44.21%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.3684061455726624\n",
      "  batch 100 loss: 1.3535618901252746\n",
      "  batch 150 loss: 1.3389731526374817\n",
      "  batch 200 loss: 1.339848837852478\n",
      "  batch 250 loss: 1.3327360916137696\n",
      "  batch 300 loss: 1.3227224850654602\n",
      "  batch 350 loss: 1.3526763272285462\n",
      "  batch 400 loss: 1.3147708070278168\n",
      "  batch 450 loss: 1.2798287439346314\n",
      "  batch 500 loss: 1.3000550007820129\n",
      "  batch 550 loss: 1.2732956182956696\n",
      "  batch 600 loss: 1.2738957691192627\n",
      "  batch 650 loss: 1.2151175391674043\n",
      "  batch 700 loss: 1.253869765996933\n",
      "  batch 750 loss: 1.2987085723876952\n",
      "  batch 800 loss: 1.2255431878566743\n",
      "  batch 850 loss: 1.2555584228038787\n",
      "  batch 900 loss: 1.2137484705448152\n",
      "LOSS train 1.21375 valid 1.32483, valid PER 40.33%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.1941548812389373\n",
      "  batch 100 loss: 1.2130785524845122\n",
      "  batch 150 loss: 1.1694297146797181\n",
      "  batch 200 loss: 1.1653110480308533\n",
      "  batch 250 loss: 1.2146375179290771\n",
      "  batch 300 loss: 1.2170017564296722\n",
      "  batch 350 loss: 1.1375383484363555\n",
      "  batch 400 loss: 1.1697754275798797\n",
      "  batch 450 loss: 1.1702902626991272\n",
      "  batch 500 loss: 1.144199583530426\n",
      "  batch 550 loss: 1.162099916934967\n",
      "  batch 600 loss: 1.2018970894813537\n",
      "  batch 650 loss: 1.1803014814853667\n",
      "  batch 700 loss: 1.1171356415748597\n",
      "  batch 750 loss: 1.1110323548316956\n",
      "  batch 800 loss: 1.0921315324306489\n",
      "  batch 850 loss: 1.1246341419219972\n",
      "  batch 900 loss: 1.175079960823059\n",
      "LOSS train 1.17508 valid 1.18144, valid PER 35.37%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.0747027266025544\n",
      "  batch 100 loss: 1.0651078414916992\n",
      "  batch 150 loss: 1.1059597539901733\n",
      "  batch 200 loss: 1.0671797227859496\n",
      "  batch 250 loss: 1.0546436274051667\n",
      "  batch 300 loss: 1.0696679544448853\n",
      "  batch 350 loss: 1.0824176836013795\n",
      "  batch 400 loss: 1.1118460559844972\n",
      "  batch 450 loss: 1.0689314448833465\n",
      "  batch 500 loss: 1.0717270731925965\n",
      "  batch 550 loss: 1.050272959470749\n",
      "  batch 600 loss: 1.114691435098648\n",
      "  batch 650 loss: 1.0560192370414734\n",
      "  batch 700 loss: 1.1186010241508484\n",
      "  batch 750 loss: 1.0421731269359589\n",
      "  batch 800 loss: 1.0748108208179474\n",
      "  batch 850 loss: 1.0496756982803346\n",
      "  batch 900 loss: 1.0651129651069642\n",
      "LOSS train 1.06511 valid 1.09335, valid PER 33.61%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.0552891540527343\n",
      "  batch 100 loss: 1.0011209404468537\n",
      "  batch 150 loss: 0.9981720995903015\n",
      "  batch 200 loss: 1.0151646542549133\n",
      "  batch 250 loss: 1.0384030377864837\n",
      "  batch 300 loss: 1.0237357711791992\n",
      "  batch 350 loss: 1.0087206912040712\n",
      "  batch 400 loss: 1.0006133246421813\n",
      "  batch 450 loss: 1.0636516988277436\n",
      "  batch 500 loss: 0.9992968225479126\n",
      "  batch 550 loss: 1.0370673894882203\n",
      "  batch 600 loss: 1.0123120415210725\n",
      "  batch 650 loss: 1.0194060456752778\n",
      "  batch 700 loss: 1.0219361257553101\n",
      "  batch 750 loss: 0.9883065271377564\n",
      "  batch 800 loss: 0.9940120673179627\n",
      "  batch 850 loss: 0.9756065058708191\n",
      "  batch 900 loss: 0.9816468179225921\n",
      "LOSS train 0.98165 valid 1.04834, valid PER 33.11%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.9651545977592468\n",
      "  batch 100 loss: 1.01008868932724\n",
      "  batch 150 loss: 0.9675522685050965\n",
      "  batch 200 loss: 0.9723746061325074\n",
      "  batch 250 loss: 0.9734094643592834\n",
      "  batch 300 loss: 0.9264846479892731\n",
      "  batch 350 loss: 0.9567459094524383\n",
      "  batch 400 loss: 0.9713945066928864\n",
      "  batch 450 loss: 0.9627650511264801\n",
      "  batch 500 loss: 0.9535596907138825\n",
      "  batch 550 loss: 0.9263715493679047\n",
      "  batch 600 loss: 0.9738093912601471\n",
      "  batch 650 loss: 0.9552232205867768\n",
      "  batch 700 loss: 0.958254736661911\n",
      "  batch 750 loss: 0.9373604369163513\n",
      "  batch 800 loss: 0.94296524643898\n",
      "  batch 850 loss: 0.9659375572204589\n",
      "  batch 900 loss: 0.9935721552371979\n",
      "LOSS train 0.99357 valid 0.98897, valid PER 31.06%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.9268206858634949\n",
      "  batch 100 loss: 0.896395492553711\n",
      "  batch 150 loss: 0.8931644594669342\n",
      "  batch 200 loss: 0.9220879566669464\n",
      "  batch 250 loss: 0.8910364377498626\n",
      "  batch 300 loss: 0.8868682873249054\n",
      "  batch 350 loss: 0.9652230250835419\n",
      "  batch 400 loss: 0.9177316415309906\n",
      "  batch 450 loss: 0.9641478323936462\n",
      "  batch 500 loss: 0.9595586311817169\n",
      "  batch 550 loss: 0.8987628543376922\n",
      "  batch 600 loss: 0.9297498154640198\n",
      "  batch 650 loss: 0.9480630266666412\n",
      "  batch 700 loss: 0.9300563073158264\n",
      "  batch 750 loss: 0.9506191730499267\n",
      "  batch 800 loss: 0.9366594123840332\n",
      "  batch 850 loss: 0.9216694045066833\n",
      "  batch 900 loss: 0.8884355628490448\n",
      "LOSS train 0.88844 valid 0.97703, valid PER 30.41%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.8318603217601777\n",
      "  batch 100 loss: 0.884087301492691\n",
      "  batch 150 loss: 0.8861563038825989\n",
      "  batch 200 loss: 0.8524282723665237\n",
      "  batch 250 loss: 0.8862279808521271\n",
      "  batch 300 loss: 0.8823886466026306\n",
      "  batch 350 loss: 0.8911975717544556\n",
      "  batch 400 loss: 0.8986594951152802\n",
      "  batch 450 loss: 0.8804992759227752\n",
      "  batch 500 loss: 0.8421366894245148\n",
      "  batch 550 loss: 0.9122195827960968\n",
      "  batch 600 loss: 0.8983194148540496\n",
      "  batch 650 loss: 0.885439110994339\n",
      "  batch 700 loss: 0.8876114606857299\n",
      "  batch 750 loss: 0.893166309595108\n",
      "  batch 800 loss: 0.9093559646606445\n",
      "  batch 850 loss: 0.9526266229152679\n",
      "  batch 900 loss: 0.885286191701889\n",
      "Epoch 00009: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.88529 valid 1.01267, valid PER 31.26%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.8124945378303527\n",
      "  batch 100 loss: 0.8055037033557891\n",
      "  batch 150 loss: 0.8226989567279815\n",
      "  batch 200 loss: 0.8143446886539459\n",
      "  batch 250 loss: 0.7975250339508057\n",
      "  batch 300 loss: 0.7513473224639893\n",
      "  batch 350 loss: 0.7859917092323303\n",
      "  batch 400 loss: 0.7646525347232819\n",
      "  batch 450 loss: 0.7536269772052765\n",
      "  batch 500 loss: 0.784136306643486\n",
      "  batch 550 loss: 0.7966057121753692\n",
      "  batch 600 loss: 0.7708533948659897\n",
      "  batch 650 loss: 0.7848869943618775\n",
      "  batch 700 loss: 0.7863087785243988\n",
      "  batch 750 loss: 0.7577380228042603\n",
      "  batch 800 loss: 0.7887596845626831\n",
      "  batch 850 loss: 0.7906540048122406\n",
      "  batch 900 loss: 0.8043456828594208\n",
      "LOSS train 0.80435 valid 0.88951, valid PER 28.32%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.7270999163389206\n",
      "  batch 100 loss: 0.6926797235012054\n",
      "  batch 150 loss: 0.7257217502593994\n",
      "  batch 200 loss: 0.7703187125921249\n",
      "  batch 250 loss: 0.7577253216505051\n",
      "  batch 300 loss: 0.7159390342235565\n",
      "  batch 350 loss: 0.7521571719646454\n",
      "  batch 400 loss: 0.7413135123252869\n",
      "  batch 450 loss: 0.7388005501031876\n",
      "  batch 500 loss: 0.7291910076141357\n",
      "  batch 550 loss: 0.7398443299531937\n",
      "  batch 600 loss: 0.7336276149749756\n",
      "  batch 650 loss: 0.7817890810966491\n",
      "  batch 700 loss: 0.7245823317766189\n",
      "  batch 750 loss: 0.7343057179450989\n",
      "  batch 800 loss: 0.7722058391571045\n",
      "  batch 850 loss: 0.7815562927722931\n",
      "  batch 900 loss: 0.7787305158376694\n",
      "LOSS train 0.77873 valid 0.87387, valid PER 27.45%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.715120176076889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 0.7130303448438644\n",
      "  batch 150 loss: 0.6817633861303329\n",
      "  batch 200 loss: 0.721118580698967\n",
      "  batch 250 loss: 0.7276902294158936\n",
      "  batch 300 loss: 0.6977686738967895\n",
      "  batch 350 loss: 0.7144143289327621\n",
      "  batch 400 loss: 0.7453233313560486\n",
      "  batch 450 loss: 0.7283589160442352\n",
      "  batch 500 loss: 0.7314473873376847\n",
      "  batch 550 loss: 0.6786418908834457\n",
      "  batch 600 loss: 0.6990543532371521\n",
      "  batch 650 loss: 0.7377239680290222\n",
      "  batch 700 loss: 0.7179063057899475\n",
      "  batch 750 loss: 0.7159662920236588\n",
      "  batch 800 loss: 0.7024925684928894\n",
      "  batch 850 loss: 0.7389034390449524\n",
      "  batch 900 loss: 0.7483813053369522\n",
      "LOSS train 0.74838 valid 0.86546, valid PER 27.07%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.6643338096141815\n",
      "  batch 100 loss: 0.7000603151321411\n",
      "  batch 150 loss: 0.675392650961876\n",
      "  batch 200 loss: 0.6954650175571442\n",
      "  batch 250 loss: 0.7076703292131424\n",
      "  batch 300 loss: 0.6943372750282287\n",
      "  batch 350 loss: 0.6767704182863236\n",
      "  batch 400 loss: 0.7219039410352707\n",
      "  batch 450 loss: 0.730391258597374\n",
      "  batch 500 loss: 0.6896331959962845\n",
      "  batch 550 loss: 0.7129147136211396\n",
      "  batch 600 loss: 0.6805643779039383\n",
      "  batch 650 loss: 0.7114720451831817\n",
      "  batch 700 loss: 0.7118539494276047\n",
      "  batch 750 loss: 0.6692273885011673\n",
      "  batch 800 loss: 0.6890223741531372\n",
      "  batch 850 loss: 0.736308541893959\n",
      "  batch 900 loss: 0.7267662835121155\n",
      "LOSS train 0.72677 valid 0.86401, valid PER 26.93%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.6618132549524307\n",
      "  batch 100 loss: 0.6810091584920883\n",
      "  batch 150 loss: 0.6595216435194016\n",
      "  batch 200 loss: 0.6549625205993652\n",
      "  batch 250 loss: 0.6685427463054657\n",
      "  batch 300 loss: 0.6879703330993653\n",
      "  batch 350 loss: 0.6632941973209381\n",
      "  batch 400 loss: 0.6709993535280228\n",
      "  batch 450 loss: 0.6669993484020234\n",
      "  batch 500 loss: 0.6981227684020996\n",
      "  batch 550 loss: 0.701979369521141\n",
      "  batch 600 loss: 0.664098596572876\n",
      "  batch 650 loss: 0.6828959840536117\n",
      "  batch 700 loss: 0.6981265449523926\n",
      "  batch 750 loss: 0.660691664814949\n",
      "  batch 800 loss: 0.6500680017471313\n",
      "  batch 850 loss: 0.6968067520856858\n",
      "  batch 900 loss: 0.6734546166658402\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.67345 valid 0.86842, valid PER 27.28%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6479135382175446\n",
      "  batch 100 loss: 0.5975705766677857\n",
      "  batch 150 loss: 0.6177643191814423\n",
      "  batch 200 loss: 0.6271097850799561\n",
      "  batch 250 loss: 0.6333444714546204\n",
      "  batch 300 loss: 0.6149780929088593\n",
      "  batch 350 loss: 0.6075989121198654\n",
      "  batch 400 loss: 0.611786944270134\n",
      "  batch 450 loss: 0.610185233950615\n",
      "  batch 500 loss: 0.5809189283847809\n",
      "  batch 550 loss: 0.6184980994462967\n",
      "  batch 600 loss: 0.6367400020360947\n",
      "  batch 650 loss: 0.6306958544254303\n",
      "  batch 700 loss: 0.6386390906572342\n",
      "  batch 750 loss: 0.6100491791963577\n",
      "  batch 800 loss: 0.6051426315307618\n",
      "  batch 850 loss: 0.5932064068317413\n",
      "  batch 900 loss: 0.6230041217803955\n",
      "LOSS train 0.62300 valid 0.83167, valid PER 25.96%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.6155494630336762\n",
      "  batch 100 loss: 0.571412296295166\n",
      "  batch 150 loss: 0.586422301530838\n",
      "  batch 200 loss: 0.5837630665302277\n",
      "  batch 250 loss: 0.6086962383985519\n",
      "  batch 300 loss: 0.5906953841447831\n",
      "  batch 350 loss: 0.5995950603485107\n",
      "  batch 400 loss: 0.6017578065395355\n",
      "  batch 450 loss: 0.6116943091154099\n",
      "  batch 500 loss: 0.5727535063028335\n",
      "  batch 550 loss: 0.5766775584220887\n",
      "  batch 600 loss: 0.5931682693958282\n",
      "  batch 650 loss: 0.6048849123716354\n",
      "  batch 700 loss: 0.5812893015146255\n",
      "  batch 750 loss: 0.6114340281486511\n",
      "  batch 800 loss: 0.5880699914693832\n",
      "  batch 850 loss: 0.584814972281456\n",
      "  batch 900 loss: 0.6036447042226791\n",
      "LOSS train 0.60364 valid 0.82980, valid PER 25.70%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.5736091703176498\n",
      "  batch 100 loss: 0.571659438610077\n",
      "  batch 150 loss: 0.5662393426895141\n",
      "  batch 200 loss: 0.5646408724784852\n",
      "  batch 250 loss: 0.5710114121437073\n",
      "  batch 300 loss: 0.5740541297197342\n",
      "  batch 350 loss: 0.5647015845775605\n",
      "  batch 400 loss: 0.6219773125648499\n",
      "  batch 450 loss: 0.5918455243110656\n",
      "  batch 500 loss: 0.5763207924365997\n",
      "  batch 550 loss: 0.577925934791565\n",
      "  batch 600 loss: 0.6077850556373596\n",
      "  batch 650 loss: 0.5726189237833023\n",
      "  batch 700 loss: 0.5763638418912888\n",
      "  batch 750 loss: 0.5680963015556335\n",
      "  batch 800 loss: 0.5448379987478256\n",
      "  batch 850 loss: 0.5948243188858032\n",
      "  batch 900 loss: 0.5624313569068908\n",
      "LOSS train 0.56243 valid 0.82628, valid PER 25.56%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.5610733389854431\n",
      "  batch 100 loss: 0.5645517474412918\n",
      "  batch 150 loss: 0.5779447132349014\n",
      "  batch 200 loss: 0.5563725632429123\n",
      "  batch 250 loss: 0.5736808538436889\n",
      "  batch 300 loss: 0.5552841919660568\n",
      "  batch 350 loss: 0.5569614207744599\n",
      "  batch 400 loss: 0.5453054487705231\n",
      "  batch 450 loss: 0.5893133074045181\n",
      "  batch 500 loss: 0.5651579505205154\n",
      "  batch 550 loss: 0.5668991613388061\n",
      "  batch 600 loss: 0.5442318713665009\n",
      "  batch 650 loss: 0.5541839921474456\n",
      "  batch 700 loss: 0.5970878863334655\n",
      "  batch 750 loss: 0.5709063380956649\n",
      "  batch 800 loss: 0.5454978132247925\n",
      "  batch 850 loss: 0.5539191496372223\n",
      "  batch 900 loss: 0.5909526669979095\n",
      "Epoch 00018: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.59095 valid 0.84937, valid PER 25.72%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5300193780660629\n",
      "  batch 100 loss: 0.5159981471300125\n",
      "  batch 150 loss: 0.5238410145044327\n",
      "  batch 200 loss: 0.5460877478122711\n",
      "  batch 250 loss: 0.5402103865146637\n",
      "  batch 300 loss: 0.5296645605564118\n",
      "  batch 350 loss: 0.5245968216657638\n",
      "  batch 400 loss: 0.5378067880868912\n",
      "  batch 450 loss: 0.5409761011600495\n",
      "  batch 500 loss: 0.5495061534643173\n",
      "  batch 550 loss: 0.5260884600877762\n",
      "  batch 600 loss: 0.5308656287193299\n",
      "  batch 650 loss: 0.5733807498216629\n",
      "  batch 700 loss: 0.5083579832315445\n",
      "  batch 750 loss: 0.5081007033586502\n",
      "  batch 800 loss: 0.5527399152517318\n",
      "  batch 850 loss: 0.5399490141868591\n",
      "  batch 900 loss: 0.5299871736764907\n",
      "LOSS train 0.52999 valid 0.82473, valid PER 25.34%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.510085963010788\n",
      "  batch 100 loss: 0.5194916552305222\n",
      "  batch 150 loss: 0.5167591917514801\n",
      "  batch 200 loss: 0.5296339339017868\n",
      "  batch 250 loss: 0.5076056933403015\n",
      "  batch 300 loss: 0.5292777395248414\n",
      "  batch 350 loss: 0.49839388132095336\n",
      "  batch 400 loss: 0.5203698474168778\n",
      "  batch 450 loss: 0.5172838032245636\n",
      "  batch 500 loss: 0.492922845184803\n",
      "  batch 550 loss: 0.5487795996665955\n",
      "  batch 600 loss: 0.4955440843105316\n",
      "  batch 650 loss: 0.5230042636394501\n",
      "  batch 700 loss: 0.5200096327066421\n",
      "  batch 750 loss: 0.49091377317905427\n",
      "  batch 800 loss: 0.5315767133235931\n",
      "  batch 850 loss: 0.5386079663038253\n",
      "  batch 900 loss: 0.527362065911293\n",
      "LOSS train 0.52736 valid 0.82374, valid PER 25.28%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231209_193306/model_20\n",
      "Currently using dropout rate of 0.1\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.2410813903808595\n",
      "  batch 100 loss: 3.403750419616699\n",
      "  batch 150 loss: 3.3094040060043337\n",
      "  batch 200 loss: 3.2489990615844726\n",
      "  batch 250 loss: 3.192182192802429\n",
      "  batch 300 loss: 3.0568379831314085\n",
      "  batch 350 loss: 2.9043865966796876\n",
      "  batch 400 loss: 2.748207664489746\n",
      "  batch 450 loss: 2.6701633977890014\n",
      "  batch 500 loss: 2.53303231716156\n",
      "  batch 550 loss: 2.438584079742432\n",
      "  batch 600 loss: 2.3646439027786257\n",
      "  batch 650 loss: 2.264903335571289\n",
      "  batch 700 loss: 2.236009831428528\n",
      "  batch 750 loss: 2.154514789581299\n",
      "  batch 800 loss: 2.114242479801178\n",
      "  batch 850 loss: 2.0475088930130005\n",
      "  batch 900 loss: 2.002305669784546\n",
      "LOSS train 2.00231 valid 1.91908, valid PER 72.58%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.9283607339859008\n",
      "  batch 100 loss: 1.8588764023780824\n",
      "  batch 150 loss: 1.7942776036262513\n",
      "  batch 200 loss: 1.8098021864891052\n",
      "  batch 250 loss: 1.7908059573173523\n",
      "  batch 300 loss: 1.7477834558486938\n",
      "  batch 350 loss: 1.6429672980308532\n",
      "  batch 400 loss: 1.6379615378379822\n",
      "  batch 450 loss: 1.5805827021598815\n",
      "  batch 500 loss: 1.6081867170333863\n",
      "  batch 550 loss: 1.602065815925598\n",
      "  batch 600 loss: 1.549401385784149\n",
      "  batch 650 loss: 1.6082541203498841\n",
      "  batch 700 loss: 1.5231326460838317\n",
      "  batch 750 loss: 1.5062686347961425\n",
      "  batch 800 loss: 1.4499340057373047\n",
      "  batch 850 loss: 1.4439644980430604\n",
      "  batch 900 loss: 1.4742969703674316\n",
      "LOSS train 1.47430 valid 1.41071, valid PER 47.85%\n",
      "EPOCH 3:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 50 loss: 1.4124713921546936\n",
      "  batch 100 loss: 1.4164020538330078\n",
      "  batch 150 loss: 1.3866048312187196\n",
      "  batch 200 loss: 1.3661624813079833\n",
      "  batch 250 loss: 1.347643632888794\n",
      "  batch 300 loss: 1.344867112636566\n",
      "  batch 350 loss: 1.3798460340499878\n",
      "  batch 400 loss: 1.3616048645973207\n",
      "  batch 450 loss: 1.3251932215690614\n",
      "  batch 500 loss: 1.337709937095642\n",
      "  batch 550 loss: 1.3134056520462036\n",
      "  batch 600 loss: 1.31200803399086\n",
      "  batch 650 loss: 1.2547938823699951\n",
      "  batch 700 loss: 1.2699825048446656\n",
      "  batch 750 loss: 1.345962507724762\n",
      "  batch 800 loss: 1.2692055797576904\n",
      "  batch 850 loss: 1.3057332730293274\n",
      "  batch 900 loss: 1.2334891939163208\n",
      "LOSS train 1.23349 valid 1.25444, valid PER 40.05%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.2374767899513244\n",
      "  batch 100 loss: 1.23666219830513\n",
      "  batch 150 loss: 1.1944410634040832\n",
      "  batch 200 loss: 1.2149111711978913\n",
      "  batch 250 loss: 1.2289338529109954\n",
      "  batch 300 loss: 1.272038996219635\n",
      "  batch 350 loss: 1.1680487787723541\n",
      "  batch 400 loss: 1.192639697790146\n",
      "  batch 450 loss: 1.2063337194919586\n",
      "  batch 500 loss: 1.170604679584503\n",
      "  batch 550 loss: 1.1958993971347809\n",
      "  batch 600 loss: 1.1920965194702149\n",
      "  batch 650 loss: 1.1999394738674163\n",
      "  batch 700 loss: 1.1475186383724212\n",
      "  batch 750 loss: 1.1270526802539826\n",
      "  batch 800 loss: 1.1160692429542542\n",
      "  batch 850 loss: 1.1633739280700683\n",
      "  batch 900 loss: 1.1738527953624724\n",
      "LOSS train 1.17385 valid 1.13115, valid PER 36.15%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.1271833956241608\n",
      "  batch 100 loss: 1.1027514815330506\n",
      "  batch 150 loss: 1.159965922832489\n",
      "  batch 200 loss: 1.0807076442241668\n",
      "  batch 250 loss: 1.0951857376098633\n",
      "  batch 300 loss: 1.1032012164592744\n",
      "  batch 350 loss: 1.0952392041683197\n",
      "  batch 400 loss: 1.1084818375110626\n",
      "  batch 450 loss: 1.0812926769256592\n",
      "  batch 500 loss: 1.128724148273468\n",
      "  batch 550 loss: 1.0820790123939514\n",
      "  batch 600 loss: 1.1311802315711974\n",
      "  batch 650 loss: 1.069847285747528\n",
      "  batch 700 loss: 1.127954397201538\n",
      "  batch 750 loss: 1.068986656665802\n",
      "  batch 800 loss: 1.0861996591091156\n",
      "  batch 850 loss: 1.0841411817073823\n",
      "  batch 900 loss: 1.090454616546631\n",
      "LOSS train 1.09045 valid 1.08529, valid PER 34.08%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.0715123474597932\n",
      "  batch 100 loss: 1.038613543510437\n",
      "  batch 150 loss: 1.032782528400421\n",
      "  batch 200 loss: 1.032187168598175\n",
      "  batch 250 loss: 1.0537419736385345\n",
      "  batch 300 loss: 1.045102413892746\n",
      "  batch 350 loss: 1.0363611006736755\n",
      "  batch 400 loss: 1.0393498349189758\n",
      "  batch 450 loss: 1.0638499450683594\n",
      "  batch 500 loss: 1.0300598633289337\n",
      "  batch 550 loss: 1.0451193761825561\n",
      "  batch 600 loss: 1.0191611683368682\n",
      "  batch 650 loss: 1.0546182489395142\n",
      "  batch 700 loss: 1.025735605955124\n",
      "  batch 750 loss: 1.017424955368042\n",
      "  batch 800 loss: 1.0373664116859436\n",
      "  batch 850 loss: 0.9991338217258453\n",
      "  batch 900 loss: 1.022674036026001\n",
      "LOSS train 1.02267 valid 1.04205, valid PER 32.86%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.0038546562194823\n",
      "  batch 100 loss: 1.0334784269332886\n",
      "  batch 150 loss: 0.9965865314006805\n",
      "  batch 200 loss: 0.9607834708690643\n",
      "  batch 250 loss: 0.9689298045635223\n",
      "  batch 300 loss: 0.956403855085373\n",
      "  batch 350 loss: 0.9768777096271515\n",
      "  batch 400 loss: 0.983596533536911\n",
      "  batch 450 loss: 1.0087923884391785\n",
      "  batch 500 loss: 0.9726161932945252\n",
      "  batch 550 loss: 0.9762070977687836\n",
      "  batch 600 loss: 0.994254400730133\n",
      "  batch 650 loss: 1.0030233144760132\n",
      "  batch 700 loss: 0.9957228469848632\n",
      "  batch 750 loss: 1.0000971055030823\n",
      "  batch 800 loss: 0.9840106654167176\n",
      "  batch 850 loss: 0.9869526588916778\n",
      "  batch 900 loss: 1.0238522708415985\n",
      "LOSS train 1.02385 valid 1.02925, valid PER 32.31%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.9793125629425049\n",
      "  batch 100 loss: 0.9527279996871948\n",
      "  batch 150 loss: 0.9406623816490174\n",
      "  batch 200 loss: 0.952123339176178\n",
      "  batch 250 loss: 0.9632505464553833\n",
      "  batch 300 loss: 0.9008275365829468\n",
      "  batch 350 loss: 0.9562609958648681\n",
      "  batch 400 loss: 0.9155063807964325\n",
      "  batch 450 loss: 0.9660863935947418\n",
      "  batch 500 loss: 0.9708788394927979\n",
      "  batch 550 loss: 0.9415551364421845\n",
      "  batch 600 loss: 0.9719951581954956\n",
      "  batch 650 loss: 0.9824739408493042\n",
      "  batch 700 loss: 0.9454868507385253\n",
      "  batch 750 loss: 0.9584360492229461\n",
      "  batch 800 loss: 0.9497540891170502\n",
      "  batch 850 loss: 0.9448686063289642\n",
      "  batch 900 loss: 0.9504270648956299\n",
      "LOSS train 0.95043 valid 0.99893, valid PER 30.82%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.8867599165439606\n",
      "  batch 100 loss: 0.9071197855472565\n",
      "  batch 150 loss: 0.9274484944343567\n",
      "  batch 200 loss: 0.8955568087100982\n",
      "  batch 250 loss: 0.9160965919494629\n",
      "  batch 300 loss: 0.9443814265727997\n",
      "  batch 350 loss: 0.9297435307502746\n",
      "  batch 400 loss: 0.9161789703369141\n",
      "  batch 450 loss: 0.918837479352951\n",
      "  batch 500 loss: 0.8783537316322326\n",
      "  batch 550 loss: 0.9298429822921753\n",
      "  batch 600 loss: 0.9156136798858643\n",
      "  batch 650 loss: 0.9058040535449982\n",
      "  batch 700 loss: 0.8763898122310638\n",
      "  batch 750 loss: 0.9098777675628662\n",
      "  batch 800 loss: 0.9235081553459168\n",
      "  batch 850 loss: 0.9452690660953522\n",
      "  batch 900 loss: 0.9125570166110992\n",
      "LOSS train 0.91256 valid 0.97563, valid PER 30.90%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.8618900668621063\n",
      "  batch 100 loss: 0.8545520031452178\n",
      "  batch 150 loss: 0.9001124262809753\n",
      "  batch 200 loss: 0.8962630999088287\n",
      "  batch 250 loss: 0.8939168083667756\n",
      "  batch 300 loss: 0.8465025806427002\n",
      "  batch 350 loss: 0.8573371553421021\n",
      "  batch 400 loss: 0.8449776721000671\n",
      "  batch 450 loss: 0.8419589972496033\n",
      "  batch 500 loss: 0.8758864510059357\n",
      "  batch 550 loss: 0.9130039012432098\n",
      "  batch 600 loss: 0.8679121565818787\n",
      "  batch 650 loss: 0.8715580010414123\n",
      "  batch 700 loss: 0.8788337957859039\n",
      "  batch 750 loss: 0.8490004253387451\n",
      "  batch 800 loss: 0.907479807138443\n",
      "  batch 850 loss: 0.9007252168655395\n",
      "  batch 900 loss: 0.9020840942859649\n",
      "LOSS train 0.90208 valid 0.93604, valid PER 30.19%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.8089375555515289\n",
      "  batch 100 loss: 0.7815523278713227\n",
      "  batch 150 loss: 0.8015630722045899\n",
      "  batch 200 loss: 0.8525462663173675\n",
      "  batch 250 loss: 0.8519859063625336\n",
      "  batch 300 loss: 0.8478497612476349\n",
      "  batch 350 loss: 0.8574645173549652\n",
      "  batch 400 loss: 0.8840583312511444\n",
      "  batch 450 loss: 0.882873545885086\n",
      "  batch 500 loss: 0.833554105758667\n",
      "  batch 550 loss: 0.8434808003902435\n",
      "  batch 600 loss: 0.8192789041996003\n",
      "  batch 650 loss: 0.884606829881668\n",
      "  batch 700 loss: 0.8105837166309356\n",
      "  batch 750 loss: 0.868762137889862\n",
      "  batch 800 loss: 0.8960142993927002\n",
      "  batch 850 loss: 0.8876680684089661\n",
      "  batch 900 loss: 0.9057463002204895\n",
      "Epoch 00011: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.90575 valid 0.96698, valid PER 30.94%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.8141915822029113\n",
      "  batch 100 loss: 0.7677050757408143\n",
      "  batch 150 loss: 0.7332689136266708\n",
      "  batch 200 loss: 0.7625621747970581\n",
      "  batch 250 loss: 0.7666979330778122\n",
      "  batch 300 loss: 0.7575052535533905\n",
      "  batch 350 loss: 0.7428794980049134\n",
      "  batch 400 loss: 0.7748197972774505\n",
      "  batch 450 loss: 0.7421849846839905\n",
      "  batch 500 loss: 0.7541884005069732\n",
      "  batch 550 loss: 0.702019808292389\n",
      "  batch 600 loss: 0.7288084614276886\n",
      "  batch 650 loss: 0.7616158902645112\n",
      "  batch 700 loss: 0.7510267090797424\n",
      "  batch 750 loss: 0.7301610565185547\n",
      "  batch 800 loss: 0.7143176317214965\n",
      "  batch 850 loss: 0.7728792762756348\n",
      "  batch 900 loss: 0.7748392599821091\n",
      "LOSS train 0.77484 valid 0.87235, valid PER 27.68%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7243668282032013\n",
      "  batch 100 loss: 0.730880411863327\n",
      "  batch 150 loss: 0.7004533267021179\n",
      "  batch 200 loss: 0.7294833689928055\n",
      "  batch 250 loss: 0.7238730007410049\n",
      "  batch 300 loss: 0.7114288759231567\n",
      "  batch 350 loss: 0.6978576880693436\n",
      "  batch 400 loss: 0.7185006880760193\n",
      "  batch 450 loss: 0.7367086863517761\n",
      "  batch 500 loss: 0.6917076748609543\n",
      "  batch 550 loss: 0.7133338457345962\n",
      "  batch 600 loss: 0.6944370883703231\n",
      "  batch 650 loss: 0.722264730334282\n",
      "  batch 700 loss: 0.7405754852294922\n",
      "  batch 750 loss: 0.6849207252264022\n",
      "  batch 800 loss: 0.7172010099887848\n",
      "  batch 850 loss: 0.7763007861375809\n",
      "  batch 900 loss: 0.7409458184242248\n",
      "LOSS train 0.74095 valid 0.86231, valid PER 27.48%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.690549293756485\n",
      "  batch 100 loss: 0.6975587630271911\n",
      "  batch 150 loss: 0.697029412984848\n",
      "  batch 200 loss: 0.691503091454506\n",
      "  batch 250 loss: 0.6994021493196487\n",
      "  batch 300 loss: 0.709209715127945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 350 loss: 0.6830681312084198\n",
      "  batch 400 loss: 0.6733462285995483\n",
      "  batch 450 loss: 0.6726241570711136\n",
      "  batch 500 loss: 0.7026231038570404\n",
      "  batch 550 loss: 0.7130435907840729\n",
      "  batch 600 loss: 0.6820482271909714\n",
      "  batch 650 loss: 0.7153279852867126\n",
      "  batch 700 loss: 0.7199150443077087\n",
      "  batch 750 loss: 0.6893043661117554\n",
      "  batch 800 loss: 0.6718603956699372\n",
      "  batch 850 loss: 0.6981209981441497\n",
      "  batch 900 loss: 0.7076051557064056\n",
      "LOSS train 0.70761 valid 0.86032, valid PER 27.02%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.6804235732555389\n",
      "  batch 100 loss: 0.6769647318124771\n",
      "  batch 150 loss: 0.6745946675539016\n",
      "  batch 200 loss: 0.6736989867687225\n",
      "  batch 250 loss: 0.6812232285737991\n",
      "  batch 300 loss: 0.6580342996120453\n",
      "  batch 350 loss: 0.6682374316453934\n",
      "  batch 400 loss: 0.6634026020765305\n",
      "  batch 450 loss: 0.6767765283584595\n",
      "  batch 500 loss: 0.6353346568346023\n",
      "  batch 550 loss: 0.6990605318546295\n",
      "  batch 600 loss: 0.6863044607639313\n",
      "  batch 650 loss: 0.6865588283538818\n",
      "  batch 700 loss: 0.6996888893842698\n",
      "  batch 750 loss: 0.6790068048238754\n",
      "  batch 800 loss: 0.678195550441742\n",
      "  batch 850 loss: 0.6627018404006958\n",
      "  batch 900 loss: 0.6852664780616761\n",
      "LOSS train 0.68527 valid 0.84312, valid PER 26.48%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.6727275574207305\n",
      "  batch 100 loss: 0.6278075093030929\n",
      "  batch 150 loss: 0.6432155007123947\n",
      "  batch 200 loss: 0.6357476192712784\n",
      "  batch 250 loss: 0.6836154276132583\n",
      "  batch 300 loss: 0.6616296422481537\n",
      "  batch 350 loss: 0.6784680849313736\n",
      "  batch 400 loss: 0.6844335824251175\n",
      "  batch 450 loss: 0.6779575550556183\n",
      "  batch 500 loss: 0.6428604090213775\n",
      "  batch 550 loss: 0.6442429232597351\n",
      "  batch 600 loss: 0.6605438226461411\n",
      "  batch 650 loss: 0.6697501063346862\n",
      "  batch 700 loss: 0.648614147901535\n",
      "  batch 750 loss: 0.6424925696849823\n",
      "  batch 800 loss: 0.6657197260856629\n",
      "  batch 850 loss: 0.6583149021863938\n",
      "  batch 900 loss: 0.6691748368740081\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.66917 valid 0.86878, valid PER 26.94%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.664132816195488\n",
      "  batch 100 loss: 0.6345254319906235\n",
      "  batch 150 loss: 0.5901173835992813\n",
      "  batch 200 loss: 0.5960240125656128\n",
      "  batch 250 loss: 0.6200803029537201\n",
      "  batch 300 loss: 0.6219379782676697\n",
      "  batch 350 loss: 0.6027489191293717\n",
      "  batch 400 loss: 0.6431406366825104\n",
      "  batch 450 loss: 0.6136538696289062\n",
      "  batch 500 loss: 0.5924782341718674\n",
      "  batch 550 loss: 0.614164308309555\n",
      "  batch 600 loss: 0.6397373580932617\n",
      "  batch 650 loss: 0.5970276385545731\n",
      "  batch 700 loss: 0.5945180451869965\n",
      "  batch 750 loss: 0.5876576817035675\n",
      "  batch 800 loss: 0.5754036623239517\n",
      "  batch 850 loss: 0.6209119564294815\n",
      "  batch 900 loss: 0.5883949953317642\n",
      "LOSS train 0.58839 valid 0.83058, valid PER 25.65%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.597133606672287\n",
      "  batch 100 loss: 0.5788566488027572\n",
      "  batch 150 loss: 0.6000285828113556\n",
      "  batch 200 loss: 0.5862371641397476\n",
      "  batch 250 loss: 0.5962937659025193\n",
      "  batch 300 loss: 0.5756284010410309\n",
      "  batch 350 loss: 0.5901957023143768\n",
      "  batch 400 loss: 0.5671153330802917\n",
      "  batch 450 loss: 0.5935212296247482\n",
      "  batch 500 loss: 0.5747391247749328\n",
      "  batch 550 loss: 0.5903523272275925\n",
      "  batch 600 loss: 0.551768245100975\n",
      "  batch 650 loss: 0.5714941000938416\n",
      "  batch 700 loss: 0.6085687220096588\n",
      "  batch 750 loss: 0.5694845706224442\n",
      "  batch 800 loss: 0.5652467566728592\n",
      "  batch 850 loss: 0.5659254992008209\n",
      "  batch 900 loss: 0.5961040818691253\n",
      "Epoch 00018: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.59610 valid 0.83384, valid PER 25.68%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5309796077013016\n",
      "  batch 100 loss: 0.5239844024181366\n",
      "  batch 150 loss: 0.5437715876102448\n",
      "  batch 200 loss: 0.567277415394783\n",
      "  batch 250 loss: 0.5734633415937423\n",
      "  batch 300 loss: 0.5538800847530365\n",
      "  batch 350 loss: 0.5403453361988068\n",
      "  batch 400 loss: 0.5600206035375596\n",
      "  batch 450 loss: 0.5600044077634811\n",
      "  batch 500 loss: 0.5709476190805435\n",
      "  batch 550 loss: 0.5440931099653245\n",
      "  batch 600 loss: 0.5629490733146667\n",
      "  batch 650 loss: 0.5899600851535797\n",
      "  batch 700 loss: 0.5220329880714416\n",
      "  batch 750 loss: 0.5288157707452774\n",
      "  batch 800 loss: 0.5723733973503112\n",
      "  batch 850 loss: 0.5540537428855896\n",
      "  batch 900 loss: 0.5499704360961915\n",
      "LOSS train 0.54997 valid 0.82729, valid PER 25.16%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.5382667809724808\n",
      "  batch 100 loss: 0.5391358137130737\n",
      "  batch 150 loss: 0.5324721783399582\n",
      "  batch 200 loss: 0.5419787979125976\n",
      "  batch 250 loss: 0.5283880418539048\n",
      "  batch 300 loss: 0.544759048819542\n",
      "  batch 350 loss: 0.5150158968567848\n",
      "  batch 400 loss: 0.5368574029207229\n",
      "  batch 450 loss: 0.544924846291542\n",
      "  batch 500 loss: 0.5145343285799027\n",
      "  batch 550 loss: 0.5742490363121032\n",
      "  batch 600 loss: 0.5210715556144714\n",
      "  batch 650 loss: 0.5379778796434402\n",
      "  batch 700 loss: 0.539370846748352\n",
      "  batch 750 loss: 0.5094182735681534\n",
      "  batch 800 loss: 0.557168116569519\n",
      "  batch 850 loss: 0.5586116003990174\n",
      "  batch 900 loss: 0.540359239578247\n",
      "Epoch 00020: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.54036 valid 0.83063, valid PER 25.28%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231209_194205/model_19\n",
      "Currently using dropout rate of 0.2\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.240419588088989\n",
      "  batch 100 loss: 3.403317666053772\n",
      "  batch 150 loss: 3.3097582197189332\n",
      "  batch 200 loss: 3.2495583629608156\n",
      "  batch 250 loss: 3.192878966331482\n",
      "  batch 300 loss: 3.059412536621094\n",
      "  batch 350 loss: 2.9063615512847902\n",
      "  batch 400 loss: 2.7535662126541136\n",
      "  batch 450 loss: 2.671296682357788\n",
      "  batch 500 loss: 2.538790464401245\n",
      "  batch 550 loss: 2.444647641181946\n",
      "  batch 600 loss: 2.3741005659103394\n",
      "  batch 650 loss: 2.2799698448181154\n",
      "  batch 700 loss: 2.2476922416687013\n",
      "  batch 750 loss: 2.1688910031318667\n",
      "  batch 800 loss: 2.1318330550193787\n",
      "  batch 850 loss: 2.06268807888031\n",
      "  batch 900 loss: 2.018194317817688\n",
      "LOSS train 2.01819 valid 1.94013, valid PER 73.06%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.9498171639442443\n",
      "  batch 100 loss: 1.8736313152313233\n",
      "  batch 150 loss: 1.8240283274650573\n",
      "  batch 200 loss: 1.8321501302719116\n",
      "  batch 250 loss: 1.8152842402458191\n",
      "  batch 300 loss: 1.7786750674247742\n",
      "  batch 350 loss: 1.675254638195038\n",
      "  batch 400 loss: 1.665710825920105\n",
      "  batch 450 loss: 1.6142324376106263\n",
      "  batch 500 loss: 1.6385166454315185\n",
      "  batch 550 loss: 1.6426575303077697\n",
      "  batch 600 loss: 1.5886292505264281\n",
      "  batch 650 loss: 1.6424668216705323\n",
      "  batch 700 loss: 1.5613100218772888\n",
      "  batch 750 loss: 1.5512038016319274\n",
      "  batch 800 loss: 1.4931712174415588\n",
      "  batch 850 loss: 1.4699320030212402\n",
      "  batch 900 loss: 1.4968976521492003\n",
      "LOSS train 1.49690 valid 1.41346, valid PER 46.94%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4476747393608094\n",
      "  batch 100 loss: 1.4574940395355225\n",
      "  batch 150 loss: 1.4047853660583496\n",
      "  batch 200 loss: 1.3917433500289917\n",
      "  batch 250 loss: 1.3879612541198731\n",
      "  batch 300 loss: 1.4051383447647094\n",
      "  batch 350 loss: 1.4196251940727234\n",
      "  batch 400 loss: 1.3935256838798522\n",
      "  batch 450 loss: 1.3529568123817444\n",
      "  batch 500 loss: 1.3560369992256165\n",
      "  batch 550 loss: 1.3363279366493226\n",
      "  batch 600 loss: 1.3308002495765685\n",
      "  batch 650 loss: 1.3058766365051269\n",
      "  batch 700 loss: 1.3098930931091308\n",
      "  batch 750 loss: 1.376509326696396\n",
      "  batch 800 loss: 1.308800050020218\n",
      "  batch 850 loss: 1.3273119974136351\n",
      "  batch 900 loss: 1.2675344955921173\n",
      "LOSS train 1.26753 valid 1.23218, valid PER 38.87%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.2483096313476563\n",
      "  batch 100 loss: 1.2724989366531372\n",
      "  batch 150 loss: 1.2232574498653412\n",
      "  batch 200 loss: 1.249317216873169\n",
      "  batch 250 loss: 1.2747587060928345\n",
      "  batch 300 loss: 1.2928671622276307\n",
      "  batch 350 loss: 1.198710868358612\n",
      "  batch 400 loss: 1.2422913110256195\n",
      "  batch 450 loss: 1.2209503638744355\n",
      "  batch 500 loss: 1.2095489048957824\n",
      "  batch 550 loss: 1.2217456459999085\n",
      "  batch 600 loss: 1.2645241391658784\n",
      "  batch 650 loss: 1.2380164778232574\n",
      "  batch 700 loss: 1.1779758262634277\n",
      "  batch 750 loss: 1.1860240387916565\n",
      "  batch 800 loss: 1.1743141424655914\n",
      "  batch 850 loss: 1.1865890574455262\n",
      "  batch 900 loss: 1.2340938079357147\n",
      "LOSS train 1.23409 valid 1.16550, valid PER 36.79%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.1602965950965882\n",
      "  batch 100 loss: 1.1370063591003419\n",
      "  batch 150 loss: 1.193463498353958\n",
      "  batch 200 loss: 1.1171632957458497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 250 loss: 1.1263668286800383\n",
      "  batch 300 loss: 1.1321280455589295\n",
      "  batch 350 loss: 1.1406924223899841\n",
      "  batch 400 loss: 1.1430746030807495\n",
      "  batch 450 loss: 1.1445577669143676\n",
      "  batch 500 loss: 1.1481684637069702\n",
      "  batch 550 loss: 1.1147120308876037\n",
      "  batch 600 loss: 1.1620667099952697\n",
      "  batch 650 loss: 1.1246913027763368\n",
      "  batch 700 loss: 1.1724723386764526\n",
      "  batch 750 loss: 1.1272145116329193\n",
      "  batch 800 loss: 1.1421124172210693\n",
      "  batch 850 loss: 1.1473607337474823\n",
      "  batch 900 loss: 1.1295252895355226\n",
      "LOSS train 1.12953 valid 1.12988, valid PER 36.14%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.1210280549526215\n",
      "  batch 100 loss: 1.0773859930038452\n",
      "  batch 150 loss: 1.0770939218997955\n",
      "  batch 200 loss: 1.0951190757751466\n",
      "  batch 250 loss: 1.0994775307178497\n",
      "  batch 300 loss: 1.0754377603530885\n",
      "  batch 350 loss: 1.0861828207969666\n",
      "  batch 400 loss: 1.066983026266098\n",
      "  batch 450 loss: 1.1156850790977477\n",
      "  batch 500 loss: 1.0897267591953277\n",
      "  batch 550 loss: 1.129799188375473\n",
      "  batch 600 loss: 1.0669039499759674\n",
      "  batch 650 loss: 1.0784859597682952\n",
      "  batch 700 loss: 1.0855947041511536\n",
      "  batch 750 loss: 1.0661038744449616\n",
      "  batch 800 loss: 1.0775280821323394\n",
      "  batch 850 loss: 1.045345003604889\n",
      "  batch 900 loss: 1.0630113625526427\n",
      "LOSS train 1.06301 valid 1.06759, valid PER 33.74%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.075530298948288\n",
      "  batch 100 loss: 1.0723889458179474\n",
      "  batch 150 loss: 1.0602301895618438\n",
      "  batch 200 loss: 1.0343040430545807\n",
      "  batch 250 loss: 1.0426364481449126\n",
      "  batch 300 loss: 0.9935549008846283\n",
      "  batch 350 loss: 1.0176779973506926\n",
      "  batch 400 loss: 1.0327264082431793\n",
      "  batch 450 loss: 1.0441598546504975\n",
      "  batch 500 loss: 1.028847848176956\n",
      "  batch 550 loss: 1.0218825137615204\n",
      "  batch 600 loss: 1.0065355670452119\n",
      "  batch 650 loss: 1.0296358239650727\n",
      "  batch 700 loss: 1.0349358892440796\n",
      "  batch 750 loss: 1.0472198975086213\n",
      "  batch 800 loss: 1.0004698169231414\n",
      "  batch 850 loss: 1.0045240795612336\n",
      "  batch 900 loss: 1.0517260110378266\n",
      "LOSS train 1.05173 valid 1.02994, valid PER 32.37%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.9893044650554657\n",
      "  batch 100 loss: 0.9830517446994782\n",
      "  batch 150 loss: 0.9654617536067963\n",
      "  batch 200 loss: 0.9899623966217042\n",
      "  batch 250 loss: 1.0003156054019928\n",
      "  batch 300 loss: 0.9589356887340545\n",
      "  batch 350 loss: 1.0108282709121703\n",
      "  batch 400 loss: 0.9744010198116303\n",
      "  batch 450 loss: 1.0087094867229462\n",
      "  batch 500 loss: 1.0381917250156403\n",
      "  batch 550 loss: 0.9750394570827484\n",
      "  batch 600 loss: 0.9902067458629609\n",
      "  batch 650 loss: 1.006041100025177\n",
      "  batch 700 loss: 0.9870206451416016\n",
      "  batch 750 loss: 0.9898287761211395\n",
      "  batch 800 loss: 0.9905980563163758\n",
      "  batch 850 loss: 0.9796474611759186\n",
      "  batch 900 loss: 0.9862174725532532\n",
      "LOSS train 0.98622 valid 1.00252, valid PER 31.59%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.9339486646652222\n",
      "  batch 100 loss: 0.9603576064109802\n",
      "  batch 150 loss: 0.9725169396400452\n",
      "  batch 200 loss: 0.90004953622818\n",
      "  batch 250 loss: 0.9530817663669586\n",
      "  batch 300 loss: 0.9673428046703338\n",
      "  batch 350 loss: 0.9752459752559662\n",
      "  batch 400 loss: 0.9605705392360687\n",
      "  batch 450 loss: 0.9623718500137329\n",
      "  batch 500 loss: 0.8977415776252746\n",
      "  batch 550 loss: 0.9518226194381714\n",
      "  batch 600 loss: 0.951787793636322\n",
      "  batch 650 loss: 0.9482372570037841\n",
      "  batch 700 loss: 0.942519987821579\n",
      "  batch 750 loss: 0.922999095916748\n",
      "  batch 800 loss: 0.9696432161331177\n",
      "  batch 850 loss: 0.9743818867206574\n",
      "  batch 900 loss: 0.9155718767642975\n",
      "LOSS train 0.91557 valid 0.96407, valid PER 30.53%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.8899200677871704\n",
      "  batch 100 loss: 0.8878184401988983\n",
      "  batch 150 loss: 0.9403881311416626\n",
      "  batch 200 loss: 0.9252372467517853\n",
      "  batch 250 loss: 0.9066473686695099\n",
      "  batch 300 loss: 0.8803156852722168\n",
      "  batch 350 loss: 0.9146813535690308\n",
      "  batch 400 loss: 0.8876039683818817\n",
      "  batch 450 loss: 0.9064789807796478\n",
      "  batch 500 loss: 0.9441591477394105\n",
      "  batch 550 loss: 0.931005927324295\n",
      "  batch 600 loss: 0.9711994183063507\n",
      "  batch 650 loss: 0.9113698995113373\n",
      "  batch 700 loss: 0.9505716514587402\n",
      "  batch 750 loss: 0.9365712988376618\n",
      "  batch 800 loss: 0.9172834503650665\n",
      "  batch 850 loss: 0.940031247138977\n",
      "  batch 900 loss: 0.9878964412212372\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.98790 valid 1.01099, valid PER 31.89%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.8768423581123352\n",
      "  batch 100 loss: 0.8170051860809326\n",
      "  batch 150 loss: 0.8152826344966888\n",
      "  batch 200 loss: 0.8514371800422669\n",
      "  batch 250 loss: 0.8607255482673645\n",
      "  batch 300 loss: 0.8130680322647095\n",
      "  batch 350 loss: 0.8431758427619934\n",
      "  batch 400 loss: 0.8312465786933899\n",
      "  batch 450 loss: 0.8224364531040191\n",
      "  batch 500 loss: 0.7966328883171081\n",
      "  batch 550 loss: 0.8306608164310455\n",
      "  batch 600 loss: 0.7928410464525223\n",
      "  batch 650 loss: 0.85937380194664\n",
      "  batch 700 loss: 0.775573284626007\n",
      "  batch 750 loss: 0.7970382499694825\n",
      "  batch 800 loss: 0.8369114100933075\n",
      "  batch 850 loss: 0.843601588010788\n",
      "  batch 900 loss: 0.840057989358902\n",
      "LOSS train 0.84006 valid 0.91713, valid PER 28.97%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.8113375592231751\n",
      "  batch 100 loss: 0.7795886826515198\n",
      "  batch 150 loss: 0.7597312700748443\n",
      "  batch 200 loss: 0.7908088850975037\n",
      "  batch 250 loss: 0.7997883379459381\n",
      "  batch 300 loss: 0.7876449751853943\n",
      "  batch 350 loss: 0.7877227389812469\n",
      "  batch 400 loss: 0.8117127466201782\n",
      "  batch 450 loss: 0.8195319843292236\n",
      "  batch 500 loss: 0.8172973042726517\n",
      "  batch 550 loss: 0.7557833087444306\n",
      "  batch 600 loss: 0.7947353863716126\n",
      "  batch 650 loss: 0.8341270220279694\n",
      "  batch 700 loss: 0.8015784585475921\n",
      "  batch 750 loss: 0.7904401111602783\n",
      "  batch 800 loss: 0.7807856678962708\n",
      "  batch 850 loss: 0.8164950668811798\n",
      "  batch 900 loss: 0.8325120902061462\n",
      "LOSS train 0.83251 valid 0.89304, valid PER 28.62%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7496702885627746\n",
      "  batch 100 loss: 0.7746245467662811\n",
      "  batch 150 loss: 0.7449922120571136\n",
      "  batch 200 loss: 0.775497619509697\n",
      "  batch 250 loss: 0.7841343641281128\n",
      "  batch 300 loss: 0.7579903078079223\n",
      "  batch 350 loss: 0.7419211345911026\n",
      "  batch 400 loss: 0.7832384896278382\n",
      "  batch 450 loss: 0.7990924882888794\n",
      "  batch 500 loss: 0.7492275530099869\n",
      "  batch 550 loss: 0.7750452852249146\n",
      "  batch 600 loss: 0.7580858182907104\n",
      "  batch 650 loss: 0.7922346568107606\n",
      "  batch 700 loss: 0.7961384892463684\n",
      "  batch 750 loss: 0.7476594269275665\n",
      "  batch 800 loss: 0.7906580460071564\n",
      "  batch 850 loss: 0.80969067633152\n",
      "  batch 900 loss: 0.7919767713546753\n",
      "LOSS train 0.79198 valid 0.87284, valid PER 27.38%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.7439582073688507\n",
      "  batch 100 loss: 0.7677833652496338\n",
      "  batch 150 loss: 0.7378050398826599\n",
      "  batch 200 loss: 0.7294252580404281\n",
      "  batch 250 loss: 0.7319122087955475\n",
      "  batch 300 loss: 0.7665481066703796\n",
      "  batch 350 loss: 0.7587158513069153\n",
      "  batch 400 loss: 0.7521949684619904\n",
      "  batch 450 loss: 0.7396744203567505\n",
      "  batch 500 loss: 0.763882332444191\n",
      "  batch 550 loss: 0.774084837436676\n",
      "  batch 600 loss: 0.737657687664032\n",
      "  batch 650 loss: 0.7642860472202301\n",
      "  batch 700 loss: 0.7807353389263153\n",
      "  batch 750 loss: 0.724582200050354\n",
      "  batch 800 loss: 0.7170000165700913\n",
      "  batch 850 loss: 0.7641431617736817\n",
      "  batch 900 loss: 0.751980117559433\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.75198 valid 0.88642, valid PER 28.14%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.7278796207904815\n",
      "  batch 100 loss: 0.6884170311689377\n",
      "  batch 150 loss: 0.7027047395706176\n",
      "  batch 200 loss: 0.7190170478820801\n",
      "  batch 250 loss: 0.7001753044128418\n",
      "  batch 300 loss: 0.6798884290456771\n",
      "  batch 350 loss: 0.6967485231161118\n",
      "  batch 400 loss: 0.6953855431079865\n",
      "  batch 450 loss: 0.6975901901721955\n",
      "  batch 500 loss: 0.6691931766271592\n",
      "  batch 550 loss: 0.6941104692220688\n",
      "  batch 600 loss: 0.7054701900482178\n",
      "  batch 650 loss: 0.7075631874799728\n",
      "  batch 700 loss: 0.7140060305595398\n",
      "  batch 750 loss: 0.6937190425395966\n",
      "  batch 800 loss: 0.690958468914032\n",
      "  batch 850 loss: 0.667626376748085\n",
      "  batch 900 loss: 0.6921840715408325\n",
      "LOSS train 0.69218 valid 0.85489, valid PER 27.25%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.7022931098937988\n",
      "  batch 100 loss: 0.636552209854126\n",
      "  batch 150 loss: 0.6780192041397095\n",
      "  batch 200 loss: 0.6652322572469711\n",
      "  batch 250 loss: 0.6964429634809494\n",
      "  batch 300 loss: 0.6798243933916092\n",
      "  batch 350 loss: 0.6947286969423294\n",
      "  batch 400 loss: 0.6961376947164536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 450 loss: 0.6877789056301117\n",
      "  batch 500 loss: 0.6694679528474807\n",
      "  batch 550 loss: 0.6596268856525421\n",
      "  batch 600 loss: 0.675288051366806\n",
      "  batch 650 loss: 0.6895640683174133\n",
      "  batch 700 loss: 0.6588101857900619\n",
      "  batch 750 loss: 0.6615969389677048\n",
      "  batch 800 loss: 0.675544280409813\n",
      "  batch 850 loss: 0.6718381321430207\n",
      "  batch 900 loss: 0.6795142406225204\n",
      "LOSS train 0.67951 valid 0.84318, valid PER 26.54%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.6715884816646576\n",
      "  batch 100 loss: 0.6654471242427826\n",
      "  batch 150 loss: 0.6455340385437012\n",
      "  batch 200 loss: 0.6466799306869507\n",
      "  batch 250 loss: 0.6653047299385071\n",
      "  batch 300 loss: 0.658716213107109\n",
      "  batch 350 loss: 0.6387039971351623\n",
      "  batch 400 loss: 0.7036819934844971\n",
      "  batch 450 loss: 0.6695623189210892\n",
      "  batch 500 loss: 0.6479550737142563\n",
      "  batch 550 loss: 0.6641303032636643\n",
      "  batch 600 loss: 0.6986031228303909\n",
      "  batch 650 loss: 0.6362096673250198\n",
      "  batch 700 loss: 0.6629941529035568\n",
      "  batch 750 loss: 0.6468743020296097\n",
      "  batch 800 loss: 0.6263957834243774\n",
      "  batch 850 loss: 0.6881409698724746\n",
      "  batch 900 loss: 0.6388791221380233\n",
      "LOSS train 0.63888 valid 0.84211, valid PER 26.20%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.6377237927913666\n",
      "  batch 100 loss: 0.6523197340965271\n",
      "  batch 150 loss: 0.6653505671024322\n",
      "  batch 200 loss: 0.641485475897789\n",
      "  batch 250 loss: 0.6547862255573272\n",
      "  batch 300 loss: 0.6328726869821548\n",
      "  batch 350 loss: 0.6479930573701859\n",
      "  batch 400 loss: 0.6210408997535706\n",
      "  batch 450 loss: 0.65586332321167\n",
      "  batch 500 loss: 0.6569178140163422\n",
      "  batch 550 loss: 0.6521987164020538\n",
      "  batch 600 loss: 0.6289962196350097\n",
      "  batch 650 loss: 0.6406303530931473\n",
      "  batch 700 loss: 0.669560878276825\n",
      "  batch 750 loss: 0.6266367018222809\n",
      "  batch 800 loss: 0.6377531868219376\n",
      "  batch 850 loss: 0.6176803028583526\n",
      "  batch 900 loss: 0.6705704635381698\n",
      "Epoch 00018: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.67057 valid 0.84737, valid PER 26.06%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.5990255188941955\n",
      "  batch 100 loss: 0.5974366807937622\n",
      "  batch 150 loss: 0.6027576690912246\n",
      "  batch 200 loss: 0.6164322394132614\n",
      "  batch 250 loss: 0.625167031288147\n",
      "  batch 300 loss: 0.6096175837516785\n",
      "  batch 350 loss: 0.6063499039411545\n",
      "  batch 400 loss: 0.6247238337993621\n",
      "  batch 450 loss: 0.6198996955156326\n",
      "  batch 500 loss: 0.6298311829566956\n",
      "  batch 550 loss: 0.605758650302887\n",
      "  batch 600 loss: 0.6234656965732575\n",
      "  batch 650 loss: 0.6584643000364303\n",
      "  batch 700 loss: 0.5892542433738709\n",
      "  batch 750 loss: 0.5938483119010926\n",
      "  batch 800 loss: 0.6275097203254699\n",
      "  batch 850 loss: 0.6296365457773209\n",
      "  batch 900 loss: 0.6098120814561844\n",
      "LOSS train 0.60981 valid 0.83083, valid PER 25.88%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.5965063118934631\n",
      "  batch 100 loss: 0.6017869555950165\n",
      "  batch 150 loss: 0.611918432712555\n",
      "  batch 200 loss: 0.6127722239494324\n",
      "  batch 250 loss: 0.6049840414524078\n",
      "  batch 300 loss: 0.6122292351722717\n",
      "  batch 350 loss: 0.5788007074594498\n",
      "  batch 400 loss: 0.6017367470264435\n",
      "  batch 450 loss: 0.6099870038032532\n",
      "  batch 500 loss: 0.5755538254976272\n",
      "  batch 550 loss: 0.6382980704307556\n",
      "  batch 600 loss: 0.568852714896202\n",
      "  batch 650 loss: 0.6121227544546127\n",
      "  batch 700 loss: 0.6002941250801086\n",
      "  batch 750 loss: 0.573866571187973\n",
      "  batch 800 loss: 0.6189353746175766\n",
      "  batch 850 loss: 0.6158937919139862\n",
      "  batch 900 loss: 0.6064260548353195\n",
      "Epoch 00020: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.60643 valid 0.83533, valid PER 25.73%\n",
      "Training finished in 9.0 minutes.\n",
      "Model saved to checkpoints/20231209_195105/model_19\n",
      "Currently using dropout rate of 0.3\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.23630573272705\n",
      "  batch 100 loss: 3.4035994577407838\n",
      "  batch 150 loss: 3.3071682453155518\n",
      "  batch 200 loss: 3.2492120027542115\n",
      "  batch 250 loss: 3.1929391288757323\n",
      "  batch 300 loss: 3.059784560203552\n",
      "  batch 350 loss: 2.911270923614502\n",
      "  batch 400 loss: 2.7567628908157347\n",
      "  batch 450 loss: 2.6729919290542603\n",
      "  batch 500 loss: 2.5472418022155763\n",
      "  batch 550 loss: 2.4485504484176634\n",
      "  batch 600 loss: 2.380059866905212\n",
      "  batch 650 loss: 2.2888785457611083\n",
      "  batch 700 loss: 2.2596699094772337\n",
      "  batch 750 loss: 2.1806920742988587\n",
      "  batch 800 loss: 2.148752052783966\n",
      "  batch 850 loss: 2.081801631450653\n",
      "  batch 900 loss: 2.0357200622558596\n",
      "LOSS train 2.03572 valid 1.97370, valid PER 74.44%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.9810813879966735\n",
      "  batch 100 loss: 1.9019310450553895\n",
      "  batch 150 loss: 1.8546573901176453\n",
      "  batch 200 loss: 1.8757314229011535\n",
      "  batch 250 loss: 1.8585821175575257\n",
      "  batch 300 loss: 1.8118818807601929\n",
      "  batch 350 loss: 1.7060463452339172\n",
      "  batch 400 loss: 1.7126044511795044\n",
      "  batch 450 loss: 1.6546411752700805\n",
      "  batch 500 loss: 1.6812395000457763\n",
      "  batch 550 loss: 1.6704206776618957\n",
      "  batch 600 loss: 1.6244070196151734\n",
      "  batch 650 loss: 1.6645314908027649\n",
      "  batch 700 loss: 1.5883204412460328\n",
      "  batch 750 loss: 1.5705853199958801\n",
      "  batch 800 loss: 1.523762629032135\n",
      "  batch 850 loss: 1.5154216861724854\n",
      "  batch 900 loss: 1.5411158871650696\n",
      "LOSS train 1.54112 valid 1.43504, valid PER 47.86%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4862634205818177\n",
      "  batch 100 loss: 1.4710759615898132\n",
      "  batch 150 loss: 1.4415462827682495\n",
      "  batch 200 loss: 1.4320970511436462\n",
      "  batch 250 loss: 1.4077676582336425\n",
      "  batch 300 loss: 1.4188920211791993\n",
      "  batch 350 loss: 1.4518646931648254\n",
      "  batch 400 loss: 1.4234961318969725\n",
      "  batch 450 loss: 1.3806086122989654\n",
      "  batch 500 loss: 1.4036827206611633\n",
      "  batch 550 loss: 1.3724593257904052\n",
      "  batch 600 loss: 1.365151504278183\n",
      "  batch 650 loss: 1.3083579421043396\n",
      "  batch 700 loss: 1.3332386350631713\n",
      "  batch 750 loss: 1.382410945892334\n",
      "  batch 800 loss: 1.318401882648468\n",
      "  batch 850 loss: 1.3668052434921265\n",
      "  batch 900 loss: 1.2989302444458009\n",
      "LOSS train 1.29893 valid 1.28687, valid PER 41.50%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.2761157608032228\n",
      "  batch 100 loss: 1.3089765763282777\n",
      "  batch 150 loss: 1.253288564682007\n",
      "  batch 200 loss: 1.2630899834632874\n",
      "  batch 250 loss: 1.2895886826515197\n",
      "  batch 300 loss: 1.3292033529281617\n",
      "  batch 350 loss: 1.1991223335266112\n",
      "  batch 400 loss: 1.2713485968112945\n",
      "  batch 450 loss: 1.2482902371883393\n",
      "  batch 500 loss: 1.2276268661022187\n",
      "  batch 550 loss: 1.2548214852809907\n",
      "  batch 600 loss: 1.2738717341423034\n",
      "  batch 650 loss: 1.267934091091156\n",
      "  batch 700 loss: 1.2022620475292205\n",
      "  batch 750 loss: 1.2063704550266265\n",
      "  batch 800 loss: 1.181769812107086\n",
      "  batch 850 loss: 1.208238799571991\n",
      "  batch 900 loss: 1.237347537279129\n",
      "LOSS train 1.23735 valid 1.17015, valid PER 37.28%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.192098753452301\n",
      "  batch 100 loss: 1.182031078338623\n",
      "  batch 150 loss: 1.2074417006969451\n",
      "  batch 200 loss: 1.1475215876102447\n",
      "  batch 250 loss: 1.1489107143878936\n",
      "  batch 300 loss: 1.1678238832950592\n",
      "  batch 350 loss: 1.1735542035102844\n",
      "  batch 400 loss: 1.1792666482925416\n",
      "  batch 450 loss: 1.1645442676544189\n",
      "  batch 500 loss: 1.1729622840881349\n",
      "  batch 550 loss: 1.1455817127227783\n",
      "  batch 600 loss: 1.1908120369911195\n",
      "  batch 650 loss: 1.1370237517356871\n",
      "  batch 700 loss: 1.182518764734268\n",
      "  batch 750 loss: 1.1368651616573333\n",
      "  batch 800 loss: 1.140379047393799\n",
      "  batch 850 loss: 1.1327125346660614\n",
      "  batch 900 loss: 1.139374042749405\n",
      "LOSS train 1.13937 valid 1.08271, valid PER 35.08%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.126704077720642\n",
      "  batch 100 loss: 1.1006286883354186\n",
      "  batch 150 loss: 1.0819050395488738\n",
      "  batch 200 loss: 1.0809426283836365\n",
      "  batch 250 loss: 1.1154005753993987\n",
      "  batch 300 loss: 1.1184185636043549\n",
      "  batch 350 loss: 1.10273534655571\n",
      "  batch 400 loss: 1.0890900421142578\n",
      "  batch 450 loss: 1.0974048674106598\n",
      "  batch 500 loss: 1.0879335951805116\n",
      "  batch 550 loss: 1.1097918105125428\n",
      "  batch 600 loss: 1.0826758563518524\n",
      "  batch 650 loss: 1.1317718470096587\n",
      "  batch 700 loss: 1.1313830757141112\n",
      "  batch 750 loss: 1.1344403421878815\n",
      "  batch 800 loss: 1.126213824748993\n",
      "  batch 850 loss: 1.0842824482917786\n",
      "  batch 900 loss: 1.1226078951358796\n",
      "Epoch 00006: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 1.12261 valid 1.09092, valid PER 34.96%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.0668455171585083\n",
      "  batch 100 loss: 1.0500854659080505\n",
      "  batch 150 loss: 0.9989104843139649\n",
      "  batch 200 loss: 0.9890513598918915\n",
      "  batch 250 loss: 0.9795330476760864\n",
      "  batch 300 loss: 0.9453978025913239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 350 loss: 0.9705314767360688\n",
      "  batch 400 loss: 0.9769237768650055\n",
      "  batch 450 loss: 0.9647226679325104\n",
      "  batch 500 loss: 0.9653905010223389\n",
      "  batch 550 loss: 0.953463739156723\n",
      "  batch 600 loss: 0.983247742652893\n",
      "  batch 650 loss: 0.9504810607433319\n",
      "  batch 700 loss: 0.9881583452224731\n",
      "  batch 750 loss: 0.9624038124084473\n",
      "  batch 800 loss: 0.9683730769157409\n",
      "  batch 850 loss: 0.9825802886486054\n",
      "  batch 900 loss: 1.0036060452461242\n",
      "LOSS train 1.00361 valid 0.98764, valid PER 31.66%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.951329540014267\n",
      "  batch 100 loss: 0.9319286501407623\n",
      "  batch 150 loss: 0.927796368598938\n",
      "  batch 200 loss: 0.9275277984142304\n",
      "  batch 250 loss: 0.9382867646217347\n",
      "  batch 300 loss: 0.8936598765850067\n",
      "  batch 350 loss: 0.9379580199718476\n",
      "  batch 400 loss: 0.9150162827968598\n",
      "  batch 450 loss: 0.9696805381774902\n",
      "  batch 500 loss: 0.9656519091129303\n",
      "  batch 550 loss: 0.8945220172405243\n",
      "  batch 600 loss: 0.9614198207855225\n",
      "  batch 650 loss: 0.9776541590690613\n",
      "  batch 700 loss: 0.9319574058055877\n",
      "  batch 750 loss: 0.9303835880756378\n",
      "  batch 800 loss: 0.9336733245849609\n",
      "  batch 850 loss: 0.9190022087097168\n",
      "  batch 900 loss: 0.932897471189499\n",
      "LOSS train 0.93290 valid 0.97439, valid PER 30.80%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.8843053650856018\n",
      "  batch 100 loss: 0.9153802013397216\n",
      "  batch 150 loss: 0.9161584115028382\n",
      "  batch 200 loss: 0.8751538062095642\n",
      "  batch 250 loss: 0.9202638697624207\n",
      "  batch 300 loss: 0.9245929837226867\n",
      "  batch 350 loss: 0.9324104321002961\n",
      "  batch 400 loss: 0.9238147592544556\n",
      "  batch 450 loss: 0.9109373331069947\n",
      "  batch 500 loss: 0.8774639225006103\n",
      "  batch 550 loss: 0.9185193300247192\n",
      "  batch 600 loss: 0.9135999095439911\n",
      "  batch 650 loss: 0.9015213644504547\n",
      "  batch 700 loss: 0.874651745557785\n",
      "  batch 750 loss: 0.8965528666973114\n",
      "  batch 800 loss: 0.9239740717411041\n",
      "  batch 850 loss: 0.9525749564170838\n",
      "  batch 900 loss: 0.8710736382007599\n",
      "LOSS train 0.87107 valid 0.94263, valid PER 29.62%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.870696429014206\n",
      "  batch 100 loss: 0.8631822192668914\n",
      "  batch 150 loss: 0.8867745697498322\n",
      "  batch 200 loss: 0.9015890264511108\n",
      "  batch 250 loss: 0.9052231764793396\n",
      "  batch 300 loss: 0.8550387072563171\n",
      "  batch 350 loss: 0.8976562952995301\n",
      "  batch 400 loss: 0.8506177484989166\n",
      "  batch 450 loss: 0.8530129647254944\n",
      "  batch 500 loss: 0.8990549254417419\n",
      "  batch 550 loss: 0.8880446720123291\n",
      "  batch 600 loss: 0.8783075416088104\n",
      "  batch 650 loss: 0.8668211305141449\n",
      "  batch 700 loss: 0.8857255482673645\n",
      "  batch 750 loss: 0.8382737588882446\n",
      "  batch 800 loss: 0.8796437978744507\n",
      "  batch 850 loss: 0.8831866323947907\n",
      "  batch 900 loss: 0.8962690484523773\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.89627 valid 0.95959, valid PER 29.91%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.826974093914032\n",
      "  batch 100 loss: 0.772947713136673\n",
      "  batch 150 loss: 0.7942654824256897\n",
      "  batch 200 loss: 0.8651079249382019\n",
      "  batch 250 loss: 0.8373671841621398\n",
      "  batch 300 loss: 0.808241651058197\n",
      "  batch 350 loss: 0.8280585968494415\n",
      "  batch 400 loss: 0.8191259109973907\n",
      "  batch 450 loss: 0.8331054031848908\n",
      "  batch 500 loss: 0.7965040564537048\n",
      "  batch 550 loss: 0.810057624578476\n",
      "  batch 600 loss: 0.7889571344852447\n",
      "  batch 650 loss: 0.8555285906791688\n",
      "  batch 700 loss: 0.7750192058086395\n",
      "  batch 750 loss: 0.7847641134262084\n",
      "  batch 800 loss: 0.8272045123577118\n",
      "  batch 850 loss: 0.8400839352607727\n",
      "  batch 900 loss: 0.8412230944633484\n",
      "LOSS train 0.84122 valid 0.87973, valid PER 27.62%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.8022971665859222\n",
      "  batch 100 loss: 0.7768541669845581\n",
      "  batch 150 loss: 0.7608813780546189\n",
      "  batch 200 loss: 0.7938590741157532\n",
      "  batch 250 loss: 0.8052782940864563\n",
      "  batch 300 loss: 0.7954743254184723\n",
      "  batch 350 loss: 0.8011307168006897\n",
      "  batch 400 loss: 0.8024841094017029\n",
      "  batch 450 loss: 0.806981235742569\n",
      "  batch 500 loss: 0.8177467513084412\n",
      "  batch 550 loss: 0.741323761343956\n",
      "  batch 600 loss: 0.7821329748630523\n",
      "  batch 650 loss: 0.8183567810058594\n",
      "  batch 700 loss: 0.8016262841224671\n",
      "  batch 750 loss: 0.7885524761676789\n",
      "  batch 800 loss: 0.7739407044649124\n",
      "  batch 850 loss: 0.8047367453575134\n",
      "  batch 900 loss: 0.8127636349201203\n",
      "LOSS train 0.81276 valid 0.87177, valid PER 27.49%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.7408469915390015\n",
      "  batch 100 loss: 0.7926117348670959\n",
      "  batch 150 loss: 0.7504717773199081\n",
      "  batch 200 loss: 0.7900206935405731\n",
      "  batch 250 loss: 0.7772157746553421\n",
      "  batch 300 loss: 0.7587914776802063\n",
      "  batch 350 loss: 0.7475764203071594\n",
      "  batch 400 loss: 0.7837962454557419\n",
      "  batch 450 loss: 0.7984094417095184\n",
      "  batch 500 loss: 0.7515640997886658\n",
      "  batch 550 loss: 0.7905181521177291\n",
      "  batch 600 loss: 0.757807058095932\n",
      "  batch 650 loss: 0.7794448089599609\n",
      "  batch 700 loss: 0.7842689275741577\n",
      "  batch 750 loss: 0.7501049995422363\n",
      "  batch 800 loss: 0.7587760746479034\n",
      "  batch 850 loss: 0.8058609116077423\n",
      "  batch 900 loss: 0.8024200904369354\n",
      "LOSS train 0.80242 valid 0.86455, valid PER 27.00%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.758293753862381\n",
      "  batch 100 loss: 0.7699529641866684\n",
      "  batch 150 loss: 0.7530139565467835\n",
      "  batch 200 loss: 0.745546823143959\n",
      "  batch 250 loss: 0.759190046787262\n",
      "  batch 300 loss: 0.7910937809944153\n",
      "  batch 350 loss: 0.7462253880500793\n",
      "  batch 400 loss: 0.7602271819114685\n",
      "  batch 450 loss: 0.7317958557605744\n",
      "  batch 500 loss: 0.7615231907367707\n",
      "  batch 550 loss: 0.7727234864234924\n",
      "  batch 600 loss: 0.746154745221138\n",
      "  batch 650 loss: 0.7699849998950958\n",
      "  batch 700 loss: 0.786277015209198\n",
      "  batch 750 loss: 0.7364716708660126\n",
      "  batch 800 loss: 0.7185523962974548\n",
      "  batch 850 loss: 0.7711865627765655\n",
      "  batch 900 loss: 0.7598005998134613\n",
      "Epoch 00014: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.75980 valid 0.87480, valid PER 27.16%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.7586000293493271\n",
      "  batch 100 loss: 0.7179355651140213\n",
      "  batch 150 loss: 0.7406616282463073\n",
      "  batch 200 loss: 0.7466870379447937\n",
      "  batch 250 loss: 0.7346785676479339\n",
      "  batch 300 loss: 0.702288316488266\n",
      "  batch 350 loss: 0.7353015631437302\n",
      "  batch 400 loss: 0.7224775660037994\n",
      "  batch 450 loss: 0.7207955944538117\n",
      "  batch 500 loss: 0.6912231826782227\n",
      "  batch 550 loss: 0.7214206582307816\n",
      "  batch 600 loss: 0.7361807876825333\n",
      "  batch 650 loss: 0.7491635227203369\n",
      "  batch 700 loss: 0.7489688134193421\n",
      "  batch 750 loss: 0.7277557146549225\n",
      "  batch 800 loss: 0.7267429900169372\n",
      "  batch 850 loss: 0.702686413526535\n",
      "  batch 900 loss: 0.7204497325420379\n",
      "LOSS train 0.72045 valid 0.85381, valid PER 27.02%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.7375251162052154\n",
      "  batch 100 loss: 0.6919431978464127\n",
      "  batch 150 loss: 0.7126129215955734\n",
      "  batch 200 loss: 0.70671266913414\n",
      "  batch 250 loss: 0.7423789775371552\n",
      "  batch 300 loss: 0.7177400678396225\n",
      "  batch 350 loss: 0.7177429002523422\n",
      "  batch 400 loss: 0.7365192502737046\n",
      "  batch 450 loss: 0.7322903978824615\n",
      "  batch 500 loss: 0.6998867803812027\n",
      "  batch 550 loss: 0.7008000963926315\n",
      "  batch 600 loss: 0.7115580070018769\n",
      "  batch 650 loss: 0.7372680652141571\n",
      "  batch 700 loss: 0.7025560683012009\n",
      "  batch 750 loss: 0.7125131505727768\n",
      "  batch 800 loss: 0.6963666343688965\n",
      "  batch 850 loss: 0.7124155402183533\n",
      "  batch 900 loss: 0.7091574573516846\n",
      "Epoch 00016: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.70916 valid 0.85427, valid PER 26.56%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.7124306112527847\n",
      "  batch 100 loss: 0.7065905916690827\n",
      "  batch 150 loss: 0.6893772357702255\n",
      "  batch 200 loss: 0.6861581939458847\n",
      "  batch 250 loss: 0.7025825679302216\n",
      "  batch 300 loss: 0.6915526503324508\n",
      "  batch 350 loss: 0.6705907678604126\n",
      "  batch 400 loss: 0.7320469665527344\n",
      "  batch 450 loss: 0.6992480456829071\n",
      "  batch 500 loss: 0.6873517096042633\n",
      "  batch 550 loss: 0.7038927614688874\n",
      "  batch 600 loss: 0.7213895589113235\n",
      "  batch 650 loss: 0.67472875893116\n",
      "  batch 700 loss: 0.6896377247571945\n",
      "  batch 750 loss: 0.6766615855693817\n",
      "  batch 800 loss: 0.6602773356437683\n",
      "  batch 850 loss: 0.7054829877614975\n",
      "  batch 900 loss: 0.6625336337089539\n",
      "LOSS train 0.66253 valid 0.84295, valid PER 26.21%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.6765957951545716\n",
      "  batch 100 loss: 0.7023972976207733\n",
      "  batch 150 loss: 0.7026247608661652\n",
      "  batch 200 loss: 0.6792836314439774\n",
      "  batch 250 loss: 0.7078465908765793\n",
      "  batch 300 loss: 0.6802757740020752\n",
      "  batch 350 loss: 0.6895520001649856\n",
      "  batch 400 loss: 0.65952332675457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 450 loss: 0.7024477958679199\n",
      "  batch 500 loss: 0.6875839895009994\n",
      "  batch 550 loss: 0.6883374184370041\n",
      "  batch 600 loss: 0.6566585552692413\n",
      "  batch 650 loss: 0.6746353662014007\n",
      "  batch 700 loss: 0.697833258509636\n",
      "  batch 750 loss: 0.6781710612773896\n",
      "  batch 800 loss: 0.6637485337257385\n",
      "  batch 850 loss: 0.663476352095604\n",
      "  batch 900 loss: 0.7079851365089417\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.70799 valid 0.84899, valid PER 26.48%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.6571967673301696\n",
      "  batch 100 loss: 0.6528203129768372\n",
      "  batch 150 loss: 0.6612490504980088\n",
      "  batch 200 loss: 0.6811364185810089\n",
      "  batch 250 loss: 0.6986423897743225\n",
      "  batch 300 loss: 0.6737143975496293\n",
      "  batch 350 loss: 0.6787929058074951\n",
      "  batch 400 loss: 0.6850334757566452\n",
      "  batch 450 loss: 0.6785662531852722\n",
      "  batch 500 loss: 0.6906520080566406\n",
      "  batch 550 loss: 0.6669913005828857\n",
      "  batch 600 loss: 0.6845183271169663\n",
      "  batch 650 loss: 0.7104526370763778\n",
      "  batch 700 loss: 0.6490281254053116\n",
      "  batch 750 loss: 0.661016132235527\n",
      "  batch 800 loss: 0.6922733706235885\n",
      "  batch 850 loss: 0.6836287593841552\n",
      "  batch 900 loss: 0.68682936668396\n",
      "LOSS train 0.68683 valid 0.84206, valid PER 26.11%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.6716170138120652\n",
      "  batch 100 loss: 0.6786842310428619\n",
      "  batch 150 loss: 0.6743924224376678\n",
      "  batch 200 loss: 0.6726662623882294\n",
      "  batch 250 loss: 0.6648901516199112\n",
      "  batch 300 loss: 0.6877211630344391\n",
      "  batch 350 loss: 0.653233351111412\n",
      "  batch 400 loss: 0.6763043600320816\n",
      "  batch 450 loss: 0.6743513309955597\n",
      "  batch 500 loss: 0.6522007989883423\n",
      "  batch 550 loss: 0.7146389293670654\n",
      "  batch 600 loss: 0.6360421931743622\n",
      "  batch 650 loss: 0.664980937242508\n",
      "  batch 700 loss: 0.6632775831222534\n",
      "  batch 750 loss: 0.6474673193693161\n",
      "  batch 800 loss: 0.6883120614290238\n",
      "  batch 850 loss: 0.6873435413837433\n",
      "  batch 900 loss: 0.664610647559166\n",
      "LOSS train 0.66461 valid 0.84082, valid PER 25.88%\n",
      "Training finished in 9.0 minutes.\n",
      "Model saved to checkpoints/20231209_200006/model_20\n",
      "Currently using dropout rate of 0.4\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.225503296852112\n",
      "  batch 100 loss: 3.4034410381317137\n",
      "  batch 150 loss: 3.303575720787048\n",
      "  batch 200 loss: 3.249274020195007\n",
      "  batch 250 loss: 3.1923697996139526\n",
      "  batch 300 loss: 3.0525808334350586\n",
      "  batch 350 loss: 2.9182229804992676\n",
      "  batch 400 loss: 2.7633262634277345\n",
      "  batch 450 loss: 2.681493821144104\n",
      "  batch 500 loss: 2.5552146291732787\n",
      "  batch 550 loss: 2.4549827194213867\n",
      "  batch 600 loss: 2.391281909942627\n",
      "  batch 650 loss: 2.3007584953308107\n",
      "  batch 700 loss: 2.2755910921096802\n",
      "  batch 750 loss: 2.199520106315613\n",
      "  batch 800 loss: 2.1726337909698485\n",
      "  batch 850 loss: 2.111443600654602\n",
      "  batch 900 loss: 2.060173647403717\n",
      "LOSS train 2.06017 valid 2.01728, valid PER 75.30%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 2.0084941530227662\n",
      "  batch 100 loss: 1.9327684617042542\n",
      "  batch 150 loss: 1.8984461808204651\n",
      "  batch 200 loss: 1.9008432483673097\n",
      "  batch 250 loss: 1.8891399526596069\n",
      "  batch 300 loss: 1.8455887866020202\n",
      "  batch 350 loss: 1.7621116065979003\n",
      "  batch 400 loss: 1.7557505750656128\n",
      "  batch 450 loss: 1.7007772707939148\n",
      "  batch 500 loss: 1.7125636792182923\n",
      "  batch 550 loss: 1.7008188462257385\n",
      "  batch 600 loss: 1.651673686504364\n",
      "  batch 650 loss: 1.7001715612411499\n",
      "  batch 700 loss: 1.6333200860023498\n",
      "  batch 750 loss: 1.6337647581100463\n",
      "  batch 800 loss: 1.5681129312515258\n",
      "  batch 850 loss: 1.5653376507759094\n",
      "  batch 900 loss: 1.5838299679756165\n",
      "LOSS train 1.58383 valid 1.48368, valid PER 51.13%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.5393353700637817\n",
      "  batch 100 loss: 1.5300536346435547\n",
      "  batch 150 loss: 1.4907631993293762\n",
      "  batch 200 loss: 1.4771097207069397\n",
      "  batch 250 loss: 1.4560732221603394\n",
      "  batch 300 loss: 1.4673692870140076\n",
      "  batch 350 loss: 1.4827219557762146\n",
      "  batch 400 loss: 1.4642332434654235\n",
      "  batch 450 loss: 1.4270450949668885\n",
      "  batch 500 loss: 1.4206696939468384\n",
      "  batch 550 loss: 1.404806306362152\n",
      "  batch 600 loss: 1.3877936005592346\n",
      "  batch 650 loss: 1.3612386536598207\n",
      "  batch 700 loss: 1.3744875693321228\n",
      "  batch 750 loss: 1.4360863399505615\n",
      "  batch 800 loss: 1.3553938674926758\n",
      "  batch 850 loss: 1.3911124444007874\n",
      "  batch 900 loss: 1.3295695543289185\n",
      "LOSS train 1.32957 valid 1.29414, valid PER 42.19%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.3117023491859436\n",
      "  batch 100 loss: 1.3420425939559937\n",
      "  batch 150 loss: 1.2840537548065185\n",
      "  batch 200 loss: 1.304236751794815\n",
      "  batch 250 loss: 1.3214321827888489\n",
      "  batch 300 loss: 1.3506965470314025\n",
      "  batch 350 loss: 1.2482196736335753\n",
      "  batch 400 loss: 1.2891210436820983\n",
      "  batch 450 loss: 1.2858592450618744\n",
      "  batch 500 loss: 1.2730873262882232\n",
      "  batch 550 loss: 1.2949447774887084\n",
      "  batch 600 loss: 1.299024682044983\n",
      "  batch 650 loss: 1.2924767279624938\n",
      "  batch 700 loss: 1.2357485890388489\n",
      "  batch 750 loss: 1.2270634198188781\n",
      "  batch 800 loss: 1.2065372240543366\n",
      "  batch 850 loss: 1.241625382900238\n",
      "  batch 900 loss: 1.2738678300380706\n",
      "LOSS train 1.27387 valid 1.15016, valid PER 36.88%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.2251005935668946\n",
      "  batch 100 loss: 1.2186845636367798\n",
      "  batch 150 loss: 1.2334349071979522\n",
      "  batch 200 loss: 1.1702894675731659\n",
      "  batch 250 loss: 1.1719990181922912\n",
      "  batch 300 loss: 1.184991329908371\n",
      "  batch 350 loss: 1.2080508160591126\n",
      "  batch 400 loss: 1.2098573136329651\n",
      "  batch 450 loss: 1.1787975704669953\n",
      "  batch 500 loss: 1.1994436848163605\n",
      "  batch 550 loss: 1.1483927536010743\n",
      "  batch 600 loss: 1.2263632786273957\n",
      "  batch 650 loss: 1.1661660802364349\n",
      "  batch 700 loss: 1.201427640914917\n",
      "  batch 750 loss: 1.1431205022335051\n",
      "  batch 800 loss: 1.164200119972229\n",
      "  batch 850 loss: 1.1617996001243591\n",
      "  batch 900 loss: 1.1777532923221588\n",
      "Epoch 00005: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 1.17775 valid 1.15040, valid PER 37.22%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.1311515605449676\n",
      "  batch 100 loss: 1.0561606943607331\n",
      "  batch 150 loss: 1.0591312921047211\n",
      "  batch 200 loss: 1.0486645817756652\n",
      "  batch 250 loss: 1.086324143409729\n",
      "  batch 300 loss: 1.0715416300296783\n",
      "  batch 350 loss: 1.0618300712108613\n",
      "  batch 400 loss: 1.043939197063446\n",
      "  batch 450 loss: 1.0602054440975188\n",
      "  batch 500 loss: 1.0637042379379273\n",
      "  batch 550 loss: 1.0848571181297302\n",
      "  batch 600 loss: 1.0535923588275908\n",
      "  batch 650 loss: 1.0744854974746705\n",
      "  batch 700 loss: 1.063343230485916\n",
      "  batch 750 loss: 1.0383492517471313\n",
      "  batch 800 loss: 1.0522202003002166\n",
      "  batch 850 loss: 1.0251712536811828\n",
      "  batch 900 loss: 1.044438862800598\n",
      "LOSS train 1.04444 valid 1.03519, valid PER 32.78%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.0377467751502991\n",
      "  batch 100 loss: 1.066885244846344\n",
      "  batch 150 loss: 1.0082799172401429\n",
      "  batch 200 loss: 1.008911657333374\n",
      "  batch 250 loss: 1.0070712065696716\n",
      "  batch 300 loss: 0.9907258987426758\n",
      "  batch 350 loss: 1.007497878074646\n",
      "  batch 400 loss: 1.0220957565307618\n",
      "  batch 450 loss: 1.0067334723472596\n",
      "  batch 500 loss: 0.9991493463516236\n",
      "  batch 550 loss: 1.0043196856975556\n",
      "  batch 600 loss: 1.0258878898620605\n",
      "  batch 650 loss: 1.0041306352615356\n",
      "  batch 700 loss: 1.0211981630325317\n",
      "  batch 750 loss: 0.9965733897686004\n",
      "  batch 800 loss: 0.9969800853729248\n",
      "  batch 850 loss: 1.009568692445755\n",
      "  batch 900 loss: 1.0354975748062134\n",
      "LOSS train 1.03550 valid 0.99709, valid PER 32.36%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.993223773241043\n",
      "  batch 100 loss: 0.9745947766304016\n",
      "  batch 150 loss: 0.9790573298931122\n",
      "  batch 200 loss: 0.9862329411506653\n",
      "  batch 250 loss: 0.9879899215698242\n",
      "  batch 300 loss: 0.9424321484565735\n",
      "  batch 350 loss: 1.0017061388492585\n",
      "  batch 400 loss: 0.9703132104873657\n",
      "  batch 450 loss: 1.0051563632488252\n",
      "  batch 500 loss: 1.0086548376083373\n",
      "  batch 550 loss: 0.9470062756538391\n",
      "  batch 600 loss: 0.9851275503635406\n",
      "  batch 650 loss: 1.0251600742340088\n",
      "  batch 700 loss: 0.9847095227241516\n",
      "  batch 750 loss: 0.9731425738334656\n",
      "  batch 800 loss: 0.9805297839641571\n",
      "  batch 850 loss: 0.9755165493488311\n",
      "  batch 900 loss: 0.9767023861408234\n",
      "LOSS train 0.97670 valid 0.99691, valid PER 31.31%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.9369927561283111\n",
      "  batch 100 loss: 0.9642573165893554\n",
      "  batch 150 loss: 0.96477942943573\n",
      "  batch 200 loss: 0.9165579473972321\n",
      "  batch 250 loss: 0.9627930009365082\n",
      "  batch 300 loss: 0.9632484364509583\n",
      "  batch 350 loss: 0.979313793182373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 400 loss: 0.9682376062870026\n",
      "  batch 450 loss: 0.9630557763576507\n",
      "  batch 500 loss: 0.9215477693080902\n",
      "  batch 550 loss: 0.9720089423656464\n",
      "  batch 600 loss: 0.9698984253406525\n",
      "  batch 650 loss: 0.9438456380367279\n",
      "  batch 700 loss: 0.9316515064239502\n",
      "  batch 750 loss: 0.9590212059020996\n",
      "  batch 800 loss: 0.9771483039855957\n",
      "  batch 850 loss: 0.9863555002212524\n",
      "  batch 900 loss: 0.9233079123497009\n",
      "LOSS train 0.92331 valid 0.96376, valid PER 30.43%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.9229220414161682\n",
      "  batch 100 loss: 0.9079756247997284\n",
      "  batch 150 loss: 0.9436542534828186\n",
      "  batch 200 loss: 0.9607348930835724\n",
      "  batch 250 loss: 0.9497406005859375\n",
      "  batch 300 loss: 0.8954160416126251\n",
      "  batch 350 loss: 0.9333334529399872\n",
      "  batch 400 loss: 0.8923700320720672\n",
      "  batch 450 loss: 0.8912749457359314\n",
      "  batch 500 loss: 0.942800315618515\n",
      "  batch 550 loss: 0.9439060842990875\n",
      "  batch 600 loss: 0.9191083168983459\n",
      "  batch 650 loss: 0.9195622062683105\n",
      "  batch 700 loss: 0.9471275746822357\n",
      "  batch 750 loss: 0.9209184908866882\n",
      "  batch 800 loss: 0.9471172499656677\n",
      "  batch 850 loss: 0.9434671640396118\n",
      "  batch 900 loss: 0.9491580390930175\n",
      "LOSS train 0.94916 valid 0.95695, valid PER 30.59%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.8853203856945038\n",
      "  batch 100 loss: 0.8780877482891083\n",
      "  batch 150 loss: 0.8760414457321167\n",
      "  batch 200 loss: 0.9362551534175872\n",
      "  batch 250 loss: 0.9176906406879425\n",
      "  batch 300 loss: 0.9025541031360627\n",
      "  batch 350 loss: 0.9240920543670654\n",
      "  batch 400 loss: 0.9091476464271545\n",
      "  batch 450 loss: 0.9185140359401703\n",
      "  batch 500 loss: 0.882698495388031\n",
      "  batch 550 loss: 0.9013639843463898\n",
      "  batch 600 loss: 0.8791014063358307\n",
      "  batch 650 loss: 0.9531283903121949\n",
      "  batch 700 loss: 0.880704517364502\n",
      "  batch 750 loss: 0.8737376010417939\n",
      "  batch 800 loss: 0.9261256909370422\n",
      "  batch 850 loss: 0.9372545981407165\n",
      "  batch 900 loss: 0.9172048878669739\n",
      "LOSS train 0.91720 valid 0.93230, valid PER 29.42%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.888389949798584\n",
      "  batch 100 loss: 0.8686883676052094\n",
      "  batch 150 loss: 0.845851377248764\n",
      "  batch 200 loss: 0.877947553396225\n",
      "  batch 250 loss: 0.9082610023021698\n",
      "  batch 300 loss: 0.8812949049472809\n",
      "  batch 350 loss: 0.89610431432724\n",
      "  batch 400 loss: 0.9056768071651459\n",
      "  batch 450 loss: 0.8834336447715759\n",
      "  batch 500 loss: 0.9085068452358246\n",
      "  batch 550 loss: 0.8387979304790497\n",
      "  batch 600 loss: 0.8815035319328308\n",
      "  batch 650 loss: 0.9196487390995025\n",
      "  batch 700 loss: 0.8768768632411956\n",
      "  batch 750 loss: 0.8773587644100189\n",
      "  batch 800 loss: 0.8778915596008301\n",
      "  batch 850 loss: 0.90342480301857\n",
      "  batch 900 loss: 0.9188266479969025\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.91883 valid 0.96160, valid PER 30.42%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.8261638748645782\n",
      "  batch 100 loss: 0.8445680058002472\n",
      "  batch 150 loss: 0.8141051614284516\n",
      "  batch 200 loss: 0.8393841540813446\n",
      "  batch 250 loss: 0.8281133127212524\n",
      "  batch 300 loss: 0.804357328414917\n",
      "  batch 350 loss: 0.813890026807785\n",
      "  batch 400 loss: 0.8306176996231079\n",
      "  batch 450 loss: 0.8698529541492462\n",
      "  batch 500 loss: 0.7846946454048157\n",
      "  batch 550 loss: 0.82158642411232\n",
      "  batch 600 loss: 0.8008890986442566\n",
      "  batch 650 loss: 0.8150682914257049\n",
      "  batch 700 loss: 0.8240562331676483\n",
      "  batch 750 loss: 0.7936714506149292\n",
      "  batch 800 loss: 0.8157012975215912\n",
      "  batch 850 loss: 0.8338779056072235\n",
      "  batch 900 loss: 0.8335610628128052\n",
      "LOSS train 0.83356 valid 0.90515, valid PER 28.96%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.8016626822948456\n",
      "  batch 100 loss: 0.8026089954376221\n",
      "  batch 150 loss: 0.8050173664093018\n",
      "  batch 200 loss: 0.7956588315963745\n",
      "  batch 250 loss: 0.804332686662674\n",
      "  batch 300 loss: 0.8315668487548828\n",
      "  batch 350 loss: 0.7746594500541687\n",
      "  batch 400 loss: 0.7834650242328643\n",
      "  batch 450 loss: 0.7766969990730286\n",
      "  batch 500 loss: 0.8112067151069641\n",
      "  batch 550 loss: 0.8184197473526001\n",
      "  batch 600 loss: 0.7936049318313598\n",
      "  batch 650 loss: 0.8091349029541015\n",
      "  batch 700 loss: 0.8174629664421081\n",
      "  batch 750 loss: 0.7794794297218323\n",
      "  batch 800 loss: 0.7497176718711853\n",
      "  batch 850 loss: 0.8071338498592376\n",
      "  batch 900 loss: 0.8150424253940582\n",
      "LOSS train 0.81504 valid 0.89436, valid PER 28.60%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.8044415241479874\n",
      "  batch 100 loss: 0.7713397812843322\n",
      "  batch 150 loss: 0.7862735855579376\n",
      "  batch 200 loss: 0.7986222910881042\n",
      "  batch 250 loss: 0.7861950469017028\n",
      "  batch 300 loss: 0.7577106416225433\n",
      "  batch 350 loss: 0.7790738379955292\n",
      "  batch 400 loss: 0.776403249502182\n",
      "  batch 450 loss: 0.7807555693387985\n",
      "  batch 500 loss: 0.7516338777542114\n",
      "  batch 550 loss: 0.7857482671737671\n",
      "  batch 600 loss: 0.8043922734260559\n",
      "  batch 650 loss: 0.8013743424415588\n",
      "  batch 700 loss: 0.7951815247535705\n",
      "  batch 750 loss: 0.7928675758838654\n",
      "  batch 800 loss: 0.7780671900510788\n",
      "  batch 850 loss: 0.7609323811531067\n",
      "  batch 900 loss: 0.7733966612815857\n",
      "LOSS train 0.77340 valid 0.88170, valid PER 27.60%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.7873109304904937\n",
      "  batch 100 loss: 0.7326617914438248\n",
      "  batch 150 loss: 0.7778337895870209\n",
      "  batch 200 loss: 0.752068641781807\n",
      "  batch 250 loss: 0.8015072154998779\n",
      "  batch 300 loss: 0.7620444107055664\n",
      "  batch 350 loss: 0.7845246285200119\n",
      "  batch 400 loss: 0.7872748053073884\n",
      "  batch 450 loss: 0.799930316209793\n",
      "  batch 500 loss: 0.7598056209087372\n",
      "  batch 550 loss: 0.7614203834533692\n",
      "  batch 600 loss: 0.7714550256729126\n",
      "  batch 650 loss: 0.7953594481945038\n",
      "  batch 700 loss: 0.7679099464416503\n",
      "  batch 750 loss: 0.7706183934211731\n",
      "  batch 800 loss: 0.7760811591148377\n",
      "  batch 850 loss: 0.7621648705005646\n",
      "  batch 900 loss: 0.7644824743270874\n",
      "LOSS train 0.76448 valid 0.87773, valid PER 27.51%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.7741297048330307\n",
      "  batch 100 loss: 0.7620282399654389\n",
      "  batch 150 loss: 0.7378983122110366\n",
      "  batch 200 loss: 0.7393352973461151\n",
      "  batch 250 loss: 0.7578672540187835\n",
      "  batch 300 loss: 0.753172293305397\n",
      "  batch 350 loss: 0.7308844816684723\n",
      "  batch 400 loss: 0.8014242613315582\n",
      "  batch 450 loss: 0.7683898174762726\n",
      "  batch 500 loss: 0.7646070128679275\n",
      "  batch 550 loss: 0.7636912906169891\n",
      "  batch 600 loss: 0.8009767580032349\n",
      "  batch 650 loss: 0.765512113571167\n",
      "  batch 700 loss: 0.7388069236278534\n",
      "  batch 750 loss: 0.7482909715175629\n",
      "  batch 800 loss: 0.7289798128604889\n",
      "  batch 850 loss: 0.7720456272363663\n",
      "  batch 900 loss: 0.7528422826528549\n",
      "LOSS train 0.75284 valid 0.87617, valid PER 26.67%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.7459017634391785\n",
      "  batch 100 loss: 0.7605699908733368\n",
      "  batch 150 loss: 0.7676418590545654\n",
      "  batch 200 loss: 0.7536237728595734\n",
      "  batch 250 loss: 0.7723863708972931\n",
      "  batch 300 loss: 0.7609923827648163\n",
      "  batch 350 loss: 0.7656271409988403\n",
      "  batch 400 loss: 0.7233453643321991\n",
      "  batch 450 loss: 0.7708559989929199\n",
      "  batch 500 loss: 0.7497373926639557\n",
      "  batch 550 loss: 0.7418025577068329\n",
      "  batch 600 loss: 0.7234465485811233\n",
      "  batch 650 loss: 0.7315260273218155\n",
      "  batch 700 loss: 0.7638848173618317\n",
      "  batch 750 loss: 0.7402962291240692\n",
      "  batch 800 loss: 0.7215910041332245\n",
      "  batch 850 loss: 0.7235719287395477\n",
      "  batch 900 loss: 0.7649945521354675\n",
      "LOSS train 0.76499 valid 0.87501, valid PER 27.12%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.6969032227993012\n",
      "  batch 100 loss: 0.7054893690347671\n",
      "  batch 150 loss: 0.7243716323375702\n",
      "  batch 200 loss: 0.7430842137336731\n",
      "  batch 250 loss: 0.7574283552169799\n",
      "  batch 300 loss: 0.7316914224624633\n",
      "  batch 350 loss: 0.7331364357471466\n",
      "  batch 400 loss: 0.7499067068099976\n",
      "  batch 450 loss: 0.7422946411371231\n",
      "  batch 500 loss: 0.7447978228330612\n",
      "  batch 550 loss: 0.7187685704231263\n"
     ]
    }
   ],
   "source": [
    "import model_uni_directional_LSTM\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start tuning For uni directional 1 Layer LSTM\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 210,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 210,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_uni_directional_LSTM.BiLSTM(args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For uni directional 1 Layer LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d8e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0.5]\n",
    "Optimiser = [\"SGD_Scheduler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a098418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning For uni directional 1 Layer LSTM\n",
      "Currently using SGD_Scheduler optimiser\n",
      "Currently using dropout rate of 0.5\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.186969437599182\n",
      "  batch 100 loss: 3.397619910240173\n",
      "  batch 150 loss: 3.299616980552673\n",
      "  batch 200 loss: 3.245315599441528\n",
      "  batch 250 loss: 3.1675468111038207\n",
      "  batch 300 loss: 3.0078634881973265\n",
      "  batch 350 loss: 2.8707586669921876\n",
      "  batch 400 loss: 2.7618620109558107\n",
      "  batch 450 loss: 2.6840696287155152\n",
      "  batch 500 loss: 2.5458651304244997\n",
      "  batch 550 loss: 2.4563596439361572\n",
      "  batch 600 loss: 2.389314250946045\n",
      "  batch 650 loss: 2.3026261281967164\n",
      "  batch 700 loss: 2.2769479751586914\n",
      "  batch 750 loss: 2.2202400183677673\n",
      "  batch 800 loss: 2.1928952169418334\n",
      "  batch 850 loss: 2.1146387338638304\n",
      "  batch 900 loss: 2.082296385765076\n",
      "LOSS train 2.08230 valid 2.02669, valid PER 74.22%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 2.0402814316749573\n",
      "  batch 100 loss: 1.97362407207489\n",
      "  batch 150 loss: 1.9304421377182006\n",
      "  batch 200 loss: 1.9384505558013916\n",
      "  batch 250 loss: 1.9346366119384766\n",
      "  batch 300 loss: 1.8826894927024842\n",
      "  batch 350 loss: 1.8066075873374938\n",
      "  batch 400 loss: 1.817167308330536\n",
      "  batch 450 loss: 1.743496723175049\n",
      "  batch 500 loss: 1.7620174741744996\n",
      "  batch 550 loss: 1.7538437008857728\n",
      "  batch 600 loss: 1.7177157497406006\n",
      "  batch 650 loss: 1.7439931106567383\n",
      "  batch 700 loss: 1.6887245106697082\n",
      "  batch 750 loss: 1.6713541984558105\n",
      "  batch 800 loss: 1.6185108017921448\n",
      "  batch 850 loss: 1.6046331548690795\n",
      "  batch 900 loss: 1.6309342098236084\n",
      "LOSS train 1.63093 valid 1.54552, valid PER 57.12%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.5862631607055664\n",
      "  batch 100 loss: 1.580238833427429\n",
      "  batch 150 loss: 1.5339291381835938\n",
      "  batch 200 loss: 1.5356384110450745\n",
      "  batch 250 loss: 1.5076810574531556\n",
      "  batch 300 loss: 1.5254621386528016\n",
      "  batch 350 loss: 1.5325627279281617\n",
      "  batch 400 loss: 1.527419888973236\n",
      "  batch 450 loss: 1.4896390008926392\n",
      "  batch 500 loss: 1.4866071677207946\n",
      "  batch 550 loss: 1.4694712805747985\n",
      "  batch 600 loss: 1.4501516604423523\n",
      "  batch 650 loss: 1.4083968019485473\n",
      "  batch 700 loss: 1.4381029319763183\n",
      "  batch 750 loss: 1.4799204349517823\n",
      "  batch 800 loss: 1.4067094135284424\n",
      "  batch 850 loss: 1.4464589619636536\n",
      "  batch 900 loss: 1.3703546357154845\n",
      "LOSS train 1.37035 valid 1.32141, valid PER 44.21%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.3707770895957947\n",
      "  batch 100 loss: 1.3831903290748597\n",
      "  batch 150 loss: 1.3213761067390442\n",
      "  batch 200 loss: 1.3500363540649414\n",
      "  batch 250 loss: 1.3623632645606996\n",
      "  batch 300 loss: 1.393007652759552\n",
      "  batch 350 loss: 1.2770912957191467\n",
      "  batch 400 loss: 1.3299058747291566\n",
      "  batch 450 loss: 1.3274481630325317\n",
      "  batch 500 loss: 1.302939178943634\n",
      "  batch 550 loss: 1.3162872672080994\n",
      "  batch 600 loss: 1.345482141971588\n",
      "  batch 650 loss: 1.3386340057849884\n",
      "  batch 700 loss: 1.2885160779953002\n",
      "  batch 750 loss: 1.2991698825359344\n",
      "  batch 800 loss: 1.2672603809833527\n",
      "  batch 850 loss: 1.275833033323288\n",
      "  batch 900 loss: 1.3255666601657867\n",
      "LOSS train 1.32557 valid 1.21625, valid PER 39.10%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.2581306719779968\n",
      "  batch 100 loss: 1.231613391637802\n",
      "  batch 150 loss: 1.2646024966239928\n",
      "  batch 200 loss: 1.2061773681640624\n",
      "  batch 250 loss: 1.2288574302196502\n",
      "  batch 300 loss: 1.2452226614952087\n",
      "  batch 350 loss: 1.241670310497284\n",
      "  batch 400 loss: 1.2403648328781127\n",
      "  batch 450 loss: 1.2228520834445953\n",
      "  batch 500 loss: 1.2374389910697936\n",
      "  batch 550 loss: 1.1787865996360778\n",
      "  batch 600 loss: 1.2742181193828583\n",
      "  batch 650 loss: 1.2138528108596802\n",
      "  batch 700 loss: 1.248318727016449\n",
      "  batch 750 loss: 1.1929467380046845\n",
      "  batch 800 loss: 1.219036705493927\n",
      "  batch 850 loss: 1.2027022016048432\n",
      "  batch 900 loss: 1.2063291776180267\n",
      "LOSS train 1.20633 valid 1.14536, valid PER 36.42%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.2030803704261779\n",
      "  batch 100 loss: 1.1611720907688141\n",
      "  batch 150 loss: 1.1769824969768523\n",
      "  batch 200 loss: 1.1835003864765168\n",
      "  batch 250 loss: 1.1779753589630126\n",
      "  batch 300 loss: 1.166083036661148\n",
      "  batch 350 loss: 1.160925896167755\n",
      "  batch 400 loss: 1.1332600212097168\n",
      "  batch 450 loss: 1.1973671233654022\n",
      "  batch 500 loss: 1.1714029598236084\n",
      "  batch 550 loss: 1.1753362822532654\n",
      "  batch 600 loss: 1.1423302853107453\n",
      "  batch 650 loss: 1.1459937822818755\n",
      "  batch 700 loss: 1.1737237799167632\n",
      "  batch 750 loss: 1.1120436811447143\n",
      "  batch 800 loss: 1.1422781872749328\n",
      "  batch 850 loss: 1.1164599311351777\n",
      "  batch 900 loss: 1.140199283361435\n",
      "LOSS train 1.14020 valid 1.09382, valid PER 34.32%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.1121058392524719\n",
      "  batch 100 loss: 1.1490687835216522\n",
      "  batch 150 loss: 1.1084873712062835\n",
      "  batch 200 loss: 1.1039975774288178\n",
      "  batch 250 loss: 1.1218468534946442\n",
      "  batch 300 loss: 1.0641319286823272\n",
      "  batch 350 loss: 1.0943721556663513\n",
      "  batch 400 loss: 1.101170837879181\n",
      "  batch 450 loss: 1.0939891731739044\n",
      "  batch 500 loss: 1.1076365792751313\n",
      "  batch 550 loss: 1.0984845077991485\n",
      "  batch 600 loss: 1.116784679889679\n",
      "  batch 650 loss: 1.1082140004634857\n",
      "  batch 700 loss: 1.125103681087494\n",
      "  batch 750 loss: 1.1036895990371705\n",
      "  batch 800 loss: 1.094279509782791\n",
      "  batch 850 loss: 1.1282348728179932\n",
      "  batch 900 loss: 1.1422517025470733\n",
      "LOSS train 1.14225 valid 1.06330, valid PER 33.78%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.0957244396209718\n",
      "  batch 100 loss: 1.0556659853458406\n",
      "  batch 150 loss: 1.051617877483368\n",
      "  batch 200 loss: 1.0695886659622191\n",
      "  batch 250 loss: 1.05559485912323\n",
      "  batch 300 loss: 1.023332998752594\n",
      "  batch 350 loss: 1.0933777403831482\n",
      "  batch 400 loss: 1.0418673264980316\n",
      "  batch 450 loss: 1.0554052662849427\n",
      "  batch 500 loss: 1.0939717829227447\n",
      "  batch 550 loss: 1.0348575186729432\n",
      "  batch 600 loss: 1.0517641615867614\n",
      "  batch 650 loss: 1.1222254490852357\n",
      "  batch 700 loss: 1.0556896674633025\n",
      "  batch 750 loss: 1.0670091724395752\n",
      "  batch 800 loss: 1.0681732964515687\n",
      "  batch 850 loss: 1.0564733958244323\n",
      "  batch 900 loss: 1.0331683921813966\n",
      "LOSS train 1.03317 valid 1.01583, valid PER 31.84%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 1.0054363441467284\n",
      "  batch 100 loss: 1.0396966445446014\n",
      "  batch 150 loss: 1.0264858067035676\n",
      "  batch 200 loss: 0.9740749859809875\n",
      "  batch 250 loss: 1.0489911484718322\n",
      "  batch 300 loss: 1.0356484878063201\n",
      "  batch 350 loss: 1.0591147196292878\n",
      "  batch 400 loss: 1.0314744639396667\n",
      "  batch 450 loss: 1.0169223415851594\n",
      "  batch 500 loss: 1.0029202365875245\n",
      "  batch 550 loss: 1.0894531404972077\n",
      "  batch 600 loss: 1.0639430832862855\n",
      "  batch 650 loss: 1.0678932535648347\n",
      "  batch 700 loss: 1.0579462838172913\n",
      "  batch 750 loss: 1.0278719878196716\n",
      "  batch 800 loss: 1.0679650843143462\n",
      "  batch 850 loss: 1.1126352536678314\n",
      "  batch 900 loss: 1.050523691177368\n",
      "Epoch 00009: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 1.05052 valid 1.04515, valid PER 32.82%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.9582265412807465\n",
      "  batch 100 loss: 0.9545987808704376\n",
      "  batch 150 loss: 0.9744830548763275\n",
      "  batch 200 loss: 0.973658664226532\n",
      "  batch 250 loss: 0.9604023623466492\n",
      "  batch 300 loss: 0.8954186367988587\n",
      "  batch 350 loss: 0.9425477135181427\n",
      "  batch 400 loss: 0.9149094319343567\n",
      "  batch 450 loss: 0.9022405028343201\n",
      "  batch 500 loss: 0.9536870634555816\n",
      "  batch 550 loss: 0.9434672701358795\n",
      "  batch 600 loss: 0.9285815978050231\n",
      "  batch 650 loss: 0.9216485321521759\n",
      "  batch 700 loss: 0.9083209264278412\n",
      "  batch 750 loss: 0.8968370997905731\n",
      "  batch 800 loss: 0.9290680980682373\n",
      "  batch 850 loss: 0.9599690735340118\n",
      "  batch 900 loss: 0.9433028137683869\n",
      "LOSS train 0.94330 valid 0.92338, valid PER 29.45%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.8919844591617584\n",
      "  batch 100 loss: 0.8554292285442352\n",
      "  batch 150 loss: 0.8660943973064422\n",
      "  batch 200 loss: 0.9318847060203552\n",
      "  batch 250 loss: 0.9166565573215485\n",
      "  batch 300 loss: 0.8632024931907654\n",
      "  batch 350 loss: 0.8936400496959687\n",
      "  batch 400 loss: 0.9037695217132569\n",
      "  batch 450 loss: 0.9098772704601288\n",
      "  batch 500 loss: 0.8682189631462097\n",
      "  batch 550 loss: 0.8732359874248504\n",
      "  batch 600 loss: 0.8541861188411712\n",
      "  batch 650 loss: 0.944811555147171\n",
      "  batch 700 loss: 0.8822462546825409\n",
      "  batch 750 loss: 0.8778067028522492\n",
      "  batch 800 loss: 0.898351126909256\n",
      "  batch 850 loss: 0.922419683933258\n",
      "  batch 900 loss: 0.9190439939498901\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.91904 valid 0.93085, valid PER 29.46%\n",
      "EPOCH 12:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 50 loss: 0.8658894455432892\n",
      "  batch 100 loss: 0.8490141677856445\n",
      "  batch 150 loss: 0.8124578773975373\n",
      "  batch 200 loss: 0.848634408712387\n",
      "  batch 250 loss: 0.8627073240280151\n",
      "  batch 300 loss: 0.8424561607837677\n",
      "  batch 350 loss: 0.8525070834159851\n",
      "  batch 400 loss: 0.8700098133087159\n",
      "  batch 450 loss: 0.849846156835556\n",
      "  batch 500 loss: 0.8500027275085449\n",
      "  batch 550 loss: 0.78084756731987\n",
      "  batch 600 loss: 0.8209719610214233\n",
      "  batch 650 loss: 0.8580285489559174\n",
      "  batch 700 loss: 0.8265656125545502\n",
      "  batch 750 loss: 0.8130369752645492\n",
      "  batch 800 loss: 0.8165104645490646\n",
      "  batch 850 loss: 0.8441041958332062\n",
      "  batch 900 loss: 0.854948388338089\n",
      "LOSS train 0.85495 valid 0.88633, valid PER 28.07%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.8003463280200959\n",
      "  batch 100 loss: 0.8320787823200226\n",
      "  batch 150 loss: 0.7940618348121643\n",
      "  batch 200 loss: 0.8403228461742401\n",
      "  batch 250 loss: 0.815920889377594\n",
      "  batch 300 loss: 0.7913719844818116\n",
      "  batch 350 loss: 0.8133838403224946\n",
      "  batch 400 loss: 0.8247128641605377\n",
      "  batch 450 loss: 0.8531033265590667\n",
      "  batch 500 loss: 0.7720016157627105\n",
      "  batch 550 loss: 0.8198776388168335\n",
      "  batch 600 loss: 0.8089561295509339\n",
      "  batch 650 loss: 0.8277952700853348\n",
      "  batch 700 loss: 0.8384198421239852\n",
      "  batch 750 loss: 0.7849702310562133\n",
      "  batch 800 loss: 0.8156388795375824\n",
      "  batch 850 loss: 0.8458785140514373\n",
      "  batch 900 loss: 0.8332007443904876\n",
      "Epoch 00013: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.83320 valid 0.88920, valid PER 28.20%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.7742489850521088\n",
      "  batch 100 loss: 0.8072509324550629\n",
      "  batch 150 loss: 0.7730317282676696\n",
      "  batch 200 loss: 0.7782809686660767\n",
      "  batch 250 loss: 0.7868225252628327\n",
      "  batch 300 loss: 0.8132464367151261\n",
      "  batch 350 loss: 0.7758445870876313\n",
      "  batch 400 loss: 0.7777083778381347\n",
      "  batch 450 loss: 0.763184562921524\n",
      "  batch 500 loss: 0.7935054266452789\n",
      "  batch 550 loss: 0.8048377215862275\n",
      "  batch 600 loss: 0.7717056173086166\n",
      "  batch 650 loss: 0.7886903059482574\n",
      "  batch 700 loss: 0.794860845208168\n",
      "  batch 750 loss: 0.7632565987110138\n",
      "  batch 800 loss: 0.7349598729610443\n",
      "  batch 850 loss: 0.8119532346725464\n",
      "  batch 900 loss: 0.8072864925861358\n",
      "LOSS train 0.80729 valid 0.87932, valid PER 27.38%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.7918701612949371\n",
      "  batch 100 loss: 0.775320086479187\n",
      "  batch 150 loss: 0.7683493947982788\n",
      "  batch 200 loss: 0.787532514333725\n",
      "  batch 250 loss: 0.7733277785778045\n",
      "  batch 300 loss: 0.7652716720104218\n",
      "  batch 350 loss: 0.7638143062591553\n",
      "  batch 400 loss: 0.7517125976085662\n",
      "  batch 450 loss: 0.765308958888054\n",
      "  batch 500 loss: 0.7413274508714676\n",
      "  batch 550 loss: 0.7694190144538879\n",
      "  batch 600 loss: 0.787229677438736\n",
      "  batch 650 loss: 0.7908290946483612\n",
      "  batch 700 loss: 0.8011501586437225\n",
      "  batch 750 loss: 0.7803864681720734\n",
      "  batch 800 loss: 0.7578133988380432\n",
      "  batch 850 loss: 0.7454028940200805\n",
      "  batch 900 loss: 0.7814300054311752\n",
      "LOSS train 0.78143 valid 0.86510, valid PER 27.54%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.8038847184181214\n",
      "  batch 100 loss: 0.7349525082111359\n",
      "  batch 150 loss: 0.771228187084198\n",
      "  batch 200 loss: 0.7600727224349976\n",
      "  batch 250 loss: 0.7899969685077667\n",
      "  batch 300 loss: 0.7692080473899842\n",
      "  batch 350 loss: 0.7709726649522781\n",
      "  batch 400 loss: 0.772331389784813\n",
      "  batch 450 loss: 0.779351772069931\n",
      "  batch 500 loss: 0.738254582285881\n",
      "  batch 550 loss: 0.7603898465633392\n",
      "  batch 600 loss: 0.7572786128520965\n",
      "  batch 650 loss: 0.7863134098052978\n",
      "  batch 700 loss: 0.7481454277038574\n",
      "  batch 750 loss: 0.7538644796609879\n",
      "  batch 800 loss: 0.7583270573616028\n",
      "  batch 850 loss: 0.7559496229887008\n",
      "  batch 900 loss: 0.7440776193141937\n",
      "LOSS train 0.74408 valid 0.86205, valid PER 26.84%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.7654928994178772\n",
      "  batch 100 loss: 0.7545201635360718\n",
      "  batch 150 loss: 0.7371439427137375\n",
      "  batch 200 loss: 0.7588813710212707\n",
      "  batch 250 loss: 0.7516592574119568\n",
      "  batch 300 loss: 0.7407669651508332\n",
      "  batch 350 loss: 0.727838723063469\n",
      "  batch 400 loss: 0.7926819503307343\n",
      "  batch 450 loss: 0.7674024796485901\n",
      "  batch 500 loss: 0.7478638416528702\n",
      "  batch 550 loss: 0.7632474601268768\n",
      "  batch 600 loss: 0.7915404343605041\n",
      "  batch 650 loss: 0.7334388375282288\n",
      "  batch 700 loss: 0.7445499432086945\n",
      "  batch 750 loss: 0.7333460587263108\n",
      "  batch 800 loss: 0.709811054468155\n",
      "  batch 850 loss: 0.7601889508962631\n",
      "  batch 900 loss: 0.7378516590595245\n",
      "LOSS train 0.73785 valid 0.86182, valid PER 26.74%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.7415672159194946\n",
      "  batch 100 loss: 0.7368411767482758\n",
      "  batch 150 loss: 0.7711918926239014\n",
      "  batch 200 loss: 0.7434509181976319\n",
      "  batch 250 loss: 0.7592691165208817\n",
      "  batch 300 loss: 0.7513177251815796\n",
      "  batch 350 loss: 0.7541103005409241\n",
      "  batch 400 loss: 0.7192303758859634\n",
      "  batch 450 loss: 0.759143043756485\n",
      "  batch 500 loss: 0.7519473361968995\n",
      "  batch 550 loss: 0.7486974638700485\n",
      "  batch 600 loss: 0.7174663704633713\n",
      "  batch 650 loss: 0.7301422619819641\n",
      "  batch 700 loss: 0.7622829842567443\n",
      "  batch 750 loss: 0.7380736422538757\n",
      "  batch 800 loss: 0.7216264355182648\n",
      "  batch 850 loss: 0.7248732852935791\n",
      "  batch 900 loss: 0.766191589832306\n",
      "LOSS train 0.76619 valid 0.85871, valid PER 27.03%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.6984748077392579\n",
      "  batch 100 loss: 0.7133833134174347\n",
      "  batch 150 loss: 0.7372731018066406\n",
      "  batch 200 loss: 0.7228013873100281\n",
      "  batch 250 loss: 0.7632971620559692\n",
      "  batch 300 loss: 0.7441866928339005\n",
      "  batch 350 loss: 0.7302275884151459\n",
      "  batch 400 loss: 0.7471629202365875\n",
      "  batch 450 loss: 0.7443732523918152\n",
      "  batch 500 loss: 0.7439203011989594\n",
      "  batch 550 loss: 0.7192757338285446\n",
      "  batch 600 loss: 0.7372515094280243\n",
      "  batch 650 loss: 0.7871344935894012\n",
      "  batch 700 loss: 0.6967810100317001\n",
      "  batch 750 loss: 0.7178063309192657\n",
      "  batch 800 loss: 0.7533392441272736\n",
      "  batch 850 loss: 0.7449789595603943\n",
      "  batch 900 loss: 0.7393347108364106\n",
      "Epoch 00019: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.73933 valid 0.86271, valid PER 26.89%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.7278940320014954\n",
      "  batch 100 loss: 0.7213766300678253\n",
      "  batch 150 loss: 0.7134141594171524\n",
      "  batch 200 loss: 0.7248766767978668\n",
      "  batch 250 loss: 0.7244100016355515\n",
      "  batch 300 loss: 0.7454638248682022\n",
      "  batch 350 loss: 0.6946883088350296\n",
      "  batch 400 loss: 0.7161118495464325\n",
      "  batch 450 loss: 0.7287001550197602\n",
      "  batch 500 loss: 0.6902619618177414\n",
      "  batch 550 loss: 0.7558519035577774\n",
      "  batch 600 loss: 0.6968931835889817\n",
      "  batch 650 loss: 0.7227103960514069\n",
      "  batch 700 loss: 0.72203109562397\n",
      "  batch 750 loss: 0.7029540312290191\n",
      "  batch 800 loss: 0.7417074167728424\n",
      "  batch 850 loss: 0.7565942966938018\n",
      "  batch 900 loss: 0.7053437554836273\n",
      "LOSS train 0.70534 valid 0.85338, valid PER 26.62%\n",
      "Training finished in 10.0 minutes.\n",
      "Model saved to checkpoints/20231209_202437/model_20\n",
      "Finish SGD_Scheduler optimiser\n",
      "End tuning For uni directional 1 Layer LSTM\n"
     ]
    }
   ],
   "source": [
    "import model_uni_directional_LSTM\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start tuning For uni directional 1 Layer LSTM\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    if opt==\"Adam\":\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 210,\n",
    "            'concat': 1,\n",
    "            'lr': 0.001,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "    else:\n",
    "        args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 210,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "        args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_uni_directional_LSTM.BiLSTM(args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        if opt==\"Adam\":\n",
    "            model_path = adam_trainer(model_with_dropout, args)\n",
    "        else:\n",
    "            model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For uni directional 1 Layer LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32671a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ea487bd",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8634c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently using cpu only\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_dataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "# First find the unique phones in train.json, and then\n",
    "# create a file named vocab.txt, each line in this \n",
    "# file is a unique phone, in total there should be \n",
    "# 40 lines\n",
    "\n",
    "vocab = {}\n",
    "phonemes = []\n",
    "with open(\"vocab_39.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab[text.strip()] = id\n",
    "        phonemes.append(text)\n",
    "phonemes = phonemes[1:]\n",
    "from collections import namedtuple\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"currently using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"currently using cpu only\")\n",
    "\n",
    "args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'val_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 128,\n",
    "        'concat': 1,\n",
    "        'lr': 0.5,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "args = namedtuple('x', args)(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "056a2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can uncomment the following line and change model path to the model you want to decode\n",
    "model_path=\"checkpoints/20231206_144057/model_20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1cf069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_regularisation_dropout_between_layer\n",
    "args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "args = namedtuple('x', args)(**args)\n",
    "model = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "087018a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints/20231206_144057/model_20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (lstm): LSTM(23, 128, num_layers=2, bidirectional=True)\n",
       "  (proj): Linear(in_features=256, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print('Loading model from {}'.format(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8f075e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB: 14.44%, DEL: 6.58%, INS: 2.93%, COR: 78.98%, PER: 23.95%\n"
     ]
    }
   ],
   "source": [
    "from decoder import decode\n",
    "results = decode(model, args, args.test_json)\n",
    "print(\"SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74253846",
   "metadata": {},
   "source": [
    "# Decode With Blank Penalty -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e6a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently using cpu only\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_dataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "# First find the unique phones in train.json, and then\n",
    "# create a file named vocab.txt, each line in this \n",
    "# file is a unique phone, in total there should be \n",
    "# 40 lines\n",
    "\n",
    "vocab = {}\n",
    "phonemes = []\n",
    "with open(\"vocab_39.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab[text.strip()] = id\n",
    "        phonemes.append(text)\n",
    "phonemes = phonemes[1:]\n",
    "from collections import namedtuple\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"currently using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"currently using cpu only\")\n",
    "\n",
    "args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'val_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 128,\n",
    "        'concat': 1,\n",
    "        'lr': 0.5,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "args = namedtuple('x', args)(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0eb4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can uncomment the following line and change model path to the model you want to decode\n",
    "model_path=\"checkpoints/20231206_144057/model_20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba2effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_regularisation_dropout_between_layer\n",
    "args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "args = namedtuple('x', args)(**args)\n",
    "model = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49047903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints/20231206_144057/model_20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (lstm): LSTM(23, 128, num_layers=2, bidirectional=True)\n",
       "  (proj): Linear(in_features=256, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print('Loading model from {}'.format(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c26455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import decode\n",
    "results = decode(model, args, args.test_json)\n",
    "print(\"SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b1ec7",
   "metadata": {},
   "source": [
    "# -0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581249d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently using cpu only\n",
      "Loading model from checkpoints/20231206_144057/model_20\n",
      "SUB: 14.47%, DEL: 6.53%, INS: 2.95%, COR: 78.99%, PER: 23.95%\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_dataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "# First find the unique phones in train.json, and then\n",
    "# create a file named vocab.txt, each line in this \n",
    "# file is a unique phone, in total there should be \n",
    "# 40 lines\n",
    "\n",
    "vocab = {}\n",
    "phonemes = []\n",
    "with open(\"vocab_39.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab[text.strip()] = id\n",
    "        phonemes.append(text)\n",
    "phonemes = phonemes[1:]\n",
    "from collections import namedtuple\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"currently using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"currently using cpu only\")\n",
    "### You can uncomment the following line and change model path to the model you want to decode\n",
    "model_path=\"checkpoints/20231206_144057/model_20\"\n",
    "import model_regularisation_dropout_between_layer\n",
    "args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "args = namedtuple('x', args)(**args)\n",
    "model = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab),0)\n",
    "import torch\n",
    "print('Loading model from {}'.format(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "from decoder import decode\n",
    "results = decode(model, args, args.test_json)\n",
    "print(\"SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049ee79",
   "metadata": {},
   "source": [
    "# -0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2d21ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently using cpu only\n",
      "Loading model from checkpoints/20231206_144057/model_20\n",
      "SUB: 14.49%, DEL: 6.51%, INS: 2.98%, COR: 79.01%, PER: 23.97%\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_dataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "# First find the unique phones in train.json, and then\n",
    "# create a file named vocab.txt, each line in this \n",
    "# file is a unique phone, in total there should be \n",
    "# 40 lines\n",
    "\n",
    "vocab = {}\n",
    "phonemes = []\n",
    "with open(\"vocab_39.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab[text.strip()] = id\n",
    "        phonemes.append(text)\n",
    "phonemes = phonemes[1:]\n",
    "from collections import namedtuple\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"currently using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"currently using cpu only\")\n",
    "### You can uncomment the following line and change model path to the model you want to decode\n",
    "model_path=\"checkpoints/20231206_144057/model_20\"\n",
    "import model_regularisation_dropout_between_layer\n",
    "args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "args = namedtuple('x', args)(**args)\n",
    "model = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab),0)\n",
    "import torch\n",
    "print('Loading model from {}'.format(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "from decoder import decode\n",
    "results = decode(model, args, args.test_json)\n",
    "print(\"SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8c349",
   "metadata": {},
   "source": [
    "# -0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb9cf71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently using cpu only\n",
      "Loading model from checkpoints/20231206_144057/model_20\n",
      "SUB: 14.49%, DEL: 6.49%, INS: 2.98%, COR: 79.02%, PER: 23.95%\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_dataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "# First find the unique phones in train.json, and then\n",
    "# create a file named vocab.txt, each line in this \n",
    "# file is a unique phone, in total there should be \n",
    "# 40 lines\n",
    "\n",
    "vocab = {}\n",
    "phonemes = []\n",
    "with open(\"vocab_39.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab[text.strip()] = id\n",
    "        phonemes.append(text)\n",
    "phonemes = phonemes[1:]\n",
    "from collections import namedtuple\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"currently using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"currently using cpu only\")\n",
    "### You can uncomment the following line and change model path to the model you want to decode\n",
    "model_path=\"checkpoints/20231206_144057/model_20\"\n",
    "import model_regularisation_dropout_between_layer\n",
    "args = {'seed': 123,\n",
    "            'train_json': 'train_fbank.json',\n",
    "            'val_json': 'dev_fbank.json',\n",
    "            'test_json': 'test_fbank.json',\n",
    "            'batch_size': 4,\n",
    "            'num_layers': 2,\n",
    "            'fbank_dims': 23,\n",
    "            'model_dims': 128,\n",
    "            'concat': 1,\n",
    "            'lr': 0.5,\n",
    "            'vocab': vocab,\n",
    "            'report_interval': 50,\n",
    "            'num_epochs': 20,\n",
    "            'device': device,\n",
    "           }\n",
    "\n",
    "args = namedtuple('x', args)(**args)\n",
    "model = model_regularisation_dropout_between_layer.BiLSTM(2, args.fbank_dims * args.concat, args.model_dims, len(args.vocab),0)\n",
    "import torch\n",
    "print('Loading model from {}'.format(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "from decoder import decode\n",
    "results = decode(model, args, args.test_json)\n",
    "print(\"SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%\".format(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ed4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2721a934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dacbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d770855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b012cea7",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670f4f8",
   "metadata": {},
   "source": [
    "## First set: Using Adam optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c1dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0]\n",
    "Optimiser = [\"Adam\"]\n",
    "Starting_Learning_rate = [0.0005, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff737b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning For wider 1 Layer LSTM Using Adam Optimiser\n",
      "Currently using Adam optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 7.185514311790467\n",
      "  batch 100 loss: 3.1382794761657715\n",
      "  batch 150 loss: 2.773013391494751\n",
      "  batch 200 loss: 2.4820990467071535\n",
      "  batch 250 loss: 2.2901057195663452\n",
      "  batch 300 loss: 2.1064032411575315\n",
      "  batch 350 loss: 1.9516385650634767\n",
      "  batch 400 loss: 1.945366744995117\n",
      "  batch 450 loss: 1.8494585585594177\n",
      "  batch 500 loss: 1.7611941242218017\n",
      "  batch 550 loss: 1.7059362936019897\n",
      "  batch 600 loss: 1.6795011687278747\n",
      "  batch 650 loss: 1.6228000044822692\n",
      "  batch 700 loss: 1.6349195981025695\n",
      "  batch 750 loss: 1.583092019557953\n",
      "  batch 800 loss: 1.5757031726837158\n",
      "  batch 850 loss: 1.541705322265625\n",
      "  batch 900 loss: 1.5175226902961731\n",
      "LOSS train 1.51752 valid 1.53086, valid PER 55.17%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.4617726612091064\n",
      "  batch 100 loss: 1.4255144119262695\n",
      "  batch 150 loss: 1.3737903308868409\n",
      "  batch 200 loss: 1.4083413505554199\n",
      "  batch 250 loss: 1.403373339176178\n",
      "  batch 300 loss: 1.362054054737091\n",
      "  batch 350 loss: 1.2908964490890502\n",
      "  batch 400 loss: 1.3088325548171997\n",
      "  batch 450 loss: 1.2539366579055786\n",
      "  batch 500 loss: 1.2984623765945436\n",
      "  batch 550 loss: 1.2917373919486999\n",
      "  batch 600 loss: 1.251425071954727\n",
      "  batch 650 loss: 1.2619733953475951\n",
      "  batch 700 loss: 1.263844439983368\n",
      "  batch 750 loss: 1.2342570209503174\n",
      "  batch 800 loss: 1.1761396634578705\n",
      "  batch 850 loss: 1.188772360086441\n",
      "  batch 900 loss: 1.2230451893806458\n",
      "LOSS train 1.22305 valid 1.22102, valid PER 41.31%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.1621449899673462\n",
      "  batch 100 loss: 1.1408043897151947\n",
      "  batch 150 loss: 1.121768629550934\n",
      "  batch 200 loss: 1.1246998381614686\n",
      "  batch 250 loss: 1.1086050605773925\n",
      "  batch 300 loss: 1.101131603717804\n",
      "  batch 350 loss: 1.1414934992790222\n",
      "  batch 400 loss: 1.0981245279312133\n",
      "  batch 450 loss: 1.091650414466858\n",
      "  batch 500 loss: 1.0660707747936249\n",
      "  batch 550 loss: 1.0896803963184356\n",
      "  batch 600 loss: 1.0391671442985535\n",
      "  batch 650 loss: 1.048731231689453\n",
      "  batch 700 loss: 1.072050713300705\n",
      "  batch 750 loss: 1.1174447846412658\n",
      "  batch 800 loss: 1.0540928983688354\n",
      "  batch 850 loss: 1.0956891834735871\n",
      "  batch 900 loss: 1.019350792169571\n",
      "LOSS train 1.01935 valid 1.08253, valid PER 35.02%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.9663036644458771\n",
      "  batch 100 loss: 1.0059397804737091\n",
      "  batch 150 loss: 0.9520060074329376\n",
      "  batch 200 loss: 1.0011962842941284\n",
      "  batch 250 loss: 1.0039237332344055\n",
      "  batch 300 loss: 0.9876060605049133\n",
      "  batch 350 loss: 0.942640643119812\n",
      "  batch 400 loss: 0.9881465661525727\n",
      "  batch 450 loss: 0.9745603883266449\n",
      "  batch 500 loss: 0.9543924188613891\n",
      "  batch 550 loss: 0.982299166917801\n",
      "  batch 600 loss: 1.014651107788086\n",
      "  batch 650 loss: 0.9649605345726013\n",
      "  batch 700 loss: 0.9574592018127441\n",
      "  batch 750 loss: 0.9353800189495086\n",
      "  batch 800 loss: 0.9155508506298066\n",
      "  batch 850 loss: 0.9624210691452026\n",
      "  batch 900 loss: 0.9784070754051208\n",
      "LOSS train 0.97841 valid 1.01454, valid PER 31.50%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.8842839670181274\n",
      "  batch 100 loss: 0.8687895786762238\n",
      "  batch 150 loss: 0.9308549749851227\n",
      "  batch 200 loss: 0.8571305549144745\n",
      "  batch 250 loss: 0.8719114112854004\n",
      "  batch 300 loss: 0.8961392223834992\n",
      "  batch 350 loss: 0.8722113871574402\n",
      "  batch 400 loss: 0.8958788275718689\n",
      "  batch 450 loss: 0.8775791442394256\n",
      "  batch 500 loss: 0.8908154404163361\n",
      "  batch 550 loss: 0.8720615530014038\n",
      "  batch 600 loss: 0.9165545511245727\n",
      "  batch 650 loss: 0.8866846323013305\n",
      "  batch 700 loss: 0.9006524837017059\n",
      "  batch 750 loss: 0.8533769023418426\n",
      "  batch 800 loss: 0.8805110764503479\n",
      "  batch 850 loss: 0.8697596991062164\n",
      "  batch 900 loss: 0.8892488634586334\n",
      "LOSS train 0.88925 valid 0.97273, valid PER 30.95%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8447023785114288\n",
      "  batch 100 loss: 0.7756488627195358\n",
      "  batch 150 loss: 0.7764919102191925\n",
      "  batch 200 loss: 0.7933780908584595\n",
      "  batch 250 loss: 0.8454376006126404\n",
      "  batch 300 loss: 0.825592292547226\n",
      "  batch 350 loss: 0.8144641816616058\n",
      "  batch 400 loss: 0.7919652724266052\n",
      "  batch 450 loss: 0.8080027425289154\n",
      "  batch 500 loss: 0.8025986421108245\n",
      "  batch 550 loss: 0.821231837272644\n",
      "  batch 600 loss: 0.7971342587471009\n",
      "  batch 650 loss: 0.8084803771972656\n",
      "  batch 700 loss: 0.8207459890842438\n",
      "  batch 750 loss: 0.7904560053348542\n",
      "  batch 800 loss: 0.806209956407547\n",
      "  batch 850 loss: 0.7872795283794403\n",
      "  batch 900 loss: 0.8213097012042999\n",
      "LOSS train 0.82131 valid 0.93564, valid PER 29.39%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.7534654253721237\n",
      "  batch 100 loss: 0.7521533119678497\n",
      "  batch 150 loss: 0.7147661167383194\n",
      "  batch 200 loss: 0.7185979050397873\n",
      "  batch 250 loss: 0.7370674037933349\n",
      "  batch 300 loss: 0.722605539560318\n",
      "  batch 350 loss: 0.7413197672367096\n",
      "  batch 400 loss: 0.7623863685131073\n",
      "  batch 450 loss: 0.7564376413822174\n",
      "  batch 500 loss: 0.7337415993213654\n",
      "  batch 550 loss: 0.7308659881353379\n",
      "  batch 600 loss: 0.7428687715530395\n",
      "  batch 650 loss: 0.7381759476661682\n",
      "  batch 700 loss: 0.7659218072891235\n",
      "  batch 750 loss: 0.7326527512073517\n",
      "  batch 800 loss: 0.7409408450126648\n",
      "  batch 850 loss: 0.7626017749309539\n",
      "  batch 900 loss: 0.7764838606119155\n",
      "LOSS train 0.77648 valid 0.94112, valid PER 29.24%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.6642262196540832\n",
      "  batch 100 loss: 0.6519543826580048\n",
      "  batch 150 loss: 0.6762972939014434\n",
      "  batch 200 loss: 0.6690135943889618\n",
      "  batch 250 loss: 0.6770055055618286\n",
      "  batch 300 loss: 0.6280205500125885\n",
      "  batch 350 loss: 0.7089224374294281\n",
      "  batch 400 loss: 0.6607786810398102\n",
      "  batch 450 loss: 0.6840857684612274\n",
      "  batch 500 loss: 0.7062543857097626\n",
      "  batch 550 loss: 0.6725780773162842\n",
      "  batch 600 loss: 0.7036194580793381\n",
      "  batch 650 loss: 0.7131770706176758\n",
      "  batch 700 loss: 0.6792538541555405\n",
      "  batch 750 loss: 0.6872516405582428\n",
      "  batch 800 loss: 0.6777393692731857\n",
      "  batch 850 loss: 0.6921831893920899\n",
      "  batch 900 loss: 0.7166562819480896\n",
      "LOSS train 0.71666 valid 0.90324, valid PER 27.62%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.5825305902957916\n",
      "  batch 100 loss: 0.594835970401764\n",
      "  batch 150 loss: 0.6125135743618011\n",
      "  batch 200 loss: 0.5753634250164033\n",
      "  batch 250 loss: 0.6355270308256149\n",
      "  batch 300 loss: 0.6176143473386765\n",
      "  batch 350 loss: 0.652593988776207\n",
      "  batch 400 loss: 0.607263605594635\n",
      "  batch 450 loss: 0.6280892419815064\n",
      "  batch 500 loss: 0.6224598222970963\n",
      "  batch 550 loss: 0.65078169465065\n",
      "  batch 600 loss: 0.6516430968046188\n",
      "  batch 650 loss: 0.6328893387317658\n",
      "  batch 700 loss: 0.6094748252630233\n",
      "  batch 750 loss: 0.6307970345020294\n",
      "  batch 800 loss: 0.6383223456144332\n",
      "  batch 850 loss: 0.6472534567117691\n",
      "  batch 900 loss: 0.6025259912014007\n",
      "LOSS train 0.60253 valid 0.90684, valid PER 26.95%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.536279730796814\n",
      "  batch 100 loss: 0.5272876697778702\n",
      "  batch 150 loss: 0.566297112107277\n",
      "  batch 200 loss: 0.5712231981754303\n",
      "  batch 250 loss: 0.5862035471200943\n",
      "  batch 300 loss: 0.5371846139431\n",
      "  batch 350 loss: 0.5698265743255615\n",
      "  batch 400 loss: 0.5517715841531754\n",
      "  batch 450 loss: 0.5627781558036804\n",
      "  batch 500 loss: 0.576231672167778\n",
      "  batch 550 loss: 0.5937057572603226\n",
      "  batch 600 loss: 0.5691654181480408\n",
      "  batch 650 loss: 0.5566342705488205\n",
      "  batch 700 loss: 0.5961235880851745\n",
      "  batch 750 loss: 0.5779548913240433\n",
      "  batch 800 loss: 0.5815903854370117\n",
      "  batch 850 loss: 0.5847692930698395\n",
      "  batch 900 loss: 0.6149152153730393\n",
      "LOSS train 0.61492 valid 0.89137, valid PER 26.90%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.47907091975212096\n",
      "  batch 100 loss: 0.4711704915761948\n",
      "  batch 150 loss: 0.4887977880239487\n",
      "  batch 200 loss: 0.5178841203451157\n",
      "  batch 250 loss: 0.523654751777649\n",
      "  batch 300 loss: 0.4851668137311935\n",
      "  batch 350 loss: 0.5151596522331238\n",
      "  batch 400 loss: 0.5394270515441895\n",
      "  batch 450 loss: 0.5258828341960907\n",
      "  batch 500 loss: 0.506553019285202\n",
      "  batch 550 loss: 0.532591198682785\n",
      "  batch 600 loss: 0.5171012753248214\n",
      "  batch 650 loss: 0.5637304937839508\n",
      "  batch 700 loss: 0.5114811944961548\n",
      "  batch 750 loss: 0.5385447418689728\n",
      "  batch 800 loss: 0.5556611430644989\n",
      "  batch 850 loss: 0.5566507858037949\n",
      "  batch 900 loss: 0.5577361333370209\n",
      "LOSS train 0.55774 valid 0.91526, valid PER 26.76%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.4501932698488236\n",
      "  batch 100 loss: 0.4477584129571915\n",
      "  batch 150 loss: 0.4220507031679153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.44970256119966506\n",
      "  batch 250 loss: 0.46298507452011106\n",
      "  batch 300 loss: 0.4530462634563446\n",
      "  batch 350 loss: 0.4568026512861252\n",
      "  batch 400 loss: 0.47578509509563444\n",
      "  batch 450 loss: 0.4580912682414055\n",
      "  batch 500 loss: 0.4729467940330505\n",
      "  batch 550 loss: 0.4466679900884628\n",
      "  batch 600 loss: 0.47028467297554016\n",
      "  batch 650 loss: 0.5095950663089752\n",
      "  batch 700 loss: 0.4962752741575241\n",
      "  batch 750 loss: 0.4787877720594406\n",
      "  batch 800 loss: 0.47444204419851305\n",
      "  batch 850 loss: 0.541334348320961\n",
      "  batch 900 loss: 0.5161500668525696\n",
      "LOSS train 0.51615 valid 0.91884, valid PER 26.32%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.4069992509484291\n",
      "  batch 100 loss: 0.40771480947732924\n",
      "  batch 150 loss: 0.4096907499432564\n",
      "  batch 200 loss: 0.41857319951057437\n",
      "  batch 250 loss: 0.4089272791147232\n",
      "  batch 300 loss: 0.40105365991592407\n",
      "  batch 350 loss: 0.4133379566669464\n",
      "  batch 400 loss: 0.4304838877916336\n",
      "  batch 450 loss: 0.44486793607473374\n",
      "  batch 500 loss: 0.4149145808815956\n",
      "  batch 550 loss: 0.4356935161352158\n",
      "  batch 600 loss: 0.4423330110311508\n",
      "  batch 650 loss: 0.4403754377365112\n",
      "  batch 700 loss: 0.4580890852212906\n",
      "  batch 750 loss: 0.41753750026226044\n",
      "  batch 800 loss: 0.4348622238636017\n",
      "  batch 850 loss: 0.4523045307397842\n",
      "  batch 900 loss: 0.4639377504587173\n",
      "LOSS train 0.46394 valid 0.94701, valid PER 26.30%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.36043881326913835\n",
      "  batch 100 loss: 0.36313399791717527\n",
      "  batch 150 loss: 0.3537624952197075\n",
      "  batch 200 loss: 0.34703801602125167\n",
      "  batch 250 loss: 0.3670991089940071\n",
      "  batch 300 loss: 0.40186486452817916\n",
      "  batch 350 loss: 0.36570715010166166\n",
      "  batch 400 loss: 0.3883755671977997\n",
      "  batch 450 loss: 0.3854802483320236\n",
      "  batch 500 loss: 0.38485453844070433\n",
      "  batch 550 loss: 0.4064845731854439\n",
      "  batch 600 loss: 0.3740769854187965\n",
      "  batch 650 loss: 0.39278611570596694\n",
      "  batch 700 loss: 0.41831603527069094\n",
      "  batch 750 loss: 0.39713972717523577\n",
      "  batch 800 loss: 0.3933254215121269\n",
      "  batch 850 loss: 0.4118273496627808\n",
      "  batch 900 loss: 0.4034065026044846\n",
      "LOSS train 0.40341 valid 0.97662, valid PER 26.50%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.3169768971204758\n",
      "  batch 100 loss: 0.31362715244293216\n",
      "  batch 150 loss: 0.3257606127858162\n",
      "  batch 200 loss: 0.32662329345941543\n",
      "  batch 250 loss: 0.359202378988266\n",
      "  batch 300 loss: 0.3271208828687668\n",
      "  batch 350 loss: 0.3399152597784996\n",
      "  batch 400 loss: 0.35462666541337967\n",
      "  batch 450 loss: 0.34691228091716764\n",
      "  batch 500 loss: 0.33475282698869707\n",
      "  batch 550 loss: 0.33812645822763443\n",
      "  batch 600 loss: 0.3435442942380905\n",
      "  batch 650 loss: 0.36235741794109344\n",
      "  batch 700 loss: 0.36817315012216567\n",
      "  batch 750 loss: 0.379935556948185\n",
      "  batch 800 loss: 0.3420302805304527\n",
      "  batch 850 loss: 0.3471398088335991\n",
      "  batch 900 loss: 0.3753247782588005\n",
      "LOSS train 0.37532 valid 1.00635, valid PER 26.22%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.29142010569572446\n",
      "  batch 100 loss: 0.2767665407061577\n",
      "  batch 150 loss: 0.30559218883514405\n",
      "  batch 200 loss: 0.3042693743109703\n",
      "  batch 250 loss: 0.3042813527584076\n",
      "  batch 300 loss: 0.29392526745796205\n",
      "  batch 350 loss: 0.3111520549654961\n",
      "  batch 400 loss: 0.30931882351636886\n",
      "  batch 450 loss: 0.3094062373042107\n",
      "  batch 500 loss: 0.3045519974827766\n",
      "  batch 550 loss: 0.2953558185696602\n",
      "  batch 600 loss: 0.3086413612961769\n",
      "  batch 650 loss: 0.3270565590262413\n",
      "  batch 700 loss: 0.32837700337171555\n",
      "  batch 750 loss: 0.31772468268871307\n",
      "  batch 800 loss: 0.3404549264907837\n",
      "  batch 850 loss: 0.3243828409910202\n",
      "  batch 900 loss: 0.3376867738366127\n",
      "LOSS train 0.33769 valid 1.02087, valid PER 26.12%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.24211568742990494\n",
      "  batch 100 loss: 0.25202522352337836\n",
      "  batch 150 loss: 0.25450701713562013\n",
      "  batch 200 loss: 0.266298291683197\n",
      "  batch 250 loss: 0.2705014270544052\n",
      "  batch 300 loss: 0.2771275925636292\n",
      "  batch 350 loss: 0.2661115774512291\n",
      "  batch 400 loss: 0.30099675595760345\n",
      "  batch 450 loss: 0.2742067229747772\n",
      "  batch 500 loss: 0.2908699369430542\n",
      "  batch 550 loss: 0.2882635433971882\n",
      "  batch 600 loss: 0.28885635137557986\n",
      "  batch 650 loss: 0.2902312338352203\n",
      "  batch 700 loss: 0.2916713863611221\n",
      "  batch 750 loss: 0.28998568162322047\n",
      "  batch 800 loss: 0.28288811177015305\n",
      "  batch 850 loss: 0.30529631167650223\n",
      "  batch 900 loss: 0.2872930097579956\n",
      "LOSS train 0.28729 valid 1.06537, valid PER 26.38%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.22692399263381957\n",
      "  batch 100 loss: 0.2193863223493099\n",
      "  batch 150 loss: 0.24652559399604798\n",
      "  batch 200 loss: 0.24208093479275702\n",
      "  batch 250 loss: 0.24570614099502563\n",
      "  batch 300 loss: 0.2326736518740654\n",
      "  batch 350 loss: 0.24548964500427245\n",
      "  batch 400 loss: 0.24325181886553765\n",
      "  batch 450 loss: 0.24353793784976005\n",
      "  batch 500 loss: 0.24870053976774215\n",
      "  batch 550 loss: 0.2613874849677086\n",
      "  batch 600 loss: 0.2457446736097336\n",
      "  batch 650 loss: 0.2476136276125908\n",
      "  batch 700 loss: 0.2597831228375435\n",
      "  batch 750 loss: 0.24587635427713395\n",
      "  batch 800 loss: 0.26457374274730683\n",
      "  batch 850 loss: 0.2612145988643169\n",
      "  batch 900 loss: 0.27341558277606964\n",
      "LOSS train 0.27342 valid 1.08546, valid PER 26.64%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.19754404187202454\n",
      "  batch 100 loss: 0.1856399330496788\n",
      "  batch 150 loss: 0.20808714389801025\n",
      "  batch 200 loss: 0.20879583984613417\n",
      "  batch 250 loss: 0.20719073146581649\n",
      "  batch 300 loss: 0.21442587837576865\n",
      "  batch 350 loss: 0.21988097980618476\n",
      "  batch 400 loss: 0.23550878286361696\n",
      "  batch 450 loss: 0.22525896728038788\n",
      "  batch 500 loss: 0.23237534061074258\n",
      "  batch 550 loss: 0.22565498560667038\n",
      "  batch 600 loss: 0.22092083647847174\n",
      "  batch 650 loss: 0.24977785155177115\n",
      "  batch 700 loss: 0.22341276541352273\n",
      "  batch 750 loss: 0.22540604203939438\n",
      "  batch 800 loss: 0.22833814188838006\n",
      "  batch 850 loss: 0.23271950483322143\n",
      "  batch 900 loss: 0.24002905771136285\n",
      "LOSS train 0.24003 valid 1.10925, valid PER 26.07%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.17657526955008507\n",
      "  batch 100 loss: 0.18444731160998346\n",
      "  batch 150 loss: 0.17381806135177613\n",
      "  batch 200 loss: 0.17899533733725548\n",
      "  batch 250 loss: 0.1928427644073963\n",
      "  batch 300 loss: 0.21480114340782167\n",
      "  batch 350 loss: 0.17615252003073692\n",
      "  batch 400 loss: 0.20936926692724228\n",
      "  batch 450 loss: 0.20464825078845025\n",
      "  batch 500 loss: 0.19441429570317267\n",
      "  batch 550 loss: 0.22682569429278374\n",
      "  batch 600 loss: 0.19353585079312324\n",
      "  batch 650 loss: 0.22469639793038368\n",
      "  batch 700 loss: 0.20910787910223008\n",
      "  batch 750 loss: 0.19541481763124466\n",
      "  batch 800 loss: 0.22713401421904564\n",
      "  batch 850 loss: 0.22072473868727685\n",
      "  batch 900 loss: 0.21270331829786301\n",
      "LOSS train 0.21270 valid 1.15836, valid PER 26.53%\n",
      "Training finished in 10.0 minutes.\n",
      "Model saved to checkpoints/20231209_154909/model_10\n",
      "Currently using Adam optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.725419969558716\n",
      "  batch 100 loss: 3.131415491104126\n",
      "  batch 150 loss: 2.8572384023666384\n",
      "  batch 200 loss: 2.5580813550949095\n",
      "  batch 250 loss: 2.3051037788391113\n",
      "  batch 300 loss: 2.0695704102516173\n",
      "  batch 350 loss: 1.8963649272918701\n",
      "  batch 400 loss: 1.8792508220672608\n",
      "  batch 450 loss: 1.7865841937065126\n",
      "  batch 500 loss: 1.695541660785675\n",
      "  batch 550 loss: 1.6346722435951233\n",
      "  batch 600 loss: 1.5899159216880798\n",
      "  batch 650 loss: 1.5266898274421692\n",
      "  batch 700 loss: 1.5333589005470276\n",
      "  batch 750 loss: 1.4946924948692322\n",
      "  batch 800 loss: 1.4678789210319518\n",
      "  batch 850 loss: 1.45129460811615\n",
      "  batch 900 loss: 1.4254409575462341\n",
      "LOSS train 1.42544 valid 1.41620, valid PER 46.31%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.363154308795929\n",
      "  batch 100 loss: 1.3405389845371247\n",
      "  batch 150 loss: 1.2833780527114869\n",
      "  batch 200 loss: 1.3276919364929198\n",
      "  batch 250 loss: 1.3003866541385651\n",
      "  batch 300 loss: 1.2829124164581298\n",
      "  batch 350 loss: 1.2256599104404449\n",
      "  batch 400 loss: 1.2579601442813872\n",
      "  batch 450 loss: 1.2113274288177491\n",
      "  batch 500 loss: 1.2489476370811463\n",
      "  batch 550 loss: 1.264058541059494\n",
      "  batch 600 loss: 1.198660409450531\n",
      "  batch 650 loss: 1.2177889776229858\n",
      "  batch 700 loss: 1.2124919950962068\n",
      "  batch 750 loss: 1.1987437880039216\n",
      "  batch 800 loss: 1.1384559786319732\n",
      "  batch 850 loss: 1.1478799891471863\n",
      "  batch 900 loss: 1.1798109805583954\n",
      "LOSS train 1.17981 valid 1.17656, valid PER 37.69%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.1201516497135162\n",
      "  batch 100 loss: 1.0980381107330321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 150 loss: 1.0695122492313385\n",
      "  batch 200 loss: 1.087842642068863\n",
      "  batch 250 loss: 1.070361157655716\n",
      "  batch 300 loss: 1.0728024065494537\n",
      "  batch 350 loss: 1.1096388936042785\n",
      "  batch 400 loss: 1.0716164553165435\n",
      "  batch 450 loss: 1.0418659400939942\n",
      "  batch 500 loss: 1.055646973848343\n",
      "  batch 550 loss: 1.0472581195831299\n",
      "  batch 600 loss: 1.020816365480423\n",
      "  batch 650 loss: 1.0242637419700622\n",
      "  batch 700 loss: 1.048163491487503\n",
      "  batch 750 loss: 1.0698262739181519\n",
      "  batch 800 loss: 1.0184289014339447\n",
      "  batch 850 loss: 1.0493556261062622\n",
      "  batch 900 loss: 0.9817560005187989\n",
      "LOSS train 0.98176 valid 1.09159, valid PER 33.52%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.9546652746200561\n",
      "  batch 100 loss: 0.9660443782806396\n",
      "  batch 150 loss: 0.956738611459732\n",
      "  batch 200 loss: 0.9821926057338715\n",
      "  batch 250 loss: 0.9759292936325074\n",
      "  batch 300 loss: 0.9679156577587128\n",
      "  batch 350 loss: 0.9242408013343811\n",
      "  batch 400 loss: 0.9636321437358856\n",
      "  batch 450 loss: 0.9416297161579132\n",
      "  batch 500 loss: 0.9383600735664368\n",
      "  batch 550 loss: 0.9737054872512817\n",
      "  batch 600 loss: 0.97695307970047\n",
      "  batch 650 loss: 0.9521491670608521\n",
      "  batch 700 loss: 0.9098448836803437\n",
      "  batch 750 loss: 0.9051488363742828\n",
      "  batch 800 loss: 0.8833185029029846\n",
      "  batch 850 loss: 0.9293921184539795\n",
      "  batch 900 loss: 0.9575717926025391\n",
      "LOSS train 0.95757 valid 0.97703, valid PER 29.98%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.8423480927944184\n",
      "  batch 100 loss: 0.8392267000675201\n",
      "  batch 150 loss: 0.8994448471069336\n",
      "  batch 200 loss: 0.8190055215358734\n",
      "  batch 250 loss: 0.8625684130191803\n",
      "  batch 300 loss: 0.8622894382476807\n",
      "  batch 350 loss: 0.8566847825050354\n",
      "  batch 400 loss: 0.8585130786895752\n",
      "  batch 450 loss: 0.8326112818717957\n",
      "  batch 500 loss: 0.8689442265033722\n",
      "  batch 550 loss: 0.8231590223312378\n",
      "  batch 600 loss: 0.887968726158142\n",
      "  batch 650 loss: 0.8528735423088074\n",
      "  batch 700 loss: 0.909721667766571\n",
      "  batch 750 loss: 0.8384184122085572\n",
      "  batch 800 loss: 0.8539438807964325\n",
      "  batch 850 loss: 0.8566162455081939\n",
      "  batch 900 loss: 0.8710335540771484\n",
      "LOSS train 0.87103 valid 0.92962, valid PER 29.04%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.8106662487983703\n",
      "  batch 100 loss: 0.7535292887687683\n",
      "  batch 150 loss: 0.7532400834560394\n",
      "  batch 200 loss: 0.7808642530441284\n",
      "  batch 250 loss: 0.8145212578773499\n",
      "  batch 300 loss: 0.7924525839090347\n",
      "  batch 350 loss: 0.799416434764862\n",
      "  batch 400 loss: 0.7802509498596192\n",
      "  batch 450 loss: 0.808649195432663\n",
      "  batch 500 loss: 0.7874460363388062\n",
      "  batch 550 loss: 0.809841171503067\n",
      "  batch 600 loss: 0.7585264205932617\n",
      "  batch 650 loss: 0.8051596701145172\n",
      "  batch 700 loss: 0.8022051548957825\n",
      "  batch 750 loss: 0.7759117162227631\n",
      "  batch 800 loss: 0.7837242162227631\n",
      "  batch 850 loss: 0.7549913865327835\n",
      "  batch 900 loss: 0.811245106458664\n",
      "LOSS train 0.81125 valid 0.90321, valid PER 28.15%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.7151987361907959\n",
      "  batch 100 loss: 0.7355178618431091\n",
      "  batch 150 loss: 0.6912304764986038\n",
      "  batch 200 loss: 0.6873208749294281\n",
      "  batch 250 loss: 0.7065952700376511\n",
      "  batch 300 loss: 0.7179746448993682\n",
      "  batch 350 loss: 0.7357463037967682\n",
      "  batch 400 loss: 0.7345658242702484\n",
      "  batch 450 loss: 0.7073552411794662\n",
      "  batch 500 loss: 0.7140478760004043\n",
      "  batch 550 loss: 0.7015356993675232\n",
      "  batch 600 loss: 0.7354722291231155\n",
      "  batch 650 loss: 0.7192262142896653\n",
      "  batch 700 loss: 0.7506980133056641\n",
      "  batch 750 loss: 0.7250390207767486\n",
      "  batch 800 loss: 0.7301968771219254\n",
      "  batch 850 loss: 0.7229976803064346\n",
      "  batch 900 loss: 0.7581953573226928\n",
      "LOSS train 0.75820 valid 0.90087, valid PER 27.47%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.640317143201828\n",
      "  batch 100 loss: 0.6310415303707123\n",
      "  batch 150 loss: 0.6611171436309814\n",
      "  batch 200 loss: 0.6454256099462509\n",
      "  batch 250 loss: 0.643782793879509\n",
      "  batch 300 loss: 0.6130100953578949\n",
      "  batch 350 loss: 0.6896652615070343\n",
      "  batch 400 loss: 0.6352676838636399\n",
      "  batch 450 loss: 0.6669084441661834\n",
      "  batch 500 loss: 0.6865719038248063\n",
      "  batch 550 loss: 0.6438207864761353\n",
      "  batch 600 loss: 0.667026025056839\n",
      "  batch 650 loss: 0.6897069811820984\n",
      "  batch 700 loss: 0.6470404833555221\n",
      "  batch 750 loss: 0.652843649983406\n",
      "  batch 800 loss: 0.6732056361436843\n",
      "  batch 850 loss: 0.6850518268346787\n",
      "  batch 900 loss: 0.6838490921258926\n",
      "LOSS train 0.68385 valid 0.87877, valid PER 26.36%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.5592514872550964\n",
      "  batch 100 loss: 0.5726690137386322\n",
      "  batch 150 loss: 0.5857339882850647\n",
      "  batch 200 loss: 0.5673036515712738\n",
      "  batch 250 loss: 0.5859149730205536\n",
      "  batch 300 loss: 0.5982993006706238\n",
      "  batch 350 loss: 0.6257731336355209\n",
      "  batch 400 loss: 0.6041270434856415\n",
      "  batch 450 loss: 0.5979244965314865\n",
      "  batch 500 loss: 0.5931650042533875\n",
      "  batch 550 loss: 0.61437344789505\n",
      "  batch 600 loss: 0.6418119645118714\n",
      "  batch 650 loss: 0.6194676864147186\n",
      "  batch 700 loss: 0.6006829178333283\n",
      "  batch 750 loss: 0.620948578119278\n",
      "  batch 800 loss: 0.6335645371675491\n",
      "  batch 850 loss: 0.637030475139618\n",
      "  batch 900 loss: 0.5879464811086654\n",
      "LOSS train 0.58795 valid 0.88394, valid PER 26.22%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.5184643697738648\n",
      "  batch 100 loss: 0.5062033611536026\n",
      "  batch 150 loss: 0.5453167563676834\n",
      "  batch 200 loss: 0.5525943762063981\n",
      "  batch 250 loss: 0.5687299686670303\n",
      "  batch 300 loss: 0.5192218071222305\n",
      "  batch 350 loss: 0.5436006683111191\n",
      "  batch 400 loss: 0.5385388141870499\n",
      "  batch 450 loss: 0.527201926112175\n",
      "  batch 500 loss: 0.5461380845308303\n",
      "  batch 550 loss: 0.5993346852064133\n",
      "  batch 600 loss: 0.5485702747106552\n",
      "  batch 650 loss: 0.5390939402580261\n",
      "  batch 700 loss: 0.5668397378921509\n",
      "  batch 750 loss: 0.5719057077169418\n",
      "  batch 800 loss: 0.5637213009595871\n",
      "  batch 850 loss: 0.5895838606357574\n",
      "  batch 900 loss: 0.6025218898057938\n",
      "LOSS train 0.60252 valid 0.88060, valid PER 25.74%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.4600975090265274\n",
      "  batch 100 loss: 0.4339063203334808\n",
      "  batch 150 loss: 0.45241129338741304\n",
      "  batch 200 loss: 0.49018140316009523\n",
      "  batch 250 loss: 0.49493524968624114\n",
      "  batch 300 loss: 0.47286802530288696\n",
      "  batch 350 loss: 0.5021160525083542\n",
      "  batch 400 loss: 0.5337286740541458\n",
      "  batch 450 loss: 0.5205544888973236\n",
      "  batch 500 loss: 0.4809732913970947\n",
      "  batch 550 loss: 0.5130750495195389\n",
      "  batch 600 loss: 0.4846565759181976\n",
      "  batch 650 loss: 0.5483149468898774\n",
      "  batch 700 loss: 0.4975435757637024\n",
      "  batch 750 loss: 0.49533373713493345\n",
      "  batch 800 loss: 0.5465671050548554\n",
      "  batch 850 loss: 0.545157779455185\n",
      "  batch 900 loss: 0.5354889333248138\n",
      "LOSS train 0.53549 valid 0.89859, valid PER 25.74%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.4305065733194351\n",
      "  batch 100 loss: 0.4413445955514908\n",
      "  batch 150 loss: 0.38896760731935504\n",
      "  batch 200 loss: 0.42220607280731204\n",
      "  batch 250 loss: 0.43231695592403413\n",
      "  batch 300 loss: 0.43044267743825915\n",
      "  batch 350 loss: 0.432154044508934\n",
      "  batch 400 loss: 0.44601300299167634\n",
      "  batch 450 loss: 0.46835159182548525\n",
      "  batch 500 loss: 0.4680491292476654\n",
      "  batch 550 loss: 0.44485586673021316\n",
      "  batch 600 loss: 0.46738758027553556\n",
      "  batch 650 loss: 0.4852295631170273\n",
      "  batch 700 loss: 0.4777823120355606\n",
      "  batch 750 loss: 0.44027253091335294\n",
      "  batch 800 loss: 0.45682572782039643\n",
      "  batch 850 loss: 0.5094165652990341\n",
      "  batch 900 loss: 0.5042863309383392\n",
      "LOSS train 0.50429 valid 0.92634, valid PER 26.55%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.3761662718653679\n",
      "  batch 100 loss: 0.38130104273557663\n",
      "  batch 150 loss: 0.3791240784525871\n",
      "  batch 200 loss: 0.39302189767360685\n",
      "  batch 250 loss: 0.38448890030384064\n",
      "  batch 300 loss: 0.4049025949835777\n",
      "  batch 350 loss: 0.4024113643169403\n",
      "  batch 400 loss: 0.4014155286550522\n",
      "  batch 450 loss: 0.40525991141796114\n",
      "  batch 500 loss: 0.39624165594577787\n",
      "  batch 550 loss: 0.42689513981342314\n",
      "  batch 600 loss: 0.42190460324287415\n",
      "  batch 650 loss: 0.4201537597179413\n",
      "  batch 700 loss: 0.4506070759892464\n",
      "  batch 750 loss: 0.4076284506917\n",
      "  batch 800 loss: 0.4299733364582062\n",
      "  batch 850 loss: 0.4367478573322296\n",
      "  batch 900 loss: 0.45273394346237184\n",
      "LOSS train 0.45273 valid 0.94707, valid PER 25.94%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.34842492908239364\n",
      "  batch 100 loss: 0.3352781018614769\n",
      "  batch 150 loss: 0.35479688704013823\n",
      "  batch 200 loss: 0.32671478420495986\n",
      "  batch 250 loss: 0.33780102550983426\n",
      "  batch 300 loss: 0.37652736514806745\n",
      "  batch 350 loss: 0.34633899718523026\n",
      "  batch 400 loss: 0.36345664024353025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 450 loss: 0.36926706016063693\n",
      "  batch 500 loss: 0.3810656416416168\n",
      "  batch 550 loss: 0.39444401413202285\n",
      "  batch 600 loss: 0.3584222912788391\n",
      "  batch 650 loss: 0.38745640516281127\n",
      "  batch 700 loss: 0.4174047869443893\n",
      "  batch 750 loss: 0.39436991691589357\n",
      "  batch 800 loss: 0.3808287978172302\n",
      "  batch 850 loss: 0.39787999629974363\n",
      "  batch 900 loss: 0.4058504810929298\n",
      "LOSS train 0.40585 valid 0.97322, valid PER 26.36%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.2941726258397102\n",
      "  batch 100 loss: 0.30057130694389345\n",
      "  batch 150 loss: 0.3000204434990883\n",
      "  batch 200 loss: 0.3247966220974922\n",
      "  batch 250 loss: 0.3340429958701134\n",
      "  batch 300 loss: 0.322901716530323\n",
      "  batch 350 loss: 0.341340546309948\n",
      "  batch 400 loss: 0.3358198544383049\n",
      "  batch 450 loss: 0.33072943091392515\n",
      "  batch 500 loss: 0.3240824156999588\n",
      "  batch 550 loss: 0.36176232069730757\n",
      "  batch 600 loss: 0.35114766895771027\n",
      "  batch 650 loss: 0.3501885625720024\n",
      "  batch 700 loss: 0.37811822682619095\n",
      "  batch 750 loss: 0.35144683986902236\n",
      "  batch 800 loss: 0.3466771709918976\n",
      "  batch 850 loss: 0.3373114958405495\n",
      "  batch 900 loss: 0.3567378440499306\n",
      "LOSS train 0.35674 valid 0.98792, valid PER 25.78%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.28477085024118426\n",
      "  batch 100 loss: 0.27738795429468155\n",
      "  batch 150 loss: 0.2950402468442917\n",
      "  batch 200 loss: 0.27355539441108706\n",
      "  batch 250 loss: 0.2908210420608521\n",
      "  batch 300 loss: 0.27741664469242094\n",
      "  batch 350 loss: 0.29653753876686095\n",
      "  batch 400 loss: 0.30301261484622954\n",
      "  batch 450 loss: 0.3028309080004692\n",
      "  batch 500 loss: 0.2894231250882149\n",
      "  batch 550 loss: 0.29694246292114257\n",
      "  batch 600 loss: 0.2961953601241112\n",
      "  batch 650 loss: 0.3096806302666664\n",
      "  batch 700 loss: 0.31413283735513686\n",
      "  batch 750 loss: 0.3126352393627167\n",
      "  batch 800 loss: 0.33506606876850126\n",
      "  batch 850 loss: 0.3291694262623787\n",
      "  batch 900 loss: 0.32342095136642457\n",
      "LOSS train 0.32342 valid 1.02618, valid PER 26.07%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.23425740838050843\n",
      "  batch 100 loss: 0.24743182018399237\n",
      "  batch 150 loss: 0.2504065552353859\n",
      "  batch 200 loss: 0.2443105798959732\n",
      "  batch 250 loss: 0.25254046499729155\n",
      "  batch 300 loss: 0.2638368555903435\n",
      "  batch 350 loss: 0.2534449329972267\n",
      "  batch 400 loss: 0.2816715270280838\n",
      "  batch 450 loss: 0.27371836215257644\n",
      "  batch 500 loss: 0.2760627222061157\n",
      "  batch 550 loss: 0.277218736410141\n",
      "  batch 600 loss: 0.27312047094106673\n",
      "  batch 650 loss: 0.28724786788225176\n",
      "  batch 700 loss: 0.28470078453421593\n",
      "  batch 750 loss: 0.2828206357359886\n",
      "  batch 800 loss: 0.29508770555257796\n",
      "  batch 850 loss: 0.3067024128139019\n",
      "  batch 900 loss: 0.29262507021427153\n",
      "LOSS train 0.29263 valid 1.05056, valid PER 26.50%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.21774589017033577\n",
      "  batch 100 loss: 0.19623422369360924\n",
      "  batch 150 loss: 0.234358911216259\n",
      "  batch 200 loss: 0.24429567649960518\n",
      "  batch 250 loss: 0.2448965722322464\n",
      "  batch 300 loss: 0.22681368619203568\n",
      "  batch 350 loss: 0.2460688491165638\n",
      "  batch 400 loss: 0.241080372184515\n",
      "  batch 450 loss: 0.2426147210597992\n",
      "  batch 500 loss: 0.2282794439792633\n",
      "  batch 550 loss: 0.264714435338974\n",
      "  batch 600 loss: 0.25538449198007585\n",
      "  batch 650 loss: 0.24433904021978378\n",
      "  batch 700 loss: 0.2704722464084625\n",
      "  batch 750 loss: 0.25103634014725684\n",
      "  batch 800 loss: 0.2620074224472046\n",
      "  batch 850 loss: 0.2607919093966484\n",
      "  batch 900 loss: 0.2682441899180412\n",
      "LOSS train 0.26824 valid 1.07572, valid PER 26.21%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.1956179764866829\n",
      "  batch 100 loss: 0.18601743400096893\n",
      "  batch 150 loss: 0.20480158433318138\n",
      "  batch 200 loss: 0.20745960459113122\n",
      "  batch 250 loss: 0.20965509966015816\n",
      "  batch 300 loss: 0.21384837433695794\n",
      "  batch 350 loss: 0.22470004469156266\n",
      "  batch 400 loss: 0.23024687334895133\n",
      "  batch 450 loss: 0.23654553890228272\n",
      "  batch 500 loss: 0.23486268013715744\n",
      "  batch 550 loss: 0.22452129259705544\n",
      "  batch 600 loss: 0.22559122771024703\n",
      "  batch 650 loss: 0.25192727625370026\n",
      "  batch 700 loss: 0.2265750312805176\n",
      "  batch 750 loss: 0.23117398500442504\n",
      "  batch 800 loss: 0.25214836359024045\n",
      "  batch 850 loss: 0.2529551011323929\n",
      "  batch 900 loss: 0.26225892156362535\n",
      "LOSS train 0.26226 valid 1.11912, valid PER 26.32%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.1967095999419689\n",
      "  batch 100 loss: 0.1815583412349224\n",
      "  batch 150 loss: 0.1765895402431488\n",
      "  batch 200 loss: 0.1858421641588211\n",
      "  batch 250 loss: 0.18658380806446076\n",
      "  batch 300 loss: 0.21041705042123796\n",
      "  batch 350 loss: 0.19368897572159768\n",
      "  batch 400 loss: 0.20529231131076814\n",
      "  batch 450 loss: 0.20814658671617509\n",
      "  batch 500 loss: 0.1969002465903759\n",
      "  batch 550 loss: 0.2190474085509777\n",
      "  batch 600 loss: 0.22419836565852166\n",
      "  batch 650 loss: 0.21953530639410018\n",
      "  batch 700 loss: 0.23476379215717316\n",
      "  batch 750 loss: 0.21074474945664406\n",
      "  batch 800 loss: 0.23596254318952561\n",
      "  batch 850 loss: 0.22065778762102128\n",
      "  batch 900 loss: 0.2424957886338234\n",
      "LOSS train 0.24250 valid 1.13718, valid PER 25.75%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231209_155944/model_8\n",
      "End tuning For Wider 1 Layer LSTM For Adam optimiser\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start tuning For wider 1 Layer LSTM Using Adam Optimiser\")\n",
    "\n",
    "for starting_lr in Starting_Learning_rate:\n",
    "    print(\"Currently using Adam optimiser\")\n",
    "    args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'val_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 512,\n",
    "        'concat': 1,\n",
    "        'lr': starting_lr,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "    args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_regularisation_dropout.BiLSTM(args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        model_path = adam_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "print(\"End tuning For Wider 1 Layer LSTM For Adam optimiser\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35ee66",
   "metadata": {},
   "source": [
    "## Second set: Using SGD Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01135724",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0]\n",
    "Optimiser = [\"SGD_Scheduler\"]\n",
    "Starting_Learning_rate = [0.5, 0.7, 1.0, 1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2caafec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning For wider 1 Layer LSTM Using SGD Optimiser\n",
      "Currently using SGD optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 5.084213075637817\n",
      "  batch 100 loss: 3.279260263442993\n",
      "  batch 150 loss: 3.090498342514038\n",
      "  batch 200 loss: 2.8440057468414306\n",
      "  batch 250 loss: 2.6850899982452394\n",
      "  batch 300 loss: 2.517392535209656\n",
      "  batch 350 loss: 2.4043646907806395\n",
      "  batch 400 loss: 2.373643870353699\n",
      "  batch 450 loss: 2.2924081897735595\n",
      "  batch 500 loss: 2.179419894218445\n",
      "  batch 550 loss: 2.1380615186691285\n",
      "  batch 600 loss: 2.071744055747986\n",
      "  batch 650 loss: 1.9753476119041442\n",
      "  batch 700 loss: 1.9737450432777406\n",
      "  batch 750 loss: 1.9024597954750062\n",
      "  batch 800 loss: 1.8859078216552734\n",
      "  batch 850 loss: 1.8554858541488648\n",
      "  batch 900 loss: 1.8198056840896606\n",
      "LOSS train 1.81981 valid 1.76480, valid PER 66.21%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.774866716861725\n",
      "  batch 100 loss: 1.708726739883423\n",
      "  batch 150 loss: 1.6797557306289672\n",
      "  batch 200 loss: 1.6890949058532714\n",
      "  batch 250 loss: 1.6788325667381288\n",
      "  batch 300 loss: 1.641678683757782\n",
      "  batch 350 loss: 1.564692542552948\n",
      "  batch 400 loss: 1.5646648001670838\n",
      "  batch 450 loss: 1.5259839153289796\n",
      "  batch 500 loss: 1.560435094833374\n",
      "  batch 550 loss: 1.5381787252426147\n",
      "  batch 600 loss: 1.4999614572525024\n",
      "  batch 650 loss: 1.5193086338043214\n",
      "  batch 700 loss: 1.4834504437446594\n",
      "  batch 750 loss: 1.465515100955963\n",
      "  batch 800 loss: 1.4064817547798156\n",
      "  batch 850 loss: 1.4144221234321595\n",
      "  batch 900 loss: 1.4235394740104674\n",
      "LOSS train 1.42354 valid 1.39754, valid PER 46.59%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.3950368285179138\n",
      "  batch 100 loss: 1.3455301356315612\n",
      "  batch 150 loss: 1.328262016773224\n",
      "  batch 200 loss: 1.3074436068534852\n",
      "  batch 250 loss: 1.29878586769104\n",
      "  batch 300 loss: 1.2882893538475038\n",
      "  batch 350 loss: 1.3350280451774597\n",
      "  batch 400 loss: 1.3005910682678223\n",
      "  batch 450 loss: 1.2540192139148711\n",
      "  batch 500 loss: 1.2280860030651093\n",
      "  batch 550 loss: 1.2500668680667877\n",
      "  batch 600 loss: 1.2265009927749633\n",
      "  batch 650 loss: 1.1996959173679351\n",
      "  batch 700 loss: 1.2190449464321136\n",
      "  batch 750 loss: 1.2725052344799042\n",
      "  batch 800 loss: 1.1912493419647217\n",
      "  batch 850 loss: 1.224803774356842\n",
      "  batch 900 loss: 1.1466201984882354\n",
      "LOSS train 1.14662 valid 1.21064, valid PER 36.56%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.1321044337749482\n",
      "  batch 100 loss: 1.155317177772522\n",
      "  batch 150 loss: 1.1147885501384736\n",
      "  batch 200 loss: 1.155358065366745\n",
      "  batch 250 loss: 1.1433393156528473\n",
      "  batch 300 loss: 1.150873420238495\n",
      "  batch 350 loss: 1.0794710183143617\n",
      "  batch 400 loss: 1.1241044175624848\n",
      "  batch 450 loss: 1.1003001022338867\n",
      "  batch 500 loss: 1.0813414549827576\n",
      "  batch 550 loss: 1.132149523496628\n",
      "  batch 600 loss: 1.116741269826889\n",
      "  batch 650 loss: 1.0953951144218446\n",
      "  batch 700 loss: 1.080310720205307\n",
      "  batch 750 loss: 1.058819363117218\n",
      "  batch 800 loss: 1.032014787197113\n",
      "  batch 850 loss: 1.0615078294277192\n",
      "  batch 900 loss: 1.1029619872570038\n",
      "LOSS train 1.10296 valid 1.08065, valid PER 34.10%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.0230136179924012\n",
      "  batch 100 loss: 1.0087156891822815\n",
      "  batch 150 loss: 1.0624833965301514\n",
      "  batch 200 loss: 1.006997172832489\n",
      "  batch 250 loss: 1.0048893463611603\n",
      "  batch 300 loss: 1.017228217124939\n",
      "  batch 350 loss: 1.0004379832744599\n",
      "  batch 400 loss: 1.0162275886535646\n",
      "  batch 450 loss: 0.9869532835483551\n",
      "  batch 500 loss: 1.0220304107666016\n",
      "  batch 550 loss: 0.9771855974197388\n",
      "  batch 600 loss: 1.026770133972168\n",
      "  batch 650 loss: 1.0043811988830567\n",
      "  batch 700 loss: 1.0315723085403443\n",
      "  batch 750 loss: 0.9652171456813812\n",
      "  batch 800 loss: 1.0001013040542603\n",
      "  batch 850 loss: 0.9848236644268036\n",
      "  batch 900 loss: 0.9896320259571075\n",
      "LOSS train 0.98963 valid 1.02832, valid PER 31.13%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9804727149009704\n",
      "  batch 100 loss: 0.9191338062286377\n",
      "  batch 150 loss: 0.9199764955043793\n",
      "  batch 200 loss: 0.9266960382461548\n",
      "  batch 250 loss: 0.9622386467456817\n",
      "  batch 300 loss: 0.9389383804798126\n",
      "  batch 350 loss: 0.940602308511734\n",
      "  batch 400 loss: 0.9268565642833709\n",
      "  batch 450 loss: 0.9475025820732117\n",
      "  batch 500 loss: 0.9321019947528839\n",
      "  batch 550 loss: 0.9489150810241699\n",
      "  batch 600 loss: 0.9159305250644684\n",
      "  batch 650 loss: 0.9346032667160035\n",
      "  batch 700 loss: 0.9440105211734772\n",
      "  batch 750 loss: 0.9146073651313782\n",
      "  batch 800 loss: 0.9179368579387664\n",
      "  batch 850 loss: 0.8920537173748017\n",
      "  batch 900 loss: 0.9251265966892243\n",
      "LOSS train 0.92513 valid 1.01296, valid PER 30.74%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8973075997829437\n",
      "  batch 100 loss: 0.8981966590881347\n",
      "  batch 150 loss: 0.8623222005367279\n",
      "  batch 200 loss: 0.8536581856012344\n",
      "  batch 250 loss: 0.8593496036529541\n",
      "  batch 300 loss: 0.8518474519252777\n",
      "  batch 350 loss: 0.8721547317504883\n",
      "  batch 400 loss: 0.8663663113117218\n",
      "  batch 450 loss: 0.8669855093955994\n",
      "  batch 500 loss: 0.865008944272995\n",
      "  batch 550 loss: 0.8550443732738495\n",
      "  batch 600 loss: 0.888579705953598\n",
      "  batch 650 loss: 0.8590791559219361\n",
      "  batch 700 loss: 0.8906621611118317\n",
      "  batch 750 loss: 0.8518853116035462\n",
      "  batch 800 loss: 0.8640688252449036\n",
      "  batch 850 loss: 0.8853121483325959\n",
      "  batch 900 loss: 0.8926422154903412\n",
      "LOSS train 0.89264 valid 0.95904, valid PER 29.48%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.799726003408432\n",
      "  batch 100 loss: 0.7921074771881104\n",
      "  batch 150 loss: 0.8167128658294678\n",
      "  batch 200 loss: 0.7970176017284394\n",
      "  batch 250 loss: 0.8108940285444259\n",
      "  batch 300 loss: 0.7666087365150451\n",
      "  batch 350 loss: 0.8401445186138153\n",
      "  batch 400 loss: 0.8099963235855102\n",
      "  batch 450 loss: 0.8179263412952423\n",
      "  batch 500 loss: 0.8472447168827056\n",
      "  batch 550 loss: 0.7695940828323364\n",
      "  batch 600 loss: 0.8410607218742371\n",
      "  batch 650 loss: 0.8504565989971161\n",
      "  batch 700 loss: 0.8013632988929749\n",
      "  batch 750 loss: 0.8130654501914978\n",
      "  batch 800 loss: 0.8224922734498977\n",
      "  batch 850 loss: 0.8218607711791992\n",
      "  batch 900 loss: 0.8394577312469482\n",
      "LOSS train 0.83946 valid 0.94902, valid PER 28.41%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.7214582455158234\n",
      "  batch 100 loss: 0.7669320315122604\n",
      "  batch 150 loss: 0.7557116127014161\n",
      "  batch 200 loss: 0.7525921189785003\n",
      "  batch 250 loss: 0.7816880023479462\n",
      "  batch 300 loss: 0.7875309717655182\n",
      "  batch 350 loss: 0.8099361503124237\n",
      "  batch 400 loss: 0.7815597188472748\n",
      "  batch 450 loss: 0.7620113921165467\n",
      "  batch 500 loss: 0.7572254711389541\n",
      "  batch 550 loss: 0.7863007509708404\n",
      "  batch 600 loss: 0.784934834241867\n",
      "  batch 650 loss: 0.780244140625\n",
      "  batch 700 loss: 0.7502901023626327\n",
      "  batch 750 loss: 0.7458859044313431\n",
      "  batch 800 loss: 0.7808954298496247\n",
      "  batch 850 loss: 0.7983850336074829\n",
      "  batch 900 loss: 0.7323280954360962\n",
      "Epoch 00009: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.73233 valid 0.95306, valid PER 27.97%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6413271772861481\n",
      "  batch 100 loss: 0.6453309834003449\n",
      "  batch 150 loss: 0.6646823889017105\n",
      "  batch 200 loss: 0.6635364490747452\n",
      "  batch 250 loss: 0.6617325669527054\n",
      "  batch 300 loss: 0.6212138915061951\n",
      "  batch 350 loss: 0.6366229295730591\n",
      "  batch 400 loss: 0.6179290968179703\n",
      "  batch 450 loss: 0.6187739223241806\n",
      "  batch 500 loss: 0.6464663106203079\n",
      "  batch 550 loss: 0.6728928899765014\n",
      "  batch 600 loss: 0.6317301726341248\n",
      "  batch 650 loss: 0.6263652288913727\n",
      "  batch 700 loss: 0.6474927008152008\n",
      "  batch 750 loss: 0.630341534614563\n",
      "  batch 800 loss: 0.6449462759494782\n",
      "  batch 850 loss: 0.6463522171974182\n",
      "  batch 900 loss: 0.6570315051078797\n",
      "LOSS train 0.65703 valid 0.87062, valid PER 26.52%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.57307736992836\n",
      "  batch 100 loss: 0.5451472449302673\n",
      "  batch 150 loss: 0.5791963881254196\n",
      "  batch 200 loss: 0.6164246690273285\n",
      "  batch 250 loss: 0.6028329509496689\n",
      "  batch 300 loss: 0.5883957266807556\n",
      "  batch 350 loss: 0.5995159304141998\n",
      "  batch 400 loss: 0.6164659976959228\n",
      "  batch 450 loss: 0.611372908949852\n",
      "  batch 500 loss: 0.5870755326747894\n",
      "  batch 550 loss: 0.6062274044752121\n",
      "  batch 600 loss: 0.5841540515422821\n",
      "  batch 650 loss: 0.6292522686719895\n",
      "  batch 700 loss: 0.575480328798294\n",
      "  batch 750 loss: 0.5869390147924424\n",
      "  batch 800 loss: 0.609484710097313\n",
      "  batch 850 loss: 0.6233140307664872\n",
      "  batch 900 loss: 0.6180747205018997\n",
      "LOSS train 0.61807 valid 0.86586, valid PER 25.65%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.5577002692222596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 0.5626668834686279\n",
      "  batch 150 loss: 0.507908553481102\n",
      "  batch 200 loss: 0.5416071891784668\n",
      "  batch 250 loss: 0.5562965106964112\n",
      "  batch 300 loss: 0.5538198417425155\n",
      "  batch 350 loss: 0.551470667719841\n",
      "  batch 400 loss: 0.5668042111396789\n",
      "  batch 450 loss: 0.5619343858957291\n",
      "  batch 500 loss: 0.5758544409275055\n",
      "  batch 550 loss: 0.5365975707769394\n",
      "  batch 600 loss: 0.5517321610450745\n",
      "  batch 650 loss: 0.5748515766859055\n",
      "  batch 700 loss: 0.5796791338920593\n",
      "  batch 750 loss: 0.5475450879335404\n",
      "  batch 800 loss: 0.5521903282403946\n",
      "  batch 850 loss: 0.6164826232194901\n",
      "  batch 900 loss: 0.5989756894111633\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.59898 valid 0.86699, valid PER 25.60%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.49491141438484193\n",
      "  batch 100 loss: 0.4981077820062637\n",
      "  batch 150 loss: 0.46924112617969516\n",
      "  batch 200 loss: 0.49934529423713686\n",
      "  batch 250 loss: 0.48229796230793\n",
      "  batch 300 loss: 0.4832914388179779\n",
      "  batch 350 loss: 0.4753624075651169\n",
      "  batch 400 loss: 0.4918773066997528\n",
      "  batch 450 loss: 0.4792683947086334\n",
      "  batch 500 loss: 0.4722419899702072\n",
      "  batch 550 loss: 0.5103842318058014\n",
      "  batch 600 loss: 0.47146714448928834\n",
      "  batch 650 loss: 0.49475405931472777\n",
      "  batch 700 loss: 0.5127209687232971\n",
      "  batch 750 loss: 0.4578361624479294\n",
      "  batch 800 loss: 0.47305252730846403\n",
      "  batch 850 loss: 0.5054605063796044\n",
      "  batch 900 loss: 0.5162622624635697\n",
      "LOSS train 0.51626 valid 0.85902, valid PER 25.17%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.44491163969039915\n",
      "  batch 100 loss: 0.45205968856811524\n",
      "  batch 150 loss: 0.45654902666807173\n",
      "  batch 200 loss: 0.4399463421106338\n",
      "  batch 250 loss: 0.4527835714817047\n",
      "  batch 300 loss: 0.48120455861091616\n",
      "  batch 350 loss: 0.42992778301239015\n",
      "  batch 400 loss: 0.46065581411123274\n",
      "  batch 450 loss: 0.4607035517692566\n",
      "  batch 500 loss: 0.45775673031806946\n",
      "  batch 550 loss: 0.47445326268672944\n",
      "  batch 600 loss: 0.4403473237156868\n",
      "  batch 650 loss: 0.46214754462242125\n",
      "  batch 700 loss: 0.4908561962842941\n",
      "  batch 750 loss: 0.4463615995645523\n",
      "  batch 800 loss: 0.4348686072230339\n",
      "  batch 850 loss: 0.4787385141849518\n",
      "  batch 900 loss: 0.46837348997592926\n",
      "Epoch 00014: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.46837 valid 0.87836, valid PER 25.35%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.4268372851610184\n",
      "  batch 100 loss: 0.4155187374353409\n",
      "  batch 150 loss: 0.41525382697582247\n",
      "  batch 200 loss: 0.4179027259349823\n",
      "  batch 250 loss: 0.4370971119403839\n",
      "  batch 300 loss: 0.4045018610358238\n",
      "  batch 350 loss: 0.4141725128889084\n",
      "  batch 400 loss: 0.4184805202484131\n",
      "  batch 450 loss: 0.4175573471188545\n",
      "  batch 500 loss: 0.38339014947414396\n",
      "  batch 550 loss: 0.4088450211286545\n",
      "  batch 600 loss: 0.42904401004314424\n",
      "  batch 650 loss: 0.43120587557554246\n",
      "  batch 700 loss: 0.43634788393974305\n",
      "  batch 750 loss: 0.41660175144672396\n",
      "  batch 800 loss: 0.4050761514902115\n",
      "  batch 850 loss: 0.3923920863866806\n",
      "  batch 900 loss: 0.42080159425735475\n",
      "Epoch 00015: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.42080 valid 0.87544, valid PER 25.13%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.3977649217844009\n",
      "  batch 100 loss: 0.377947593331337\n",
      "  batch 150 loss: 0.38295076668262484\n",
      "  batch 200 loss: 0.38795896887779235\n",
      "  batch 250 loss: 0.39372399598360064\n",
      "  batch 300 loss: 0.38925998836755754\n",
      "  batch 350 loss: 0.39130633980035784\n",
      "  batch 400 loss: 0.39792257845401763\n",
      "  batch 450 loss: 0.4015907350182533\n",
      "  batch 500 loss: 0.3694962319731712\n",
      "  batch 550 loss: 0.38020283728837967\n",
      "  batch 600 loss: 0.3789198684692383\n",
      "  batch 650 loss: 0.38788369864225386\n",
      "  batch 700 loss: 0.38366068542003634\n",
      "  batch 750 loss: 0.3990212941169739\n",
      "  batch 800 loss: 0.40130334079265595\n",
      "  batch 850 loss: 0.37646844148635866\n",
      "  batch 900 loss: 0.40320156514644623\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.40320 valid 0.87829, valid PER 24.86%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.3706627628207207\n",
      "  batch 100 loss: 0.387055047750473\n",
      "  batch 150 loss: 0.3739761754870415\n",
      "  batch 200 loss: 0.37160574436187743\n",
      "  batch 250 loss: 0.38169557631015777\n",
      "  batch 300 loss: 0.3750025573372841\n",
      "  batch 350 loss: 0.35667914122343064\n",
      "  batch 400 loss: 0.3959518080949783\n",
      "  batch 450 loss: 0.3804959359765053\n",
      "  batch 500 loss: 0.36714431941509246\n",
      "  batch 550 loss: 0.37261327624320983\n",
      "  batch 600 loss: 0.38471799045801164\n",
      "  batch 650 loss: 0.3597851926088333\n",
      "  batch 700 loss: 0.36687761783599854\n",
      "  batch 750 loss: 0.3725594121217728\n",
      "  batch 800 loss: 0.3569296523928642\n",
      "  batch 850 loss: 0.38023572862148286\n",
      "  batch 900 loss: 0.3631150242686272\n",
      "Epoch 00017: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.36312 valid 0.88293, valid PER 24.71%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.36475868403911593\n",
      "  batch 100 loss: 0.36576213270425795\n",
      "  batch 150 loss: 0.3929289162158966\n",
      "  batch 200 loss: 0.36894616574048994\n",
      "  batch 250 loss: 0.38538460940122604\n",
      "  batch 300 loss: 0.3488568091392517\n",
      "  batch 350 loss: 0.357953388094902\n",
      "  batch 400 loss: 0.3582911029458046\n",
      "  batch 450 loss: 0.37597829967737195\n",
      "  batch 500 loss: 0.36449749499559403\n",
      "  batch 550 loss: 0.3733178618550301\n",
      "  batch 600 loss: 0.35262556463479994\n",
      "  batch 650 loss: 0.3477690151333809\n",
      "  batch 700 loss: 0.36958749026060106\n",
      "  batch 750 loss: 0.34676735013723375\n",
      "  batch 800 loss: 0.3712161499261856\n",
      "  batch 850 loss: 0.35983953982591627\n",
      "  batch 900 loss: 0.3800301820039749\n",
      "Epoch 00018: reducing learning rate of group 0 to 3.9062e-03.\n",
      "LOSS train 0.38003 valid 0.88405, valid PER 24.88%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.3660873115062714\n",
      "  batch 100 loss: 0.3566968235373497\n",
      "  batch 150 loss: 0.3570238599181175\n",
      "  batch 200 loss: 0.3628006449341774\n",
      "  batch 250 loss: 0.36323215752840043\n",
      "  batch 300 loss: 0.3670381769537926\n",
      "  batch 350 loss: 0.3503938117623329\n",
      "  batch 400 loss: 0.3592755806446075\n",
      "  batch 450 loss: 0.3770223453640938\n",
      "  batch 500 loss: 0.362985805273056\n",
      "  batch 550 loss: 0.3476020848751068\n",
      "  batch 600 loss: 0.34282801777124405\n",
      "  batch 650 loss: 0.3926160123944282\n",
      "  batch 700 loss: 0.3596101477742195\n",
      "  batch 750 loss: 0.3527715241909027\n",
      "  batch 800 loss: 0.36652968227863314\n",
      "  batch 850 loss: 0.3610138511657715\n",
      "  batch 900 loss: 0.36737997978925707\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.9531e-03.\n",
      "LOSS train 0.36738 valid 0.88491, valid PER 24.82%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.37456262290477754\n",
      "  batch 100 loss: 0.35982218623161316\n",
      "  batch 150 loss: 0.35551601618528367\n",
      "  batch 200 loss: 0.3618667060136795\n",
      "  batch 250 loss: 0.35766502350568774\n",
      "  batch 300 loss: 0.3564430445432663\n",
      "  batch 350 loss: 0.3392058444023132\n",
      "  batch 400 loss: 0.35875786453485486\n",
      "  batch 450 loss: 0.35905605882406233\n",
      "  batch 500 loss: 0.34311121940612793\n",
      "  batch 550 loss: 0.382314413189888\n",
      "  batch 600 loss: 0.3445682561397552\n",
      "  batch 650 loss: 0.3588387405872345\n",
      "  batch 700 loss: 0.3615800112485886\n",
      "  batch 750 loss: 0.338876858651638\n",
      "  batch 800 loss: 0.3832846838235855\n",
      "  batch 850 loss: 0.36717778325080874\n",
      "  batch 900 loss: 0.3691208264231682\n",
      "Epoch 00020: reducing learning rate of group 0 to 9.7656e-04.\n",
      "LOSS train 0.36912 valid 0.88549, valid PER 24.85%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231209_160825/model_13\n",
      "Currently using SGD optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.874590668678284\n",
      "  batch 100 loss: 3.2780465030670167\n",
      "  batch 150 loss: 3.000844292640686\n",
      "  batch 200 loss: 2.738853669166565\n",
      "  batch 250 loss: 2.6197555351257322\n",
      "  batch 300 loss: 2.4477523374557495\n",
      "  batch 350 loss: 2.357774419784546\n",
      "  batch 400 loss: 2.324733934402466\n",
      "  batch 450 loss: 2.200628855228424\n",
      "  batch 500 loss: 2.085264894962311\n",
      "  batch 550 loss: 2.026858069896698\n",
      "  batch 600 loss: 1.960392999649048\n",
      "  batch 650 loss: 1.859564051628113\n",
      "  batch 700 loss: 1.8729270601272583\n",
      "  batch 750 loss: 1.80668288230896\n",
      "  batch 800 loss: 1.7867854452133178\n",
      "  batch 850 loss: 1.7502900910377504\n",
      "  batch 900 loss: 1.7083862113952637\n",
      "LOSS train 1.70839 valid 1.68275, valid PER 62.43%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.6576803183555604\n",
      "  batch 100 loss: 1.6079183340072631\n",
      "  batch 150 loss: 1.5853368330001831\n",
      "  batch 200 loss: 1.5948298954963684\n",
      "  batch 250 loss: 1.5944708490371704\n",
      "  batch 300 loss: 1.549876902103424\n",
      "  batch 350 loss: 1.4745134830474853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 400 loss: 1.4955157947540283\n",
      "  batch 450 loss: 1.445053277015686\n",
      "  batch 500 loss: 1.4861087727546691\n",
      "  batch 550 loss: 1.4657780528068542\n",
      "  batch 600 loss: 1.4295513868331908\n",
      "  batch 650 loss: 1.4573638939857483\n",
      "  batch 700 loss: 1.4118761134147644\n",
      "  batch 750 loss: 1.4063637137413025\n",
      "  batch 800 loss: 1.3592495822906494\n",
      "  batch 850 loss: 1.3499480080604553\n",
      "  batch 900 loss: 1.3910088968276977\n",
      "LOSS train 1.39101 valid 1.35717, valid PER 43.47%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.3413076567649842\n",
      "  batch 100 loss: 1.2998478031158447\n",
      "  batch 150 loss: 1.2973154497146606\n",
      "  batch 200 loss: 1.271260895729065\n",
      "  batch 250 loss: 1.275689162015915\n",
      "  batch 300 loss: 1.2659638667106627\n",
      "  batch 350 loss: 1.3231982004642486\n",
      "  batch 400 loss: 1.2813828027248382\n",
      "  batch 450 loss: 1.267249881029129\n",
      "  batch 500 loss: 1.2329501593112946\n",
      "  batch 550 loss: 1.2468984973430635\n",
      "  batch 600 loss: 1.2121263718605042\n",
      "  batch 650 loss: 1.2103162705898285\n",
      "  batch 700 loss: 1.2230168390274048\n",
      "  batch 750 loss: 1.2779148626327514\n",
      "  batch 800 loss: 1.2066721296310425\n",
      "  batch 850 loss: 1.2182696950435639\n",
      "  batch 900 loss: 1.1640271246433258\n",
      "LOSS train 1.16403 valid 1.23794, valid PER 38.58%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.1466254472732544\n",
      "  batch 100 loss: 1.1605543327331542\n",
      "  batch 150 loss: 1.1095261669158936\n",
      "  batch 200 loss: 1.1539202451705932\n",
      "  batch 250 loss: 1.1405152916908263\n",
      "  batch 300 loss: 1.1450869047641754\n",
      "  batch 350 loss: 1.0734959268569946\n",
      "  batch 400 loss: 1.1104910159111023\n",
      "  batch 450 loss: 1.0966289567947387\n",
      "  batch 500 loss: 1.0923183369636535\n",
      "  batch 550 loss: 1.1149096131324767\n",
      "  batch 600 loss: 1.128034416437149\n",
      "  batch 650 loss: 1.1086803901195526\n",
      "  batch 700 loss: 1.0841211318969726\n",
      "  batch 750 loss: 1.0597650277614594\n",
      "  batch 800 loss: 1.0432696378231048\n",
      "  batch 850 loss: 1.0676418888568877\n",
      "  batch 900 loss: 1.1147294449806213\n",
      "LOSS train 1.11473 valid 1.10172, valid PER 34.03%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9979356133937836\n",
      "  batch 100 loss: 1.001265299320221\n",
      "  batch 150 loss: 1.049499475955963\n",
      "  batch 200 loss: 1.0005827224254609\n",
      "  batch 250 loss: 1.0074476957321168\n",
      "  batch 300 loss: 1.0045444774627685\n",
      "  batch 350 loss: 1.0000458657741547\n",
      "  batch 400 loss: 0.9917567217350006\n",
      "  batch 450 loss: 0.9839701390266419\n",
      "  batch 500 loss: 0.9991947841644288\n",
      "  batch 550 loss: 0.9700855755805969\n",
      "  batch 600 loss: 1.0350631844997407\n",
      "  batch 650 loss: 0.9869742333889008\n",
      "  batch 700 loss: 1.0457074213027955\n",
      "  batch 750 loss: 0.9509384024143219\n",
      "  batch 800 loss: 0.9794808721542358\n",
      "  batch 850 loss: 0.9777481412887573\n",
      "  batch 900 loss: 0.9830545103549957\n",
      "LOSS train 0.98305 valid 1.04080, valid PER 31.75%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9391448891162872\n",
      "  batch 100 loss: 0.8955446612834931\n",
      "  batch 150 loss: 0.873477635383606\n",
      "  batch 200 loss: 0.9045166635513305\n",
      "  batch 250 loss: 0.939458875656128\n",
      "  batch 300 loss: 0.9085173779726028\n",
      "  batch 350 loss: 0.9313566303253173\n",
      "  batch 400 loss: 0.9007362020015717\n",
      "  batch 450 loss: 0.9366590142250061\n",
      "  batch 500 loss: 0.9135473465919495\n",
      "  batch 550 loss: 0.9348854613304138\n",
      "  batch 600 loss: 0.9095315980911255\n",
      "  batch 650 loss: 0.9313655483722687\n",
      "  batch 700 loss: 0.919016307592392\n",
      "  batch 750 loss: 0.8971156179904938\n",
      "  batch 800 loss: 0.8917668950557709\n",
      "  batch 850 loss: 0.8883543729782104\n",
      "  batch 900 loss: 0.9102807796001434\n",
      "LOSS train 0.91028 valid 1.02861, valid PER 30.81%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8554459834098815\n",
      "  batch 100 loss: 0.8653876185417175\n",
      "  batch 150 loss: 0.8386238539218902\n",
      "  batch 200 loss: 0.8249396711587906\n",
      "  batch 250 loss: 0.8193812978267669\n",
      "  batch 300 loss: 0.8333672404289245\n",
      "  batch 350 loss: 0.8565555763244629\n",
      "  batch 400 loss: 0.842301949262619\n",
      "  batch 450 loss: 0.8558844810724259\n",
      "  batch 500 loss: 0.8528458392620086\n",
      "  batch 550 loss: 0.8261477553844452\n",
      "  batch 600 loss: 0.8481051290035247\n",
      "  batch 650 loss: 0.8439520931243897\n",
      "  batch 700 loss: 0.8629576969146728\n",
      "  batch 750 loss: 0.8264544856548309\n",
      "  batch 800 loss: 0.8261072087287903\n",
      "  batch 850 loss: 0.8457605993747711\n",
      "  batch 900 loss: 0.8705592668056488\n",
      "LOSS train 0.87056 valid 0.98792, valid PER 30.26%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.7746493136882782\n",
      "  batch 100 loss: 0.7619449210166931\n",
      "  batch 150 loss: 0.7695860946178437\n",
      "  batch 200 loss: 0.7660193181037903\n",
      "  batch 250 loss: 0.771149435043335\n",
      "  batch 300 loss: 0.7251872396469117\n",
      "  batch 350 loss: 0.8179816126823425\n",
      "  batch 400 loss: 0.7367196303606033\n",
      "  batch 450 loss: 0.795439475774765\n",
      "  batch 500 loss: 0.819058700799942\n",
      "  batch 550 loss: 0.752912580370903\n",
      "  batch 600 loss: 0.7973130249977112\n",
      "  batch 650 loss: 0.8136130499839783\n",
      "  batch 700 loss: 0.7668016427755355\n",
      "  batch 750 loss: 0.7800194227695465\n",
      "  batch 800 loss: 0.7990953630208969\n",
      "  batch 850 loss: 0.7905418491363525\n",
      "  batch 900 loss: 0.8074258100986481\n",
      "LOSS train 0.80743 valid 0.97053, valid PER 28.09%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.6801646596193314\n",
      "  batch 100 loss: 0.7058762311935425\n",
      "  batch 150 loss: 0.7189597058296203\n",
      "  batch 200 loss: 0.6891999197006226\n",
      "  batch 250 loss: 0.7349035310745239\n",
      "  batch 300 loss: 0.731558308005333\n",
      "  batch 350 loss: 0.7376183307170868\n",
      "  batch 400 loss: 0.7139157772064209\n",
      "  batch 450 loss: 0.7124865674972534\n",
      "  batch 500 loss: 0.7203088104724884\n",
      "  batch 550 loss: 0.7352948015928269\n",
      "  batch 600 loss: 0.7514899051189423\n",
      "  batch 650 loss: 0.7283390879631042\n",
      "  batch 700 loss: 0.7088780325651168\n",
      "  batch 750 loss: 0.7244000953435897\n",
      "  batch 800 loss: 0.7430377423763275\n",
      "  batch 850 loss: 0.7569790840148926\n",
      "  batch 900 loss: 0.7053834873437882\n",
      "LOSS train 0.70538 valid 0.94196, valid PER 28.00%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.6248771160840988\n",
      "  batch 100 loss: 0.6380026233196259\n",
      "  batch 150 loss: 0.6676277208328247\n",
      "  batch 200 loss: 0.6885145902633667\n",
      "  batch 250 loss: 0.6846557867527008\n",
      "  batch 300 loss: 0.6389586555957795\n",
      "  batch 350 loss: 0.6550693702697754\n",
      "  batch 400 loss: 0.6461286848783493\n",
      "  batch 450 loss: 0.6488621383905411\n",
      "  batch 500 loss: 0.668333300948143\n",
      "  batch 550 loss: 0.6917527318000793\n",
      "  batch 600 loss: 0.6719024205207824\n",
      "  batch 650 loss: 0.6444393002986908\n",
      "  batch 700 loss: 0.7000304675102234\n",
      "  batch 750 loss: 0.6825529825687409\n",
      "  batch 800 loss: 0.685764086842537\n",
      "  batch 850 loss: 0.6929505127668381\n",
      "  batch 900 loss: 0.6951223146915436\n",
      "Epoch 00010: reducing learning rate of group 0 to 3.5000e-01.\n",
      "LOSS train 0.69512 valid 0.95858, valid PER 28.46%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.5407097029685974\n",
      "  batch 100 loss: 0.4785176968574524\n",
      "  batch 150 loss: 0.5086652034521103\n",
      "  batch 200 loss: 0.5427295470237732\n",
      "  batch 250 loss: 0.5366496926546097\n",
      "  batch 300 loss: 0.4917738550901413\n",
      "  batch 350 loss: 0.5314883953332901\n",
      "  batch 400 loss: 0.5289062428474426\n",
      "  batch 450 loss: 0.5313820576667786\n",
      "  batch 500 loss: 0.5163363474607467\n",
      "  batch 550 loss: 0.521435158252716\n",
      "  batch 600 loss: 0.5036862713098526\n",
      "  batch 650 loss: 0.5609511315822602\n",
      "  batch 700 loss: 0.49332826256752016\n",
      "  batch 750 loss: 0.5037766379117966\n",
      "  batch 800 loss: 0.5321326529979706\n",
      "  batch 850 loss: 0.5509927552938462\n",
      "  batch 900 loss: 0.5501807326078415\n",
      "LOSS train 0.55018 valid 0.91465, valid PER 25.98%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.47173830956220625\n",
      "  batch 100 loss: 0.443957285284996\n",
      "  batch 150 loss: 0.40894061386585234\n",
      "  batch 200 loss: 0.4481672602891922\n",
      "  batch 250 loss: 0.45974140107631684\n",
      "  batch 300 loss: 0.4454608923196793\n",
      "  batch 350 loss: 0.4454305452108383\n",
      "  batch 400 loss: 0.4691852128505707\n",
      "  batch 450 loss: 0.4689592516422272\n",
      "  batch 500 loss: 0.46843843966722487\n",
      "  batch 550 loss: 0.43622623026371005\n",
      "  batch 600 loss: 0.4742312830686569\n",
      "  batch 650 loss: 0.481643568277359\n",
      "  batch 700 loss: 0.46696353375911714\n",
      "  batch 750 loss: 0.4596622234582901\n",
      "  batch 800 loss: 0.4670079717040062\n",
      "  batch 850 loss: 0.4984107917547226\n",
      "  batch 900 loss: 0.48185077995061876\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.7500e-01.\n",
      "LOSS train 0.48185 valid 0.91750, valid PER 25.87%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.3863942256569862\n",
      "  batch 100 loss: 0.3704685914516449\n",
      "  batch 150 loss: 0.3592963418364525\n",
      "  batch 200 loss: 0.37311763405799864\n",
      "  batch 250 loss: 0.36657976150512694\n",
      "  batch 300 loss: 0.3632808256149292\n",
      "  batch 350 loss: 0.3489663749933243\n",
      "  batch 400 loss: 0.38144824087619783\n",
      "  batch 450 loss: 0.3646405193209648\n",
      "  batch 500 loss: 0.35024526983499527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 550 loss: 0.3876370874047279\n",
      "  batch 600 loss: 0.3639501443505287\n",
      "  batch 650 loss: 0.3872870236635208\n",
      "  batch 700 loss: 0.3830036637187004\n",
      "  batch 750 loss: 0.3439823570847511\n",
      "  batch 800 loss: 0.35943577229976653\n",
      "  batch 850 loss: 0.3784014376997948\n",
      "  batch 900 loss: 0.39757623463869096\n",
      "Epoch 00013: reducing learning rate of group 0 to 8.7500e-02.\n",
      "LOSS train 0.39758 valid 0.93474, valid PER 25.50%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.31717868596315385\n",
      "  batch 100 loss: 0.3255603665113449\n",
      "  batch 150 loss: 0.31248593121767043\n",
      "  batch 200 loss: 0.29944984972476957\n",
      "  batch 250 loss: 0.3122228875756264\n",
      "  batch 300 loss: 0.3226291432976723\n",
      "  batch 350 loss: 0.298842137157917\n",
      "  batch 400 loss: 0.29614907413721087\n",
      "  batch 450 loss: 0.3132588747143745\n",
      "  batch 500 loss: 0.31753602921962737\n",
      "  batch 550 loss: 0.32253783077001574\n",
      "  batch 600 loss: 0.29359151363372804\n",
      "  batch 650 loss: 0.3093507650494576\n",
      "  batch 700 loss: 0.3253193932771683\n",
      "  batch 750 loss: 0.30269073039293287\n",
      "  batch 800 loss: 0.2994882172346115\n",
      "  batch 850 loss: 0.3254102173447609\n",
      "  batch 900 loss: 0.3016619113087654\n",
      "Epoch 00014: reducing learning rate of group 0 to 4.3750e-02.\n",
      "LOSS train 0.30166 valid 0.94347, valid PER 25.20%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.2711669099330902\n",
      "  batch 100 loss: 0.28002063542604444\n",
      "  batch 150 loss: 0.28149814695119857\n",
      "  batch 200 loss: 0.28082119435071945\n",
      "  batch 250 loss: 0.2989488783478737\n",
      "  batch 300 loss: 0.27508161813020704\n",
      "  batch 350 loss: 0.2761402654647827\n",
      "  batch 400 loss: 0.29178238242864607\n",
      "  batch 450 loss: 0.2759539577364922\n",
      "  batch 500 loss: 0.26633582413196566\n",
      "  batch 550 loss: 0.2799601489305496\n",
      "  batch 600 loss: 0.2804124453663826\n",
      "  batch 650 loss: 0.2940125098824501\n",
      "  batch 700 loss: 0.29264096409082413\n",
      "  batch 750 loss: 0.28353665232658387\n",
      "  batch 800 loss: 0.26309860408306124\n",
      "  batch 850 loss: 0.2590153628587723\n",
      "  batch 900 loss: 0.28657611057162286\n",
      "Epoch 00015: reducing learning rate of group 0 to 2.1875e-02.\n",
      "LOSS train 0.28658 valid 0.94690, valid PER 25.17%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.2662061819434166\n",
      "  batch 100 loss: 0.2563060837984085\n",
      "  batch 150 loss: 0.2676976165175438\n",
      "  batch 200 loss: 0.2575318697094917\n",
      "  batch 250 loss: 0.27128754258155824\n",
      "  batch 300 loss: 0.2617544747889042\n",
      "  batch 350 loss: 0.26350839614868166\n",
      "  batch 400 loss: 0.27314287453889846\n",
      "  batch 450 loss: 0.2760967069864273\n",
      "  batch 500 loss: 0.2536842888593674\n",
      "  batch 550 loss: 0.24377100318670272\n",
      "  batch 600 loss: 0.2498947149515152\n",
      "  batch 650 loss: 0.27335041359066964\n",
      "  batch 700 loss: 0.25983829170465467\n",
      "  batch 750 loss: 0.2598448720574379\n",
      "  batch 800 loss: 0.27218700528144835\n",
      "  batch 850 loss: 0.2577878224849701\n",
      "  batch 900 loss: 0.27662473142147065\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0937e-02.\n",
      "LOSS train 0.27662 valid 0.95416, valid PER 25.30%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.26227012813091277\n",
      "  batch 100 loss: 0.2644214400649071\n",
      "  batch 150 loss: 0.25046959176659583\n",
      "  batch 200 loss: 0.25503778845071795\n",
      "  batch 250 loss: 0.2588631348311901\n",
      "  batch 300 loss: 0.2541347122192383\n",
      "  batch 350 loss: 0.24611383244395257\n",
      "  batch 400 loss: 0.270256569981575\n",
      "  batch 450 loss: 0.2535333248972893\n",
      "  batch 500 loss: 0.25225297451019285\n",
      "  batch 550 loss: 0.2540914924442768\n",
      "  batch 600 loss: 0.25519805908203125\n",
      "  batch 650 loss: 0.25688165754079817\n",
      "  batch 700 loss: 0.2570158152282238\n",
      "  batch 750 loss: 0.2478795638680458\n",
      "  batch 800 loss: 0.23623134523630143\n",
      "  batch 850 loss: 0.26060095965862273\n",
      "  batch 900 loss: 0.2427917844057083\n",
      "Epoch 00017: reducing learning rate of group 0 to 5.4687e-03.\n",
      "LOSS train 0.24279 valid 0.95747, valid PER 25.24%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.2403203223645687\n",
      "  batch 100 loss: 0.252862751185894\n",
      "  batch 150 loss: 0.2698238308727741\n",
      "  batch 200 loss: 0.2603337424993515\n",
      "  batch 250 loss: 0.2712099748849869\n",
      "  batch 300 loss: 0.24550393462181092\n",
      "  batch 350 loss: 0.24281679302453996\n",
      "  batch 400 loss: 0.24468196123838426\n",
      "  batch 450 loss: 0.25476277887821197\n",
      "  batch 500 loss: 0.24263547524809836\n",
      "  batch 550 loss: 0.2662352308630943\n",
      "  batch 600 loss: 0.24065129727125167\n",
      "  batch 650 loss: 0.2368299551308155\n",
      "  batch 700 loss: 0.2583690586686134\n",
      "  batch 750 loss: 0.24295855388045312\n",
      "  batch 800 loss: 0.240740677267313\n",
      "  batch 850 loss: 0.25112598717212675\n",
      "  batch 900 loss: 0.2515401214361191\n",
      "Epoch 00018: reducing learning rate of group 0 to 2.7344e-03.\n",
      "LOSS train 0.25154 valid 0.95876, valid PER 25.24%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.2510209259390831\n",
      "  batch 100 loss: 0.24481701135635375\n",
      "  batch 150 loss: 0.24572824627161027\n",
      "  batch 200 loss: 0.2558774623274803\n",
      "  batch 250 loss: 0.2467115552723408\n",
      "  batch 300 loss: 0.25601678892970087\n",
      "  batch 350 loss: 0.24629827380180358\n",
      "  batch 400 loss: 0.2457806697487831\n",
      "  batch 450 loss: 0.2606064024567604\n",
      "  batch 500 loss: 0.2548686784505844\n",
      "  batch 550 loss: 0.2348979100584984\n",
      "  batch 600 loss: 0.23586211159825324\n",
      "  batch 650 loss: 0.26614177986979487\n",
      "  batch 700 loss: 0.24134484991431238\n",
      "  batch 750 loss: 0.23213557198643683\n",
      "  batch 800 loss: 0.2569001677632332\n",
      "  batch 850 loss: 0.2504023593664169\n",
      "  batch 900 loss: 0.2554453927278519\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.3672e-03.\n",
      "LOSS train 0.25545 valid 0.95945, valid PER 25.26%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.2670484408736229\n",
      "  batch 100 loss: 0.2431855246424675\n",
      "  batch 150 loss: 0.24198496595025062\n",
      "  batch 200 loss: 0.2442129597067833\n",
      "  batch 250 loss: 0.24603436306118964\n",
      "  batch 300 loss: 0.2561351861059666\n",
      "  batch 350 loss: 0.2290073235332966\n",
      "  batch 400 loss: 0.24834777504205705\n",
      "  batch 450 loss: 0.25183737218379976\n",
      "  batch 500 loss: 0.22824367463588716\n",
      "  batch 550 loss: 0.2684801626205444\n",
      "  batch 600 loss: 0.24362364947795867\n",
      "  batch 650 loss: 0.2574863818287849\n",
      "  batch 700 loss: 0.24081628501415253\n",
      "  batch 750 loss: 0.23791419878602027\n",
      "  batch 800 loss: 0.25701309204101563\n",
      "  batch 850 loss: 0.25206920444965364\n",
      "  batch 900 loss: 0.24871036738157273\n",
      "Epoch 00020: reducing learning rate of group 0 to 6.8359e-04.\n",
      "LOSS train 0.24871 valid 0.96005, valid PER 25.26%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231209_161653/model_11\n",
      "Currently using SGD optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.619661698341369\n",
      "  batch 100 loss: 3.1130731868743897\n",
      "  batch 150 loss: 2.8658093070983885\n",
      "  batch 200 loss: 2.719969434738159\n",
      "  batch 250 loss: 2.712317929267883\n",
      "  batch 300 loss: 2.529215087890625\n",
      "  batch 350 loss: 2.392859525680542\n",
      "  batch 400 loss: 2.359589695930481\n",
      "  batch 450 loss: 2.2812403988838197\n",
      "  batch 500 loss: 2.217523601055145\n",
      "  batch 550 loss: 2.156904993057251\n",
      "  batch 600 loss: 2.007791485786438\n",
      "  batch 650 loss: 1.862450828552246\n",
      "  batch 700 loss: 1.8813041853904724\n",
      "  batch 750 loss: 1.8100911831855775\n",
      "  batch 800 loss: 1.7721000409126282\n",
      "  batch 850 loss: 1.7351365303993225\n",
      "  batch 900 loss: 1.6935344624519348\n",
      "LOSS train 1.69353 valid 1.63373, valid PER 57.65%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.6382261085510255\n",
      "  batch 100 loss: 1.5843217158317566\n",
      "  batch 150 loss: 1.5198885846138\n",
      "  batch 200 loss: 1.527416582107544\n",
      "  batch 250 loss: 1.5175679183006288\n",
      "  batch 300 loss: 1.4905181479454042\n",
      "  batch 350 loss: 1.387812190055847\n",
      "  batch 400 loss: 1.411451370716095\n",
      "  batch 450 loss: 1.3414259433746338\n",
      "  batch 500 loss: 1.4009621381759643\n",
      "  batch 550 loss: 1.4119948506355287\n",
      "  batch 600 loss: 1.3545129823684692\n",
      "  batch 650 loss: 1.3631339836120606\n",
      "  batch 700 loss: 1.3595090532302856\n",
      "  batch 750 loss: 1.3229305338859558\n",
      "  batch 800 loss: 1.2979544830322265\n",
      "  batch 850 loss: 1.282353426218033\n",
      "  batch 900 loss: 1.3198222851753234\n",
      "LOSS train 1.31982 valid 1.25963, valid PER 37.48%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.239190204143524\n",
      "  batch 100 loss: 1.2580414378643037\n",
      "  batch 150 loss: 1.2265431833267213\n",
      "  batch 200 loss: 1.1911523783206939\n",
      "  batch 250 loss: 1.2031384265422822\n",
      "  batch 300 loss: 1.177749559879303\n",
      "  batch 350 loss: 1.2351849162578583\n",
      "  batch 400 loss: 1.1924833858013153\n",
      "  batch 450 loss: 1.1864585852622986\n",
      "  batch 500 loss: 1.1668746340274812\n",
      "  batch 550 loss: 1.1889317440986633\n",
      "  batch 600 loss: 1.1641122829914092\n",
      "  batch 650 loss: 1.1295896661281586\n",
      "  batch 700 loss: 1.1699271631240844\n",
      "  batch 750 loss: 1.1935613667964935\n",
      "  batch 800 loss: 1.1338855004310608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 850 loss: 1.1587940633296967\n",
      "  batch 900 loss: 1.1254368066787719\n",
      "LOSS train 1.12544 valid 1.17209, valid PER 35.22%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0716103172302247\n",
      "  batch 100 loss: 1.1115892624855042\n",
      "  batch 150 loss: 1.074433025121689\n",
      "  batch 200 loss: 1.0931191670894622\n",
      "  batch 250 loss: 1.0945008051395417\n",
      "  batch 300 loss: 1.0939276552200317\n",
      "  batch 350 loss: 1.0294990456104278\n",
      "  batch 400 loss: 1.0622120213508606\n",
      "  batch 450 loss: 1.0439597153663636\n",
      "  batch 500 loss: 1.0268917155265809\n",
      "  batch 550 loss: 1.0646444475650787\n",
      "  batch 600 loss: 1.0816781759262084\n",
      "  batch 650 loss: 1.0509575641155242\n",
      "  batch 700 loss: 1.051159827709198\n",
      "  batch 750 loss: 1.0231579673290252\n",
      "  batch 800 loss: 0.9938838684558868\n",
      "  batch 850 loss: 1.0421117460727691\n",
      "  batch 900 loss: 1.0773703587055206\n",
      "LOSS train 1.07737 valid 1.11236, valid PER 33.81%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9526908957958221\n",
      "  batch 100 loss: 0.9806990480422973\n",
      "  batch 150 loss: 1.0104088866710663\n",
      "  batch 200 loss: 0.9444855415821075\n",
      "  batch 250 loss: 0.9571813869476319\n",
      "  batch 300 loss: 0.9677864360809326\n",
      "  batch 350 loss: 0.9745777976512909\n",
      "  batch 400 loss: 0.9656870722770691\n",
      "  batch 450 loss: 0.9712502062320709\n",
      "  batch 500 loss: 0.9803982162475586\n",
      "  batch 550 loss: 0.925859968662262\n",
      "  batch 600 loss: 0.9991848158836365\n",
      "  batch 650 loss: 0.9770200181007386\n",
      "  batch 700 loss: 0.9963863468170167\n",
      "  batch 750 loss: 0.9354574263095856\n",
      "  batch 800 loss: 0.9673713076114655\n",
      "  batch 850 loss: 0.9640041947364807\n",
      "  batch 900 loss: 0.9831577253341675\n",
      "LOSS train 0.98316 valid 1.00942, valid PER 30.17%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9334280693531036\n",
      "  batch 100 loss: 0.8934752607345581\n",
      "  batch 150 loss: 0.8680139422416687\n",
      "  batch 200 loss: 0.873999092578888\n",
      "  batch 250 loss: 0.9361583280563355\n",
      "  batch 300 loss: 0.8914532899856568\n",
      "  batch 350 loss: 0.905301285982132\n",
      "  batch 400 loss: 0.8685190272331238\n",
      "  batch 450 loss: 0.9208089363574982\n",
      "  batch 500 loss: 0.9361346995830536\n",
      "  batch 550 loss: 0.9152109098434448\n",
      "  batch 600 loss: 0.8752911770343781\n",
      "  batch 650 loss: 0.9142237782478333\n",
      "  batch 700 loss: 0.9295294177532196\n",
      "  batch 750 loss: 0.8818105709552765\n",
      "  batch 800 loss: 0.8957017827033996\n",
      "  batch 850 loss: 0.8737376391887665\n",
      "  batch 900 loss: 0.8943905818462372\n",
      "LOSS train 0.89439 valid 1.00559, valid PER 30.35%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8397275996208191\n",
      "  batch 100 loss: 0.83058030128479\n",
      "  batch 150 loss: 0.8284489643573761\n",
      "  batch 200 loss: 0.8182243752479553\n",
      "  batch 250 loss: 0.807528612613678\n",
      "  batch 300 loss: 0.8245893573760986\n",
      "  batch 350 loss: 0.8259882891178131\n",
      "  batch 400 loss: 0.8285412931442261\n",
      "  batch 450 loss: 0.8357199656963349\n",
      "  batch 500 loss: 0.8198580825328827\n",
      "  batch 550 loss: 0.8293509018421173\n",
      "  batch 600 loss: 0.8409846448898315\n",
      "  batch 650 loss: 0.8404399037361145\n",
      "  batch 700 loss: 0.8649277108907699\n",
      "  batch 750 loss: 0.8361708152294159\n",
      "  batch 800 loss: 0.8219805693626404\n",
      "  batch 850 loss: 0.8447126007080078\n",
      "  batch 900 loss: 0.8729157984256745\n",
      "LOSS train 0.87292 valid 0.94453, valid PER 29.06%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.7571118533611297\n",
      "  batch 100 loss: 0.7550154101848602\n",
      "  batch 150 loss: 0.7607654368877411\n",
      "  batch 200 loss: 0.7572863256931305\n",
      "  batch 250 loss: 0.7660829848051072\n",
      "  batch 300 loss: 0.7233931446075439\n",
      "  batch 350 loss: 0.7858560717105866\n",
      "  batch 400 loss: 0.7629811173677444\n",
      "  batch 450 loss: 0.7996304929256439\n",
      "  batch 500 loss: 0.8296591091156006\n",
      "  batch 550 loss: 0.7601122689247132\n",
      "  batch 600 loss: 0.7872528314590455\n",
      "  batch 650 loss: 0.8242377316951752\n",
      "  batch 700 loss: 0.7656751281023025\n",
      "  batch 750 loss: 0.7911480617523193\n",
      "  batch 800 loss: 0.8041508126258851\n",
      "  batch 850 loss: 0.7667843866348266\n",
      "  batch 900 loss: 0.7826394838094711\n",
      "Epoch 00008: reducing learning rate of group 0 to 5.0000e-01.\n",
      "LOSS train 0.78264 valid 0.94685, valid PER 28.38%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.6384867745637893\n",
      "  batch 100 loss: 0.6471251279115677\n",
      "  batch 150 loss: 0.635216658115387\n",
      "  batch 200 loss: 0.5970976501703262\n",
      "  batch 250 loss: 0.630062837600708\n",
      "  batch 300 loss: 0.6258000135421753\n",
      "  batch 350 loss: 0.6427876484394074\n",
      "  batch 400 loss: 0.6112387132644653\n",
      "  batch 450 loss: 0.6128707075119019\n",
      "  batch 500 loss: 0.6043671184778213\n",
      "  batch 550 loss: 0.627317231297493\n",
      "  batch 600 loss: 0.6340196990966797\n",
      "  batch 650 loss: 0.6177034491300583\n",
      "  batch 700 loss: 0.6012494122982025\n",
      "  batch 750 loss: 0.6039114218950271\n",
      "  batch 800 loss: 0.6251248985528945\n",
      "  batch 850 loss: 0.6264039713144303\n",
      "  batch 900 loss: 0.5902899920940399\n",
      "LOSS train 0.59029 valid 0.86411, valid PER 25.99%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.5272015714645386\n",
      "  batch 100 loss: 0.5378758609294891\n",
      "  batch 150 loss: 0.5664043116569519\n",
      "  batch 200 loss: 0.5744196552038193\n",
      "  batch 250 loss: 0.5784778702259064\n",
      "  batch 300 loss: 0.5374510091543198\n",
      "  batch 350 loss: 0.5774118173122406\n",
      "  batch 400 loss: 0.5332084625959397\n",
      "  batch 450 loss: 0.5431449919939041\n",
      "  batch 500 loss: 0.5672704899311065\n",
      "  batch 550 loss: 0.5804433405399323\n",
      "  batch 600 loss: 0.5666926288604737\n",
      "  batch 650 loss: 0.5540501767396927\n",
      "  batch 700 loss: 0.5680902445316315\n",
      "  batch 750 loss: 0.5604944479465485\n",
      "  batch 800 loss: 0.5777944660186768\n",
      "  batch 850 loss: 0.5749457269906998\n",
      "  batch 900 loss: 0.5809223586320877\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-01.\n",
      "LOSS train 0.58092 valid 0.86621, valid PER 26.10%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.473113175034523\n",
      "  batch 100 loss: 0.43142084538936615\n",
      "  batch 150 loss: 0.45085569500923156\n",
      "  batch 200 loss: 0.4915145355463028\n",
      "  batch 250 loss: 0.47584682404994966\n",
      "  batch 300 loss: 0.4450491428375244\n",
      "  batch 350 loss: 0.45524529308080675\n",
      "  batch 400 loss: 0.47313223719596864\n",
      "  batch 450 loss: 0.4794319248199463\n",
      "  batch 500 loss: 0.4455174612998962\n",
      "  batch 550 loss: 0.46647556364536286\n",
      "  batch 600 loss: 0.4503982308506966\n",
      "  batch 650 loss: 0.4960150772333145\n",
      "  batch 700 loss: 0.44472337901592257\n",
      "  batch 750 loss: 0.4552702331542969\n",
      "  batch 800 loss: 0.48257973611354826\n",
      "  batch 850 loss: 0.48519691288471223\n",
      "  batch 900 loss: 0.49772671699523924\n",
      "LOSS train 0.49773 valid 0.85688, valid PER 24.86%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.4284186488389969\n",
      "  batch 100 loss: 0.4124345642328262\n",
      "  batch 150 loss: 0.3790523573756218\n",
      "  batch 200 loss: 0.42710598349571227\n",
      "  batch 250 loss: 0.427489321231842\n",
      "  batch 300 loss: 0.40838745176792146\n",
      "  batch 350 loss: 0.4132792991399765\n",
      "  batch 400 loss: 0.4327276021242142\n",
      "  batch 450 loss: 0.43050838708877565\n",
      "  batch 500 loss: 0.4376501560211182\n",
      "  batch 550 loss: 0.4114464047551155\n",
      "  batch 600 loss: 0.4388331252336502\n",
      "  batch 650 loss: 0.4384599307179451\n",
      "  batch 700 loss: 0.4386331325769424\n",
      "  batch 750 loss: 0.4262052491307259\n",
      "  batch 800 loss: 0.4385538253188133\n",
      "  batch 850 loss: 0.46774659514427186\n",
      "  batch 900 loss: 0.44861791133880613\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.2500e-01.\n",
      "LOSS train 0.44862 valid 0.86892, valid PER 25.10%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.37547584116458893\n",
      "  batch 100 loss: 0.38673779129981994\n",
      "  batch 150 loss: 0.361085903942585\n",
      "  batch 200 loss: 0.3743186354637146\n",
      "  batch 250 loss: 0.3610889405012131\n",
      "  batch 300 loss: 0.36270976543426514\n",
      "  batch 350 loss: 0.35511609345674516\n",
      "  batch 400 loss: 0.37997106671333314\n",
      "  batch 450 loss: 0.36776696354150773\n",
      "  batch 500 loss: 0.3498843777179718\n",
      "  batch 550 loss: 0.3769434803724289\n",
      "  batch 600 loss: 0.35969732284545897\n",
      "  batch 650 loss: 0.37600206702947614\n",
      "  batch 700 loss: 0.39149139225482943\n",
      "  batch 750 loss: 0.3508655440807342\n",
      "  batch 800 loss: 0.372244873046875\n",
      "  batch 850 loss: 0.3634208580851555\n",
      "  batch 900 loss: 0.3851805275678635\n",
      "Epoch 00013: reducing learning rate of group 0 to 6.2500e-02.\n",
      "LOSS train 0.38518 valid 0.87081, valid PER 24.81%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.34762743443250654\n",
      "  batch 100 loss: 0.3267630705237389\n",
      "  batch 150 loss: 0.3318604850769043\n",
      "  batch 200 loss: 0.31603926509618757\n",
      "  batch 250 loss: 0.3283867561817169\n",
      "  batch 300 loss: 0.3550629103183746\n",
      "  batch 350 loss: 0.31933145195245743\n",
      "  batch 400 loss: 0.3335987976193428\n",
      "  batch 450 loss: 0.33474370419979094\n",
      "  batch 500 loss: 0.3276229906082153\n",
      "  batch 550 loss: 0.3458560475707054\n",
      "  batch 600 loss: 0.3216732320189476\n",
      "  batch 650 loss: 0.3375802853703499\n",
      "  batch 700 loss: 0.36059747993946073\n",
      "  batch 750 loss: 0.3307736799120903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 800 loss: 0.3192412179708481\n",
      "  batch 850 loss: 0.33288459211587906\n",
      "  batch 900 loss: 0.33084321618080137\n",
      "Epoch 00014: reducing learning rate of group 0 to 3.1250e-02.\n",
      "LOSS train 0.33084 valid 0.87673, valid PER 24.74%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.3164902064204216\n",
      "  batch 100 loss: 0.3113329672813416\n",
      "  batch 150 loss: 0.31692228704690933\n",
      "  batch 200 loss: 0.3201861813664436\n",
      "  batch 250 loss: 0.33551207959651946\n",
      "  batch 300 loss: 0.31006830364465715\n",
      "  batch 350 loss: 0.32236216247081756\n",
      "  batch 400 loss: 0.31574627727270127\n",
      "  batch 450 loss: 0.30256917506456377\n",
      "  batch 500 loss: 0.30583421379327774\n",
      "  batch 550 loss: 0.3064282888174057\n",
      "  batch 600 loss: 0.31817650467157366\n",
      "  batch 650 loss: 0.3285538575053215\n",
      "  batch 700 loss: 0.3315695136785507\n",
      "  batch 750 loss: 0.31693208575248716\n",
      "  batch 800 loss: 0.3034723097085953\n",
      "  batch 850 loss: 0.2953485381603241\n",
      "  batch 900 loss: 0.3162604469060898\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.5625e-02.\n",
      "LOSS train 0.31626 valid 0.88068, valid PER 24.82%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.32041915714740754\n",
      "  batch 100 loss: 0.30049687922000884\n",
      "  batch 150 loss: 0.31076851606369016\n",
      "  batch 200 loss: 0.297412174642086\n",
      "  batch 250 loss: 0.30366066068410874\n",
      "  batch 300 loss: 0.3024523067474365\n",
      "  batch 350 loss: 0.2961526420712471\n",
      "  batch 400 loss: 0.3195383539795876\n",
      "  batch 450 loss: 0.31610944598913193\n",
      "  batch 500 loss: 0.2858793127536774\n",
      "  batch 550 loss: 0.2901476031541824\n",
      "  batch 600 loss: 0.2987871164083481\n",
      "  batch 650 loss: 0.3034566968679428\n",
      "  batch 700 loss: 0.30286071956157684\n",
      "  batch 750 loss: 0.3079905104637146\n",
      "  batch 800 loss: 0.3143265852332115\n",
      "  batch 850 loss: 0.30388081580400467\n",
      "  batch 900 loss: 0.3151612648367882\n",
      "Epoch 00016: reducing learning rate of group 0 to 7.8125e-03.\n",
      "LOSS train 0.31516 valid 0.88540, valid PER 24.74%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.30751277089118956\n",
      "  batch 100 loss: 0.30697304755449295\n",
      "  batch 150 loss: 0.29346313923597334\n",
      "  batch 200 loss: 0.2884543216228485\n",
      "  batch 250 loss: 0.31980129301548005\n",
      "  batch 300 loss: 0.3023060297966003\n",
      "  batch 350 loss: 0.28669523596763613\n",
      "  batch 400 loss: 0.32054355144500735\n",
      "  batch 450 loss: 0.30025975197553634\n",
      "  batch 500 loss: 0.28605022341012953\n",
      "  batch 550 loss: 0.3015453028678894\n",
      "  batch 600 loss: 0.3043580976128578\n",
      "  batch 650 loss: 0.3030759277939796\n",
      "  batch 700 loss: 0.30355632185935977\n",
      "  batch 750 loss: 0.2879671260714531\n",
      "  batch 800 loss: 0.27586199820041657\n",
      "  batch 850 loss: 0.30811909258365633\n",
      "  batch 900 loss: 0.2832543039321899\n",
      "Epoch 00017: reducing learning rate of group 0 to 3.9062e-03.\n",
      "LOSS train 0.28325 valid 0.88712, valid PER 24.81%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.29324732065200804\n",
      "  batch 100 loss: 0.30224443882703783\n",
      "  batch 150 loss: 0.30101123243570327\n",
      "  batch 200 loss: 0.2911375370621681\n",
      "  batch 250 loss: 0.3176413995027542\n",
      "  batch 300 loss: 0.2858050686120987\n",
      "  batch 350 loss: 0.29321343660354615\n",
      "  batch 400 loss: 0.29293737828731536\n",
      "  batch 450 loss: 0.3074604034423828\n",
      "  batch 500 loss: 0.30049526929855347\n",
      "  batch 550 loss: 0.3152342414855957\n",
      "  batch 600 loss: 0.2855684286355972\n",
      "  batch 650 loss: 0.27998496145009993\n",
      "  batch 700 loss: 0.3057864585518837\n",
      "  batch 750 loss: 0.28606657207012176\n",
      "  batch 800 loss: 0.29880829811096193\n",
      "  batch 850 loss: 0.28574547320604327\n",
      "  batch 900 loss: 0.29922176122665406\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.9531e-03.\n",
      "LOSS train 0.29922 valid 0.88807, valid PER 24.88%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.30617512345314024\n",
      "  batch 100 loss: 0.28567907005548476\n",
      "  batch 150 loss: 0.29870152920484544\n",
      "  batch 200 loss: 0.30057827562093736\n",
      "  batch 250 loss: 0.29863263517618177\n",
      "  batch 300 loss: 0.2945982326567173\n",
      "  batch 350 loss: 0.28338948607444764\n",
      "  batch 400 loss: 0.2857068312168121\n",
      "  batch 450 loss: 0.3064010229706764\n",
      "  batch 500 loss: 0.29672539204359055\n",
      "  batch 550 loss: 0.2879090344905853\n",
      "  batch 600 loss: 0.2847974160313606\n",
      "  batch 650 loss: 0.32041954874992373\n",
      "  batch 700 loss: 0.28892216384410857\n",
      "  batch 750 loss: 0.28450111985206605\n",
      "  batch 800 loss: 0.29995239078998565\n",
      "  batch 850 loss: 0.29555214136838914\n",
      "  batch 900 loss: 0.29158022165298464\n",
      "Epoch 00019: reducing learning rate of group 0 to 9.7656e-04.\n",
      "LOSS train 0.29158 valid 0.88850, valid PER 24.84%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.3068813246488571\n",
      "  batch 100 loss: 0.29956130430102346\n",
      "  batch 150 loss: 0.2831696370244026\n",
      "  batch 200 loss: 0.2870629489421844\n",
      "  batch 250 loss: 0.2913871401548386\n",
      "  batch 300 loss: 0.30494064211845395\n",
      "  batch 350 loss: 0.27963810458779337\n",
      "  batch 400 loss: 0.2972517192363739\n",
      "  batch 450 loss: 0.3031338280439377\n",
      "  batch 500 loss: 0.27568891406059265\n",
      "  batch 550 loss: 0.31290170788764954\n",
      "  batch 600 loss: 0.28163681238889693\n",
      "  batch 650 loss: 0.29614356845617296\n",
      "  batch 700 loss: 0.2900992473959923\n",
      "  batch 750 loss: 0.283589034229517\n",
      "  batch 800 loss: 0.3072728204727173\n",
      "  batch 850 loss: 0.30465966075658796\n",
      "  batch 900 loss: 0.29321778506040574\n",
      "Epoch 00020: reducing learning rate of group 0 to 4.8828e-04.\n",
      "LOSS train 0.29322 valid 0.88876, valid PER 24.88%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231209_162521/model_11\n",
      "Currently using SGD optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 2240552\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.661665215492248\n",
      "  batch 100 loss: 3.25090681552887\n",
      "  batch 150 loss: 3.0160288429260254\n",
      "  batch 200 loss: 2.81784779548645\n",
      "  batch 250 loss: 2.798632583618164\n",
      "  batch 300 loss: 2.62138427734375\n",
      "  batch 350 loss: 2.485672788619995\n",
      "  batch 400 loss: 2.4443233370780946\n",
      "  batch 450 loss: 2.3487854409217834\n",
      "  batch 500 loss: 2.2825487852096558\n",
      "  batch 550 loss: 2.1570428156852723\n",
      "  batch 600 loss: 2.100818192958832\n",
      "  batch 650 loss: 1.8882164669036865\n",
      "  batch 700 loss: 1.8630916500091552\n",
      "  batch 750 loss: 1.8156489467620849\n",
      "  batch 800 loss: 1.7776271176338196\n",
      "  batch 850 loss: 1.7246552419662475\n",
      "  batch 900 loss: 1.6611563611030578\n",
      "LOSS train 1.66116 valid 1.72923, valid PER 60.19%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.6501839685440063\n",
      "  batch 100 loss: 1.5893759441375732\n",
      "  batch 150 loss: 1.5393833923339844\n",
      "  batch 200 loss: 1.5722531056404114\n",
      "  batch 250 loss: 1.5740041375160216\n",
      "  batch 300 loss: 1.5183381724357605\n",
      "  batch 350 loss: 1.3966338419914246\n",
      "  batch 400 loss: 1.4179153490066527\n",
      "  batch 450 loss: 1.3481105017662047\n",
      "  batch 500 loss: 1.4075149095058441\n",
      "  batch 550 loss: 1.3941756081581116\n",
      "  batch 600 loss: 1.3127030754089355\n",
      "  batch 650 loss: 1.3618500769138335\n",
      "  batch 700 loss: 1.358855973482132\n",
      "  batch 750 loss: 1.3184353482723237\n",
      "  batch 800 loss: 1.2821767258644103\n",
      "  batch 850 loss: 1.2867625415325166\n",
      "  batch 900 loss: 1.291886305809021\n",
      "LOSS train 1.29189 valid 1.21363, valid PER 38.72%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.2463753974437715\n",
      "  batch 100 loss: 1.2346022570133208\n",
      "  batch 150 loss: 1.2275202894210815\n",
      "  batch 200 loss: 1.1740646588802337\n",
      "  batch 250 loss: 1.192872977256775\n",
      "  batch 300 loss: 1.1671390223503113\n",
      "  batch 350 loss: 1.2332188844680787\n",
      "  batch 400 loss: 1.1992278015613556\n",
      "  batch 450 loss: 1.1941604840755462\n",
      "  batch 500 loss: 1.141587688922882\n",
      "  batch 550 loss: 1.1519847524166107\n",
      "  batch 600 loss: 1.134029586315155\n",
      "  batch 650 loss: 1.1025120937824249\n",
      "  batch 700 loss: 1.1516348099708558\n",
      "  batch 750 loss: 1.1936552548408508\n",
      "  batch 800 loss: 1.1101718389987945\n",
      "  batch 850 loss: 1.1371890532970428\n",
      "  batch 900 loss: 1.0782269585132598\n",
      "LOSS train 1.07823 valid 1.20134, valid PER 35.44%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.0549595403671264\n",
      "  batch 100 loss: 1.0641395342350006\n",
      "  batch 150 loss: 1.024880485534668\n",
      "  batch 200 loss: 1.052887854576111\n",
      "  batch 250 loss: 1.0633913254737855\n",
      "  batch 300 loss: 1.082060159444809\n",
      "  batch 350 loss: 1.0083153903484345\n",
      "  batch 400 loss: 1.0511253905296325\n",
      "  batch 450 loss: 1.0542985486984253\n",
      "  batch 500 loss: 1.0291179764270781\n",
      "  batch 550 loss: 1.0529416704177856\n",
      "  batch 600 loss: 1.0784531211853028\n",
      "  batch 650 loss: 1.0567562186717987\n",
      "  batch 700 loss: 1.027292983531952\n",
      "  batch 750 loss: 1.0272808504104614\n",
      "  batch 800 loss: 0.9987535870075226\n",
      "  batch 850 loss: 1.045805594921112\n",
      "  batch 900 loss: 1.076927205324173\n",
      "LOSS train 1.07693 valid 1.06642, valid PER 32.36%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.9397182452678681\n",
      "  batch 100 loss: 0.9427171850204468\n",
      "  batch 150 loss: 1.0152308785915374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.9340521252155304\n",
      "  batch 250 loss: 0.9687004792690277\n",
      "  batch 300 loss: 0.9645647311210632\n",
      "  batch 350 loss: 0.9485770857334137\n",
      "  batch 400 loss: 0.9585914933681488\n",
      "  batch 450 loss: 0.9563530802726745\n",
      "  batch 500 loss: 0.9687843108177185\n",
      "  batch 550 loss: 0.9389273178577423\n",
      "  batch 600 loss: 1.0064745116233826\n",
      "  batch 650 loss: 0.9751174664497375\n",
      "  batch 700 loss: 1.0467513597011566\n",
      "  batch 750 loss: 1.0102285766601562\n",
      "  batch 800 loss: 0.9739977443218231\n",
      "  batch 850 loss: 0.9655930197238922\n",
      "  batch 900 loss: 0.9669645738601684\n",
      "LOSS train 0.96696 valid 1.04439, valid PER 32.04%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.9248822951316833\n",
      "  batch 100 loss: 0.889702582359314\n",
      "  batch 150 loss: 0.8625107336044312\n",
      "  batch 200 loss: 0.9019035804271698\n",
      "  batch 250 loss: 0.9522956776618957\n",
      "  batch 300 loss: 0.8921441078186035\n",
      "  batch 350 loss: 0.8829591941833496\n",
      "  batch 400 loss: 0.8755054533481598\n",
      "  batch 450 loss: 0.9478652381896973\n",
      "  batch 500 loss: 0.9485787427425385\n",
      "  batch 550 loss: 0.934004420042038\n",
      "  batch 600 loss: 0.8877142477035522\n",
      "  batch 650 loss: 0.9136203873157501\n",
      "  batch 700 loss: 0.9097999274730683\n",
      "  batch 750 loss: 0.8973463380336761\n",
      "  batch 800 loss: 0.9174669253826141\n",
      "  batch 850 loss: 0.8984907078742981\n",
      "  batch 900 loss: 0.9122561228275299\n",
      "LOSS train 0.91226 valid 1.01419, valid PER 30.40%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.8550818467140198\n",
      "  batch 100 loss: 0.8607824146747589\n",
      "  batch 150 loss: 0.8588862872123718\n",
      "  batch 200 loss: 0.8340367901325226\n",
      "  batch 250 loss: 0.8292503273487091\n",
      "  batch 300 loss: 0.8214048326015473\n",
      "  batch 350 loss: 0.8546489000320434\n",
      "  batch 400 loss: 0.8601314890384674\n",
      "  batch 450 loss: 0.873668247461319\n",
      "  batch 500 loss: 0.8583310770988465\n",
      "  batch 550 loss: 0.8388987445831299\n",
      "  batch 600 loss: 0.8779010808467865\n",
      "  batch 650 loss: 0.8412699222564697\n",
      "  batch 700 loss: 0.8882847321033478\n",
      "  batch 750 loss: 0.8561634600162507\n",
      "  batch 800 loss: 0.8766277480125427\n",
      "  batch 850 loss: 0.8536127507686615\n",
      "  batch 900 loss: 0.8987377464771271\n",
      "LOSS train 0.89874 valid 0.95424, valid PER 28.98%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.766179506778717\n",
      "  batch 100 loss: 0.7767139279842377\n",
      "  batch 150 loss: 0.7879556030035019\n",
      "  batch 200 loss: 0.7802099668979645\n",
      "  batch 250 loss: 0.7722299402952194\n",
      "  batch 300 loss: 0.7589275723695755\n",
      "  batch 350 loss: 0.8308831620216369\n",
      "  batch 400 loss: 0.784546080827713\n",
      "  batch 450 loss: 0.8242012357711792\n",
      "  batch 500 loss: 0.8266660213470459\n",
      "  batch 550 loss: 0.7635033059120179\n",
      "  batch 600 loss: 0.8055538201332092\n",
      "  batch 650 loss: 0.8420279490947723\n",
      "  batch 700 loss: 0.7881334817409515\n",
      "  batch 750 loss: 0.7958192229270935\n",
      "  batch 800 loss: 0.8251235842704773\n",
      "  batch 850 loss: 0.8055524456501008\n",
      "  batch 900 loss: 0.8150050836801529\n",
      "Epoch 00008: reducing learning rate of group 0 to 7.5000e-01.\n",
      "LOSS train 0.81501 valid 1.00477, valid PER 28.27%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.6668245649337768\n",
      "  batch 100 loss: 0.6672959226369858\n",
      "  batch 150 loss: 0.6537886667251587\n",
      "  batch 200 loss: 0.6187959122657776\n",
      "  batch 250 loss: 0.6459068602323532\n",
      "  batch 300 loss: 0.6378895020484925\n",
      "  batch 350 loss: 0.6477549242973327\n",
      "  batch 400 loss: 0.6190856260061264\n",
      "  batch 450 loss: 0.6161182779073715\n",
      "  batch 500 loss: 0.5946963429450989\n",
      "  batch 550 loss: 0.6240673291683198\n",
      "  batch 600 loss: 0.6519229114055634\n",
      "  batch 650 loss: 0.6187261039018631\n",
      "  batch 700 loss: 0.5885078448057175\n",
      "  batch 750 loss: 0.617482795715332\n",
      "  batch 800 loss: 0.6230991888046264\n",
      "  batch 850 loss: 0.6317671227455139\n",
      "  batch 900 loss: 0.5928492695093155\n",
      "LOSS train 0.59285 valid 0.86102, valid PER 26.16%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.5327008980512619\n",
      "  batch 100 loss: 0.5334396487474442\n",
      "  batch 150 loss: 0.5641011935472489\n",
      "  batch 200 loss: 0.5681473082304\n",
      "  batch 250 loss: 0.572925187945366\n",
      "  batch 300 loss: 0.5344015002250672\n",
      "  batch 350 loss: 0.5619622826576233\n",
      "  batch 400 loss: 0.5364527690410614\n",
      "  batch 450 loss: 0.5332289338111877\n",
      "  batch 500 loss: 0.5718003356456757\n",
      "  batch 550 loss: 0.5858532202243805\n",
      "  batch 600 loss: 0.5633493006229401\n",
      "  batch 650 loss: 0.5655707728862762\n",
      "  batch 700 loss: 0.5744151103496552\n",
      "  batch 750 loss: 0.5623443740606308\n",
      "  batch 800 loss: 0.5831214600801468\n",
      "  batch 850 loss: 0.5778427028656006\n",
      "  batch 900 loss: 0.5768761074543\n",
      "Epoch 00010: reducing learning rate of group 0 to 3.7500e-01.\n",
      "LOSS train 0.57688 valid 0.86496, valid PER 25.86%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.46655408322811126\n",
      "  batch 100 loss: 0.43023518174886705\n",
      "  batch 150 loss: 0.4395583111047745\n",
      "  batch 200 loss: 0.47877454102039335\n",
      "  batch 250 loss: 0.4674188619852066\n",
      "  batch 300 loss: 0.42454341530799866\n",
      "  batch 350 loss: 0.4618998008966446\n",
      "  batch 400 loss: 0.46333499014377594\n",
      "  batch 450 loss: 0.4579091680049896\n",
      "  batch 500 loss: 0.447673431634903\n",
      "  batch 550 loss: 0.4557189041376114\n",
      "  batch 600 loss: 0.4444566860795021\n",
      "  batch 650 loss: 0.48112150669097903\n",
      "  batch 700 loss: 0.42562839806079866\n",
      "  batch 750 loss: 0.4582818534970283\n",
      "  batch 800 loss: 0.4721397113800049\n",
      "  batch 850 loss: 0.4801296466588974\n",
      "  batch 900 loss: 0.476643003821373\n",
      "LOSS train 0.47664 valid 0.85782, valid PER 24.85%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.4137245932221413\n",
      "  batch 100 loss: 0.3930485573410988\n",
      "  batch 150 loss: 0.3739377465844154\n",
      "  batch 200 loss: 0.4050159949064255\n",
      "  batch 250 loss: 0.40607924848794935\n",
      "  batch 300 loss: 0.40245254516601564\n",
      "  batch 350 loss: 0.39201440513134\n",
      "  batch 400 loss: 0.4286354884505272\n",
      "  batch 450 loss: 0.42985295712947846\n",
      "  batch 500 loss: 0.43171450555324553\n",
      "  batch 550 loss: 0.3989547583460808\n",
      "  batch 600 loss: 0.4214803773164749\n",
      "  batch 650 loss: 0.4213866752386093\n",
      "  batch 700 loss: 0.4289363783597946\n",
      "  batch 750 loss: 0.4086308082938194\n",
      "  batch 800 loss: 0.4253411763906479\n",
      "  batch 850 loss: 0.4516361755132675\n",
      "  batch 900 loss: 0.4458473482728004\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.8750e-01.\n",
      "LOSS train 0.44585 valid 0.87033, valid PER 24.90%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.35685541838407514\n",
      "  batch 100 loss: 0.36341793298721314\n",
      "  batch 150 loss: 0.33499269366264345\n",
      "  batch 200 loss: 0.34933205872774126\n",
      "  batch 250 loss: 0.34644775062799454\n",
      "  batch 300 loss: 0.33887665569782255\n",
      "  batch 350 loss: 0.3274455034732819\n",
      "  batch 400 loss: 0.34611812442541123\n",
      "  batch 450 loss: 0.3479821589589119\n",
      "  batch 500 loss: 0.33255033791065214\n",
      "  batch 550 loss: 0.3572968935966492\n",
      "  batch 600 loss: 0.3428157275915146\n",
      "  batch 650 loss: 0.3644380474090576\n",
      "  batch 700 loss: 0.3713749876618385\n",
      "  batch 750 loss: 0.3349425274133682\n",
      "  batch 800 loss: 0.339101305603981\n",
      "  batch 850 loss: 0.3655450290441513\n",
      "  batch 900 loss: 0.35865341186523436\n",
      "Epoch 00013: reducing learning rate of group 0 to 9.3750e-02.\n",
      "LOSS train 0.35865 valid 0.88059, valid PER 24.61%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.3222522300481796\n",
      "  batch 100 loss: 0.2992166543006897\n",
      "  batch 150 loss: 0.30734054297208785\n",
      "  batch 200 loss: 0.29652017205953596\n",
      "  batch 250 loss: 0.31221745133399964\n",
      "  batch 300 loss: 0.32967494934797287\n",
      "  batch 350 loss: 0.2891867870092392\n",
      "  batch 400 loss: 0.30736715227365496\n",
      "  batch 450 loss: 0.30904948741197585\n",
      "  batch 500 loss: 0.3079945516586304\n",
      "  batch 550 loss: 0.33236459255218503\n",
      "  batch 600 loss: 0.29473999723792077\n",
      "  batch 650 loss: 0.31092236906290055\n",
      "  batch 700 loss: 0.33045565605163574\n",
      "  batch 750 loss: 0.3031432330608368\n",
      "  batch 800 loss: 0.2997403672337532\n",
      "  batch 850 loss: 0.32105095744132994\n",
      "  batch 900 loss: 0.3119621604681015\n",
      "Epoch 00014: reducing learning rate of group 0 to 4.6875e-02.\n",
      "LOSS train 0.31196 valid 0.89309, valid PER 24.91%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.2860120162367821\n",
      "  batch 100 loss: 0.2863529819250107\n",
      "  batch 150 loss: 0.29609941571950915\n",
      "  batch 200 loss: 0.28932421386241913\n",
      "  batch 250 loss: 0.3047476601600647\n",
      "  batch 300 loss: 0.2956152093410492\n",
      "  batch 350 loss: 0.2862758156657219\n",
      "  batch 400 loss: 0.2950382193922996\n",
      "  batch 450 loss: 0.27270083874464035\n",
      "  batch 500 loss: 0.2814047133922577\n",
      "  batch 550 loss: 0.2905901864171028\n",
      "  batch 600 loss: 0.2920107024908066\n",
      "  batch 650 loss: 0.30699351489543913\n",
      "  batch 700 loss: 0.29892750948667524\n",
      "  batch 750 loss: 0.2897529548406601\n",
      "  batch 800 loss: 0.2746302425861359\n",
      "  batch 850 loss: 0.2711346527934074\n",
      "  batch 900 loss: 0.2905327796936035\n",
      "Epoch 00015: reducing learning rate of group 0 to 2.3438e-02.\n",
      "LOSS train 0.29053 valid 0.89770, valid PER 24.82%\n",
      "EPOCH 16:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 50 loss: 0.291055403649807\n",
      "  batch 100 loss: 0.2786509099602699\n",
      "  batch 150 loss: 0.28050599902868273\n",
      "  batch 200 loss: 0.27124046385288236\n",
      "  batch 250 loss: 0.28743150025606157\n",
      "  batch 300 loss: 0.2709192422032356\n",
      "  batch 350 loss: 0.2828856107592583\n",
      "  batch 400 loss: 0.27981853783130645\n",
      "  batch 450 loss: 0.29188023805618285\n",
      "  batch 500 loss: 0.2639802795648575\n",
      "  batch 550 loss: 0.2667907354235649\n",
      "  batch 600 loss: 0.27451126992702485\n",
      "  batch 650 loss: 0.287089632153511\n",
      "  batch 700 loss: 0.26710293084383013\n",
      "  batch 750 loss: 0.27865264981985094\n",
      "  batch 800 loss: 0.27663732320070267\n",
      "  batch 850 loss: 0.27191267102956773\n",
      "  batch 900 loss: 0.2892590942978859\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.1719e-02.\n",
      "LOSS train 0.28926 valid 0.90320, valid PER 24.95%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.2728401251137257\n",
      "  batch 100 loss: 0.275545814037323\n",
      "  batch 150 loss: 0.27271121308207513\n",
      "  batch 200 loss: 0.26684260070323945\n",
      "  batch 250 loss: 0.28078650891780854\n",
      "  batch 300 loss: 0.2852397319674492\n",
      "  batch 350 loss: 0.26510005205869674\n",
      "  batch 400 loss: 0.28797383576631547\n",
      "  batch 450 loss: 0.2729329180717468\n",
      "  batch 500 loss: 0.25544903516769407\n",
      "  batch 550 loss: 0.27106923460960386\n",
      "  batch 600 loss: 0.2740124389529228\n",
      "  batch 650 loss: 0.27538020357489584\n",
      "  batch 700 loss: 0.2790797598659992\n",
      "  batch 750 loss: 0.2622116020321846\n",
      "  batch 800 loss: 0.25471981316804887\n",
      "  batch 850 loss: 0.27917376220226287\n",
      "  batch 900 loss: 0.26704921424388883\n",
      "Epoch 00017: reducing learning rate of group 0 to 5.8594e-03.\n",
      "LOSS train 0.26705 valid 0.90511, valid PER 24.88%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.2581596076488495\n",
      "  batch 100 loss: 0.26891582578420636\n",
      "  batch 150 loss: 0.2815506565570831\n",
      "  batch 200 loss: 0.2715388199687004\n",
      "  batch 250 loss: 0.28867813348770144\n",
      "  batch 300 loss: 0.25911927431821824\n",
      "  batch 350 loss: 0.26251627385616305\n",
      "  batch 400 loss: 0.2665118214488029\n",
      "  batch 450 loss: 0.2819743499159813\n",
      "  batch 500 loss: 0.26911439538002013\n",
      "  batch 550 loss: 0.28316372379660604\n",
      "  batch 600 loss: 0.262963627576828\n",
      "  batch 650 loss: 0.25176627293229104\n",
      "  batch 700 loss: 0.2804188847541809\n",
      "  batch 750 loss: 0.2495076748728752\n",
      "  batch 800 loss: 0.2698677811026573\n",
      "  batch 850 loss: 0.2681292289495468\n",
      "  batch 900 loss: 0.2758758652210236\n",
      "Epoch 00018: reducing learning rate of group 0 to 2.9297e-03.\n",
      "LOSS train 0.27588 valid 0.90651, valid PER 24.98%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.27730605512857437\n",
      "  batch 100 loss: 0.25411322712898254\n",
      "  batch 150 loss: 0.26972968369722367\n",
      "  batch 200 loss: 0.27306579023599625\n",
      "  batch 250 loss: 0.27224810391664506\n",
      "  batch 300 loss: 0.27354430556297304\n",
      "  batch 350 loss: 0.2600974103808403\n",
      "  batch 400 loss: 0.26203317910432816\n",
      "  batch 450 loss: 0.28297931343317034\n",
      "  batch 500 loss: 0.2690239003300667\n",
      "  batch 550 loss: 0.2598280808329582\n",
      "  batch 600 loss: 0.2577933120727539\n",
      "  batch 650 loss: 0.2882532849907875\n",
      "  batch 700 loss: 0.2605452537536621\n",
      "  batch 750 loss: 0.25447996735572814\n",
      "  batch 800 loss: 0.26829371839761734\n",
      "  batch 850 loss: 0.26915610700845716\n",
      "  batch 900 loss: 0.2685963264107704\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.4648e-03.\n",
      "LOSS train 0.26860 valid 0.90695, valid PER 25.01%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.2788449949026108\n",
      "  batch 100 loss: 0.2721936786174774\n",
      "  batch 150 loss: 0.2561315655708313\n",
      "  batch 200 loss: 0.26285254418849946\n",
      "  batch 250 loss: 0.27061888128519057\n",
      "  batch 300 loss: 0.27622796446084974\n",
      "  batch 350 loss: 0.25571359634399415\n",
      "  batch 400 loss: 0.26796367168426516\n",
      "  batch 450 loss: 0.271353677213192\n",
      "  batch 500 loss: 0.2569101694226265\n",
      "  batch 550 loss: 0.2806481519341469\n",
      "  batch 600 loss: 0.2555242791771889\n",
      "  batch 650 loss: 0.2737187460064888\n",
      "  batch 700 loss: 0.2665926092863083\n",
      "  batch 750 loss: 0.2541374792158604\n",
      "  batch 800 loss: 0.2763585638999939\n",
      "  batch 850 loss: 0.27358695209026335\n",
      "  batch 900 loss: 0.26313937962055206\n",
      "Epoch 00020: reducing learning rate of group 0 to 7.3242e-04.\n",
      "LOSS train 0.26314 valid 0.90732, valid PER 25.00%\n",
      "Training finished in 8.0 minutes.\n",
      "Model saved to checkpoints/20231209_163347/model_11\n",
      "End tuning For Wider 1 Layer LSTM For SGD optimiser\n"
     ]
    }
   ],
   "source": [
    "import model_regularisation_dropout\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start tuning For wider 1 Layer LSTM Using SGD Optimiser\")\n",
    "\n",
    "for starting_lr in Starting_Learning_rate:\n",
    "    print(\"Currently using SGD optimiser\")\n",
    "    args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'val_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 512,\n",
    "        'concat': 1,\n",
    "        'lr': starting_lr,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "    args = namedtuple('x', args)(**args)\n",
    "        \n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "        model_with_dropout = model_regularisation_dropout.BiLSTM(args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "        num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "        print('Total number of model parameters is {}'.format(num_params))\n",
    "        start = datetime.now()\n",
    "        model_with_dropout.to(args.device)\n",
    "        model_path = sgd_trainer(model_with_dropout, args)\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "        print('Model saved to {}'.format(model_path))\n",
    "\n",
    "print(\"End tuning For Wider 1 Layer LSTM For SGD optimiser\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50b928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95c917ab",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd4aeef",
   "metadata": {},
   "source": [
    "# Adam learning rate [0.005]\n",
    "# Dropout Rate =[0, 0.1, 0.3, 0.5, 0.7]\n",
    "# Model Structure 2 layer unidirectional LSTM, 210 dim in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7706df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0, 0.1, 0.3, 0.5, 0.7]\n",
    "starting_lrs = [0.005]\n",
    "Optimiser = [\"Adam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc910fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning For uni directional 2 Layer LSTM\n",
      "Currently using Adam optimiser\n",
      "Currently using dropout rate of 0\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.346335139274597\n",
      "  batch 100 loss: 3.123088126182556\n",
      "  batch 150 loss: 2.8972212362289427\n",
      "  batch 200 loss: 2.759316143989563\n",
      "  batch 250 loss: 2.633390688896179\n",
      "  batch 300 loss: 2.457405891418457\n",
      "  batch 350 loss: 2.27980318069458\n",
      "  batch 400 loss: 2.2151732540130613\n",
      "  batch 450 loss: 2.0854905366897585\n",
      "  batch 500 loss: 2.027049510478973\n",
      "  batch 550 loss: 1.9372886395454407\n",
      "  batch 600 loss: 1.8851449275016785\n",
      "  batch 650 loss: 1.8118459391593933\n",
      "  batch 700 loss: 1.8733761239051818\n",
      "  batch 750 loss: 1.7866853785514831\n",
      "  batch 800 loss: 1.7571559047698975\n",
      "  batch 850 loss: 1.741557686328888\n",
      "  batch 900 loss: 1.729168155193329\n",
      "LOSS train 1.72917 valid 1.68293, valid PER 53.77%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.6805900597572327\n",
      "  batch 100 loss: 1.6421936058998108\n",
      "  batch 150 loss: 1.6390576481819152\n",
      "  batch 200 loss: 1.6454333639144898\n",
      "  batch 250 loss: 1.628677659034729\n",
      "  batch 300 loss: 1.6136492919921874\n",
      "  batch 350 loss: 1.5403812646865844\n",
      "  batch 400 loss: 1.6146198964118958\n",
      "  batch 450 loss: 1.5061244440078736\n",
      "  batch 500 loss: 1.5588920831680297\n",
      "  batch 550 loss: 1.571539385318756\n",
      "  batch 600 loss: 1.5275572371482848\n",
      "  batch 650 loss: 1.5306053209304809\n",
      "  batch 700 loss: 1.5392885041236877\n",
      "  batch 750 loss: 1.4921696639060975\n",
      "  batch 800 loss: 1.4255852675437928\n",
      "  batch 850 loss: 1.4189952874183656\n",
      "  batch 900 loss: 1.4471392297744752\n",
      "LOSS train 1.44714 valid 1.44427, valid PER 45.29%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4138114213943482\n",
      "  batch 100 loss: 1.383646352291107\n",
      "  batch 150 loss: 1.3938966083526612\n",
      "  batch 200 loss: 1.4333980870246887\n",
      "  batch 250 loss: 1.4110035133361816\n",
      "  batch 300 loss: 1.3686593985557556\n",
      "  batch 350 loss: 1.3723334860801697\n",
      "  batch 400 loss: 1.3582772970199586\n",
      "  batch 450 loss: 1.3673182392120362\n",
      "  batch 500 loss: 1.382331998348236\n",
      "  batch 550 loss: 1.3438371205329895\n",
      "  batch 600 loss: 1.3205411648750305\n",
      "  batch 650 loss: 1.2812210202217102\n",
      "  batch 700 loss: 1.287936955690384\n",
      "  batch 750 loss: 1.3619839572906494\n",
      "  batch 800 loss: 1.294835637807846\n",
      "  batch 850 loss: 1.3246308493614196\n",
      "  batch 900 loss: 1.2848630583286285\n",
      "LOSS train 1.28486 valid 1.30096, valid PER 40.21%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.221564620733261\n",
      "  batch 100 loss: 1.2637852358818054\n",
      "  batch 150 loss: 1.2137384116649628\n",
      "  batch 200 loss: 1.240152848958969\n",
      "  batch 250 loss: 1.273683420419693\n",
      "  batch 300 loss: 1.2545114684104919\n",
      "  batch 350 loss: 1.183315325975418\n",
      "  batch 400 loss: 1.2457076859474183\n",
      "  batch 450 loss: 1.2427559447288514\n",
      "  batch 500 loss: 1.2247398436069488\n",
      "  batch 550 loss: 1.2606972646713257\n",
      "  batch 600 loss: 1.2620630073547363\n",
      "  batch 650 loss: 1.2575220274925232\n",
      "  batch 700 loss: 1.2019032752513885\n",
      "  batch 750 loss: 1.1965738785266877\n",
      "  batch 800 loss: 1.1916371524333953\n",
      "  batch 850 loss: 1.201764258146286\n",
      "  batch 900 loss: 1.2774888813495635\n",
      "LOSS train 1.27749 valid 1.24684, valid PER 38.51%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.1843058466911316\n",
      "  batch 100 loss: 1.1514546823501588\n",
      "  batch 150 loss: 1.2006030976772308\n",
      "  batch 200 loss: 1.1569578087329864\n",
      "  batch 250 loss: 1.148093490600586\n",
      "  batch 300 loss: 1.1980010104179382\n",
      "  batch 350 loss: 1.2311751306056977\n",
      "  batch 400 loss: 1.226639859676361\n",
      "  batch 450 loss: 1.1737953150272369\n",
      "  batch 500 loss: 1.2620744502544403\n",
      "  batch 550 loss: 1.1830314254760743\n",
      "  batch 600 loss: 1.187779176235199\n",
      "  batch 650 loss: 1.1624515104293822\n",
      "  batch 700 loss: 1.1813051867485047\n",
      "  batch 750 loss: 1.1236452925205231\n",
      "  batch 800 loss: 1.1260358476638794\n",
      "  batch 850 loss: 1.1771701133251191\n",
      "  batch 900 loss: 1.1794613218307495\n",
      "LOSS train 1.17946 valid 1.22325, valid PER 37.48%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.184301393032074\n",
      "  batch 100 loss: 1.1211773347854614\n",
      "  batch 150 loss: 1.1177042639255523\n",
      "  batch 200 loss: 1.1273329985141753\n",
      "  batch 250 loss: 1.2280286300182341\n",
      "  batch 300 loss: 1.1419901883602142\n",
      "  batch 350 loss: 1.1385978090763091\n",
      "  batch 400 loss: 1.111969301700592\n",
      "  batch 450 loss: 1.1665591096878052\n",
      "  batch 500 loss: 1.1669633841514588\n",
      "  batch 550 loss: 1.1429058825969696\n",
      "  batch 600 loss: 1.1055473625659942\n",
      "  batch 650 loss: 1.141941169500351\n",
      "  batch 700 loss: 1.1213818514347076\n",
      "  batch 750 loss: 1.0788618326187134\n",
      "  batch 800 loss: 1.0876296639442444\n",
      "  batch 850 loss: 1.079750200510025\n",
      "  batch 900 loss: 1.100754805803299\n",
      "LOSS train 1.10075 valid 1.14850, valid PER 35.48%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.0638389945030213\n",
      "  batch 100 loss: 1.0895418620109558\n",
      "  batch 150 loss: 1.0626578736305237\n",
      "  batch 200 loss: 1.0779662394523621\n",
      "  batch 250 loss: 1.086147586107254\n",
      "  batch 300 loss: 1.0677290916442872\n",
      "  batch 350 loss: 1.0471811258792878\n",
      "  batch 400 loss: 1.0914172446727752\n",
      "  batch 450 loss: 1.083638653755188\n",
      "  batch 500 loss: 1.0642526853084564\n",
      "  batch 550 loss: 1.0657382559776307\n",
      "  batch 600 loss: 1.0820286417007445\n",
      "  batch 650 loss: 1.0776004254817964\n",
      "  batch 700 loss: 1.0755802690982819\n",
      "  batch 750 loss: 1.0544773602485658\n",
      "  batch 800 loss: 1.0348173904418945\n",
      "  batch 850 loss: 1.0825819838047028\n",
      "  batch 900 loss: 1.1027373707294463\n",
      "LOSS train 1.10274 valid 1.13290, valid PER 35.48%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.0870266377925872\n",
      "  batch 100 loss: 1.0437310361862182\n",
      "  batch 150 loss: 1.102265980243683\n",
      "  batch 200 loss: 1.0944721961021424\n",
      "  batch 250 loss: 1.1148456978797912\n",
      "  batch 300 loss: 1.0467997777462006\n",
      "  batch 350 loss: 1.1054167115688325\n",
      "  batch 400 loss: 1.0713551557064056\n",
      "  batch 450 loss: 1.0874370956420898\n",
      "  batch 500 loss: 1.1434245014190674\n",
      "  batch 550 loss: 1.0518997967243195\n",
      "  batch 600 loss: 1.07971372961998\n",
      "  batch 650 loss: 1.0844733572006227\n",
      "  batch 700 loss: 1.0793527507781981\n",
      "  batch 750 loss: 1.0596640455722808\n",
      "  batch 800 loss: 1.1028476142883301\n",
      "  batch 850 loss: 1.0404668545722962\n",
      "  batch 900 loss: 1.0117581641674043\n",
      "LOSS train 1.01176 valid 1.08791, valid PER 33.83%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.9736240696907044\n",
      "  batch 100 loss: 1.048067135810852\n",
      "  batch 150 loss: 1.0339593517780303\n",
      "  batch 200 loss: 0.9535671210289002\n",
      "  batch 250 loss: 1.0276022791862487\n",
      "  batch 300 loss: 1.0694653713703155\n",
      "  batch 350 loss: 1.0496470749378204\n",
      "  batch 400 loss: 1.0436104536056519\n",
      "  batch 450 loss: 1.0811587250232697\n",
      "  batch 500 loss: 1.0106819891929626\n",
      "  batch 550 loss: 1.0450898015499115\n",
      "  batch 600 loss: 1.0654912745952607\n",
      "  batch 650 loss: 1.0126245653629302\n",
      "  batch 700 loss: 1.0271622049808502\n",
      "  batch 750 loss: 1.0580253767967225\n",
      "  batch 800 loss: 1.0574635231494904\n",
      "  batch 850 loss: 1.0946881210803985\n",
      "  batch 900 loss: 1.0610425209999084\n",
      "LOSS train 1.06104 valid 1.13958, valid PER 34.97%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 1.037438042163849\n",
      "  batch 100 loss: 1.0206341636180878\n",
      "  batch 150 loss: 1.0222614407539368\n",
      "  batch 200 loss: 1.0422069466114043\n",
      "  batch 250 loss: 1.0228401148319244\n",
      "  batch 300 loss: 1.0333624005317688\n",
      "  batch 350 loss: 1.0645224678516387\n",
      "  batch 400 loss: 1.0077461647987365\n",
      "  batch 450 loss: 0.9747307097911835\n",
      "  batch 500 loss: 1.0474734222888946\n",
      "  batch 550 loss: 1.017708957195282\n",
      "  batch 600 loss: 1.0146179831027984\n",
      "  batch 650 loss: 1.0148076367378236\n",
      "  batch 700 loss: 1.0656993293762207\n",
      "  batch 750 loss: 0.9972109889984131\n",
      "  batch 800 loss: 1.0259416317939758\n",
      "  batch 850 loss: 1.031964409351349\n",
      "  batch 900 loss: 1.0405815815925599\n",
      "LOSS train 1.04058 valid 1.09992, valid PER 33.24%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.9840217137336731\n",
      "  batch 100 loss: 0.9724311065673829\n",
      "  batch 150 loss: 0.9942829608917236\n",
      "  batch 200 loss: 1.0361964535713195\n",
      "  batch 250 loss: 1.0184188735485078\n",
      "  batch 300 loss: 0.9693534326553345\n",
      "  batch 350 loss: 0.9984158146381378\n",
      "  batch 400 loss: 1.0155165934562682\n",
      "  batch 450 loss: 1.0140125524997712\n",
      "  batch 500 loss: 0.9984259510040283\n",
      "  batch 550 loss: 1.0023821938037871\n",
      "  batch 600 loss: 0.9684407711029053\n",
      "  batch 650 loss: 1.0414186227321625\n",
      "  batch 700 loss: 0.9801715743541718\n",
      "  batch 750 loss: 0.9864963066577911\n",
      "  batch 800 loss: 1.0161661434173583\n",
      "  batch 850 loss: 1.0579941499233245\n",
      "  batch 900 loss: 1.0237471497058868\n",
      "LOSS train 1.02375 valid 1.08009, valid PER 32.77%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.963344669342041\n",
      "  batch 100 loss: 0.9496836376190185\n",
      "  batch 150 loss: 0.9210996413230896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.9457796919345856\n",
      "  batch 250 loss: 1.0278704571723938\n",
      "  batch 300 loss: 0.9684745728969574\n",
      "  batch 350 loss: 0.9747871959209442\n",
      "  batch 400 loss: 0.996763927936554\n",
      "  batch 450 loss: 0.9441496992111206\n",
      "  batch 500 loss: 1.016985023021698\n",
      "  batch 550 loss: 0.9403657221794128\n",
      "  batch 600 loss: 0.9850481700897217\n",
      "  batch 650 loss: 1.0400050342082978\n",
      "  batch 700 loss: 1.0020142376422883\n",
      "  batch 750 loss: 0.9509621059894562\n",
      "  batch 800 loss: 0.9749845099449158\n",
      "  batch 850 loss: 1.0084507048130036\n",
      "  batch 900 loss: 0.9841943144798279\n",
      "LOSS train 0.98419 valid 1.06885, valid PER 32.68%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.9511243331432343\n",
      "  batch 100 loss: 0.9984589684009552\n",
      "  batch 150 loss: 0.9416204750537872\n",
      "  batch 200 loss: 0.9569619083404541\n",
      "  batch 250 loss: 0.960421462059021\n",
      "  batch 300 loss: 0.9359949278831482\n",
      "  batch 350 loss: 0.9645335602760315\n",
      "  batch 400 loss: 1.026206476688385\n",
      "  batch 450 loss: 1.0437476682662963\n",
      "  batch 500 loss: 0.9742412936687469\n",
      "  batch 550 loss: 0.983635526895523\n",
      "  batch 600 loss: 1.0008366620540619\n",
      "  batch 650 loss: 0.9896576178073883\n",
      "  batch 700 loss: 0.9762277412414551\n",
      "  batch 750 loss: 0.9307442390918732\n",
      "  batch 800 loss: 0.9934604346752167\n",
      "  batch 850 loss: 1.0703195869922637\n",
      "  batch 900 loss: 1.0270623230934144\n",
      "LOSS train 1.02706 valid 1.05395, valid PER 32.54%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.9245558857917786\n",
      "  batch 100 loss: 0.9781978917121887\n",
      "  batch 150 loss: 0.9361025977134705\n",
      "  batch 200 loss: 0.9645237910747528\n",
      "  batch 250 loss: 0.9241493415832519\n",
      "  batch 300 loss: 0.9605509471893311\n",
      "  batch 350 loss: 0.9480715966224671\n",
      "  batch 400 loss: 0.9377711689472199\n",
      "  batch 450 loss: 0.9172283720970154\n",
      "  batch 500 loss: 1.011972597837448\n",
      "  batch 550 loss: 0.9877412855625153\n",
      "  batch 600 loss: 0.951739764213562\n",
      "  batch 650 loss: 0.9853340005874633\n",
      "  batch 700 loss: 0.9712864351272583\n",
      "  batch 750 loss: 0.9459635388851165\n",
      "  batch 800 loss: 0.9229296684265137\n",
      "  batch 850 loss: 0.996967875957489\n",
      "  batch 900 loss: 0.9893154716491699\n",
      "LOSS train 0.98932 valid 1.08287, valid PER 33.23%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.9569672930240631\n",
      "  batch 100 loss: 0.9163461685180664\n",
      "  batch 150 loss: 0.8814743769168853\n",
      "  batch 200 loss: 0.9527774608135223\n",
      "  batch 250 loss: 0.9829451751708984\n",
      "  batch 300 loss: 0.9687257730960845\n",
      "  batch 350 loss: 0.9480659663677216\n",
      "  batch 400 loss: 0.9362638306617737\n",
      "  batch 450 loss: 0.9498589193820953\n",
      "  batch 500 loss: 0.9494718146324158\n",
      "  batch 550 loss: 0.989093177318573\n",
      "  batch 600 loss: 0.9594235968589783\n",
      "  batch 650 loss: 0.9443558883666993\n",
      "  batch 700 loss: 0.9703747475147247\n",
      "  batch 750 loss: 0.9638229584693909\n",
      "  batch 800 loss: 0.932138341665268\n",
      "  batch 850 loss: 0.9594704961776733\n",
      "  batch 900 loss: 0.9816958224773407\n",
      "LOSS train 0.98170 valid 1.02993, valid PER 31.46%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.9410983788967132\n",
      "  batch 100 loss: 0.8891303193569183\n",
      "  batch 150 loss: 0.9263689398765564\n",
      "  batch 200 loss: 0.9137053740024567\n",
      "  batch 250 loss: 0.9512196624279022\n",
      "  batch 300 loss: 0.9521588134765625\n",
      "  batch 350 loss: 0.9786836659908295\n",
      "  batch 400 loss: 0.9333124017715454\n",
      "  batch 450 loss: 0.9601452493667603\n",
      "  batch 500 loss: 0.9022573828697205\n",
      "  batch 550 loss: 0.9287122535705566\n",
      "  batch 600 loss: 0.9179634261131286\n",
      "  batch 650 loss: 0.9716650438308716\n",
      "  batch 700 loss: 0.9560638654232025\n",
      "  batch 750 loss: 0.9563603782653809\n",
      "  batch 800 loss: 0.9527510607242584\n",
      "  batch 850 loss: 0.9550431835651397\n",
      "  batch 900 loss: 0.9316000378131867\n",
      "LOSS train 0.93160 valid 1.07887, valid PER 32.98%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.974953500032425\n",
      "  batch 100 loss: 0.9361459302902222\n",
      "  batch 150 loss: 0.907770563364029\n",
      "  batch 200 loss: 0.8918255126476288\n",
      "  batch 250 loss: 0.9256787633895874\n",
      "  batch 300 loss: 0.9529132044315338\n",
      "  batch 350 loss: 0.9318667829036713\n",
      "  batch 400 loss: 0.977401419878006\n",
      "  batch 450 loss: 0.9819341087341309\n",
      "  batch 500 loss: 0.9389485049247742\n",
      "  batch 550 loss: 0.9652046775817871\n",
      "  batch 600 loss: 0.9985377943515777\n",
      "  batch 650 loss: 0.9196322119235992\n",
      "  batch 700 loss: 0.9285587811470032\n",
      "  batch 750 loss: 0.9119371056556702\n",
      "  batch 800 loss: 0.9206517243385315\n",
      "  batch 850 loss: 0.9318564987182617\n",
      "  batch 900 loss: 0.9097069823741912\n",
      "LOSS train 0.90971 valid 1.03332, valid PER 30.76%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.8941869556903839\n",
      "  batch 100 loss: 0.934888973236084\n",
      "  batch 150 loss: 0.9161961209774018\n",
      "  batch 200 loss: 0.8848301410675049\n",
      "  batch 250 loss: 0.8964689946174622\n",
      "  batch 300 loss: 0.8855439817905426\n",
      "  batch 350 loss: 0.9420594787597656\n",
      "  batch 400 loss: 0.8601895534992218\n",
      "  batch 450 loss: 0.9146132588386535\n",
      "  batch 500 loss: 0.8941073417663574\n",
      "  batch 550 loss: 0.917030987739563\n",
      "  batch 600 loss: 0.8937923455238342\n",
      "  batch 650 loss: 0.8803099727630616\n",
      "  batch 700 loss: 0.934816130399704\n",
      "  batch 750 loss: 0.909696831703186\n",
      "  batch 800 loss: 0.8846709609031678\n",
      "  batch 850 loss: 0.8908628368377686\n",
      "  batch 900 loss: 0.9466827213764191\n",
      "LOSS train 0.94668 valid 1.02591, valid PER 31.64%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.8284514439105988\n",
      "  batch 100 loss: 0.8716539657115936\n",
      "  batch 150 loss: 0.916281236410141\n",
      "  batch 200 loss: 0.9189810287952423\n",
      "  batch 250 loss: 0.9455173540115357\n",
      "  batch 300 loss: 0.9235749578475952\n",
      "  batch 350 loss: 0.901470102071762\n",
      "  batch 400 loss: 0.8986389183998108\n",
      "  batch 450 loss: 0.8756723475456237\n",
      "  batch 500 loss: 0.9256138956546783\n",
      "  batch 550 loss: 0.9046976840496064\n",
      "  batch 600 loss: 0.8903944492340088\n",
      "  batch 650 loss: 0.9201874971389771\n",
      "  batch 700 loss: 0.8669150793552398\n",
      "  batch 750 loss: 0.9212763833999634\n",
      "  batch 800 loss: 0.9254370176792145\n",
      "  batch 850 loss: 0.9307194936275482\n",
      "  batch 900 loss: 0.9055759179592132\n",
      "LOSS train 0.90558 valid 1.05121, valid PER 32.14%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.8639936542510986\n",
      "  batch 100 loss: 0.8944213616847992\n",
      "  batch 150 loss: 0.868144781589508\n",
      "  batch 200 loss: 0.8938569736480713\n",
      "  batch 250 loss: 0.8699315881729126\n",
      "  batch 300 loss: 0.9064585208892822\n",
      "  batch 350 loss: 0.8486272847652435\n",
      "  batch 400 loss: 0.8938753592967987\n",
      "  batch 450 loss: 0.8805402338504791\n",
      "  batch 500 loss: 0.8714016199111938\n",
      "  batch 550 loss: 0.9146387374401093\n",
      "  batch 600 loss: 0.8798906779289246\n",
      "  batch 650 loss: 0.8855302226543427\n",
      "  batch 700 loss: 0.8886166155338288\n",
      "  batch 750 loss: 0.8891838729381562\n",
      "  batch 800 loss: 0.9168761813640595\n",
      "  batch 850 loss: 0.9173885011672973\n",
      "  batch 900 loss: 0.8839264607429504\n",
      "LOSS train 0.88393 valid 1.01524, valid PER 31.15%\n",
      "Training finished in 10.0 minutes.\n",
      "Model saved to checkpoints/20231211_110857/model_20\n",
      "Currently using dropout rate of 0.1\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.527525548934936\n",
      "  batch 100 loss: 3.210155305862427\n",
      "  batch 150 loss: 2.9932041692733766\n",
      "  batch 200 loss: 2.8205918359756468\n",
      "  batch 250 loss: 2.805464758872986\n",
      "  batch 300 loss: 2.6180273199081423\n",
      "  batch 350 loss: 2.4811631870269775\n",
      "  batch 400 loss: 2.4218621921539305\n",
      "  batch 450 loss: 2.249559607505798\n",
      "  batch 500 loss: 2.137460193634033\n",
      "  batch 550 loss: 2.101159062385559\n",
      "  batch 600 loss: 2.0045972228050233\n",
      "  batch 650 loss: 1.916213321685791\n",
      "  batch 700 loss: 1.9350300240516662\n",
      "  batch 750 loss: 1.8673702692985534\n",
      "  batch 800 loss: 1.8806529688835143\n",
      "  batch 850 loss: 1.8062560963630676\n",
      "  batch 900 loss: 1.777621190547943\n",
      "LOSS train 1.77762 valid 1.77844, valid PER 57.91%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 1.7504516816139222\n",
      "  batch 100 loss: 1.7362112164497376\n",
      "  batch 150 loss: 1.686256582736969\n",
      "  batch 200 loss: 1.7126800894737244\n",
      "  batch 250 loss: 1.6958693194389343\n",
      "  batch 300 loss: 1.6902364206314087\n",
      "  batch 350 loss: 1.635694181919098\n",
      "  batch 400 loss: 1.6250413537025452\n",
      "  batch 450 loss: 1.55053076505661\n",
      "  batch 500 loss: 1.607842004299164\n",
      "  batch 550 loss: 1.6482946848869324\n",
      "  batch 600 loss: 1.5987833285331725\n",
      "  batch 650 loss: 1.5940530490875244\n",
      "  batch 700 loss: 1.5358675026893616\n",
      "  batch 750 loss: 1.518585753440857\n",
      "  batch 800 loss: 1.4909065246582032\n",
      "  batch 850 loss: 1.514536635875702\n",
      "  batch 900 loss: 1.5386173248291015\n",
      "LOSS train 1.53862 valid 1.46465, valid PER 45.94%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.4943193912506103\n",
      "  batch 100 loss: 1.423487629890442\n",
      "  batch 150 loss: 1.4377655720710754\n",
      "  batch 200 loss: 1.4356787300109863\n",
      "  batch 250 loss: 1.3829126834869385\n",
      "  batch 300 loss: 1.404154736995697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 350 loss: 1.4212997007369994\n",
      "  batch 400 loss: 1.3944188094139098\n",
      "  batch 450 loss: 1.3860944437980651\n",
      "  batch 500 loss: 1.3918337512016297\n",
      "  batch 550 loss: 1.376759672164917\n",
      "  batch 600 loss: 1.3486296474933623\n",
      "  batch 650 loss: 1.320385057926178\n",
      "  batch 700 loss: 1.3427540981769561\n",
      "  batch 750 loss: 1.4199447441101074\n",
      "  batch 800 loss: 1.3214681839942932\n",
      "  batch 850 loss: 1.3489288091659546\n",
      "  batch 900 loss: 1.3224798917770386\n",
      "LOSS train 1.32248 valid 1.38168, valid PER 42.95%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.327938266992569\n",
      "  batch 100 loss: 1.317831835746765\n",
      "  batch 150 loss: 1.2667737650871276\n",
      "  batch 200 loss: 1.2896015429496765\n",
      "  batch 250 loss: 1.301979546546936\n",
      "  batch 300 loss: 1.3257019996643067\n",
      "  batch 350 loss: 1.2707879745960236\n",
      "  batch 400 loss: 1.3070688939094544\n",
      "  batch 450 loss: 1.2689840757846833\n",
      "  batch 500 loss: 1.234335242509842\n",
      "  batch 550 loss: 1.2901889824867248\n",
      "  batch 600 loss: 1.3281439304351808\n",
      "  batch 650 loss: 1.3101835107803346\n",
      "  batch 700 loss: 1.2885697078704834\n",
      "  batch 750 loss: 1.240275912284851\n",
      "  batch 800 loss: 1.211097447872162\n",
      "  batch 850 loss: 1.2511606645584106\n",
      "  batch 900 loss: 1.272564970254898\n",
      "LOSS train 1.27256 valid 1.27240, valid PER 40.17%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.2136822104454041\n",
      "  batch 100 loss: 1.2201145958900452\n",
      "  batch 150 loss: 1.2862914049625396\n",
      "  batch 200 loss: 1.1878670239448548\n",
      "  batch 250 loss: 1.2120929777622222\n",
      "  batch 300 loss: 1.2241259598731995\n",
      "  batch 350 loss: 1.2558091723918914\n",
      "  batch 400 loss: 1.2535140407085419\n",
      "  batch 450 loss: 1.251894897222519\n",
      "  batch 500 loss: 1.2545523691177367\n",
      "  batch 550 loss: 1.1812209928035735\n",
      "  batch 600 loss: 1.2542051517963408\n",
      "  batch 650 loss: 1.2436388671398162\n",
      "  batch 700 loss: 1.2573730754852295\n",
      "  batch 750 loss: 1.2058271634578706\n",
      "  batch 800 loss: 1.2466327273845672\n",
      "  batch 850 loss: 1.2119793593883514\n",
      "  batch 900 loss: 1.2683426880836486\n",
      "LOSS train 1.26834 valid 1.25471, valid PER 37.80%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.215257440805435\n",
      "  batch 100 loss: 1.150080542564392\n",
      "  batch 150 loss: 1.1392805886268615\n",
      "  batch 200 loss: 1.2149155473709106\n",
      "  batch 250 loss: 1.211432260274887\n",
      "  batch 300 loss: 1.1617403888702393\n",
      "  batch 350 loss: 1.1797994792461395\n",
      "  batch 400 loss: 1.1709332585334777\n",
      "  batch 450 loss: 1.1918167185783386\n",
      "  batch 500 loss: 1.2292416965961457\n",
      "  batch 550 loss: 1.216962127685547\n",
      "  batch 600 loss: 1.148764852285385\n",
      "  batch 650 loss: 1.1933920967578888\n",
      "  batch 700 loss: 1.182233568429947\n",
      "  batch 750 loss: 1.1636612188816071\n",
      "  batch 800 loss: 1.2050569868087768\n",
      "  batch 850 loss: 1.1618114280700684\n",
      "  batch 900 loss: 1.184694926738739\n",
      "LOSS train 1.18469 valid 1.17750, valid PER 36.53%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.1225012230873108\n",
      "  batch 100 loss: 1.1724848437309265\n",
      "  batch 150 loss: 1.1223405659198762\n",
      "  batch 200 loss: 1.1079299521446229\n",
      "  batch 250 loss: 1.151471791267395\n",
      "  batch 300 loss: 1.0821864640712737\n",
      "  batch 350 loss: 1.1349394464492797\n",
      "  batch 400 loss: 1.1351853322982788\n",
      "  batch 450 loss: 1.14193981051445\n",
      "  batch 500 loss: 1.114397644996643\n",
      "  batch 550 loss: 1.1184621357917786\n",
      "  batch 600 loss: 1.1398441147804261\n",
      "  batch 650 loss: 1.1162830197811127\n",
      "  batch 700 loss: 1.124768191576004\n",
      "  batch 750 loss: 1.1434365129470825\n",
      "  batch 800 loss: 1.131527192592621\n",
      "  batch 850 loss: 1.1983504462242127\n",
      "  batch 900 loss: 1.1840263390541077\n",
      "LOSS train 1.18403 valid 1.16775, valid PER 35.64%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.1127159190177918\n",
      "  batch 100 loss: 1.0902141404151917\n",
      "  batch 150 loss: 1.0743452990055085\n",
      "  batch 200 loss: 1.072838432788849\n",
      "  batch 250 loss: 1.1167675614356996\n",
      "  batch 300 loss: 1.0492188334465027\n",
      "  batch 350 loss: 1.1276061582565307\n",
      "  batch 400 loss: 1.0804266154766082\n",
      "  batch 450 loss: 1.1157446146011352\n",
      "  batch 500 loss: 1.117137907743454\n",
      "  batch 550 loss: 1.0912741065025329\n",
      "  batch 600 loss: 1.1469705867767335\n",
      "  batch 650 loss: 1.1516885459423065\n",
      "  batch 700 loss: 1.0926805114746094\n",
      "  batch 750 loss: 1.111776851415634\n",
      "  batch 800 loss: 1.1042432975769043\n",
      "  batch 850 loss: 1.0919088912010193\n",
      "  batch 900 loss: 1.0876786530017852\n",
      "LOSS train 1.08768 valid 1.12770, valid PER 34.30%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 1.0507890975475311\n",
      "  batch 100 loss: 1.0769109976291658\n",
      "  batch 150 loss: 1.0943810987472533\n",
      "  batch 200 loss: 1.0534517574310303\n",
      "  batch 250 loss: 1.075360952615738\n",
      "  batch 300 loss: 1.0976870942115784\n",
      "  batch 350 loss: 1.1507572472095489\n",
      "  batch 400 loss: 1.1097444939613341\n",
      "  batch 450 loss: 1.100455093383789\n",
      "  batch 500 loss: 1.0583350348472595\n",
      "  batch 550 loss: 1.1079620957374572\n",
      "  batch 600 loss: 1.1129554879665375\n",
      "  batch 650 loss: 1.077789624929428\n",
      "  batch 700 loss: 1.053712340593338\n",
      "  batch 750 loss: 1.0822506284713744\n",
      "  batch 800 loss: 1.1149860298633576\n",
      "  batch 850 loss: 1.1406532514095307\n",
      "  batch 900 loss: 1.0788790822029113\n",
      "LOSS train 1.07888 valid 1.16283, valid PER 35.73%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 1.04889919757843\n",
      "  batch 100 loss: 1.0435080778598786\n",
      "  batch 150 loss: 1.04694202542305\n",
      "  batch 200 loss: 1.0705884850025178\n",
      "  batch 250 loss: 1.0531454920768737\n",
      "  batch 300 loss: 1.0083880805969239\n",
      "  batch 350 loss: 1.0643519926071168\n",
      "  batch 400 loss: 1.0238169920444489\n",
      "  batch 450 loss: 1.0161749291419984\n",
      "  batch 500 loss: 1.0952695143222808\n",
      "  batch 550 loss: 1.0625158143043518\n",
      "  batch 600 loss: 1.0535570979118347\n",
      "  batch 650 loss: 1.0270008051395416\n",
      "  batch 700 loss: 1.0654214704036713\n",
      "  batch 750 loss: 1.0238104212284087\n",
      "  batch 800 loss: 1.0604643821716309\n",
      "  batch 850 loss: 1.0677623534202576\n",
      "  batch 900 loss: 1.0683171570301055\n",
      "LOSS train 1.06832 valid 1.08740, valid PER 33.02%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 1.023670904636383\n",
      "  batch 100 loss: 0.9935230588912964\n",
      "  batch 150 loss: 1.0199952948093414\n",
      "  batch 200 loss: 1.098674578666687\n",
      "  batch 250 loss: 1.0644397377967834\n",
      "  batch 300 loss: 1.0340003502368926\n",
      "  batch 350 loss: 1.040911580324173\n",
      "  batch 400 loss: 1.0751358413696288\n",
      "  batch 450 loss: 1.035899623632431\n",
      "  batch 500 loss: 1.029834394454956\n",
      "  batch 550 loss: 1.0538645458221436\n",
      "  batch 600 loss: 1.017758582830429\n",
      "  batch 650 loss: 1.1073284506797791\n",
      "  batch 700 loss: 1.044403703212738\n",
      "  batch 750 loss: 1.0585727035999297\n",
      "  batch 800 loss: 1.1009339249134065\n",
      "  batch 850 loss: 1.101108490228653\n",
      "  batch 900 loss: 1.0753978204727173\n",
      "LOSS train 1.07540 valid 1.10837, valid PER 33.36%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 1.0313766860961915\n",
      "  batch 100 loss: 1.0121920084953309\n",
      "  batch 150 loss: 0.9983623719215393\n",
      "  batch 200 loss: 1.0056257939338684\n",
      "  batch 250 loss: 1.025088461637497\n",
      "  batch 300 loss: 1.0115348243713378\n",
      "  batch 350 loss: 1.0154155337810515\n",
      "  batch 400 loss: 1.0878332936763764\n",
      "  batch 450 loss: 1.0643402802944184\n",
      "  batch 500 loss: 1.079501407146454\n",
      "  batch 550 loss: 1.0030235397815703\n",
      "  batch 600 loss: 1.0442784786224366\n",
      "  batch 650 loss: 1.0885126781463623\n",
      "  batch 700 loss: 1.0653031492233276\n",
      "  batch 750 loss: 1.0274435687065124\n",
      "  batch 800 loss: 1.0444294095039368\n",
      "  batch 850 loss: 1.03308851480484\n",
      "  batch 900 loss: 1.0362054681777955\n",
      "LOSS train 1.03621 valid 1.07426, valid PER 32.32%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.9677402710914612\n",
      "  batch 100 loss: 1.012691034078598\n",
      "  batch 150 loss: 0.980473427772522\n",
      "  batch 200 loss: 0.9951044917106628\n",
      "  batch 250 loss: 1.0146631455421449\n",
      "  batch 300 loss: 0.9771715700626373\n",
      "  batch 350 loss: 0.993800505399704\n",
      "  batch 400 loss: 1.0152261328697205\n",
      "  batch 450 loss: 1.038284751176834\n",
      "  batch 500 loss: 0.9806747889518738\n",
      "  batch 550 loss: 0.966896687746048\n",
      "  batch 600 loss: 1.021102659702301\n",
      "  batch 650 loss: 1.0091303265094758\n",
      "  batch 700 loss: 1.0213910281658172\n",
      "  batch 750 loss: 0.9886696195602417\n",
      "  batch 800 loss: 1.0184745168685914\n",
      "  batch 850 loss: 1.045004540681839\n",
      "  batch 900 loss: 1.0593824982643127\n",
      "LOSS train 1.05938 valid 1.06337, valid PER 32.40%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.954978928565979\n",
      "  batch 100 loss: 0.9942574143409729\n",
      "  batch 150 loss: 0.9625914347171783\n",
      "  batch 200 loss: 1.0182002937793733\n",
      "  batch 250 loss: 1.0795786738395692\n",
      "  batch 300 loss: 1.0903295767307282\n",
      "  batch 350 loss: 1.064244853258133\n",
      "  batch 400 loss: 1.0687541449069977\n",
      "  batch 450 loss: 1.0306663119792938\n",
      "  batch 500 loss: 1.0174401354789735\n",
      "  batch 550 loss: 1.0330588042736053\n",
      "  batch 600 loss: 0.9845255947113037\n",
      "  batch 650 loss: 1.0254808378219604\n",
      "  batch 700 loss: 0.9951970279216766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 750 loss: 0.9998311042785645\n",
      "  batch 800 loss: 0.9480529737472534\n",
      "  batch 850 loss: 1.0482417905330659\n",
      "  batch 900 loss: 0.999470648765564\n",
      "LOSS train 0.99947 valid 1.08362, valid PER 33.66%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.9902499556541443\n",
      "  batch 100 loss: 0.9472691059112549\n",
      "  batch 150 loss: 0.930337985754013\n",
      "  batch 200 loss: 0.9989167439937592\n",
      "  batch 250 loss: 0.9690368342399597\n",
      "  batch 300 loss: 0.9400697934627533\n",
      "  batch 350 loss: 0.9541654324531555\n",
      "  batch 400 loss: 0.9410768043994904\n",
      "  batch 450 loss: 0.9758247375488281\n",
      "  batch 500 loss: 0.9717845344543456\n",
      "  batch 550 loss: 0.9819350671768189\n",
      "  batch 600 loss: 1.0323086631298066\n",
      "  batch 650 loss: 1.014403188228607\n",
      "  batch 700 loss: 0.977833491563797\n",
      "  batch 750 loss: 0.9849282848834991\n",
      "  batch 800 loss: 0.9972719252109528\n",
      "  batch 850 loss: 0.9390538775920868\n",
      "  batch 900 loss: 0.9971554863452912\n",
      "LOSS train 0.99716 valid 1.05026, valid PER 31.50%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.9858012926578522\n",
      "  batch 100 loss: 0.9364194929599762\n",
      "  batch 150 loss: 0.9445220184326172\n",
      "  batch 200 loss: 0.9606708145141601\n",
      "  batch 250 loss: 0.9892912423610687\n",
      "  batch 300 loss: 0.9517930269241333\n",
      "  batch 350 loss: 0.9880520331859589\n",
      "  batch 400 loss: 0.9525918602943421\n",
      "  batch 450 loss: 0.9717310202121735\n",
      "  batch 500 loss: 0.9148329269886016\n",
      "  batch 550 loss: 0.9827573919296264\n",
      "  batch 600 loss: 0.9662991356849671\n",
      "  batch 650 loss: 1.0054245865345002\n",
      "  batch 700 loss: 0.9473918533325195\n",
      "  batch 750 loss: 0.9550597214698792\n",
      "  batch 800 loss: 0.9633371484279633\n",
      "  batch 850 loss: 0.9689800930023194\n",
      "  batch 900 loss: 1.0045801508426666\n",
      "LOSS train 1.00458 valid 1.05827, valid PER 32.38%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.9577605068683624\n",
      "  batch 100 loss: 0.9366126811504364\n",
      "  batch 150 loss: 0.9293938541412353\n",
      "  batch 200 loss: 0.9439487469196319\n",
      "  batch 250 loss: 0.9454693901538849\n",
      "  batch 300 loss: 0.9647772800922394\n",
      "  batch 350 loss: 0.9750593507289886\n",
      "  batch 400 loss: 1.0111453282833098\n",
      "  batch 450 loss: 1.0124854135513306\n",
      "  batch 500 loss: 0.9526083052158356\n",
      "  batch 550 loss: 0.9608750295639038\n",
      "  batch 600 loss: 1.0004459238052368\n",
      "  batch 650 loss: 0.9451510512828827\n",
      "  batch 700 loss: 0.9658523440361023\n",
      "  batch 750 loss: 0.9478829324245452\n",
      "  batch 800 loss: 0.9281861710548401\n",
      "  batch 850 loss: 0.9808504927158356\n",
      "  batch 900 loss: 0.939631462097168\n",
      "LOSS train 0.93963 valid 1.04449, valid PER 32.00%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.935170202255249\n",
      "  batch 100 loss: 0.942744927406311\n",
      "  batch 150 loss: 0.9431465792655945\n",
      "  batch 200 loss: 0.9243805384635926\n",
      "  batch 250 loss: 0.9303731191158294\n",
      "  batch 300 loss: 0.9336041915416717\n",
      "  batch 350 loss: 0.974011892080307\n",
      "  batch 400 loss: 0.9272978782653809\n",
      "  batch 450 loss: 0.9638138031959533\n",
      "  batch 500 loss: 0.9599854385852814\n",
      "  batch 550 loss: 0.9805269002914428\n",
      "  batch 600 loss: 0.9379882538318634\n",
      "  batch 650 loss: 0.9178538703918457\n",
      "  batch 700 loss: 0.9642094767093659\n",
      "  batch 750 loss: 0.9343403017520905\n",
      "  batch 800 loss: 0.9656294846534729\n",
      "  batch 850 loss: 0.9662810730934143\n",
      "  batch 900 loss: 0.9888583064079285\n",
      "LOSS train 0.98886 valid 1.03284, valid PER 31.20%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.8568840003013611\n",
      "  batch 100 loss: 0.9082110464572907\n",
      "  batch 150 loss: 0.9538279020786286\n",
      "  batch 200 loss: 0.9472546458244324\n",
      "  batch 250 loss: 1.0097043323516846\n",
      "  batch 300 loss: 0.9899531948566437\n",
      "  batch 350 loss: 0.9348875546455383\n",
      "  batch 400 loss: 0.9715207493305207\n",
      "  batch 450 loss: 0.9596609842777252\n",
      "  batch 500 loss: 0.962195188999176\n",
      "  batch 550 loss: 0.9556082093715668\n",
      "  batch 600 loss: 0.9757259678840637\n",
      "  batch 650 loss: 0.9967898285388946\n",
      "  batch 700 loss: 0.9361289799213409\n",
      "  batch 750 loss: 0.9412363362312317\n",
      "  batch 800 loss: 0.9873820793628693\n",
      "  batch 850 loss: 0.978926705121994\n",
      "  batch 900 loss: 0.9636986517906189\n",
      "LOSS train 0.96370 valid 1.02822, valid PER 30.62%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.9031305468082428\n",
      "  batch 100 loss: 0.9125563561916351\n",
      "  batch 150 loss: 0.9202478921413422\n",
      "  batch 200 loss: 0.9324833250045776\n",
      "  batch 250 loss: 0.926018911600113\n",
      "  batch 300 loss: 0.9246933555603027\n",
      "  batch 350 loss: 0.8945395886898041\n",
      "  batch 400 loss: 0.9151423609256745\n",
      "  batch 450 loss: 0.9760320794582367\n",
      "  batch 500 loss: 0.9318331670761109\n",
      "  batch 550 loss: 0.9760171616077423\n",
      "  batch 600 loss: 0.9157275712490082\n",
      "  batch 650 loss: 0.9556110966205597\n",
      "  batch 700 loss: 0.9602244591712952\n",
      "  batch 750 loss: 0.9084428787231446\n",
      "  batch 800 loss: 0.9410602295398712\n",
      "  batch 850 loss: 0.9835442221164703\n",
      "  batch 900 loss: 0.9617755782604217\n",
      "LOSS train 0.96178 valid 1.03321, valid PER 30.40%\n",
      "Training finished in 9.0 minutes.\n",
      "Model saved to checkpoints/20231211_111927/model_19\n",
      "Currently using dropout rate of 0.3\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.5714758682250975\n",
      "  batch 100 loss: 3.2832054901123047\n",
      "  batch 150 loss: 3.264313087463379\n",
      "  batch 200 loss: 3.2511919784545897\n",
      "  batch 250 loss: 3.166998572349548\n",
      "  batch 300 loss: 2.9666867876052856\n",
      "  batch 350 loss: 2.8815561532974243\n",
      "  batch 400 loss: 2.7939066982269285\n",
      "  batch 450 loss: 2.6738561487197874\n",
      "  batch 500 loss: 2.555957245826721\n",
      "  batch 550 loss: 2.488089179992676\n",
      "  batch 600 loss: 2.41745192527771\n",
      "  batch 650 loss: 2.3044271326065062\n",
      "  batch 700 loss: 2.2896469163894655\n",
      "  batch 750 loss: 2.1807416582107546\n",
      "  batch 800 loss: 2.1469966864585874\n",
      "  batch 850 loss: 2.0965778279304503\n",
      "  batch 900 loss: 2.0518673849105835\n",
      "LOSS train 2.05187 valid 1.95138, valid PER 67.32%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 2.000357356071472\n",
      "  batch 100 loss: 1.949773919582367\n",
      "  batch 150 loss: 1.9198062491416932\n",
      "  batch 200 loss: 1.9454620742797852\n",
      "  batch 250 loss: 1.927003529071808\n",
      "  batch 300 loss: 1.8895805311203002\n",
      "  batch 350 loss: 1.8146273875236512\n",
      "  batch 400 loss: 1.8532767391204834\n",
      "  batch 450 loss: 1.7871284008026123\n",
      "  batch 500 loss: 1.8068013381958008\n",
      "  batch 550 loss: 1.818555235862732\n",
      "  batch 600 loss: 1.7765028953552247\n",
      "  batch 650 loss: 1.7842409539222717\n",
      "  batch 700 loss: 1.7766998076438905\n",
      "  batch 750 loss: 1.7376617097854614\n",
      "  batch 800 loss: 1.6800220346450805\n",
      "  batch 850 loss: 1.6968021512031555\n",
      "  batch 900 loss: 1.7240712690353392\n",
      "LOSS train 1.72407 valid 1.62948, valid PER 54.59%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.664021828174591\n",
      "  batch 100 loss: 1.637099132537842\n",
      "  batch 150 loss: 1.6119510555267333\n",
      "  batch 200 loss: 1.6320689129829407\n",
      "  batch 250 loss: 1.5930956935882568\n",
      "  batch 300 loss: 1.5734188652038574\n",
      "  batch 350 loss: 1.6279652619361877\n",
      "  batch 400 loss: 1.563117380142212\n",
      "  batch 450 loss: 1.5690218019485473\n",
      "  batch 500 loss: 1.5530602860450744\n",
      "  batch 550 loss: 1.5556710290908813\n",
      "  batch 600 loss: 1.5187987661361695\n",
      "  batch 650 loss: 1.4879593300819396\n",
      "  batch 700 loss: 1.5077368569374086\n",
      "  batch 750 loss: 1.5893629431724547\n",
      "  batch 800 loss: 1.4932415080070496\n",
      "  batch 850 loss: 1.5148935222625732\n",
      "  batch 900 loss: 1.4788758993148803\n",
      "LOSS train 1.47888 valid 1.46288, valid PER 49.47%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.4763873028755188\n",
      "  batch 100 loss: 1.458565855026245\n",
      "  batch 150 loss: 1.402854356765747\n",
      "  batch 200 loss: 1.4671458673477173\n",
      "  batch 250 loss: 1.4635908699035645\n",
      "  batch 300 loss: 1.4839710664749146\n",
      "  batch 350 loss: 1.4312671828269958\n",
      "  batch 400 loss: 1.457577702999115\n",
      "  batch 450 loss: 1.458844883441925\n",
      "  batch 500 loss: 1.4087316274642945\n",
      "  batch 550 loss: 1.460319948196411\n",
      "  batch 600 loss: 1.4710524940490723\n",
      "  batch 650 loss: 1.4347520971298218\n",
      "  batch 700 loss: 1.4207468628883362\n",
      "  batch 750 loss: 1.3857134461402894\n",
      "  batch 800 loss: 1.3727528381347656\n",
      "  batch 850 loss: 1.422221462726593\n",
      "  batch 900 loss: 1.4936558270454408\n",
      "LOSS train 1.49366 valid 1.35886, valid PER 42.62%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.4080556201934815\n",
      "  batch 100 loss: 1.4292286539077759\n",
      "  batch 150 loss: 1.4175415706634522\n",
      "  batch 200 loss: 1.3339864802360535\n",
      "  batch 250 loss: 1.3298607420921327\n",
      "  batch 300 loss: 1.3703099274635315\n",
      "  batch 350 loss: 1.3937027096748351\n",
      "  batch 400 loss: 1.383832767009735\n",
      "  batch 450 loss: 1.3600484347343444\n",
      "  batch 500 loss: 1.4216037225723266\n",
      "  batch 550 loss: 1.3461983394622803\n",
      "  batch 600 loss: 1.402187135219574\n",
      "  batch 650 loss: 1.3496502804756165\n",
      "  batch 700 loss: 1.3938739967346192\n",
      "  batch 750 loss: 1.358899803161621\n",
      "  batch 800 loss: 1.4011965608596801\n",
      "  batch 850 loss: 1.3972005438804627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 900 loss: 1.4219681859016418\n",
      "LOSS train 1.42197 valid 1.30111, valid PER 40.91%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.3492155432701112\n",
      "  batch 100 loss: 1.3503376865386962\n",
      "  batch 150 loss: 1.2969682717323303\n",
      "  batch 200 loss: 1.3504661583900452\n",
      "  batch 250 loss: 1.3463176798820495\n",
      "  batch 300 loss: 1.3174651193618774\n",
      "  batch 350 loss: 1.3264539909362794\n",
      "  batch 400 loss: 1.3297682511806488\n",
      "  batch 450 loss: 1.3667264246940614\n",
      "  batch 500 loss: 1.361070306301117\n",
      "  batch 550 loss: 1.3435300445556642\n",
      "  batch 600 loss: 1.3596639823913574\n",
      "  batch 650 loss: 1.392453029155731\n",
      "  batch 700 loss: 1.3361058640480041\n",
      "  batch 750 loss: 1.2844684028625488\n",
      "  batch 800 loss: 1.3160608291625977\n",
      "  batch 850 loss: 1.2885420608520508\n",
      "  batch 900 loss: 1.3674168014526367\n",
      "LOSS train 1.36742 valid 1.33808, valid PER 42.13%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.3344583821296692\n",
      "  batch 100 loss: 1.3165594363212585\n",
      "  batch 150 loss: 1.3294534015655517\n",
      "  batch 200 loss: 1.277997808456421\n",
      "  batch 250 loss: 1.2680610418319702\n",
      "  batch 300 loss: 1.2734625768661498\n",
      "  batch 350 loss: 1.2782715904712676\n",
      "  batch 400 loss: 1.3243388748168945\n",
      "  batch 450 loss: 1.320910303592682\n",
      "  batch 500 loss: 1.2915702223777772\n",
      "  batch 550 loss: 1.306995244026184\n",
      "  batch 600 loss: 1.3202637887001039\n",
      "  batch 650 loss: 1.3014173412322998\n",
      "  batch 700 loss: 1.2874640703201294\n",
      "  batch 750 loss: 1.2720207273960114\n",
      "  batch 800 loss: 1.2839070129394532\n",
      "  batch 850 loss: 1.3116151893138885\n",
      "  batch 900 loss: 1.3291373252868652\n",
      "LOSS train 1.32914 valid 1.25303, valid PER 39.48%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.285105220079422\n",
      "  batch 100 loss: 1.2458816421031953\n",
      "  batch 150 loss: 1.257777054309845\n",
      "  batch 200 loss: 1.238306654691696\n",
      "  batch 250 loss: 1.3324595379829407\n",
      "  batch 300 loss: 1.2345352637767792\n",
      "  batch 350 loss: 1.3190926289558411\n",
      "  batch 400 loss: 1.2496204912662505\n",
      "  batch 450 loss: 1.2815807998180389\n",
      "  batch 500 loss: 1.2930072832107544\n",
      "  batch 550 loss: 1.3465948462486268\n",
      "  batch 600 loss: 1.3128282189369203\n",
      "  batch 650 loss: 1.309336049556732\n",
      "  batch 700 loss: 1.267471764087677\n",
      "  batch 750 loss: 1.276166377067566\n",
      "  batch 800 loss: 1.2493315398693086\n",
      "  batch 850 loss: 1.2640501797199248\n",
      "  batch 900 loss: 1.2555023980140687\n",
      "LOSS train 1.25550 valid 1.26095, valid PER 39.64%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 1.2278594601154327\n",
      "  batch 100 loss: 1.2977663052082062\n",
      "  batch 150 loss: 1.2701859331130982\n",
      "  batch 200 loss: 1.198977416753769\n",
      "  batch 250 loss: 1.260049830675125\n",
      "  batch 300 loss: 1.231662813425064\n",
      "  batch 350 loss: 1.2691931104660035\n",
      "  batch 400 loss: 1.2712399935722352\n",
      "  batch 450 loss: 1.3156678450107575\n",
      "  batch 500 loss: 1.2351544165611268\n",
      "  batch 550 loss: 1.2892375528812408\n",
      "  batch 600 loss: 1.3232277679443358\n",
      "  batch 650 loss: 1.2628556382656098\n",
      "  batch 700 loss: 1.2578194522857666\n",
      "  batch 750 loss: 1.2788017988204956\n",
      "  batch 800 loss: 1.2807398748397827\n",
      "  batch 850 loss: 1.2760007357597352\n",
      "  batch 900 loss: 1.2530640387535095\n",
      "LOSS train 1.25306 valid 1.23148, valid PER 39.25%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 1.200220112800598\n",
      "  batch 100 loss: 1.241362669467926\n",
      "  batch 150 loss: 1.2856327569484711\n",
      "  batch 200 loss: 1.3461567306518554\n",
      "  batch 250 loss: 1.2922096705436708\n",
      "  batch 300 loss: 1.2152039551734923\n",
      "  batch 350 loss: 1.2772532749176024\n",
      "  batch 400 loss: 1.2471537268161774\n",
      "  batch 450 loss: 1.2076474881172181\n",
      "  batch 500 loss: 1.2750312113761901\n",
      "  batch 550 loss: 1.2610015749931336\n",
      "  batch 600 loss: 1.2355911087989808\n",
      "  batch 650 loss: 1.2559480261802674\n",
      "  batch 700 loss: 1.2606096959114075\n",
      "  batch 750 loss: 1.2402668046951293\n",
      "  batch 800 loss: 1.2435594582557679\n",
      "  batch 850 loss: 1.258968175649643\n",
      "  batch 900 loss: 1.2590942919254302\n",
      "LOSS train 1.25909 valid 1.24869, valid PER 39.97%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 1.2570537149906158\n",
      "  batch 100 loss: 1.2088856458663941\n",
      "  batch 150 loss: 1.2374949657917023\n",
      "  batch 200 loss: 1.239789776802063\n",
      "  batch 250 loss: 1.2499964547157287\n",
      "  batch 300 loss: 1.1930626404285432\n",
      "  batch 350 loss: 1.2249295949935912\n",
      "  batch 400 loss: 1.242295025587082\n",
      "  batch 450 loss: 1.2470109927654267\n",
      "  batch 500 loss: 1.1738701164722443\n",
      "  batch 550 loss: 1.198798555135727\n",
      "  batch 600 loss: 1.198499220609665\n",
      "  batch 650 loss: 1.2859945750236512\n",
      "  batch 700 loss: 1.2148423528671264\n",
      "  batch 750 loss: 1.2632628965377808\n",
      "  batch 800 loss: 1.2867027139663696\n",
      "  batch 850 loss: 1.2749421072006226\n",
      "  batch 900 loss: 1.2713447415828705\n",
      "LOSS train 1.27134 valid 1.19824, valid PER 37.83%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 1.212746649980545\n",
      "  batch 100 loss: 1.173156943321228\n",
      "  batch 150 loss: 1.1874196708202363\n",
      "  batch 200 loss: 1.2150802719593048\n",
      "  batch 250 loss: 1.2581039106845855\n",
      "  batch 300 loss: 1.1996400916576386\n",
      "  batch 350 loss: 1.2362072682380676\n",
      "  batch 400 loss: 1.2288688325881958\n",
      "  batch 450 loss: 1.2267269456386567\n",
      "  batch 500 loss: 1.2233063197135925\n",
      "  batch 550 loss: 1.1712824702262878\n",
      "  batch 600 loss: 1.2055020260810851\n",
      "  batch 650 loss: 1.2746109890937805\n",
      "  batch 700 loss: 1.2435529232025146\n",
      "  batch 750 loss: 1.2071698248386382\n",
      "  batch 800 loss: 1.1920018601417541\n",
      "  batch 850 loss: 1.2274508571624756\n",
      "  batch 900 loss: 1.239791145324707\n",
      "LOSS train 1.23979 valid 1.24078, valid PER 38.28%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 1.1820655345916748\n",
      "  batch 100 loss: 1.244375435113907\n",
      "  batch 150 loss: 1.2100045359134675\n",
      "  batch 200 loss: 1.2017404627799988\n",
      "  batch 250 loss: 1.213690584897995\n",
      "  batch 300 loss: 1.1680783724784851\n",
      "  batch 350 loss: 1.20337256193161\n",
      "  batch 400 loss: 1.2146967124938965\n",
      "  batch 450 loss: 1.244285728931427\n",
      "  batch 500 loss: 1.1715512228012086\n",
      "  batch 550 loss: 1.1943497693538665\n",
      "  batch 600 loss: 1.1955695915222169\n",
      "  batch 650 loss: 1.1892033433914184\n",
      "  batch 700 loss: 1.2024537432193756\n",
      "  batch 750 loss: 1.1852200305461884\n",
      "  batch 800 loss: 1.219037709236145\n",
      "  batch 850 loss: 1.2531669807434083\n",
      "  batch 900 loss: 1.254111329317093\n",
      "LOSS train 1.25411 valid 1.22563, valid PER 39.27%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 1.1822027504444121\n",
      "  batch 100 loss: 1.2250691664218902\n",
      "  batch 150 loss: 1.1959034144878387\n",
      "  batch 200 loss: 1.1961562192440034\n",
      "  batch 250 loss: 1.1593920648097993\n",
      "  batch 300 loss: 1.3064528620243072\n",
      "  batch 350 loss: 1.2116029131412507\n",
      "  batch 400 loss: 1.209953204393387\n",
      "  batch 450 loss: 1.1791873693466186\n",
      "  batch 500 loss: 1.2325783252716065\n",
      "  batch 550 loss: 1.2177151489257811\n",
      "  batch 600 loss: 1.1774552047252655\n",
      "  batch 650 loss: 1.237449038028717\n",
      "  batch 700 loss: 1.1989222478866577\n",
      "  batch 750 loss: 1.172215062379837\n",
      "  batch 800 loss: 1.1302276575565338\n",
      "  batch 850 loss: 1.205577062368393\n",
      "  batch 900 loss: 1.2116104233264924\n",
      "LOSS train 1.21161 valid 1.20809, valid PER 39.23%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 1.236120766401291\n",
      "  batch 100 loss: 1.1961807107925415\n",
      "  batch 150 loss: 1.2160780787467957\n",
      "  batch 200 loss: 1.2109672236442566\n",
      "  batch 250 loss: 1.2353142774105073\n",
      "  batch 300 loss: 1.2434853076934815\n",
      "  batch 350 loss: 1.1881884360313415\n",
      "  batch 400 loss: 1.1581110560894012\n",
      "  batch 450 loss: 1.179503449201584\n",
      "  batch 500 loss: 1.1489252960681915\n",
      "  batch 550 loss: 1.1896045076847077\n",
      "  batch 600 loss: 1.2192419290542602\n",
      "  batch 650 loss: 1.2182392060756684\n",
      "  batch 700 loss: 1.2210941445827483\n",
      "  batch 750 loss: 1.1783894777297974\n",
      "  batch 800 loss: 1.1569709026813506\n",
      "  batch 850 loss: 1.149973121881485\n",
      "  batch 900 loss: 1.1823163032531738\n",
      "LOSS train 1.18232 valid 1.17661, valid PER 36.39%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 1.2106068158149719\n",
      "  batch 100 loss: 1.1974527072906493\n",
      "  batch 150 loss: 1.1714621937274934\n",
      "  batch 200 loss: 1.2446843075752259\n",
      "  batch 250 loss: 1.2837870073318483\n",
      "  batch 300 loss: 1.194823670387268\n",
      "  batch 350 loss: 1.2562068450450896\n",
      "  batch 400 loss: 1.2079376995563507\n",
      "  batch 450 loss: 1.2108177161216735\n",
      "  batch 500 loss: 1.1553901576995849\n",
      "  batch 550 loss: 1.2152159416675568\n",
      "  batch 600 loss: 1.2444938516616821\n",
      "  batch 650 loss: 1.2184617388248444\n",
      "  batch 700 loss: 1.228392288684845\n",
      "  batch 750 loss: 1.1768150413036347\n",
      "  batch 800 loss: 1.1570031666755676\n",
      "  batch 850 loss: 1.1741056168079376\n",
      "  batch 900 loss: 1.1872939431667329\n",
      "LOSS train 1.18729 valid 1.20621, valid PER 37.65%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 1.1773710095882415\n",
      "  batch 100 loss: 1.1752630460262299\n",
      "  batch 150 loss: 1.1437771427631378\n",
      "  batch 200 loss: 1.147532857656479\n",
      "  batch 250 loss: 1.160541172027588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 1.1741521263122559\n",
      "  batch 350 loss: 1.154753235578537\n",
      "  batch 400 loss: 1.2108310914039613\n",
      "  batch 450 loss: 1.2988735330104828\n",
      "  batch 500 loss: 1.2569367396831512\n",
      "  batch 550 loss: 1.193630645275116\n",
      "  batch 600 loss: 1.3175043439865113\n",
      "  batch 650 loss: 1.3204483163356782\n",
      "  batch 700 loss: 1.2548356604576112\n",
      "  batch 750 loss: 1.1858949506282805\n",
      "  batch 800 loss: 1.1943905007839204\n",
      "  batch 850 loss: 1.1977395415306091\n",
      "  batch 900 loss: 1.1722902762889862\n",
      "LOSS train 1.17229 valid 1.21022, valid PER 37.92%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 1.1614924228191377\n",
      "  batch 100 loss: 1.189409888982773\n",
      "  batch 150 loss: 1.2188839280605317\n",
      "  batch 200 loss: 1.1784587168693543\n",
      "  batch 250 loss: 1.1791108787059783\n",
      "  batch 300 loss: 1.1774186432361602\n",
      "  batch 350 loss: 1.2054944515228272\n",
      "  batch 400 loss: 1.1659189093112945\n",
      "  batch 450 loss: 1.226254094839096\n",
      "  batch 500 loss: 1.1932009887695312\n",
      "  batch 550 loss: 1.1714679157733918\n",
      "  batch 600 loss: 1.176809970140457\n",
      "  batch 650 loss: 1.17180535197258\n",
      "  batch 700 loss: 1.1627570331096648\n",
      "  batch 750 loss: 1.1440171766281129\n",
      "  batch 800 loss: 1.1781154012680053\n",
      "  batch 850 loss: 1.1468688321113587\n",
      "  batch 900 loss: 1.1928949773311615\n",
      "LOSS train 1.19289 valid 1.22305, valid PER 37.87%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 1.1338926446437836\n",
      "  batch 100 loss: 1.132869542837143\n",
      "  batch 150 loss: 1.1687561392784118\n",
      "  batch 200 loss: 1.1599815797805786\n",
      "  batch 250 loss: 1.1678399753570556\n",
      "  batch 300 loss: 1.167980043888092\n",
      "  batch 350 loss: 1.123032456636429\n",
      "  batch 400 loss: 1.188721296787262\n",
      "  batch 450 loss: 1.1921300280094147\n",
      "  batch 500 loss: 1.1802593874931335\n",
      "  batch 550 loss: 1.1421216630935669\n",
      "  batch 600 loss: 1.1488516759872436\n",
      "  batch 650 loss: 1.195890109539032\n",
      "  batch 700 loss: 1.137785986661911\n",
      "  batch 750 loss: 1.1377851128578187\n",
      "  batch 800 loss: 1.1450780355930328\n",
      "  batch 850 loss: 1.1919204592704773\n",
      "  batch 900 loss: 1.1734437918663025\n",
      "LOSS train 1.17344 valid 1.17870, valid PER 36.63%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 1.1431387841701508\n",
      "  batch 100 loss: 1.1505846798419952\n",
      "  batch 150 loss: 1.1626504862308502\n",
      "  batch 200 loss: 1.155021115541458\n",
      "  batch 250 loss: 1.1662465453147888\n",
      "  batch 300 loss: 1.1871357440948487\n",
      "  batch 350 loss: 1.1657730674743652\n",
      "  batch 400 loss: 1.1695512163639068\n",
      "  batch 450 loss: 1.114269984960556\n",
      "  batch 500 loss: 1.1763113045692444\n",
      "  batch 550 loss: 1.1955494582653046\n",
      "  batch 600 loss: 1.155112726688385\n",
      "  batch 650 loss: 1.1743064868450164\n",
      "  batch 700 loss: 1.186391372680664\n",
      "  batch 750 loss: 1.1539801633358002\n",
      "  batch 800 loss: 1.2190819144248963\n",
      "  batch 850 loss: 1.2064235651493072\n",
      "  batch 900 loss: 1.1895473647117614\n",
      "LOSS train 1.18955 valid 1.16110, valid PER 36.32%\n",
      "Training finished in 9.0 minutes.\n",
      "Model saved to checkpoints/20231211_112837/model_20\n",
      "Currently using dropout rate of 0.5\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.558809375762939\n",
      "  batch 100 loss: 3.2583188104629515\n",
      "  batch 150 loss: 3.353033256530762\n",
      "  batch 200 loss: 3.3096227073669433\n",
      "  batch 250 loss: 3.2741643810272216\n",
      "  batch 300 loss: 3.2382493782043458\n",
      "  batch 350 loss: 3.22578773021698\n",
      "  batch 400 loss: 3.2478428745269774\n",
      "  batch 450 loss: 3.2108606910705566\n",
      "  batch 500 loss: 3.212123169898987\n",
      "  batch 550 loss: 3.1699974155426025\n",
      "  batch 600 loss: 3.149377913475037\n",
      "  batch 650 loss: 3.0905564403533936\n",
      "  batch 700 loss: 3.069198727607727\n",
      "  batch 750 loss: 3.0417293453216554\n",
      "  batch 800 loss: 3.0259510469436646\n",
      "  batch 850 loss: 2.96564350605011\n",
      "  batch 900 loss: 2.9093291568756103\n",
      "LOSS train 2.90933 valid 2.95919, valid PER 84.08%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 2.8845525693893435\n",
      "  batch 100 loss: 2.8622835874557495\n",
      "  batch 150 loss: 2.814659662246704\n",
      "  batch 200 loss: 2.8117018127441407\n",
      "  batch 250 loss: 2.788121666908264\n",
      "  batch 300 loss: 2.7634976005554197\n",
      "  batch 350 loss: 2.723152279853821\n",
      "  batch 400 loss: 2.7111735725402832\n",
      "  batch 450 loss: 2.704147300720215\n",
      "  batch 500 loss: 2.6849199676513673\n",
      "  batch 550 loss: 2.7003071546554565\n",
      "  batch 600 loss: 2.630481333732605\n",
      "  batch 650 loss: 2.6354905557632446\n",
      "  batch 700 loss: 2.620694327354431\n",
      "  batch 750 loss: 2.596525797843933\n",
      "  batch 800 loss: 2.544470386505127\n",
      "  batch 850 loss: 2.5660411977767943\n",
      "  batch 900 loss: 2.5625485038757323\n",
      "LOSS train 2.56255 valid 2.54104, valid PER 82.77%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 2.537954316139221\n",
      "  batch 100 loss: 2.439280366897583\n",
      "  batch 150 loss: 2.4537142753601073\n",
      "  batch 200 loss: 2.4404691743850706\n",
      "  batch 250 loss: 2.3872549819946287\n",
      "  batch 300 loss: 2.3776325607299804\n",
      "  batch 350 loss: 2.38661283493042\n",
      "  batch 400 loss: 2.344249515533447\n",
      "  batch 450 loss: 2.309650559425354\n",
      "  batch 500 loss: 2.339966683387756\n",
      "  batch 550 loss: 2.2941319227218626\n",
      "  batch 600 loss: 2.2475174474716186\n",
      "  batch 650 loss: 2.2149831366539003\n",
      "  batch 700 loss: 2.2373677563667296\n",
      "  batch 750 loss: 2.2754453468322753\n",
      "  batch 800 loss: 2.19183712720871\n",
      "  batch 850 loss: 2.1962533020973205\n",
      "  batch 900 loss: 2.147110185623169\n",
      "LOSS train 2.14711 valid 2.12261, valid PER 81.86%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 2.155324733257294\n",
      "  batch 100 loss: 2.1342716360092164\n",
      "  batch 150 loss: 2.0968747544288635\n",
      "  batch 200 loss: 2.134646575450897\n",
      "  batch 250 loss: 2.1069067454338075\n",
      "  batch 300 loss: 2.107602195739746\n",
      "  batch 350 loss: 2.0277506279945374\n",
      "  batch 400 loss: 2.0655392098426817\n",
      "  batch 450 loss: 2.0446513390541075\n",
      "  batch 500 loss: 1.9764099550247192\n",
      "  batch 550 loss: 2.0220148944854737\n",
      "  batch 600 loss: 2.0215312027931214\n",
      "  batch 650 loss: 2.001665587425232\n",
      "  batch 700 loss: 1.9494803047180176\n",
      "  batch 750 loss: 1.9177436804771424\n",
      "  batch 800 loss: 1.9120682048797608\n",
      "  batch 850 loss: 1.9371661615371705\n",
      "  batch 900 loss: 1.9505292940139771\n",
      "LOSS train 1.95053 valid 1.84156, valid PER 72.36%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.8972154545783997\n",
      "  batch 100 loss: 1.852711751461029\n",
      "  batch 150 loss: 1.9168582820892335\n",
      "  batch 200 loss: 1.8403793215751647\n",
      "  batch 250 loss: 1.8370636081695557\n",
      "  batch 300 loss: 1.8286360859870912\n",
      "  batch 350 loss: 1.8482491707801818\n",
      "  batch 400 loss: 1.8433493089675903\n",
      "  batch 450 loss: 1.823489511013031\n",
      "  batch 500 loss: 1.806317813396454\n",
      "  batch 550 loss: 1.754995105266571\n",
      "  batch 600 loss: 1.8203982901573181\n",
      "  batch 650 loss: 1.7651903104782105\n",
      "  batch 700 loss: 1.7841773676872252\n",
      "  batch 750 loss: 1.7243693494796752\n",
      "  batch 800 loss: 1.7446110272407531\n",
      "  batch 850 loss: 1.7575335931777953\n",
      "  batch 900 loss: 1.779455180168152\n",
      "LOSS train 1.77946 valid 1.66908, valid PER 62.23%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.719920403957367\n",
      "  batch 100 loss: 1.7019753932952881\n",
      "  batch 150 loss: 1.6820826172828673\n",
      "  batch 200 loss: 1.7103586101531982\n",
      "  batch 250 loss: 1.7522485589981078\n",
      "  batch 300 loss: 1.6842863082885742\n",
      "  batch 350 loss: 1.7089566135406493\n",
      "  batch 400 loss: 1.7019516181945802\n",
      "  batch 450 loss: 1.6913491630554198\n",
      "  batch 500 loss: 1.6574876379966736\n",
      "  batch 550 loss: 1.66340003490448\n",
      "  batch 600 loss: 1.6590921592712402\n",
      "  batch 650 loss: 1.712477695941925\n",
      "  batch 700 loss: 1.6555431056022645\n",
      "  batch 750 loss: 1.6342246985435487\n",
      "  batch 800 loss: 1.6265365982055664\n",
      "  batch 850 loss: 1.6221875500679017\n",
      "  batch 900 loss: 1.6437063312530518\n",
      "LOSS train 1.64371 valid 1.56471, valid PER 56.81%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.6151464223861693\n",
      "  batch 100 loss: 1.6180378770828248\n",
      "  batch 150 loss: 1.602644968032837\n",
      "  batch 200 loss: 1.5844649410247802\n",
      "  batch 250 loss: 1.5985634922981262\n",
      "  batch 300 loss: 1.6140139746665954\n",
      "  batch 350 loss: 1.580796172618866\n",
      "  batch 400 loss: 1.5979565119743346\n",
      "  batch 450 loss: 1.5895822358131408\n",
      "  batch 500 loss: 1.5739447450637818\n",
      "  batch 550 loss: 1.5740894556045533\n",
      "  batch 600 loss: 1.5986718392372132\n",
      "  batch 650 loss: 1.5583342385292054\n",
      "  batch 700 loss: 1.5614927434921264\n",
      "  batch 750 loss: 1.5372775721549987\n",
      "  batch 800 loss: 1.5735160088539124\n",
      "  batch 850 loss: 1.5733346033096314\n",
      "  batch 900 loss: 1.6028031039237975\n",
      "LOSS train 1.60280 valid 1.50588, valid PER 52.77%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.5208350348472595\n",
      "  batch 100 loss: 1.510540337562561\n",
      "  batch 150 loss: 1.5366438269615172\n",
      "  batch 200 loss: 1.4748060202598572\n",
      "  batch 250 loss: 1.524550611972809\n",
      "  batch 300 loss: 1.4630350708961486\n",
      "  batch 350 loss: 1.5540135049819945\n",
      "  batch 400 loss: 1.4953190445899964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 450 loss: 1.5479865097999572\n",
      "  batch 500 loss: 1.552217402458191\n",
      "  batch 550 loss: 1.5051586937904358\n",
      "  batch 600 loss: 1.5148541021347046\n",
      "  batch 650 loss: 1.5870400333404542\n",
      "  batch 700 loss: 1.5236979079246522\n",
      "  batch 750 loss: 1.5158755445480347\n",
      "  batch 800 loss: 1.4952186059951782\n",
      "  batch 850 loss: 1.50380713224411\n",
      "  batch 900 loss: 1.4993728947639466\n",
      "LOSS train 1.49937 valid 1.40968, valid PER 48.70%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 1.4263037276268005\n",
      "  batch 100 loss: 1.5196050930023193\n",
      "  batch 150 loss: 1.4916280221939087\n",
      "  batch 200 loss: 1.4176553678512573\n",
      "  batch 250 loss: 1.4641001915931702\n",
      "  batch 300 loss: 1.4644421410560609\n",
      "  batch 350 loss: 1.4924025464057922\n",
      "  batch 400 loss: 1.4752102994918823\n",
      "  batch 450 loss: 1.454753165245056\n",
      "  batch 500 loss: 1.4359695553779601\n",
      "  batch 550 loss: 1.4653852224349975\n",
      "  batch 600 loss: 1.4648370599746705\n",
      "  batch 650 loss: 1.4521031332015992\n",
      "  batch 700 loss: 1.4368066048622132\n",
      "  batch 750 loss: 1.4857138347625733\n",
      "  batch 800 loss: 1.4567369627952576\n",
      "  batch 850 loss: 1.5052637720108033\n",
      "  batch 900 loss: 1.434092297554016\n",
      "LOSS train 1.43409 valid 1.37509, valid PER 45.08%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 1.3938126826286317\n",
      "  batch 100 loss: 1.4284384179115295\n",
      "  batch 150 loss: 1.4914351463317872\n",
      "  batch 200 loss: 1.4539378488063812\n",
      "  batch 250 loss: 1.424122622013092\n",
      "  batch 300 loss: 1.37805988073349\n",
      "  batch 350 loss: 1.4331695652008056\n",
      "  batch 400 loss: 1.434219799041748\n",
      "  batch 450 loss: 1.3936921954154968\n",
      "  batch 500 loss: 1.4517368006706237\n",
      "  batch 550 loss: 1.460035502910614\n",
      "  batch 600 loss: 1.4690452194213868\n",
      "  batch 650 loss: 1.4264493489265442\n",
      "  batch 700 loss: 1.4266653990745544\n",
      "  batch 750 loss: 1.3864773559570311\n",
      "  batch 800 loss: 1.4327684330940247\n",
      "  batch 850 loss: 1.4329378366470338\n",
      "  batch 900 loss: 1.4190244960784912\n",
      "LOSS train 1.41902 valid 1.39254, valid PER 43.79%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 1.409899971485138\n",
      "  batch 100 loss: 1.3779117345809937\n",
      "  batch 150 loss: 1.3472187089920045\n",
      "  batch 200 loss: 1.4176691150665284\n",
      "  batch 250 loss: 1.4158188271522523\n",
      "  batch 300 loss: 1.3799598622322082\n",
      "  batch 350 loss: 1.4026667189598083\n",
      "  batch 400 loss: 1.425715570449829\n",
      "  batch 450 loss: 1.4430285859107972\n",
      "  batch 500 loss: 1.3802509808540344\n",
      "  batch 550 loss: 1.402562711238861\n",
      "  batch 600 loss: 1.3747335147857667\n",
      "  batch 650 loss: 1.4534933352470398\n",
      "  batch 700 loss: 1.367420494556427\n",
      "  batch 750 loss: 1.3712806606292725\n",
      "  batch 800 loss: 1.4069704151153564\n",
      "  batch 850 loss: 1.450248863697052\n",
      "  batch 900 loss: 1.417589590549469\n",
      "LOSS train 1.41759 valid 1.30429, valid PER 42.98%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 1.3950316190719605\n",
      "  batch 100 loss: 1.3625751161575317\n",
      "  batch 150 loss: 1.357983613014221\n",
      "  batch 200 loss: 1.3664049696922302\n",
      "  batch 250 loss: 1.4189465355873108\n",
      "  batch 300 loss: 1.3701415967941284\n",
      "  batch 350 loss: 1.4010010361671448\n",
      "  batch 400 loss: 1.394542362689972\n",
      "  batch 450 loss: 1.3813765859603881\n",
      "  batch 500 loss: 1.3822135376930236\n",
      "  batch 550 loss: 1.31335129737854\n",
      "  batch 600 loss: 1.3459381675720214\n",
      "  batch 650 loss: 1.4380029082298278\n",
      "  batch 700 loss: 1.405761320590973\n",
      "  batch 750 loss: 1.3424547410011292\n",
      "  batch 800 loss: 1.3545390701293945\n",
      "  batch 850 loss: 1.3905607295036315\n",
      "  batch 900 loss: 1.390105128288269\n",
      "LOSS train 1.39011 valid 1.34061, valid PER 41.77%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 1.312954354286194\n",
      "  batch 100 loss: 1.3590075397491455\n",
      "  batch 150 loss: 1.2906206941604614\n",
      "  batch 200 loss: 1.3539495539665223\n",
      "  batch 250 loss: 1.3518448901176452\n",
      "  batch 300 loss: 1.3468687272071838\n",
      "  batch 350 loss: 1.3429842293262482\n",
      "  batch 400 loss: 1.3620750176906586\n",
      "  batch 450 loss: 1.4114073276519776\n",
      "  batch 500 loss: 1.3177181792259216\n",
      "  batch 550 loss: 1.3708283615112304\n",
      "  batch 600 loss: 1.3578990507125854\n",
      "  batch 650 loss: 1.33514732837677\n",
      "  batch 700 loss: 1.3185246205329895\n",
      "  batch 750 loss: 1.3430686736106872\n",
      "  batch 800 loss: 1.317663516998291\n",
      "  batch 850 loss: 1.3694995260238647\n",
      "  batch 900 loss: 1.3501753115653992\n",
      "LOSS train 1.35018 valid 1.29504, valid PER 41.08%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 1.3216987133026123\n",
      "  batch 100 loss: 1.3796747374534606\n",
      "  batch 150 loss: 1.3587018394470214\n",
      "  batch 200 loss: 1.3587000250816346\n",
      "  batch 250 loss: 1.3601704168319702\n",
      "  batch 300 loss: 1.3692837524414063\n",
      "  batch 350 loss: 1.310856385231018\n",
      "  batch 400 loss: 1.3208687543869018\n",
      "  batch 450 loss: 1.3272621893882752\n",
      "  batch 500 loss: 1.374733386039734\n",
      "  batch 550 loss: 1.3509243512153626\n",
      "  batch 600 loss: 1.3003263759613037\n",
      "  batch 650 loss: 1.3438872599601746\n",
      "  batch 700 loss: 1.3564801931381225\n",
      "  batch 750 loss: 1.3462479424476623\n",
      "  batch 800 loss: 1.2841335606575013\n",
      "  batch 850 loss: 1.3703144478797913\n",
      "  batch 900 loss: 1.3859135174751283\n",
      "LOSS train 1.38591 valid 1.29969, valid PER 41.80%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 1.3526958179473878\n",
      "  batch 100 loss: 1.3341162538528442\n",
      "  batch 150 loss: 1.3199914050102235\n",
      "  batch 200 loss: 1.3493115925788879\n",
      "  batch 250 loss: 1.3524697017669678\n",
      "  batch 300 loss: 1.3236731123924255\n",
      "  batch 350 loss: 1.3180852138996124\n",
      "  batch 400 loss: 1.3271546745300293\n",
      "  batch 450 loss: 1.3338633584976196\n",
      "  batch 500 loss: 1.321728241443634\n",
      "  batch 550 loss: 1.369927841424942\n",
      "  batch 600 loss: 1.338743088245392\n",
      "  batch 650 loss: 1.3326669907569886\n",
      "  batch 700 loss: 1.3589619195461273\n",
      "  batch 750 loss: 1.3125479412078858\n",
      "  batch 800 loss: 1.3180080914497376\n",
      "  batch 850 loss: 1.306542282104492\n",
      "  batch 900 loss: 1.3377869844436645\n",
      "LOSS train 1.33779 valid 1.28363, valid PER 40.93%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 1.326339409351349\n",
      "  batch 100 loss: 1.2662063002586366\n",
      "  batch 150 loss: 1.3001544022560119\n",
      "  batch 200 loss: 1.3240891933441161\n",
      "  batch 250 loss: 1.343561589717865\n",
      "  batch 300 loss: 1.31622864484787\n",
      "  batch 350 loss: 1.321358036994934\n",
      "  batch 400 loss: 1.3096212184429168\n",
      "  batch 450 loss: 1.3595409309864044\n",
      "  batch 500 loss: 1.2809690356254577\n",
      "  batch 550 loss: 1.3717030477523804\n",
      "  batch 600 loss: 1.3604614806175233\n",
      "  batch 650 loss: 1.362035641670227\n",
      "  batch 700 loss: 1.3760471177101135\n",
      "  batch 750 loss: 1.3186772894859313\n",
      "  batch 800 loss: 1.3018593883514404\n",
      "  batch 850 loss: 1.315337722301483\n",
      "  batch 900 loss: 1.3157741141319275\n",
      "LOSS train 1.31577 valid 1.24515, valid PER 39.95%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 1.333589298725128\n",
      "  batch 100 loss: 1.2973373198509217\n",
      "  batch 150 loss: 1.2667429196834563\n",
      "  batch 200 loss: 1.2566603708267212\n",
      "  batch 250 loss: 1.2941597175598145\n",
      "  batch 300 loss: 1.3469570970535278\n",
      "  batch 350 loss: 1.3308884358406068\n",
      "  batch 400 loss: 1.357140429019928\n",
      "  batch 450 loss: 1.3340060663223268\n",
      "  batch 500 loss: 1.3106551885604858\n",
      "  batch 550 loss: 1.3338753962516785\n",
      "  batch 600 loss: 1.3862429523468018\n",
      "  batch 650 loss: 1.2899317228794098\n",
      "  batch 700 loss: 1.3241313695907593\n",
      "  batch 750 loss: 1.2990270674228668\n",
      "  batch 800 loss: 1.301001091003418\n",
      "  batch 850 loss: 1.3045188891887665\n",
      "  batch 900 loss: 1.3114379799365998\n",
      "LOSS train 1.31144 valid 1.28800, valid PER 39.61%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 1.2899488639831542\n",
      "  batch 100 loss: 1.3341673684120179\n",
      "  batch 150 loss: 1.2932823526859283\n",
      "  batch 200 loss: 1.330572123527527\n",
      "  batch 250 loss: 1.3317004644870758\n",
      "  batch 300 loss: 1.3000315058231353\n",
      "  batch 350 loss: 1.3284601426124574\n",
      "  batch 400 loss: 1.3190903568267822\n",
      "  batch 450 loss: 1.3442835927009582\n",
      "  batch 500 loss: 1.307358559370041\n",
      "  batch 550 loss: 1.2860368251800538\n",
      "  batch 600 loss: 1.2732828330993653\n",
      "  batch 650 loss: 1.2813996911048888\n",
      "  batch 700 loss: 1.281877624988556\n",
      "  batch 750 loss: 1.318265242576599\n",
      "  batch 800 loss: 1.2985855257511139\n",
      "  batch 850 loss: 1.2714102041721345\n",
      "  batch 900 loss: 1.3281528496742248\n",
      "LOSS train 1.32815 valid 1.23499, valid PER 39.14%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 1.2350558817386628\n",
      "  batch 100 loss: 1.236969895362854\n",
      "  batch 150 loss: 1.2767688250541687\n",
      "  batch 200 loss: 1.3138128757476806\n",
      "  batch 250 loss: 1.310202270746231\n",
      "  batch 300 loss: 1.272320442199707\n",
      "  batch 350 loss: 1.2600000035762786\n",
      "  batch 400 loss: 1.3432641959190368\n",
      "  batch 450 loss: 1.2918014454841613\n",
      "  batch 500 loss: 1.3284412121772766\n",
      "  batch 550 loss: 1.3169494104385375\n",
      "  batch 600 loss: 1.2826949167251587\n",
      "  batch 650 loss: 1.338424904346466\n",
      "  batch 700 loss: 1.2738863933086395\n",
      "  batch 750 loss: 1.2445650458335877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 800 loss: 1.2622269093990326\n",
      "  batch 850 loss: 1.2869526147842407\n",
      "  batch 900 loss: 1.2744816744327545\n",
      "LOSS train 1.27448 valid 1.23670, valid PER 38.89%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 1.255370750427246\n",
      "  batch 100 loss: 1.286340537071228\n",
      "  batch 150 loss: 1.2802221632003785\n",
      "  batch 200 loss: 1.2814559423923493\n",
      "  batch 250 loss: 1.261622394323349\n",
      "  batch 300 loss: 1.263218140602112\n",
      "  batch 350 loss: 1.239763733148575\n",
      "  batch 400 loss: 1.2809998846054078\n",
      "  batch 450 loss: 1.2565087187290191\n",
      "  batch 500 loss: 1.265830866098404\n",
      "  batch 550 loss: 1.3241942346096038\n",
      "  batch 600 loss: 1.2508706045150757\n",
      "  batch 650 loss: 1.3063528990745545\n",
      "  batch 700 loss: 1.3029358422756194\n",
      "  batch 750 loss: 1.2702208244800568\n",
      "  batch 800 loss: 1.2714236307144164\n",
      "  batch 850 loss: 1.2970575499534607\n",
      "  batch 900 loss: 1.3848312520980834\n",
      "LOSS train 1.38483 valid 1.23069, valid PER 38.08%\n",
      "Training finished in 9.0 minutes.\n",
      "Model saved to checkpoints/20231211_113748/model_20\n",
      "Currently using dropout rate of 0.7\n",
      "Total number of model parameters is 560320\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 4.548445558547973\n",
      "  batch 100 loss: 3.2771821069717406\n",
      "  batch 150 loss: 3.0844760370254516\n",
      "  batch 200 loss: 2.9816536569595335\n",
      "  batch 250 loss: 2.897058424949646\n",
      "  batch 300 loss: 2.7828182411193847\n",
      "  batch 350 loss: 2.6909472370147705\n",
      "  batch 400 loss: 2.664705762863159\n",
      "  batch 450 loss: 2.5787686586380003\n",
      "  batch 500 loss: 2.481042466163635\n",
      "  batch 550 loss: 2.404945526123047\n",
      "  batch 600 loss: 2.340281205177307\n",
      "  batch 650 loss: 2.2578689813613892\n",
      "  batch 700 loss: 2.2976853036880494\n",
      "  batch 750 loss: 2.2207417631149293\n",
      "  batch 800 loss: 2.1996195697784424\n",
      "  batch 850 loss: 2.1593403935432436\n",
      "  batch 900 loss: 2.124836845397949\n",
      "LOSS train 2.12484 valid 2.05741, valid PER 71.47%\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 2.082552759647369\n",
      "  batch 100 loss: 2.0941138648986817\n",
      "  batch 150 loss: 2.0367435383796693\n",
      "  batch 200 loss: 2.0564561748504637\n",
      "  batch 250 loss: 2.0856029963493348\n",
      "  batch 300 loss: 2.074923298358917\n",
      "  batch 350 loss: 1.973648464679718\n",
      "  batch 400 loss: 1.9575919532775878\n",
      "  batch 450 loss: 1.9063936018943786\n",
      "  batch 500 loss: 1.9243211197853087\n",
      "  batch 550 loss: 1.91750981092453\n",
      "  batch 600 loss: 1.8643581104278564\n",
      "  batch 650 loss: 1.8765874123573303\n",
      "  batch 700 loss: 1.8513844799995423\n",
      "  batch 750 loss: 1.8159150862693787\n",
      "  batch 800 loss: 1.754245548248291\n",
      "  batch 850 loss: 1.7703669714927672\n",
      "  batch 900 loss: 1.8161824131011963\n",
      "LOSS train 1.81618 valid 1.70906, valid PER 58.71%\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 1.7627212357521058\n",
      "  batch 100 loss: 1.727009859085083\n",
      "  batch 150 loss: 1.7435972166061402\n",
      "  batch 200 loss: 1.7349277853965759\n",
      "  batch 250 loss: 1.6879456615447999\n",
      "  batch 300 loss: 1.6995943641662599\n",
      "  batch 350 loss: 1.7257993602752686\n",
      "  batch 400 loss: 1.6757088589668274\n",
      "  batch 450 loss: 1.6818917179107666\n",
      "  batch 500 loss: 1.653363494873047\n",
      "  batch 550 loss: 1.6339441132545471\n",
      "  batch 600 loss: 1.6313401198387145\n",
      "  batch 650 loss: 1.5934172868728638\n",
      "  batch 700 loss: 1.6017182540893555\n",
      "  batch 750 loss: 1.7015256142616273\n",
      "  batch 800 loss: 1.6018670654296876\n",
      "  batch 850 loss: 1.6461152601242066\n",
      "  batch 900 loss: 1.5696101403236389\n",
      "LOSS train 1.56961 valid 1.52003, valid PER 53.27%\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 1.555053722858429\n",
      "  batch 100 loss: 1.572787311077118\n",
      "  batch 150 loss: 1.5156209444999695\n",
      "  batch 200 loss: 1.5608770418167115\n",
      "  batch 250 loss: 1.5526354265213014\n",
      "  batch 300 loss: 1.6046909284591675\n",
      "  batch 350 loss: 1.519675953388214\n",
      "  batch 400 loss: 1.5473858666419984\n",
      "  batch 450 loss: 1.5331975722312927\n",
      "  batch 500 loss: 1.5089085078239441\n",
      "  batch 550 loss: 1.5585137104988098\n",
      "  batch 600 loss: 1.5550111627578735\n",
      "  batch 650 loss: 1.5477067303657532\n",
      "  batch 700 loss: 1.4889594578742982\n",
      "  batch 750 loss: 1.466789846420288\n",
      "  batch 800 loss: 1.4538345742225647\n",
      "  batch 850 loss: 1.4898238801956176\n",
      "  batch 900 loss: 1.5402265095710754\n",
      "LOSS train 1.54023 valid 1.36789, valid PER 48.51%\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 1.4479432582855225\n",
      "  batch 100 loss: 1.453310146331787\n",
      "  batch 150 loss: 1.5216907238960267\n",
      "  batch 200 loss: 1.4676401948928832\n",
      "  batch 250 loss: 1.4501752734184266\n",
      "  batch 300 loss: 1.4750803685188294\n",
      "  batch 350 loss: 1.468288459777832\n",
      "  batch 400 loss: 1.4962564849853515\n",
      "  batch 450 loss: 1.4681114959716797\n",
      "  batch 500 loss: 1.4826489329338073\n",
      "  batch 550 loss: 1.4365453100204468\n",
      "  batch 600 loss: 1.489229941368103\n",
      "  batch 650 loss: 1.4610800313949586\n",
      "  batch 700 loss: 1.499287040233612\n",
      "  batch 750 loss: 1.4125125193595887\n",
      "  batch 800 loss: 1.4567805480957032\n",
      "  batch 850 loss: 1.4909299278259278\n",
      "  batch 900 loss: 1.4714048433303832\n",
      "LOSS train 1.47140 valid 1.32614, valid PER 44.92%\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 1.4433750486373902\n",
      "  batch 100 loss: 1.4168178510665894\n",
      "  batch 150 loss: 1.4015583753585816\n",
      "  batch 200 loss: 1.43518150806427\n",
      "  batch 250 loss: 1.4277010464668274\n",
      "  batch 300 loss: 1.3980131244659424\n",
      "  batch 350 loss: 1.4166482186317444\n",
      "  batch 400 loss: 1.4171136379241944\n",
      "  batch 450 loss: 1.4419919180870056\n",
      "  batch 500 loss: 1.4165652012825012\n",
      "  batch 550 loss: 1.4373664784431457\n",
      "  batch 600 loss: 1.4149871420860292\n",
      "  batch 650 loss: 1.4387437009811401\n",
      "  batch 700 loss: 1.4344603848457336\n",
      "  batch 750 loss: 1.3771514916419982\n",
      "  batch 800 loss: 1.394657349586487\n",
      "  batch 850 loss: 1.3817334818840026\n",
      "  batch 900 loss: 1.4149515509605408\n",
      "LOSS train 1.41495 valid 1.28872, valid PER 41.85%\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 1.3863442349433899\n",
      "  batch 100 loss: 1.4015541315078734\n",
      "  batch 150 loss: 1.384515700340271\n",
      "  batch 200 loss: 1.3904444074630737\n",
      "  batch 250 loss: 1.4007888913154602\n",
      "  batch 300 loss: 1.3576614379882812\n",
      "  batch 350 loss: 1.3374506974220275\n",
      "  batch 400 loss: 1.3684164381027222\n",
      "  batch 450 loss: 1.3866468811035155\n",
      "  batch 500 loss: 1.4050184631347655\n",
      "  batch 550 loss: 1.3850373935699463\n",
      "  batch 600 loss: 1.412758288383484\n",
      "  batch 650 loss: 1.3994965934753418\n",
      "  batch 700 loss: 1.394143180847168\n",
      "  batch 750 loss: 1.3848442220687867\n",
      "  batch 800 loss: 1.3402301955223084\n",
      "  batch 850 loss: 1.3661008048057557\n",
      "  batch 900 loss: 1.4521171689033507\n",
      "LOSS train 1.45212 valid 1.26314, valid PER 39.07%\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 1.367599503993988\n",
      "  batch 100 loss: 1.3893227529525758\n",
      "  batch 150 loss: 1.4084648633003234\n",
      "  batch 200 loss: 1.3434173440933228\n",
      "  batch 250 loss: 1.4204321122169494\n",
      "  batch 300 loss: 1.3124697422981262\n",
      "  batch 350 loss: 1.3882950568199157\n",
      "  batch 400 loss: 1.3605350708961488\n",
      "  batch 450 loss: 1.3653498792648315\n",
      "  batch 500 loss: 1.369689462184906\n",
      "  batch 550 loss: 1.3270884513854981\n",
      "  batch 600 loss: 1.345487654209137\n",
      "  batch 650 loss: 1.4049021553993226\n",
      "  batch 700 loss: 1.3336871659755707\n",
      "  batch 750 loss: 1.3424675011634826\n",
      "  batch 800 loss: 1.329106764793396\n",
      "  batch 850 loss: 1.3392366898059844\n",
      "  batch 900 loss: 1.3153943705558777\n",
      "LOSS train 1.31539 valid 1.27125, valid PER 40.48%\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 1.298374514579773\n",
      "  batch 100 loss: 1.3467032098770142\n",
      "  batch 150 loss: 1.3774513268470765\n",
      "  batch 200 loss: 1.3233358979225158\n",
      "  batch 250 loss: 1.3477013909816742\n",
      "  batch 300 loss: 1.3382778787612915\n",
      "  batch 350 loss: 1.354313657283783\n",
      "  batch 400 loss: 1.3519962978363038\n",
      "  batch 450 loss: 1.3911741328239442\n",
      "  batch 500 loss: 1.3469862735271454\n",
      "  batch 550 loss: 1.3663780450820924\n",
      "  batch 600 loss: 1.3428739547729491\n",
      "  batch 650 loss: 1.2966753673553466\n",
      "  batch 700 loss: 1.3076794564723968\n",
      "  batch 750 loss: 1.3639786911010743\n",
      "  batch 800 loss: 1.3689427590370178\n",
      "  batch 850 loss: 1.3991015076637268\n",
      "  batch 900 loss: 1.3068352127075196\n",
      "LOSS train 1.30684 valid 1.20920, valid PER 38.09%\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 1.2729888296127319\n",
      "  batch 100 loss: 1.2970629477500915\n",
      "  batch 150 loss: 1.3373251819610597\n",
      "  batch 200 loss: 1.3368581521511078\n",
      "  batch 250 loss: 1.3353065633773804\n",
      "  batch 300 loss: 1.2836904096603394\n",
      "  batch 350 loss: 1.3379372262954712\n",
      "  batch 400 loss: 1.2808837699890137\n",
      "  batch 450 loss: 1.3053661167621613\n",
      "  batch 500 loss: 1.3376394891738892\n",
      "  batch 550 loss: 1.3491247749328614\n",
      "  batch 600 loss: 1.307444863319397\n",
      "  batch 650 loss: 1.3035239958763123\n",
      "  batch 700 loss: 1.314306640625\n",
      "  batch 750 loss: 1.285667645931244\n",
      "  batch 800 loss: 1.3076703834533692\n",
      "  batch 850 loss: 1.3473715782165527\n",
      "  batch 900 loss: 1.3367765927314759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 1.33678 valid 1.22709, valid PER 38.35%\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 1.257199776172638\n",
      "  batch 100 loss: 1.2723645758628845\n",
      "  batch 150 loss: 1.2776077616214752\n",
      "  batch 200 loss: 1.3057368791103363\n",
      "  batch 250 loss: 1.2883813822269439\n",
      "  batch 300 loss: 1.2460764145851135\n",
      "  batch 350 loss: 1.2846076476573944\n",
      "  batch 400 loss: 1.2777276706695557\n",
      "  batch 450 loss: 1.3117445385456086\n",
      "  batch 500 loss: 1.2615432679653167\n",
      "  batch 550 loss: 1.2863542997837067\n",
      "  batch 600 loss: 1.2705463755130768\n",
      "  batch 650 loss: 1.3186847972869873\n",
      "  batch 700 loss: 1.2515344846248626\n",
      "  batch 750 loss: 1.2802686476707459\n",
      "  batch 800 loss: 1.301668883562088\n",
      "  batch 850 loss: 1.3474023628234864\n",
      "  batch 900 loss: 1.3555156469345093\n",
      "LOSS train 1.35552 valid 1.20341, valid PER 37.85%\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 1.2961091196537018\n",
      "  batch 100 loss: 1.252028201818466\n",
      "  batch 150 loss: 1.2761135578155518\n",
      "  batch 200 loss: 1.2966256868839263\n",
      "  batch 250 loss: 1.3259624552726745\n",
      "  batch 300 loss: 1.2818300104141236\n",
      "  batch 350 loss: 1.2906459856033325\n",
      "  batch 400 loss: 1.302976666688919\n",
      "  batch 450 loss: 1.301961908340454\n",
      "  batch 500 loss: 1.3124639010429382\n",
      "  batch 550 loss: 1.2123584508895875\n",
      "  batch 600 loss: 1.2392915165424347\n",
      "  batch 650 loss: 1.3404640197753905\n",
      "  batch 700 loss: 1.3045546746253966\n",
      "  batch 750 loss: 1.2740839147567748\n",
      "  batch 800 loss: 1.2803908610343933\n",
      "  batch 850 loss: 1.318823435306549\n",
      "  batch 900 loss: 1.3371158385276793\n",
      "LOSS train 1.33712 valid 1.21274, valid PER 38.31%\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 1.2308435320854187\n",
      "  batch 100 loss: 1.266114161014557\n",
      "  batch 150 loss: 1.302333526611328\n",
      "  batch 200 loss: 1.2706364846229554\n",
      "  batch 250 loss: 1.2771081709861756\n",
      "  batch 300 loss: 1.2510661351680756\n",
      "  batch 350 loss: 1.290267174243927\n",
      "  batch 400 loss: 1.278743076324463\n",
      "  batch 450 loss: 1.2935840737819673\n",
      "  batch 500 loss: 1.2325227522850037\n",
      "  batch 550 loss: 1.2505537903308868\n",
      "  batch 600 loss: 1.2660011899471284\n",
      "  batch 650 loss: 1.2944017267227172\n",
      "  batch 700 loss: 1.2604593300819398\n",
      "  batch 750 loss: 1.251707364320755\n",
      "  batch 800 loss: 1.2630801331996917\n",
      "  batch 850 loss: 1.300722963809967\n",
      "  batch 900 loss: 1.3009320521354675\n",
      "LOSS train 1.30093 valid 1.22249, valid PER 38.82%\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 1.25915186047554\n",
      "  batch 100 loss: 1.2562278175354005\n",
      "  batch 150 loss: 1.2338131833076478\n",
      "  batch 200 loss: 1.2543269777297974\n",
      "  batch 250 loss: 1.2497748684883119\n",
      "  batch 300 loss: 1.3387745213508606\n",
      "  batch 350 loss: 1.2807178902626037\n",
      "  batch 400 loss: 1.233070936203003\n",
      "  batch 450 loss: 1.2505586194992064\n",
      "  batch 500 loss: 1.2737331426143645\n",
      "  batch 550 loss: 1.3010195887088776\n",
      "  batch 600 loss: 1.2426867485046387\n",
      "  batch 650 loss: 1.328432502746582\n",
      "  batch 700 loss: 1.3272096848487853\n",
      "  batch 750 loss: 1.266665176153183\n",
      "  batch 800 loss: 1.2410577285289763\n",
      "  batch 850 loss: 1.2841041040420533\n",
      "  batch 900 loss: 1.3119200849533081\n",
      "LOSS train 1.31192 valid 1.20685, valid PER 37.62%\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 1.2850538682937622\n",
      "  batch 100 loss: 1.2378260850906373\n",
      "  batch 150 loss: 1.2537860226631166\n",
      "  batch 200 loss: 1.349494924545288\n",
      "  batch 250 loss: 1.2996936655044555\n",
      "  batch 300 loss: 1.265582491159439\n",
      "  batch 350 loss: 1.2912455308437347\n",
      "  batch 400 loss: 1.276246143579483\n",
      "  batch 450 loss: 1.2596972072124482\n",
      "  batch 500 loss: 1.2463218235969544\n",
      "  batch 550 loss: 1.288951268196106\n",
      "  batch 600 loss: 1.2897323846817017\n",
      "  batch 650 loss: 1.2944447660446168\n",
      "  batch 700 loss: 1.2837634444236756\n",
      "  batch 750 loss: 1.2965074801445007\n",
      "  batch 800 loss: 1.253314962387085\n",
      "  batch 850 loss: 1.21961452126503\n",
      "  batch 900 loss: 1.287526273727417\n",
      "LOSS train 1.28753 valid 1.19178, valid PER 37.15%\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 1.276737072467804\n",
      "  batch 100 loss: 1.224521485567093\n",
      "  batch 150 loss: 1.2337777650356292\n",
      "  batch 200 loss: 1.2566517639160155\n",
      "  batch 250 loss: 1.3133484292030335\n",
      "  batch 300 loss: 1.283033640384674\n",
      "  batch 350 loss: 1.276061315536499\n",
      "  batch 400 loss: 1.2625570094585419\n",
      "  batch 450 loss: 1.3337806975841522\n",
      "  batch 500 loss: 1.2079067599773408\n",
      "  batch 550 loss: 1.2826394712924958\n",
      "  batch 600 loss: 1.2570632660388947\n",
      "  batch 650 loss: 1.2615877950191499\n",
      "  batch 700 loss: 1.22670152425766\n",
      "  batch 750 loss: 1.2330857074260713\n",
      "  batch 800 loss: 1.2516867196559907\n",
      "  batch 850 loss: 1.2319424116611482\n",
      "  batch 900 loss: 1.2469393384456635\n",
      "LOSS train 1.24694 valid 1.18270, valid PER 36.96%\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 1.2430691075325013\n",
      "  batch 100 loss: 1.2397502636909485\n",
      "  batch 150 loss: 1.2160094630718232\n",
      "  batch 200 loss: 1.1951627564430236\n",
      "  batch 250 loss: 1.220648844242096\n",
      "  batch 300 loss: 1.2409328746795654\n",
      "  batch 350 loss: 1.2464045262336731\n",
      "  batch 400 loss: 1.313371193408966\n",
      "  batch 450 loss: 1.2529087162017822\n",
      "  batch 500 loss: 1.217716808319092\n",
      "  batch 550 loss: 1.2531960141658782\n",
      "  batch 600 loss: 1.3217393064498901\n",
      "  batch 650 loss: 1.2227781951427459\n",
      "  batch 700 loss: 1.2550129735469817\n",
      "  batch 750 loss: 1.1918727028369904\n",
      "  batch 800 loss: 1.2119130837917327\n",
      "  batch 850 loss: 1.249556109905243\n",
      "  batch 900 loss: 1.209460163116455\n",
      "LOSS train 1.20946 valid 1.19245, valid PER 37.36%\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 1.234272471666336\n",
      "  batch 100 loss: 1.2749357736110687\n",
      "  batch 150 loss: 1.2350328755378723\n",
      "  batch 200 loss: 1.2431068778038026\n",
      "  batch 250 loss: 1.2439299750328063\n",
      "  batch 300 loss: 1.2611227118968964\n",
      "  batch 350 loss: 1.2814903163909912\n",
      "  batch 400 loss: 1.2218249011039735\n",
      "  batch 450 loss: 1.244060127735138\n",
      "  batch 500 loss: 1.2210904991626739\n",
      "  batch 550 loss: 1.222708591222763\n",
      "  batch 600 loss: 1.2092432475090027\n",
      "  batch 650 loss: 1.2494233405590058\n",
      "  batch 700 loss: 1.3115163767337799\n",
      "  batch 750 loss: 1.2579450631141662\n",
      "  batch 800 loss: 1.2589546823501587\n",
      "  batch 850 loss: 1.2523372304439544\n",
      "  batch 900 loss: 1.2509750676155091\n",
      "LOSS train 1.25098 valid 1.17918, valid PER 36.70%\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 1.2123682236671447\n",
      "  batch 100 loss: 1.2162592661380769\n",
      "  batch 150 loss: 1.227997682094574\n",
      "  batch 200 loss: 1.2429160296916961\n",
      "  batch 250 loss: 1.2557921993732453\n",
      "  batch 300 loss: 1.241207309961319\n",
      "  batch 350 loss: 1.207498449087143\n",
      "  batch 400 loss: 1.2500634694099426\n",
      "  batch 450 loss: 1.2353272485733031\n",
      "  batch 500 loss: 1.2442380928993224\n",
      "  batch 550 loss: 1.2244477319717406\n",
      "  batch 600 loss: 1.2190657150745392\n",
      "  batch 650 loss: 1.270453188419342\n",
      "  batch 700 loss: 1.2355599975585938\n",
      "  batch 750 loss: 1.1906363666057587\n",
      "  batch 800 loss: 1.2350698363780976\n",
      "  batch 850 loss: 1.2777730476856233\n",
      "  batch 900 loss: 1.251807574033737\n",
      "LOSS train 1.25181 valid 1.16351, valid PER 37.14%\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 1.210778023004532\n",
      "  batch 100 loss: 1.2253181159496307\n",
      "  batch 150 loss: 1.206652284860611\n",
      "  batch 200 loss: 1.2205010378360748\n",
      "  batch 250 loss: 1.2048619270324707\n",
      "  batch 300 loss: 1.2510970973968505\n",
      "  batch 350 loss: 1.2240177536010741\n",
      "  batch 400 loss: 1.1948314356803893\n",
      "  batch 450 loss: 1.2351778519153596\n",
      "  batch 500 loss: 1.2447632515430451\n",
      "  batch 550 loss: 1.3078864121437073\n",
      "  batch 600 loss: 1.2539164423942566\n",
      "  batch 650 loss: 1.2657827937602997\n",
      "  batch 700 loss: 1.2977684462070465\n",
      "  batch 750 loss: 1.2428546786308288\n",
      "  batch 800 loss: 1.2489319634437561\n",
      "  batch 850 loss: 1.256642005443573\n",
      "  batch 900 loss: 1.2500976824760437\n",
      "LOSS train 1.25010 valid 1.16746, valid PER 36.04%\n",
      "Training finished in 9.0 minutes.\n",
      "Model saved to checkpoints/20231211_114700/model_19\n",
      "Finish Adam optimiser\n",
      "End tuning For uni directional 2 Layer LSTM\n"
     ]
    }
   ],
   "source": [
    "import model_uni_directional_LSTM\n",
    "from datetime import datetime\n",
    "from trainer_SGD_Scheduler import train as sgd_trainer\n",
    "from trainer_Adam import train as adam_trainer\n",
    "import torch\n",
    "from decoder import decode\n",
    "\n",
    "print(\"Start tuning For uni directional 2 Layer LSTM\")\n",
    "\n",
    "for opt in Optimiser:\n",
    "    print(\"Currently using \"+ opt +\" optimiser\")\n",
    "    for starting_lr in starting_lrs:\n",
    "        if opt==\"Adam\":\n",
    "            args = {'seed': 123,\n",
    "                'train_json': 'train_fbank.json',\n",
    "                'val_json': 'dev_fbank.json',\n",
    "                'test_json': 'test_fbank.json',\n",
    "                'batch_size': 4,\n",
    "                'num_layers': 2,\n",
    "                'fbank_dims': 23,\n",
    "                'model_dims': 210,\n",
    "                'concat': 1,\n",
    "                'lr': starting_lr,\n",
    "                'vocab': vocab,\n",
    "                'report_interval': 50,\n",
    "                'num_epochs': 20,\n",
    "                'device': device,\n",
    "               }\n",
    "\n",
    "            args = namedtuple('x', args)(**args)\n",
    "        else:\n",
    "            args = {'seed': 123,\n",
    "                'train_json': 'train_fbank.json',\n",
    "                'val_json': 'dev_fbank.json',\n",
    "                'test_json': 'test_fbank.json',\n",
    "                'batch_size': 4,\n",
    "                'num_layers': 2,\n",
    "                'fbank_dims': 23,\n",
    "                'model_dims': 210,\n",
    "                'concat': 1,\n",
    "                'lr': starting_lr,\n",
    "                'vocab': vocab,\n",
    "                'report_interval': 50,\n",
    "                'num_epochs': 20,\n",
    "                'device': device,\n",
    "               }\n",
    "\n",
    "            args = namedtuple('x', args)(**args)\n",
    "\n",
    "\n",
    "        for dropout_rate in dropout_rates:\n",
    "            print(\"Currently using dropout rate of \"+ str(dropout_rate))\n",
    "            model_with_dropout = model_uni_directional_LSTM.BiLSTM(args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab), dropout_rate)\n",
    "            num_params = sum(p.numel() for p in model_with_dropout.parameters())\n",
    "            print('Total number of model parameters is {}'.format(num_params))\n",
    "            start = datetime.now()\n",
    "            model_with_dropout.to(args.device)\n",
    "            if opt==\"Adam\":\n",
    "                model_path = adam_trainer(model_with_dropout, args)\n",
    "            else:\n",
    "                model_path = sgd_trainer(model_with_dropout, args)\n",
    "            end = datetime.now()\n",
    "            duration = (end - start).total_seconds()\n",
    "            print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "            print('Model saved to {}'.format(model_path))\n",
    "    \n",
    "    print(\"Finish \"+ opt +\" optimiser\")\n",
    "print(\"End tuning For uni directional 2 Layer LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce6e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f557aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0fbf24cf75000eabf5a00e10c8a172f625c080664c667beb546de0043e5ae1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
